07/09 03:48:12PM parser.py:28 [INFO] 
07/09 03:48:12PM parser.py:29 [INFO] Parameters:
07/09 03:48:12PM parser.py:31 [INFO] BATCH_SIZE=64
07/09 03:48:12PM parser.py:31 [INFO] CUTOUT_LENGTH=16
07/09 03:48:12PM parser.py:31 [INFO] DATA_PATH=../data/
07/09 03:48:12PM parser.py:31 [INFO] DATASET=cifar100
07/09 03:48:12PM parser.py:31 [INFO] EPOCHS=300
07/09 03:48:12PM parser.py:31 [INFO] EXP_NAME=16_E300_STEPLR-20240709-154812
07/09 03:48:12PM parser.py:31 [INFO] GPUS=[0]
07/09 03:48:12PM parser.py:31 [INFO] LOCAL_RANK=0
07/09 03:48:12PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/09 03:48:12PM parser.py:31 [INFO] MODEL_NAME=efficientnet_b8
07/09 03:48:12PM parser.py:31 [INFO] NAME=FINETUNE
07/09 03:48:12PM parser.py:31 [INFO] PATH=results/teacher/cifar100/efficientnet_b8/FINETUNE/16_E300_STEPLR-20240709-154812
07/09 03:48:12PM parser.py:31 [INFO] PLOT_PATH=results/teacher/cifar100/efficientnet_b8/FINETUNE/16_E300_STEPLR-20240709-154812/plots
07/09 03:48:12PM parser.py:31 [INFO] PRINT_FREQ=50
07/09 03:48:12PM parser.py:31 [INFO] RESUME_PATH=None
07/09 03:48:12PM parser.py:31 [INFO] SAVE=16_E300_STEPLR
07/09 03:48:12PM parser.py:31 [INFO] SEED=0
07/09 03:48:12PM parser.py:31 [INFO] TRAIN_PORTION=0.9
07/09 03:48:12PM parser.py:31 [INFO] W_GRAD_CLIP=100.0
07/09 03:48:12PM parser.py:31 [INFO] W_LR=0.1
07/09 03:48:12PM parser.py:31 [INFO] W_LR_MIN=0.001
07/09 03:48:12PM parser.py:31 [INFO] W_MOMENTUM=0.9
07/09 03:48:12PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
07/09 03:48:12PM parser.py:31 [INFO] WORKERS=4
07/09 03:48:12PM parser.py:32 [INFO] 
07/09 03:48:31PM finetuneTeacher_trainer.py:115 [INFO] --> No loaded checkpoint!
07/09 03:48:51PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][50/703]	Step 50	lr 0.1	Loss 8.1522 (8.5178)	Prec@(1,5) (1.4%, 5.5%)	
07/09 03:49:01PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][100/703]	Step 100	lr 0.1	Loss 4.9360 (7.5332)	Prec@(1,5) (1.4%, 6.2%)	
07/09 03:49:10PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][150/703]	Step 150	lr 0.1	Loss 4.7487 (6.7187)	Prec@(1,5) (1.4%, 6.6%)	
07/09 03:49:19PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][200/703]	Step 200	lr 0.1	Loss 4.6840 (6.2251)	Prec@(1,5) (1.4%, 6.5%)	
07/09 03:49:28PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][250/703]	Step 250	lr 0.1	Loss 4.5988 (5.9139)	Prec@(1,5) (1.3%, 6.4%)	
07/09 03:49:38PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][300/703]	Step 300	lr 0.1	Loss 4.5311 (5.7002)	Prec@(1,5) (1.3%, 6.4%)	
07/09 03:49:48PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][350/703]	Step 350	lr 0.1	Loss 4.5815 (5.5386)	Prec@(1,5) (1.4%, 6.6%)	
07/09 03:49:58PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][400/703]	Step 400	lr 0.1	Loss 4.5598 (5.4142)	Prec@(1,5) (1.4%, 6.7%)	
07/09 03:50:08PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][450/703]	Step 450	lr 0.1	Loss 4.5125 (5.3158)	Prec@(1,5) (1.4%, 6.8%)	
07/09 03:50:18PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][500/703]	Step 500	lr 0.1	Loss 4.6279 (5.2398)	Prec@(1,5) (1.4%, 6.9%)	
07/09 03:50:28PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][550/703]	Step 550	lr 0.1	Loss 4.4951 (5.1764)	Prec@(1,5) (1.5%, 7.1%)	
07/09 03:50:38PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][600/703]	Step 600	lr 0.1	Loss 4.5158 (5.1228)	Prec@(1,5) (1.5%, 7.2%)	
07/09 03:50:47PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][650/703]	Step 650	lr 0.1	Loss 4.4948 (5.0770)	Prec@(1,5) (1.5%, 7.3%)	
07/09 03:50:57PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][700/703]	Step 700	lr 0.1	Loss 4.5538 (5.0371)	Prec@(1,5) (1.5%, 7.4%)	
07/09 03:50:57PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [0][703/703]	Step 703	lr 0.1	Loss 4.4977 (5.0350)	Prec@(1,5) (1.5%, 7.4%)	
07/09 03:51:11PM finetuneTeacher_trainer.py:187 [INFO] Train: [  0/299] Final Prec@1 1.5044%
07/09 03:51:15PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [0][50/79]	Step 704	Loss 4.6071	Prec@(1,5) (1.2%, 7.1%)
07/09 03:51:16PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [0][78/79]	Step 704	Loss 4.6130	Prec@(1,5) (1.2%, 6.8%)
07/09 03:51:16PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  0/299] Final Prec@1 1.2400%
07/09 03:51:18PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.2400%
07/09 03:51:28PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][50/703]	Step 754	lr 0.1	Loss 4.6252 (4.5920)	Prec@(1,5) (1.9%, 7.4%)	
07/09 03:51:37PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][100/703]	Step 804	lr 0.1	Loss 4.5967 (4.5806)	Prec@(1,5) (1.8%, 7.5%)	
07/09 03:51:46PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][150/703]	Step 854	lr 0.1	Loss 4.5882 (4.5783)	Prec@(1,5) (1.8%, 7.4%)	
07/09 03:51:56PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][200/703]	Step 904	lr 0.1	Loss 4.5845 (4.5709)	Prec@(1,5) (1.7%, 7.5%)	
07/09 03:52:05PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][250/703]	Step 954	lr 0.1	Loss 4.5276 (4.5638)	Prec@(1,5) (1.7%, 7.5%)	
07/09 03:52:14PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][300/703]	Step 1004	lr 0.1	Loss 4.4248 (4.5583)	Prec@(1,5) (1.7%, 7.7%)	
07/09 03:52:23PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][350/703]	Step 1054	lr 0.1	Loss 4.5819 (4.5540)	Prec@(1,5) (1.6%, 7.7%)	
07/09 03:52:31PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][400/703]	Step 1104	lr 0.1	Loss 4.4755 (4.5493)	Prec@(1,5) (1.7%, 7.8%)	
07/09 03:52:40PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][450/703]	Step 1154	lr 0.1	Loss 4.4639 (4.5442)	Prec@(1,5) (1.7%, 7.9%)	
07/09 03:52:49PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][500/703]	Step 1204	lr 0.1	Loss 4.5509 (4.5467)	Prec@(1,5) (1.7%, 7.8%)	
07/09 03:52:58PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][550/703]	Step 1254	lr 0.1	Loss 4.6013 (4.5467)	Prec@(1,5) (1.7%, 7.8%)	
07/09 03:53:07PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][600/703]	Step 1304	lr 0.1	Loss 4.5299 (4.5447)	Prec@(1,5) (1.7%, 7.8%)	
07/09 03:53:15PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][650/703]	Step 1354	lr 0.1	Loss 4.5472 (4.5395)	Prec@(1,5) (1.7%, 7.9%)	
07/09 03:53:24PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][700/703]	Step 1404	lr 0.1	Loss 4.4685 (4.5365)	Prec@(1,5) (1.8%, 7.9%)	
07/09 03:53:24PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [1][703/703]	Step 1407	lr 0.1	Loss 4.5087 (4.5362)	Prec@(1,5) (1.8%, 7.9%)	
07/09 03:53:24PM finetuneTeacher_trainer.py:187 [INFO] Train: [  1/299] Final Prec@1 1.7578%
07/09 03:53:28PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [1][50/79]	Step 1408	Loss 4.6526	Prec@(1,5) (1.2%, 6.2%)
07/09 03:53:29PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [1][78/79]	Step 1408	Loss 4.6508	Prec@(1,5) (1.4%, 6.6%)
07/09 03:53:29PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  1/299] Final Prec@1 1.3600%
07/09 03:53:31PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.3600%
07/09 03:53:41PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][50/703]	Step 1458	lr 0.1	Loss 4.3802 (4.4869)	Prec@(1,5) (1.9%, 9.5%)	
07/09 03:53:51PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][100/703]	Step 1508	lr 0.1	Loss 4.5107 (4.4773)	Prec@(1,5) (1.9%, 10.2%)	
07/09 03:53:59PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][150/703]	Step 1558	lr 0.1	Loss 4.5714 (4.4778)	Prec@(1,5) (1.9%, 10.2%)	
07/09 03:54:09PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][200/703]	Step 1608	lr 0.1	Loss 4.5313 (4.4945)	Prec@(1,5) (1.8%, 9.6%)	
07/09 03:54:19PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][250/703]	Step 1658	lr 0.1	Loss 4.5442 (4.5070)	Prec@(1,5) (1.7%, 9.1%)	
07/09 03:54:28PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][300/703]	Step 1708	lr 0.1	Loss 4.5431 (4.5175)	Prec@(1,5) (1.7%, 8.7%)	
07/09 03:54:37PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][350/703]	Step 1758	lr 0.1	Loss 4.4828 (4.5240)	Prec@(1,5) (1.6%, 8.6%)	
07/09 03:54:46PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][400/703]	Step 1808	lr 0.1	Loss 4.4579 (4.5260)	Prec@(1,5) (1.6%, 8.6%)	
07/09 03:54:56PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][450/703]	Step 1858	lr 0.1	Loss 4.5753 (4.5294)	Prec@(1,5) (1.6%, 8.4%)	
07/09 03:55:05PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][500/703]	Step 1908	lr 0.1	Loss 4.5624 (4.5324)	Prec@(1,5) (1.6%, 8.5%)	
07/09 03:55:15PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][550/703]	Step 1958	lr 0.1	Loss 4.5466 (4.5329)	Prec@(1,5) (1.7%, 8.6%)	
07/09 03:55:24PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][600/703]	Step 2008	lr 0.1	Loss 4.5033 (4.5315)	Prec@(1,5) (1.7%, 8.7%)	
07/09 03:55:33PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][650/703]	Step 2058	lr 0.1	Loss 4.4753 (4.5308)	Prec@(1,5) (1.7%, 8.7%)	
07/09 03:55:43PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][700/703]	Step 2108	lr 0.1	Loss 4.6168 (4.5297)	Prec@(1,5) (1.7%, 8.7%)	
07/09 03:55:43PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [2][703/703]	Step 2111	lr 0.1	Loss 4.5687 (4.5299)	Prec@(1,5) (1.7%, 8.7%)	
07/09 03:55:43PM finetuneTeacher_trainer.py:187 [INFO] Train: [  2/299] Final Prec@1 1.7400%
07/09 03:55:47PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [2][50/79]	Step 2112	Loss 5.1101	Prec@(1,5) (1.1%, 5.9%)
07/09 03:55:49PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [2][78/79]	Step 2112	Loss 5.0743	Prec@(1,5) (1.0%, 6.0%)
07/09 03:55:49PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  2/299] Final Prec@1 1.0200%
07/09 03:55:49PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.3600%
07/09 03:55:59PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][50/703]	Step 2162	lr 0.1	Loss 4.5115 (4.5552)	Prec@(1,5) (1.7%, 7.8%)	
07/09 03:56:08PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][100/703]	Step 2212	lr 0.1	Loss 4.4381 (4.5478)	Prec@(1,5) (1.9%, 7.8%)	
07/09 03:56:17PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][150/703]	Step 2262	lr 0.1	Loss 4.4039 (4.5290)	Prec@(1,5) (2.2%, 8.8%)	
07/09 03:56:26PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][200/703]	Step 2312	lr 0.1	Loss 4.5185 (4.5224)	Prec@(1,5) (2.3%, 9.2%)	
07/09 03:56:36PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][250/703]	Step 2362	lr 0.1	Loss 4.4483 (4.5179)	Prec@(1,5) (2.2%, 9.3%)	
07/09 03:56:45PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][300/703]	Step 2412	lr 0.1	Loss 4.3871 (4.5093)	Prec@(1,5) (2.2%, 9.5%)	
07/09 03:56:55PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][350/703]	Step 2462	lr 0.1	Loss 4.3702 (4.4993)	Prec@(1,5) (2.3%, 9.8%)	
07/09 03:57:04PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][400/703]	Step 2512	lr 0.1	Loss 4.4608 (4.5046)	Prec@(1,5) (2.2%, 9.6%)	
07/09 03:57:14PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][450/703]	Step 2562	lr 0.1	Loss 4.5768 (4.5023)	Prec@(1,5) (2.2%, 9.7%)	
07/09 03:57:23PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][500/703]	Step 2612	lr 0.1	Loss 4.4172 (4.4984)	Prec@(1,5) (2.2%, 9.8%)	
07/09 03:57:32PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][550/703]	Step 2662	lr 0.1	Loss 4.3913 (4.4925)	Prec@(1,5) (2.2%, 9.8%)	
07/09 03:57:41PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][600/703]	Step 2712	lr 0.1	Loss 4.4409 (4.4871)	Prec@(1,5) (2.2%, 10.0%)	
07/09 03:57:51PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][650/703]	Step 2762	lr 0.1	Loss 4.3851 (4.4819)	Prec@(1,5) (2.2%, 10.2%)	
07/09 03:58:00PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][700/703]	Step 2812	lr 0.1	Loss 4.5861 (4.4766)	Prec@(1,5) (2.2%, 10.4%)	
07/09 03:58:01PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [3][703/703]	Step 2815	lr 0.1	Loss 4.3965 (4.4764)	Prec@(1,5) (2.2%, 10.4%)	
07/09 03:58:01PM finetuneTeacher_trainer.py:187 [INFO] Train: [  3/299] Final Prec@1 2.2378%
07/09 03:58:04PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [3][50/79]	Step 2816	Loss 4.9262	Prec@(1,5) (0.6%, 5.0%)
07/09 03:58:06PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [3][78/79]	Step 2816	Loss 4.9214	Prec@(1,5) (0.7%, 4.9%)
07/09 03:58:06PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  3/299] Final Prec@1 0.7600%
07/09 03:58:06PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.3600%
07/09 03:58:16PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][50/703]	Step 2866	lr 0.1	Loss 4.4137 (4.4259)	Prec@(1,5) (2.5%, 11.4%)	
07/09 03:58:26PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][100/703]	Step 2916	lr 0.1	Loss 4.4113 (4.4186)	Prec@(1,5) (2.4%, 11.1%)	
07/09 03:58:35PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][150/703]	Step 2966	lr 0.1	Loss 4.5220 (4.4149)	Prec@(1,5) (2.6%, 11.3%)	
07/09 03:58:44PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][200/703]	Step 3016	lr 0.1	Loss 4.5754 (4.4268)	Prec@(1,5) (2.4%, 11.1%)	
07/09 03:58:53PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][250/703]	Step 3066	lr 0.1	Loss 4.4027 (4.4368)	Prec@(1,5) (2.4%, 10.8%)	
07/09 03:59:03PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][300/703]	Step 3116	lr 0.1	Loss 4.3446 (4.4347)	Prec@(1,5) (2.4%, 11.0%)	
07/09 03:59:12PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][350/703]	Step 3166	lr 0.1	Loss 4.2590 (4.4296)	Prec@(1,5) (2.4%, 11.1%)	
07/09 03:59:22PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][400/703]	Step 3216	lr 0.1	Loss 4.3237 (4.4247)	Prec@(1,5) (2.4%, 11.1%)	
07/09 03:59:31PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][450/703]	Step 3266	lr 0.1	Loss 4.2861 (4.4182)	Prec@(1,5) (2.5%, 11.3%)	
07/09 03:59:40PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][500/703]	Step 3316	lr 0.1	Loss 4.2342 (4.4106)	Prec@(1,5) (2.5%, 11.4%)	
07/09 03:59:50PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][550/703]	Step 3366	lr 0.1	Loss 4.4576 (4.4077)	Prec@(1,5) (2.5%, 11.5%)	
07/09 03:59:59PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][600/703]	Step 3416	lr 0.1	Loss 4.2684 (4.4044)	Prec@(1,5) (2.5%, 11.6%)	
07/09 04:00:08PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][650/703]	Step 3466	lr 0.1	Loss 4.4049 (4.4006)	Prec@(1,5) (2.5%, 11.6%)	
07/09 04:00:18PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][700/703]	Step 3516	lr 0.1	Loss 4.3416 (4.4004)	Prec@(1,5) (2.5%, 11.7%)	
07/09 04:00:18PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [4][703/703]	Step 3519	lr 0.1	Loss 4.3771 (4.4003)	Prec@(1,5) (2.6%, 11.7%)	
07/09 04:00:18PM finetuneTeacher_trainer.py:187 [INFO] Train: [  4/299] Final Prec@1 2.5511%
07/09 04:00:22PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [4][50/79]	Step 3520	Loss 87.7007	Prec@(1,5) (1.2%, 5.1%)
07/09 04:00:24PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [4][78/79]	Step 3520	Loss 86.8103	Prec@(1,5) (1.1%, 5.1%)
07/09 04:00:24PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  4/299] Final Prec@1 1.1400%
07/09 04:00:24PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.3600%
07/09 04:00:34PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][50/703]	Step 3570	lr 0.1	Loss 4.4173 (4.4031)	Prec@(1,5) (2.3%, 12.2%)	
07/09 04:00:44PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][100/703]	Step 3620	lr 0.1	Loss 4.2732 (4.3863)	Prec@(1,5) (2.8%, 12.7%)	
07/09 04:00:53PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][150/703]	Step 3670	lr 0.1	Loss 4.4471 (4.3766)	Prec@(1,5) (2.9%, 13.0%)	
07/09 04:01:02PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][200/703]	Step 3720	lr 0.1	Loss 4.2363 (4.3637)	Prec@(1,5) (2.9%, 13.1%)	
07/09 04:01:11PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][250/703]	Step 3770	lr 0.1	Loss 4.3010 (4.3570)	Prec@(1,5) (2.9%, 13.3%)	
07/09 04:01:20PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][300/703]	Step 3820	lr 0.1	Loss 4.2858 (4.3500)	Prec@(1,5) (3.0%, 13.5%)	
07/09 04:01:30PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][350/703]	Step 3870	lr 0.1	Loss 4.2363 (4.3484)	Prec@(1,5) (3.0%, 13.6%)	
07/09 04:01:39PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][400/703]	Step 3920	lr 0.1	Loss 4.1065 (4.3451)	Prec@(1,5) (3.0%, 13.6%)	
07/09 04:01:48PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][450/703]	Step 3970	lr 0.1	Loss 4.3315 (4.3419)	Prec@(1,5) (3.0%, 13.7%)	
07/09 04:01:57PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][500/703]	Step 4020	lr 0.1	Loss 4.1548 (4.3401)	Prec@(1,5) (3.1%, 13.7%)	
07/09 04:02:07PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][550/703]	Step 4070	lr 0.1	Loss 4.2797 (4.3377)	Prec@(1,5) (3.0%, 13.7%)	
07/09 04:02:16PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][600/703]	Step 4120	lr 0.1	Loss 4.1812 (4.3367)	Prec@(1,5) (3.0%, 13.7%)	
07/09 04:02:26PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][650/703]	Step 4170	lr 0.1	Loss 4.2297 (4.3400)	Prec@(1,5) (3.0%, 13.5%)	
07/09 04:02:35PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][700/703]	Step 4220	lr 0.1	Loss 4.4688 (4.3400)	Prec@(1,5) (3.0%, 13.6%)	
07/09 04:02:36PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [5][703/703]	Step 4223	lr 0.1	Loss 4.3932 (4.3402)	Prec@(1,5) (3.0%, 13.6%)	
07/09 04:02:36PM finetuneTeacher_trainer.py:187 [INFO] Train: [  5/299] Final Prec@1 3.0156%
07/09 04:02:39PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [5][50/79]	Step 4224	Loss 138.6818	Prec@(1,5) (1.5%, 7.7%)
07/09 04:02:41PM finetuneTeacher_trainer.py:215 [INFO] Valid: Epoch: [5][78/79]	Step 4224	Loss 128.9395	Prec@(1,5) (1.3%, 7.4%)
07/09 04:02:41PM finetuneTeacher_trainer.py:222 [INFO] Valid: [  5/299] Final Prec@1 1.2800%
07/09 04:02:41PM finetuneTeacher_main.py:54 [INFO] Until now, best Prec@1 = 1.3600%
07/09 04:02:51PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [6][50/703]	Step 4274	lr 0.1	Loss 4.4434 (4.3295)	Prec@(1,5) (2.7%, 15.3%)	
07/09 04:03:00PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [6][100/703]	Step 4324	lr 0.1	Loss 4.2355 (4.3042)	Prec@(1,5) (3.2%, 15.3%)	
07/09 04:03:10PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [6][150/703]	Step 4374	lr 0.1	Loss 4.2739 (4.3034)	Prec@(1,5) (3.3%, 15.0%)	
07/09 04:03:18PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [6][200/703]	Step 4424	lr 0.1	Loss 4.2597 (4.3001)	Prec@(1,5) (3.2%, 14.8%)	
07/09 04:03:28PM finetuneTeacher_trainer.py:177 [INFO] Train: Epoch: [6][250/703]	Step 4474	lr 0.1	Loss 4.2887 (4.2942)	Prec@(1,5) (3.1%, 14.8%)	
