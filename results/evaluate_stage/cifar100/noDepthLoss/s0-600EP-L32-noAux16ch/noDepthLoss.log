11/23 01:57:41AM parser.py:28 [INFO] 
11/23 01:57:41AM parser.py:29 [INFO] Parameters:
11/23 01:57:41AM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=range(10, 12))
11/23 01:57:41AM parser.py:31 [INFO] T=10.0
11/23 01:57:41AM parser.py:31 [INFO] ADVANCED=True
11/23 01:57:41AM parser.py:31 [INFO] AUX_WEIGHT=0.0
11/23 01:57:41AM parser.py:31 [INFO] BATCH_SIZE=64
11/23 01:57:41AM parser.py:31 [INFO] CUTOUT_LENGTH=16
11/23 01:57:41AM parser.py:31 [INFO] DATA_PATH=../data/
11/23 01:57:41AM parser.py:31 [INFO] DATASET=cifar100
11/23 01:57:41AM parser.py:31 [INFO] DESCRIPTION=evaluate_32-layers_network_with_600-epochs_with_noAux_16ch
11/23 01:57:41AM parser.py:31 [INFO] DROP_PATH_PROB=0.0
11/23 01:57:41AM parser.py:31 [INFO] EPOCHS=600
11/23 01:57:41AM parser.py:31 [INFO] EXP_NAME=s0-600EP-L32-noAux16ch
11/23 01:57:41AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/23 01:57:41AM parser.py:31 [INFO] GPUS=[0]
11/23 01:57:41AM parser.py:31 [INFO] GRAD_CLIP=5.0
11/23 01:57:41AM parser.py:31 [INFO] INIT_CHANNELS=16
11/23 01:57:41AM parser.py:31 [INFO] L=0.5
11/23 01:57:41AM parser.py:31 [INFO] LAYERS=32
11/23 01:57:41AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/23 01:57:41AM parser.py:31 [INFO] LR=0.025
11/23 01:57:41AM parser.py:31 [INFO] LR_MIN=0.001
11/23 01:57:41AM parser.py:31 [INFO] MOMENTUM=0.9
11/23 01:57:41AM parser.py:31 [INFO] NAME=noDepthLoss
11/23 01:57:41AM parser.py:31 [INFO] NONKD=True
11/23 01:57:41AM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/noDepthLoss/s0-600EP-L32-noAux16ch
11/23 01:57:41AM parser.py:31 [INFO] PRINT_FREQ=50
11/23 01:57:41AM parser.py:31 [INFO] RESUME_PATH=None
11/23 01:57:41AM parser.py:31 [INFO] SAVE=s0-600EP-L32-noAux16ch
11/23 01:57:41AM parser.py:31 [INFO] SEED=0
11/23 01:57:41AM parser.py:31 [INFO] SPEC_CELL=True
11/23 01:57:41AM parser.py:31 [INFO] TEACHER_NAME=none
11/23 01:57:41AM parser.py:31 [INFO] TEACHER_PATH=none
11/23 01:57:41AM parser.py:31 [INFO] TRAIN_PORTION=0.9
11/23 01:57:41AM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
11/23 01:57:41AM parser.py:31 [INFO] WORKERS=4
11/23 01:57:41AM parser.py:32 [INFO] 
11/23 01:57:43AM evaluateStage_trainer.py:82 [INFO] --> No loaded checkpoint!
11/23 01:57:51AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][50/703]	Step 50	lr 0.025	Loss 4.5316 (4.6389)	Prec@(1,5) (1.1%, 5.2%)	
11/23 01:57:57AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][100/703]	Step 100	lr 0.025	Loss 4.5709 (4.6245)	Prec@(1,5) (1.2%, 5.7%)	
11/23 01:58:03AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][150/703]	Step 150	lr 0.025	Loss 4.6174 (4.6042)	Prec@(1,5) (1.5%, 6.9%)	
11/23 01:58:09AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][200/703]	Step 200	lr 0.025	Loss 4.4814 (4.5665)	Prec@(1,5) (2.0%, 8.5%)	
11/23 01:58:15AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][250/703]	Step 250	lr 0.025	Loss 4.2538 (4.5208)	Prec@(1,5) (2.4%, 9.9%)	
11/23 01:58:21AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][300/703]	Step 300	lr 0.025	Loss 4.3524 (4.4823)	Prec@(1,5) (2.8%, 11.2%)	
11/23 01:58:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][350/703]	Step 350	lr 0.025	Loss 4.3054 (4.4370)	Prec@(1,5) (3.3%, 12.8%)	
11/23 01:58:33AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][400/703]	Step 400	lr 0.025	Loss 3.9619 (4.3947)	Prec@(1,5) (3.8%, 14.2%)	
11/23 01:58:39AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][450/703]	Step 450	lr 0.025	Loss 3.9103 (4.3568)	Prec@(1,5) (4.2%, 15.4%)	
11/23 01:58:45AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][500/703]	Step 500	lr 0.025	Loss 3.9098 (4.3236)	Prec@(1,5) (4.6%, 16.4%)	
11/23 01:58:50AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][550/703]	Step 550	lr 0.025	Loss 3.8775 (4.2883)	Prec@(1,5) (5.0%, 17.6%)	
11/23 01:58:56AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][600/703]	Step 600	lr 0.025	Loss 3.8659 (4.2573)	Prec@(1,5) (5.4%, 18.6%)	
11/23 01:59:02AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][650/703]	Step 650	lr 0.025	Loss 4.0002 (4.2249)	Prec@(1,5) (5.8%, 19.7%)	
11/23 01:59:08AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][700/703]	Step 700	lr 0.025	Loss 3.7918 (4.1979)	Prec@(1,5) (6.1%, 20.5%)	
11/23 01:59:09AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][703/703]	Step 703	lr 0.025	Loss 3.7756 (4.1960)	Prec@(1,5) (6.1%, 20.5%)	
11/23 01:59:09AM evaluateStage_trainer.py:148 [INFO] Train: [  0/599] Final Prec@1 6.0911%
11/23 01:59:13AM evaluateStage_trainer.py:183 [INFO] Valid: [  0/599] Final Prec@1 10.7800%
11/23 01:59:13AM evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 10.7800%
11/23 01:59:21AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][50/703]	Step 754	lr 0.025	Loss 3.8656 (3.7861)	Prec@(1,5) (11.3%, 33.3%)	
11/23 01:59:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][100/703]	Step 804	lr 0.025	Loss 3.7458 (3.7626)	Prec@(1,5) (11.4%, 34.3%)	
11/23 01:59:34AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][150/703]	Step 854	lr 0.025	Loss 3.5312 (3.7422)	Prec@(1,5) (11.3%, 35.0%)	
11/23 01:59:40AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][200/703]	Step 904	lr 0.025	Loss 3.7116 (3.7336)	Prec@(1,5) (11.6%, 35.5%)	
11/23 01:59:47AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][250/703]	Step 954	lr 0.025	Loss 3.7766 (3.7160)	Prec@(1,5) (12.1%, 36.0%)	
11/23 01:59:53AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][300/703]	Step 1004	lr 0.025	Loss 3.4869 (3.6995)	Prec@(1,5) (12.4%, 36.5%)	
11/23 02:00:00AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][350/703]	Step 1054	lr 0.025	Loss 3.3075 (3.6839)	Prec@(1,5) (12.6%, 36.9%)	
11/23 02:00:07AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][400/703]	Step 1104	lr 0.025	Loss 3.3712 (3.6627)	Prec@(1,5) (13.0%, 37.5%)	
11/23 02:00:14AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][450/703]	Step 1154	lr 0.025	Loss 3.2616 (3.6487)	Prec@(1,5) (13.2%, 37.9%)	
11/23 02:00:20AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][500/703]	Step 1204	lr 0.025	Loss 3.3037 (3.6345)	Prec@(1,5) (13.4%, 38.3%)	
11/23 02:00:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][550/703]	Step 1254	lr 0.025	Loss 3.5972 (3.6173)	Prec@(1,5) (13.7%, 38.8%)	
11/23 02:00:34AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][600/703]	Step 1304	lr 0.025	Loss 3.5257 (3.6025)	Prec@(1,5) (14.0%, 39.3%)	
11/23 02:00:41AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][650/703]	Step 1354	lr 0.025	Loss 3.3805 (3.5930)	Prec@(1,5) (14.2%, 39.6%)	
11/23 02:00:48AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][700/703]	Step 1404	lr 0.025	Loss 3.5775 (3.5774)	Prec@(1,5) (14.5%, 40.0%)	
11/23 02:00:48AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][703/703]	Step 1407	lr 0.025	Loss 3.6507 (3.5768)	Prec@(1,5) (14.5%, 40.0%)	
11/23 02:00:48AM evaluateStage_trainer.py:148 [INFO] Train: [  1/599] Final Prec@1 14.5422%
11/23 02:00:52AM evaluateStage_trainer.py:183 [INFO] Valid: [  1/599] Final Prec@1 16.4800%
11/23 02:00:52AM evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 16.4800%
11/23 02:01:00AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][50/703]	Step 1458	lr 0.025	Loss 3.4141 (3.3675)	Prec@(1,5) (18.9%, 45.7%)	
11/23 02:01:07AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][100/703]	Step 1508	lr 0.025	Loss 3.1775 (3.3473)	Prec@(1,5) (19.0%, 46.0%)	
11/23 02:01:13AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][150/703]	Step 1558	lr 0.025	Loss 3.3238 (3.3381)	Prec@(1,5) (19.2%, 46.0%)	
11/23 02:01:20AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][200/703]	Step 1608	lr 0.025	Loss 3.4355 (3.3159)	Prec@(1,5) (19.3%, 46.8%)	
11/23 02:01:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][250/703]	Step 1658	lr 0.025	Loss 3.0922 (3.3027)	Prec@(1,5) (19.4%, 47.4%)	
11/23 02:01:34AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][300/703]	Step 1708	lr 0.025	Loss 3.5211 (3.2955)	Prec@(1,5) (19.6%, 47.4%)	
11/23 02:01:40AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][350/703]	Step 1758	lr 0.025	Loss 3.0651 (3.2838)	Prec@(1,5) (19.8%, 47.8%)	
11/23 02:01:47AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][400/703]	Step 1808	lr 0.025	Loss 3.3240 (3.2741)	Prec@(1,5) (19.9%, 48.2%)	
11/23 02:01:54AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][450/703]	Step 1858	lr 0.025	Loss 3.3031 (3.2598)	Prec@(1,5) (20.2%, 48.6%)	
11/23 02:02:00AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][500/703]	Step 1908	lr 0.025	Loss 2.9965 (3.2539)	Prec@(1,5) (20.2%, 48.7%)	
11/23 02:02:07AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][550/703]	Step 1958	lr 0.025	Loss 2.8148 (3.2413)	Prec@(1,5) (20.5%, 49.1%)	
11/23 02:02:14AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][600/703]	Step 2008	lr 0.025	Loss 2.7778 (3.2306)	Prec@(1,5) (20.6%, 49.4%)	
11/23 02:02:20AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][650/703]	Step 2058	lr 0.025	Loss 3.1759 (3.2217)	Prec@(1,5) (20.8%, 49.7%)	
11/23 02:02:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][700/703]	Step 2108	lr 0.025	Loss 3.2773 (3.2134)	Prec@(1,5) (21.0%, 49.9%)	
11/23 02:02:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][703/703]	Step 2111	lr 0.025	Loss 2.9319 (3.2131)	Prec@(1,5) (21.0%, 49.9%)	
11/23 02:02:28AM evaluateStage_trainer.py:148 [INFO] Train: [  2/599] Final Prec@1 20.9533%
11/23 02:02:31AM evaluateStage_trainer.py:183 [INFO] Valid: [  2/599] Final Prec@1 19.8400%
11/23 02:02:32AM evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 19.8400%
11/23 02:02:39AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][50/703]	Step 2162	lr 0.025	Loss 3.2270 (3.0719)	Prec@(1,5) (22.4%, 53.4%)	
11/23 02:02:46AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][100/703]	Step 2212	lr 0.025	Loss 3.2013 (3.0279)	Prec@(1,5) (23.7%, 54.7%)	
11/23 02:02:53AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][150/703]	Step 2262	lr 0.025	Loss 3.2898 (3.0224)	Prec@(1,5) (24.1%, 55.1%)	
11/23 02:03:00AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][200/703]	Step 2312	lr 0.025	Loss 2.8431 (3.0087)	Prec@(1,5) (24.4%, 55.4%)	
11/23 02:03:06AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][250/703]	Step 2362	lr 0.025	Loss 3.1074 (2.9926)	Prec@(1,5) (24.7%, 55.8%)	
11/23 02:03:13AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][300/703]	Step 2412	lr 0.025	Loss 3.0979 (3.0008)	Prec@(1,5) (24.6%, 55.5%)	
11/23 02:03:20AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][350/703]	Step 2462	lr 0.025	Loss 3.1835 (2.9917)	Prec@(1,5) (24.7%, 55.7%)	
11/23 02:03:27AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][400/703]	Step 2512	lr 0.025	Loss 2.6693 (2.9744)	Prec@(1,5) (25.0%, 56.2%)	
11/23 02:03:34AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][450/703]	Step 2562	lr 0.025	Loss 2.8951 (2.9713)	Prec@(1,5) (25.0%, 56.2%)	
11/23 02:03:41AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][500/703]	Step 2612	lr 0.025	Loss 2.8079 (2.9629)	Prec@(1,5) (25.3%, 56.5%)	
11/23 02:03:47AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][550/703]	Step 2662	lr 0.025	Loss 3.1339 (2.9552)	Prec@(1,5) (25.4%, 56.6%)	
11/23 02:03:54AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][600/703]	Step 2712	lr 0.025	Loss 2.9795 (2.9475)	Prec@(1,5) (25.5%, 56.8%)	
11/23 02:04:01AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][650/703]	Step 2762	lr 0.025	Loss 2.6840 (2.9408)	Prec@(1,5) (25.8%, 57.0%)	
11/23 02:04:08AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][700/703]	Step 2812	lr 0.025	Loss 2.8590 (2.9336)	Prec@(1,5) (25.9%, 57.2%)	
11/23 02:04:08AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][703/703]	Step 2815	lr 0.025	Loss 2.5772 (2.9325)	Prec@(1,5) (25.9%, 57.2%)	
11/23 02:04:08AM evaluateStage_trainer.py:148 [INFO] Train: [  3/599] Final Prec@1 25.8978%
11/23 02:04:12AM evaluateStage_trainer.py:183 [INFO] Valid: [  3/599] Final Prec@1 24.1800%
11/23 02:04:12AM evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 24.1800%
11/23 02:04:20AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][50/703]	Step 2866	lr 0.025	Loss 2.7557 (2.7649)	Prec@(1,5) (30.1%, 61.2%)	
11/23 02:04:26AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][100/703]	Step 2916	lr 0.025	Loss 2.7945 (2.7931)	Prec@(1,5) (28.9%, 60.4%)	
11/23 02:04:33AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][150/703]	Step 2966	lr 0.025	Loss 2.6809 (2.7926)	Prec@(1,5) (28.9%, 60.5%)	
11/23 02:04:40AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][200/703]	Step 3016	lr 0.025	Loss 2.7526 (2.7807)	Prec@(1,5) (29.3%, 60.7%)	
11/23 02:04:47AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][250/703]	Step 3066	lr 0.025	Loss 2.3024 (2.7749)	Prec@(1,5) (29.4%, 61.0%)	
11/23 02:04:54AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][300/703]	Step 3116	lr 0.025	Loss 2.8074 (2.7694)	Prec@(1,5) (29.7%, 61.1%)	
11/23 02:05:00AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][350/703]	Step 3166	lr 0.025	Loss 2.3088 (2.7617)	Prec@(1,5) (29.7%, 61.3%)	
11/23 02:05:07AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][400/703]	Step 3216	lr 0.025	Loss 2.9293 (2.7549)	Prec@(1,5) (29.8%, 61.4%)	
11/23 02:05:14AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][450/703]	Step 3266	lr 0.025	Loss 2.7555 (2.7524)	Prec@(1,5) (29.9%, 61.5%)	
11/23 02:05:21AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][500/703]	Step 3316	lr 0.025	Loss 2.4187 (2.7444)	Prec@(1,5) (30.0%, 61.7%)	
11/23 02:05:28AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][550/703]	Step 3366	lr 0.025	Loss 2.3944 (2.7377)	Prec@(1,5) (30.1%, 61.8%)	
11/23 02:05:34AM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][600/703]	Step 3416	lr 0.025	Loss 2.6169 (2.7325)	Prec@(1,5) (30.1%, 61.9%)	
11/23 02:05:55AM parser.py:28 [INFO] 
11/23 02:05:55AM parser.py:29 [INFO] Parameters:
11/23 02:05:55AM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=range(10, 12))
11/23 02:05:55AM parser.py:31 [INFO] T=10.0
11/23 02:05:55AM parser.py:31 [INFO] ADVANCED=True
11/23 02:05:55AM parser.py:31 [INFO] AUX_WEIGHT=0.0
11/23 02:05:55AM parser.py:31 [INFO] BATCH_SIZE=128
11/23 02:05:55AM parser.py:31 [INFO] CUTOUT_LENGTH=16
11/23 02:05:55AM parser.py:31 [INFO] DATA_PATH=../data/
11/23 02:05:55AM parser.py:31 [INFO] DATASET=cifar100
11/23 02:05:55AM parser.py:31 [INFO] DESCRIPTION=evaluate_32-layers_network_with_600-epochs_with_noAux_16ch
11/23 02:05:55AM parser.py:31 [INFO] DROP_PATH_PROB=0.0
11/23 02:05:55AM parser.py:31 [INFO] EPOCHS=600
11/23 02:05:55AM parser.py:31 [INFO] EXP_NAME=s0-600EP-L32-noAux16ch
11/23 02:05:55AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/23 02:05:55AM parser.py:31 [INFO] GPUS=[0]
11/23 02:05:55AM parser.py:31 [INFO] GRAD_CLIP=5.0
11/23 02:05:55AM parser.py:31 [INFO] INIT_CHANNELS=16
11/23 02:05:55AM parser.py:31 [INFO] L=0.5
11/23 02:05:55AM parser.py:31 [INFO] LAYERS=32
11/23 02:05:55AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/23 02:05:55AM parser.py:31 [INFO] LR=0.025
11/23 02:05:55AM parser.py:31 [INFO] LR_MIN=0.001
11/23 02:05:55AM parser.py:31 [INFO] MOMENTUM=0.9
11/23 02:05:55AM parser.py:31 [INFO] NAME=noDepthLoss
11/23 02:05:55AM parser.py:31 [INFO] NONKD=True
11/23 02:05:55AM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/noDepthLoss/s0-600EP-L32-noAux16ch
11/23 02:05:55AM parser.py:31 [INFO] PRINT_FREQ=50
11/23 02:05:55AM parser.py:31 [INFO] RESUME_PATH=None
11/23 02:05:55AM parser.py:31 [INFO] SAVE=s0-600EP-L32-noAux16ch
11/23 02:05:55AM parser.py:31 [INFO] SEED=0
11/23 02:05:55AM parser.py:31 [INFO] SPEC_CELL=True
11/23 02:05:55AM parser.py:31 [INFO] TEACHER_NAME=none
11/23 02:05:55AM parser.py:31 [INFO] TEACHER_PATH=none
11/23 02:05:55AM parser.py:31 [INFO] TRAIN_PORTION=0.9
11/23 02:05:55AM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
11/23 02:05:55AM parser.py:31 [INFO] WORKERS=4
11/23 02:05:55AM parser.py:32 [INFO] 
11/23 02:05:57AM evaluateStage_trainer.py:79 [INFO] --> No loaded checkpoint!
11/23 02:06:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][50/351]	Step 50	lr 0.025	Loss 4.6121 (4.6031)	Prec@(1,5) (1.6%, 7.2%)	
11/23 02:06:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][100/351]	Step 100	lr 0.025	Loss 4.4333 (4.5549)	Prec@(1,5) (2.2%, 9.6%)	
11/23 02:06:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][150/351]	Step 150	lr 0.025	Loss 4.2710 (4.4739)	Prec@(1,5) (3.0%, 12.1%)	
11/23 02:06:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][200/351]	Step 200	lr 0.025	Loss 3.9253 (4.3837)	Prec@(1,5) (3.9%, 15.0%)	
11/23 02:06:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][250/351]	Step 250	lr 0.025	Loss 3.9719 (4.3117)	Prec@(1,5) (4.6%, 17.1%)	
11/23 02:06:40AM parser.py:28 [INFO] 
11/23 02:06:40AM parser.py:29 [INFO] Parameters:
11/23 02:06:40AM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=range(10, 12))
11/23 02:06:40AM parser.py:31 [INFO] T=10.0
11/23 02:06:40AM parser.py:31 [INFO] ADVANCED=True
11/23 02:06:40AM parser.py:31 [INFO] AUX_WEIGHT=0.0
11/23 02:06:40AM parser.py:31 [INFO] BATCH_SIZE=256
11/23 02:06:40AM parser.py:31 [INFO] CUTOUT_LENGTH=16
11/23 02:06:40AM parser.py:31 [INFO] DATA_PATH=../data/
11/23 02:06:40AM parser.py:31 [INFO] DATASET=cifar100
11/23 02:06:40AM parser.py:31 [INFO] DESCRIPTION=evaluate_32-layers_network_with_600-epochs_with_noAux_16ch
11/23 02:06:40AM parser.py:31 [INFO] DROP_PATH_PROB=0.0
11/23 02:06:40AM parser.py:31 [INFO] EPOCHS=600
11/23 02:06:40AM parser.py:31 [INFO] EXP_NAME=s0-600EP-L32-noAux16ch
11/23 02:06:40AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/23 02:06:40AM parser.py:31 [INFO] GPUS=[0]
11/23 02:06:40AM parser.py:31 [INFO] GRAD_CLIP=5.0
11/23 02:06:40AM parser.py:31 [INFO] INIT_CHANNELS=16
11/23 02:06:40AM parser.py:31 [INFO] L=0.5
11/23 02:06:40AM parser.py:31 [INFO] LAYERS=32
11/23 02:06:40AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/23 02:06:40AM parser.py:31 [INFO] LR=0.025
11/23 02:06:40AM parser.py:31 [INFO] LR_MIN=0.001
11/23 02:06:40AM parser.py:31 [INFO] MOMENTUM=0.9
11/23 02:06:40AM parser.py:31 [INFO] NAME=noDepthLoss
11/23 02:06:40AM parser.py:31 [INFO] NONKD=True
11/23 02:06:40AM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/noDepthLoss/s0-600EP-L32-noAux16ch
11/23 02:06:40AM parser.py:31 [INFO] PRINT_FREQ=50
11/23 02:06:40AM parser.py:31 [INFO] RESUME_PATH=None
11/23 02:06:40AM parser.py:31 [INFO] SAVE=s0-600EP-L32-noAux16ch
11/23 02:06:40AM parser.py:31 [INFO] SEED=0
11/23 02:06:40AM parser.py:31 [INFO] SPEC_CELL=True
11/23 02:06:40AM parser.py:31 [INFO] TEACHER_NAME=none
11/23 02:06:40AM parser.py:31 [INFO] TEACHER_PATH=none
11/23 02:06:40AM parser.py:31 [INFO] TRAIN_PORTION=0.9
11/23 02:06:40AM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
11/23 02:06:40AM parser.py:31 [INFO] WORKERS=4
11/23 02:06:40AM parser.py:32 [INFO] 
11/23 02:06:42AM evaluateStage_trainer.py:79 [INFO] --> No loaded checkpoint!
11/23 02:06:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][50/175]	Step 50	lr 0.025	Loss 4.5322 (4.5880)	Prec@(1,5) (1.8%, 7.8%)	
11/23 02:07:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][100/175]	Step 100	lr 0.025	Loss 4.2028 (4.4850)	Prec@(1,5) (2.7%, 11.6%)	
11/23 02:07:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][150/175]	Step 150	lr 0.025	Loss 3.9309 (4.3691)	Prec@(1,5) (3.9%, 15.2%)	
11/23 02:07:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [0][175/175]	Step 175	lr 0.025	Loss 3.9619 (4.3159)	Prec@(1,5) (4.5%, 16.8%)	
11/23 02:07:28AM evaluateStage_trainer.py:145 [INFO] Train: [  0/599] Final Prec@1 4.5289%
11/23 02:07:31AM evaluateStage_trainer.py:180 [INFO] Valid: [  0/599] Final Prec@1 7.5600%
11/23 02:07:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 7.5600%
11/23 02:07:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [1][50/175]	Step 226	lr 0.025	Loss 3.7142 (3.8472)	Prec@(1,5) (10.4%, 31.7%)	
11/23 02:07:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [1][100/175]	Step 276	lr 0.025	Loss 3.6150 (3.7778)	Prec@(1,5) (11.6%, 33.7%)	
11/23 02:08:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [1][150/175]	Step 326	lr 0.025	Loss 3.7948 (3.7208)	Prec@(1,5) (12.6%, 35.5%)	
11/23 02:08:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [1][175/175]	Step 351	lr 0.025	Loss 3.5273 (3.6944)	Prec@(1,5) (13.0%, 36.2%)	
11/23 02:08:14AM evaluateStage_trainer.py:145 [INFO] Train: [  1/599] Final Prec@1 12.9778%
11/23 02:08:17AM evaluateStage_trainer.py:180 [INFO] Valid: [  1/599] Final Prec@1 14.1000%
11/23 02:08:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 14.1000%
11/23 02:08:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [2][50/175]	Step 402	lr 0.025	Loss 3.4458 (3.4515)	Prec@(1,5) (17.3%, 42.9%)	
11/23 02:08:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [2][100/175]	Step 452	lr 0.025	Loss 3.4450 (3.4037)	Prec@(1,5) (17.8%, 44.7%)	
11/23 02:08:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [2][150/175]	Step 502	lr 0.025	Loss 3.1518 (3.3677)	Prec@(1,5) (18.4%, 45.8%)	
11/23 02:08:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [2][175/175]	Step 527	lr 0.025	Loss 3.1780 (3.3460)	Prec@(1,5) (18.8%, 46.2%)	
11/23 02:09:00AM evaluateStage_trainer.py:145 [INFO] Train: [  2/599] Final Prec@1 18.7711%
11/23 02:09:02AM evaluateStage_trainer.py:180 [INFO] Valid: [  2/599] Final Prec@1 20.0200%
11/23 02:09:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 20.0200%
11/23 02:09:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [3][50/175]	Step 578	lr 0.025	Loss 3.1172 (3.1278)	Prec@(1,5) (22.7%, 52.0%)	
11/23 02:09:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [3][100/175]	Step 628	lr 0.025	Loss 2.8459 (3.1038)	Prec@(1,5) (23.1%, 52.6%)	
11/23 02:09:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [3][150/175]	Step 678	lr 0.025	Loss 2.9832 (3.0823)	Prec@(1,5) (23.4%, 53.1%)	
11/23 02:09:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [3][175/175]	Step 703	lr 0.025	Loss 2.9434 (3.0710)	Prec@(1,5) (23.7%, 53.4%)	
11/23 02:09:45AM evaluateStage_trainer.py:145 [INFO] Train: [  3/599] Final Prec@1 23.7467%
11/23 02:09:48AM evaluateStage_trainer.py:180 [INFO] Valid: [  3/599] Final Prec@1 21.8400%
11/23 02:09:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 21.8400%
11/23 02:10:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [4][50/175]	Step 754	lr 0.025	Loss 2.9493 (2.8950)	Prec@(1,5) (27.0%, 57.9%)	
11/23 02:10:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [4][100/175]	Step 804	lr 0.025	Loss 2.7674 (2.8768)	Prec@(1,5) (27.3%, 58.4%)	
11/23 02:10:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [4][150/175]	Step 854	lr 0.025	Loss 2.7889 (2.8619)	Prec@(1,5) (27.7%, 58.8%)	
11/23 02:10:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [4][175/175]	Step 879	lr 0.025	Loss 2.8711 (2.8549)	Prec@(1,5) (27.9%, 59.1%)	
11/23 02:10:31AM evaluateStage_trainer.py:145 [INFO] Train: [  4/599] Final Prec@1 27.8844%
11/23 02:10:33AM evaluateStage_trainer.py:180 [INFO] Valid: [  4/599] Final Prec@1 26.4000%
11/23 02:10:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 26.4000%
11/23 02:10:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [5][50/175]	Step 930	lr 0.025	Loss 2.7618 (2.7159)	Prec@(1,5) (30.4%, 62.9%)	
11/23 02:10:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [5][100/175]	Step 980	lr 0.025	Loss 2.6347 (2.7120)	Prec@(1,5) (30.7%, 62.7%)	
11/23 02:11:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [5][150/175]	Step 1030	lr 0.025	Loss 2.6052 (2.6878)	Prec@(1,5) (31.4%, 63.0%)	
11/23 02:11:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [5][175/175]	Step 1055	lr 0.025	Loss 2.4875 (2.6785)	Prec@(1,5) (31.6%, 63.2%)	
11/23 02:11:16AM evaluateStage_trainer.py:145 [INFO] Train: [  5/599] Final Prec@1 31.5756%
11/23 02:11:19AM evaluateStage_trainer.py:180 [INFO] Valid: [  5/599] Final Prec@1 27.2000%
11/23 02:11:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 27.2000%
11/23 02:11:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [6][50/175]	Step 1106	lr 0.02499	Loss 2.5965 (2.5526)	Prec@(1,5) (33.8%, 65.7%)	
11/23 02:11:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [6][100/175]	Step 1156	lr 0.02499	Loss 2.3830 (2.5396)	Prec@(1,5) (34.3%, 66.2%)	
11/23 02:11:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [6][150/175]	Step 1206	lr 0.02499	Loss 2.4464 (2.5214)	Prec@(1,5) (34.7%, 66.7%)	
11/23 02:12:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [6][175/175]	Step 1231	lr 0.02499	Loss 2.4249 (2.5164)	Prec@(1,5) (34.9%, 66.8%)	
11/23 02:12:02AM evaluateStage_trainer.py:145 [INFO] Train: [  6/599] Final Prec@1 34.8667%
11/23 02:12:04AM evaluateStage_trainer.py:180 [INFO] Valid: [  6/599] Final Prec@1 31.7800%
11/23 02:12:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 31.7800%
11/23 02:12:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [7][50/175]	Step 1282	lr 0.02499	Loss 2.4957 (2.3931)	Prec@(1,5) (37.1%, 68.9%)	
11/23 02:12:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [7][100/175]	Step 1332	lr 0.02499	Loss 2.2973 (2.4022)	Prec@(1,5) (37.0%, 69.2%)	
11/23 02:12:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [7][150/175]	Step 1382	lr 0.02499	Loss 2.0897 (2.3943)	Prec@(1,5) (37.2%, 69.4%)	
11/23 02:12:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [7][175/175]	Step 1407	lr 0.02499	Loss 2.3816 (2.3873)	Prec@(1,5) (37.4%, 69.6%)	
11/23 02:12:47AM evaluateStage_trainer.py:145 [INFO] Train: [  7/599] Final Prec@1 37.4178%
11/23 02:12:50AM evaluateStage_trainer.py:180 [INFO] Valid: [  7/599] Final Prec@1 33.6800%
11/23 02:12:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 33.6800%
11/23 02:13:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [8][50/175]	Step 1458	lr 0.02499	Loss 2.1127 (2.2611)	Prec@(1,5) (40.0%, 72.4%)	
11/23 02:13:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [8][100/175]	Step 1508	lr 0.02499	Loss 2.1171 (2.2568)	Prec@(1,5) (40.2%, 72.5%)	
11/23 02:13:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [8][150/175]	Step 1558	lr 0.02499	Loss 2.1281 (2.2660)	Prec@(1,5) (39.9%, 72.3%)	
11/23 02:13:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [8][175/175]	Step 1583	lr 0.02499	Loss 2.1328 (2.2679)	Prec@(1,5) (39.9%, 72.3%)	
11/23 02:13:33AM evaluateStage_trainer.py:145 [INFO] Train: [  8/599] Final Prec@1 39.8844%
11/23 02:13:35AM evaluateStage_trainer.py:180 [INFO] Valid: [  8/599] Final Prec@1 34.2000%
11/23 02:13:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 34.2000%
11/23 02:13:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [9][50/175]	Step 1634	lr 0.02499	Loss 2.0049 (2.1500)	Prec@(1,5) (42.3%, 74.4%)	
11/23 02:14:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [9][100/175]	Step 1684	lr 0.02499	Loss 2.2663 (2.1546)	Prec@(1,5) (42.1%, 74.4%)	
11/23 02:14:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [9][150/175]	Step 1734	lr 0.02499	Loss 1.9507 (2.1504)	Prec@(1,5) (42.4%, 74.4%)	
11/23 02:14:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [9][175/175]	Step 1759	lr 0.02499	Loss 2.2328 (2.1469)	Prec@(1,5) (42.5%, 74.5%)	
11/23 02:14:18AM evaluateStage_trainer.py:145 [INFO] Train: [  9/599] Final Prec@1 42.4889%
11/23 02:14:21AM evaluateStage_trainer.py:180 [INFO] Valid: [  9/599] Final Prec@1 37.1000%
11/23 02:14:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 37.1000%
11/23 02:14:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [10][50/175]	Step 1810	lr 0.02498	Loss 2.0979 (2.0737)	Prec@(1,5) (44.0%, 76.2%)	
11/23 02:14:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [10][100/175]	Step 1860	lr 0.02498	Loss 2.0174 (2.0698)	Prec@(1,5) (44.1%, 76.2%)	
11/23 02:14:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [10][150/175]	Step 1910	lr 0.02498	Loss 1.9021 (2.0666)	Prec@(1,5) (44.1%, 76.4%)	
11/23 02:15:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [10][175/175]	Step 1935	lr 0.02498	Loss 2.0613 (2.0618)	Prec@(1,5) (44.3%, 76.5%)	
11/23 02:15:04AM evaluateStage_trainer.py:145 [INFO] Train: [ 10/599] Final Prec@1 44.2933%
11/23 02:15:07AM evaluateStage_trainer.py:180 [INFO] Valid: [ 10/599] Final Prec@1 39.9000%
11/23 02:15:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 39.9000%
11/23 02:15:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [11][50/175]	Step 1986	lr 0.02498	Loss 1.9408 (1.9677)	Prec@(1,5) (45.9%, 78.4%)	
11/23 02:15:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [11][100/175]	Step 2036	lr 0.02498	Loss 1.8564 (1.9670)	Prec@(1,5) (46.5%, 78.2%)	
11/23 02:15:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [11][150/175]	Step 2086	lr 0.02498	Loss 1.9897 (1.9803)	Prec@(1,5) (46.1%, 77.9%)	
11/23 02:15:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [11][175/175]	Step 2111	lr 0.02498	Loss 1.9695 (1.9825)	Prec@(1,5) (46.2%, 78.0%)	
11/23 02:15:50AM evaluateStage_trainer.py:145 [INFO] Train: [ 11/599] Final Prec@1 46.1978%
11/23 02:15:52AM evaluateStage_trainer.py:180 [INFO] Valid: [ 11/599] Final Prec@1 37.1200%
11/23 02:15:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 39.9000%
11/23 02:16:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [12][50/175]	Step 2162	lr 0.02498	Loss 1.9877 (1.8833)	Prec@(1,5) (49.1%, 79.3%)	
11/23 02:16:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [12][100/175]	Step 2212	lr 0.02498	Loss 1.7891 (1.8991)	Prec@(1,5) (48.4%, 79.3%)	
11/23 02:16:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [12][150/175]	Step 2262	lr 0.02498	Loss 1.8002 (1.8990)	Prec@(1,5) (48.3%, 79.3%)	
11/23 02:16:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [12][175/175]	Step 2287	lr 0.02498	Loss 1.8968 (1.9065)	Prec@(1,5) (48.0%, 79.3%)	
11/23 02:16:35AM evaluateStage_trainer.py:145 [INFO] Train: [ 12/599] Final Prec@1 48.0378%
11/23 02:16:37AM evaluateStage_trainer.py:180 [INFO] Valid: [ 12/599] Final Prec@1 39.8000%
11/23 02:16:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 39.9000%
11/23 02:16:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [13][50/175]	Step 2338	lr 0.02497	Loss 1.7938 (1.8551)	Prec@(1,5) (49.2%, 80.0%)	
11/23 02:17:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [13][100/175]	Step 2388	lr 0.02497	Loss 1.8326 (1.8484)	Prec@(1,5) (49.2%, 80.2%)	
11/23 02:17:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [13][150/175]	Step 2438	lr 0.02497	Loss 1.7670 (1.8496)	Prec@(1,5) (49.2%, 80.4%)	
11/23 02:17:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [13][175/175]	Step 2463	lr 0.02497	Loss 2.1056 (1.8458)	Prec@(1,5) (49.4%, 80.4%)	
11/23 02:17:20AM evaluateStage_trainer.py:145 [INFO] Train: [ 13/599] Final Prec@1 49.3444%
11/23 02:17:22AM evaluateStage_trainer.py:180 [INFO] Valid: [ 13/599] Final Prec@1 41.5800%
11/23 02:17:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 41.5800%
11/23 02:17:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [14][50/175]	Step 2514	lr 0.02497	Loss 1.9421 (1.7331)	Prec@(1,5) (52.1%, 82.6%)	
11/23 02:17:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [14][100/175]	Step 2564	lr 0.02497	Loss 1.6679 (1.7582)	Prec@(1,5) (51.3%, 82.3%)	
11/23 02:17:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [14][150/175]	Step 2614	lr 0.02497	Loss 1.6738 (1.7712)	Prec@(1,5) (51.3%, 81.9%)	
11/23 02:18:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [14][175/175]	Step 2639	lr 0.02497	Loss 1.7030 (1.7743)	Prec@(1,5) (51.3%, 81.8%)	
11/23 02:18:05AM evaluateStage_trainer.py:145 [INFO] Train: [ 14/599] Final Prec@1 51.3111%
11/23 02:18:08AM evaluateStage_trainer.py:180 [INFO] Valid: [ 14/599] Final Prec@1 42.3000%
11/23 02:18:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 42.3000%
11/23 02:18:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [15][50/175]	Step 2690	lr 0.02496	Loss 1.6808 (1.6938)	Prec@(1,5) (52.9%, 83.3%)	
11/23 02:18:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [15][100/175]	Step 2740	lr 0.02496	Loss 1.7929 (1.7101)	Prec@(1,5) (52.7%, 82.8%)	
11/23 02:18:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [15][150/175]	Step 2790	lr 0.02496	Loss 1.8176 (1.7158)	Prec@(1,5) (52.4%, 82.9%)	
11/23 02:18:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [15][175/175]	Step 2815	lr 0.02496	Loss 1.6913 (1.7174)	Prec@(1,5) (52.4%, 82.8%)	
11/23 02:18:51AM evaluateStage_trainer.py:145 [INFO] Train: [ 15/599] Final Prec@1 52.3667%
11/23 02:18:54AM evaluateStage_trainer.py:180 [INFO] Valid: [ 15/599] Final Prec@1 43.1800%
11/23 02:18:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 43.1800%
11/23 02:19:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [16][50/175]	Step 2866	lr 0.02496	Loss 1.6275 (1.6498)	Prec@(1,5) (53.7%, 83.6%)	
11/23 02:19:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [16][100/175]	Step 2916	lr 0.02496	Loss 1.6940 (1.6503)	Prec@(1,5) (53.8%, 83.7%)	
11/23 02:19:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [16][150/175]	Step 2966	lr 0.02496	Loss 1.5450 (1.6602)	Prec@(1,5) (53.7%, 83.5%)	
11/23 02:19:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [16][175/175]	Step 2991	lr 0.02496	Loss 1.8705 (1.6661)	Prec@(1,5) (53.5%, 83.5%)	
11/23 02:19:37AM evaluateStage_trainer.py:145 [INFO] Train: [ 16/599] Final Prec@1 53.5244%
11/23 02:19:39AM evaluateStage_trainer.py:180 [INFO] Valid: [ 16/599] Final Prec@1 43.1600%
11/23 02:19:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 43.1800%
11/23 02:19:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [17][50/175]	Step 3042	lr 0.02495	Loss 1.6094 (1.6268)	Prec@(1,5) (54.2%, 84.4%)	
11/23 02:20:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [17][100/175]	Step 3092	lr 0.02495	Loss 1.4348 (1.6054)	Prec@(1,5) (55.0%, 84.7%)	
11/23 02:20:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [17][150/175]	Step 3142	lr 0.02495	Loss 1.5221 (1.6223)	Prec@(1,5) (54.6%, 84.3%)	
11/23 02:20:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [17][175/175]	Step 3167	lr 0.02495	Loss 1.5321 (1.6191)	Prec@(1,5) (54.6%, 84.3%)	
11/23 02:20:22AM evaluateStage_trainer.py:145 [INFO] Train: [ 17/599] Final Prec@1 54.6378%
11/23 02:20:24AM evaluateStage_trainer.py:180 [INFO] Valid: [ 17/599] Final Prec@1 47.4800%
11/23 02:20:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 47.4800%
11/23 02:20:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [18][50/175]	Step 3218	lr 0.02495	Loss 1.6058 (1.5348)	Prec@(1,5) (57.2%, 85.6%)	
11/23 02:20:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [18][100/175]	Step 3268	lr 0.02495	Loss 1.5627 (1.5522)	Prec@(1,5) (56.5%, 85.3%)	
11/23 02:21:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [18][150/175]	Step 3318	lr 0.02495	Loss 1.5558 (1.5549)	Prec@(1,5) (56.3%, 85.4%)	
11/23 02:21:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [18][175/175]	Step 3343	lr 0.02495	Loss 1.5453 (1.5651)	Prec@(1,5) (56.0%, 85.2%)	
11/23 02:21:07AM evaluateStage_trainer.py:145 [INFO] Train: [ 18/599] Final Prec@1 55.9867%
11/23 02:21:10AM evaluateStage_trainer.py:180 [INFO] Valid: [ 18/599] Final Prec@1 46.1600%
11/23 02:21:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 47.4800%
11/23 02:21:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [19][50/175]	Step 3394	lr 0.02494	Loss 1.4535 (1.5065)	Prec@(1,5) (57.8%, 85.9%)	
11/23 02:21:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [19][100/175]	Step 3444	lr 0.02494	Loss 1.4284 (1.5200)	Prec@(1,5) (57.3%, 85.7%)	
11/23 02:21:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [19][150/175]	Step 3494	lr 0.02494	Loss 1.5220 (1.5276)	Prec@(1,5) (57.1%, 85.6%)	
11/23 02:21:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [19][175/175]	Step 3519	lr 0.02494	Loss 1.5156 (1.5279)	Prec@(1,5) (57.1%, 85.6%)	
11/23 02:21:53AM evaluateStage_trainer.py:145 [INFO] Train: [ 19/599] Final Prec@1 57.1067%
11/23 02:21:55AM evaluateStage_trainer.py:180 [INFO] Valid: [ 19/599] Final Prec@1 47.6600%
11/23 02:21:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 47.6600%
11/23 02:22:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [20][50/175]	Step 3570	lr 0.02493	Loss 1.3958 (1.4697)	Prec@(1,5) (58.6%, 86.5%)	
11/23 02:22:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [20][100/175]	Step 3620	lr 0.02493	Loss 1.4585 (1.4849)	Prec@(1,5) (58.3%, 86.3%)	
11/23 02:22:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [20][150/175]	Step 3670	lr 0.02493	Loss 1.4847 (1.4843)	Prec@(1,5) (58.1%, 86.3%)	
11/23 02:22:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [20][175/175]	Step 3695	lr 0.02493	Loss 1.4384 (1.4906)	Prec@(1,5) (57.9%, 86.2%)	
11/23 02:22:38AM evaluateStage_trainer.py:145 [INFO] Train: [ 20/599] Final Prec@1 57.9200%
11/23 02:22:41AM evaluateStage_trainer.py:180 [INFO] Valid: [ 20/599] Final Prec@1 48.3200%
11/23 02:22:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 48.3200%
11/23 02:22:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [21][50/175]	Step 3746	lr 0.02493	Loss 1.4749 (1.4234)	Prec@(1,5) (58.9%, 87.7%)	
11/23 02:23:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [21][100/175]	Step 3796	lr 0.02493	Loss 1.6308 (1.4398)	Prec@(1,5) (58.7%, 87.2%)	
11/23 02:23:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [21][150/175]	Step 3846	lr 0.02493	Loss 1.3619 (1.4477)	Prec@(1,5) (58.5%, 87.1%)	
11/23 02:23:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [21][175/175]	Step 3871	lr 0.02493	Loss 1.4486 (1.4507)	Prec@(1,5) (58.6%, 87.0%)	
11/23 02:23:24AM evaluateStage_trainer.py:145 [INFO] Train: [ 21/599] Final Prec@1 58.5889%
11/23 02:23:26AM evaluateStage_trainer.py:180 [INFO] Valid: [ 21/599] Final Prec@1 49.9600%
11/23 02:23:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 49.9600%
11/23 02:23:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [22][50/175]	Step 3922	lr 0.02492	Loss 1.4360 (1.4022)	Prec@(1,5) (59.9%, 88.2%)	
11/23 02:23:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [22][100/175]	Step 3972	lr 0.02492	Loss 1.3557 (1.4137)	Prec@(1,5) (59.7%, 87.7%)	
11/23 02:24:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [22][150/175]	Step 4022	lr 0.02492	Loss 1.3024 (1.4192)	Prec@(1,5) (59.7%, 87.5%)	
11/23 02:24:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [22][175/175]	Step 4047	lr 0.02492	Loss 1.5355 (1.4214)	Prec@(1,5) (59.6%, 87.5%)	
11/23 02:24:09AM evaluateStage_trainer.py:145 [INFO] Train: [ 22/599] Final Prec@1 59.6244%
11/23 02:24:12AM evaluateStage_trainer.py:180 [INFO] Valid: [ 22/599] Final Prec@1 48.3400%
11/23 02:24:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 49.9600%
11/23 02:24:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [23][50/175]	Step 4098	lr 0.02491	Loss 1.3714 (1.3358)	Prec@(1,5) (61.6%, 88.7%)	
11/23 02:24:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [23][100/175]	Step 4148	lr 0.02491	Loss 1.2209 (1.3544)	Prec@(1,5) (61.2%, 88.5%)	
11/23 02:24:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [23][150/175]	Step 4198	lr 0.02491	Loss 1.4795 (1.3680)	Prec@(1,5) (60.8%, 88.2%)	
11/23 02:24:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [23][175/175]	Step 4223	lr 0.02491	Loss 1.3561 (1.3758)	Prec@(1,5) (60.7%, 88.0%)	
11/23 02:24:55AM evaluateStage_trainer.py:145 [INFO] Train: [ 23/599] Final Prec@1 60.6556%
11/23 02:24:57AM evaluateStage_trainer.py:180 [INFO] Valid: [ 23/599] Final Prec@1 49.4000%
11/23 02:24:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 49.9600%
11/23 02:25:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [24][50/175]	Step 4274	lr 0.02491	Loss 1.5162 (1.3043)	Prec@(1,5) (62.6%, 88.9%)	
11/23 02:25:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [24][100/175]	Step 4324	lr 0.02491	Loss 1.5522 (1.3243)	Prec@(1,5) (62.0%, 88.8%)	
11/23 02:25:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [24][150/175]	Step 4374	lr 0.02491	Loss 1.3644 (1.3418)	Prec@(1,5) (61.5%, 88.5%)	
11/23 02:25:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [24][175/175]	Step 4399	lr 0.02491	Loss 1.4574 (1.3445)	Prec@(1,5) (61.4%, 88.5%)	
11/23 02:25:40AM evaluateStage_trainer.py:145 [INFO] Train: [ 24/599] Final Prec@1 61.3956%
11/23 02:25:42AM evaluateStage_trainer.py:180 [INFO] Valid: [ 24/599] Final Prec@1 50.3000%
11/23 02:25:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 50.3000%
11/23 02:25:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [25][50/175]	Step 4450	lr 0.0249	Loss 1.2307 (1.2628)	Prec@(1,5) (63.5%, 89.6%)	
11/23 02:26:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [25][100/175]	Step 4500	lr 0.0249	Loss 1.3250 (1.2919)	Prec@(1,5) (63.0%, 89.3%)	
11/23 02:26:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [25][150/175]	Step 4550	lr 0.0249	Loss 1.3326 (1.3062)	Prec@(1,5) (62.7%, 89.0%)	
11/23 02:26:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [25][175/175]	Step 4575	lr 0.0249	Loss 1.2759 (1.3114)	Prec@(1,5) (62.5%, 89.0%)	
11/23 02:26:25AM evaluateStage_trainer.py:145 [INFO] Train: [ 25/599] Final Prec@1 62.4600%
11/23 02:26:28AM evaluateStage_trainer.py:180 [INFO] Valid: [ 25/599] Final Prec@1 50.6600%
11/23 02:26:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 50.6600%
11/23 02:26:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [26][50/175]	Step 4626	lr 0.02489	Loss 1.2023 (1.2365)	Prec@(1,5) (63.7%, 89.9%)	
11/23 02:26:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [26][100/175]	Step 4676	lr 0.02489	Loss 1.2986 (1.2561)	Prec@(1,5) (63.4%, 89.9%)	
11/23 02:27:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [26][150/175]	Step 4726	lr 0.02489	Loss 1.0744 (1.2789)	Prec@(1,5) (62.8%, 89.5%)	
11/23 02:27:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [26][175/175]	Step 4751	lr 0.02489	Loss 1.2546 (1.2839)	Prec@(1,5) (62.8%, 89.4%)	
11/23 02:27:11AM evaluateStage_trainer.py:145 [INFO] Train: [ 26/599] Final Prec@1 62.7644%
11/23 02:27:13AM evaluateStage_trainer.py:180 [INFO] Valid: [ 26/599] Final Prec@1 49.2200%
11/23 02:27:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 50.6600%
11/23 02:27:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [27][50/175]	Step 4802	lr 0.02488	Loss 1.1629 (1.2135)	Prec@(1,5) (64.5%, 90.9%)	
11/23 02:27:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [27][100/175]	Step 4852	lr 0.02488	Loss 1.2177 (1.2288)	Prec@(1,5) (64.0%, 90.4%)	
11/23 02:27:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [27][150/175]	Step 4902	lr 0.02488	Loss 1.3662 (1.2500)	Prec@(1,5) (63.6%, 90.1%)	
11/23 02:27:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [27][175/175]	Step 4927	lr 0.02488	Loss 1.3029 (1.2598)	Prec@(1,5) (63.3%, 89.9%)	
11/23 02:27:56AM evaluateStage_trainer.py:145 [INFO] Train: [ 27/599] Final Prec@1 63.2956%
11/23 02:27:58AM evaluateStage_trainer.py:180 [INFO] Valid: [ 27/599] Final Prec@1 49.0000%
11/23 02:27:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 50.6600%
11/23 02:28:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [28][50/175]	Step 4978	lr 0.02487	Loss 1.0589 (1.1768)	Prec@(1,5) (65.6%, 90.7%)	
11/23 02:28:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [28][100/175]	Step 5028	lr 0.02487	Loss 1.1914 (1.2002)	Prec@(1,5) (64.8%, 90.6%)	
11/23 02:28:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [28][150/175]	Step 5078	lr 0.02487	Loss 1.2803 (1.2227)	Prec@(1,5) (64.1%, 90.3%)	
11/23 02:28:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [28][175/175]	Step 5103	lr 0.02487	Loss 1.4374 (1.2290)	Prec@(1,5) (64.0%, 90.2%)	
11/23 02:28:41AM evaluateStage_trainer.py:145 [INFO] Train: [ 28/599] Final Prec@1 63.9889%
11/23 02:28:43AM evaluateStage_trainer.py:180 [INFO] Valid: [ 28/599] Final Prec@1 51.8800%
11/23 02:28:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 51.8800%
11/23 02:28:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [29][50/175]	Step 5154	lr 0.02486	Loss 1.2035 (1.1573)	Prec@(1,5) (66.5%, 91.3%)	
11/23 02:29:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [29][100/175]	Step 5204	lr 0.02486	Loss 1.1928 (1.1842)	Prec@(1,5) (65.5%, 90.8%)	
11/23 02:29:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [29][150/175]	Step 5254	lr 0.02486	Loss 1.1978 (1.1973)	Prec@(1,5) (65.1%, 90.7%)	
11/23 02:29:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [29][175/175]	Step 5279	lr 0.02486	Loss 1.3250 (1.2010)	Prec@(1,5) (65.0%, 90.7%)	
11/23 02:29:26AM evaluateStage_trainer.py:145 [INFO] Train: [ 29/599] Final Prec@1 64.9422%
11/23 02:29:29AM evaluateStage_trainer.py:180 [INFO] Valid: [ 29/599] Final Prec@1 51.5800%
11/23 02:29:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 51.8800%
11/23 02:29:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [30][50/175]	Step 5330	lr 0.02485	Loss 1.2039 (1.1417)	Prec@(1,5) (66.8%, 91.9%)	
11/23 02:29:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [30][100/175]	Step 5380	lr 0.02485	Loss 1.3415 (1.1535)	Prec@(1,5) (66.4%, 91.4%)	
11/23 02:30:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [30][150/175]	Step 5430	lr 0.02485	Loss 1.2349 (1.1662)	Prec@(1,5) (66.0%, 91.3%)	
11/23 02:30:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [30][175/175]	Step 5455	lr 0.02485	Loss 1.2008 (1.1754)	Prec@(1,5) (65.6%, 91.1%)	
11/23 02:30:11AM evaluateStage_trainer.py:145 [INFO] Train: [ 30/599] Final Prec@1 65.6111%
11/23 02:30:14AM evaluateStage_trainer.py:180 [INFO] Valid: [ 30/599] Final Prec@1 50.3200%
11/23 02:30:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 51.8800%
11/23 02:30:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [31][50/175]	Step 5506	lr 0.02484	Loss 1.1632 (1.1161)	Prec@(1,5) (67.3%, 91.9%)	
11/23 02:30:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [31][100/175]	Step 5556	lr 0.02484	Loss 1.0273 (1.1347)	Prec@(1,5) (66.6%, 91.6%)	
11/23 02:30:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [31][150/175]	Step 5606	lr 0.02484	Loss 1.1177 (1.1487)	Prec@(1,5) (66.3%, 91.4%)	
11/23 02:30:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [31][175/175]	Step 5631	lr 0.02484	Loss 1.4211 (1.1529)	Prec@(1,5) (66.1%, 91.4%)	
11/23 02:30:57AM evaluateStage_trainer.py:145 [INFO] Train: [ 31/599] Final Prec@1 66.1356%
11/23 02:31:00AM evaluateStage_trainer.py:180 [INFO] Valid: [ 31/599] Final Prec@1 49.7600%
11/23 02:31:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 51.8800%
11/23 02:31:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [32][50/175]	Step 5682	lr 0.02483	Loss 1.0898 (1.0823)	Prec@(1,5) (68.1%, 91.9%)	
11/23 02:31:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [32][100/175]	Step 5732	lr 0.02483	Loss 0.9846 (1.1013)	Prec@(1,5) (67.6%, 91.8%)	
11/23 02:31:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [32][150/175]	Step 5782	lr 0.02483	Loss 1.3468 (1.1312)	Prec@(1,5) (66.8%, 91.5%)	
11/23 02:31:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [32][175/175]	Step 5807	lr 0.02483	Loss 1.2953 (1.1368)	Prec@(1,5) (66.6%, 91.5%)	
11/23 02:31:42AM evaluateStage_trainer.py:145 [INFO] Train: [ 32/599] Final Prec@1 66.6067%
11/23 02:31:45AM evaluateStage_trainer.py:180 [INFO] Valid: [ 32/599] Final Prec@1 49.9000%
11/23 02:31:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 51.8800%
11/23 02:31:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [33][50/175]	Step 5858	lr 0.02482	Loss 1.1106 (1.0467)	Prec@(1,5) (69.5%, 92.5%)	
11/23 02:32:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [33][100/175]	Step 5908	lr 0.02482	Loss 1.1994 (1.0771)	Prec@(1,5) (68.5%, 92.1%)	
11/23 02:32:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [33][150/175]	Step 5958	lr 0.02482	Loss 1.1179 (1.0970)	Prec@(1,5) (67.8%, 92.0%)	
11/23 02:32:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [33][175/175]	Step 5983	lr 0.02482	Loss 1.3635 (1.1064)	Prec@(1,5) (67.5%, 91.9%)	
11/23 02:32:28AM evaluateStage_trainer.py:145 [INFO] Train: [ 33/599] Final Prec@1 67.4378%
11/23 02:32:30AM evaluateStage_trainer.py:180 [INFO] Valid: [ 33/599] Final Prec@1 52.2600%
11/23 02:32:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 52.2600%
11/23 02:32:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [34][50/175]	Step 6034	lr 0.02481	Loss 1.1403 (1.0293)	Prec@(1,5) (69.1%, 92.9%)	
11/23 02:32:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [34][100/175]	Step 6084	lr 0.02481	Loss 1.0933 (1.0466)	Prec@(1,5) (69.0%, 92.5%)	
11/23 02:33:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [34][150/175]	Step 6134	lr 0.02481	Loss 1.0220 (1.0750)	Prec@(1,5) (68.1%, 92.1%)	
11/23 02:33:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [34][175/175]	Step 6159	lr 0.02481	Loss 1.0217 (1.0862)	Prec@(1,5) (67.9%, 92.0%)	
11/23 02:33:13AM evaluateStage_trainer.py:145 [INFO] Train: [ 34/599] Final Prec@1 67.9089%
11/23 02:33:16AM evaluateStage_trainer.py:180 [INFO] Valid: [ 34/599] Final Prec@1 51.8600%
11/23 02:33:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 52.2600%
11/23 02:33:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [35][50/175]	Step 6210	lr 0.0248	Loss 1.0484 (1.0212)	Prec@(1,5) (69.4%, 92.9%)	
11/23 02:33:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [35][100/175]	Step 6260	lr 0.0248	Loss 0.9559 (1.0508)	Prec@(1,5) (68.8%, 92.7%)	
11/23 02:33:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [35][150/175]	Step 6310	lr 0.0248	Loss 1.1866 (1.0681)	Prec@(1,5) (68.5%, 92.5%)	
11/23 02:33:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [35][175/175]	Step 6335	lr 0.0248	Loss 1.0455 (1.0723)	Prec@(1,5) (68.4%, 92.4%)	
11/23 02:33:58AM evaluateStage_trainer.py:145 [INFO] Train: [ 35/599] Final Prec@1 68.4511%
11/23 02:34:01AM evaluateStage_trainer.py:180 [INFO] Valid: [ 35/599] Final Prec@1 53.5600%
11/23 02:34:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:34:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [36][50/175]	Step 6386	lr 0.02479	Loss 1.0208 (0.9950)	Prec@(1,5) (70.3%, 93.4%)	
11/23 02:34:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [36][100/175]	Step 6436	lr 0.02479	Loss 1.0550 (1.0156)	Prec@(1,5) (69.7%, 93.3%)	
11/23 02:34:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [36][150/175]	Step 6486	lr 0.02479	Loss 1.1132 (1.0367)	Prec@(1,5) (69.0%, 92.9%)	
11/23 02:34:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [36][175/175]	Step 6511	lr 0.02479	Loss 1.0421 (1.0454)	Prec@(1,5) (68.9%, 92.8%)	
11/23 02:34:44AM evaluateStage_trainer.py:145 [INFO] Train: [ 36/599] Final Prec@1 68.8956%
11/23 02:34:46AM evaluateStage_trainer.py:180 [INFO] Valid: [ 36/599] Final Prec@1 52.0200%
11/23 02:34:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:34:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [37][50/175]	Step 6562	lr 0.02478	Loss 0.9991 (0.9621)	Prec@(1,5) (71.2%, 93.8%)	
11/23 02:35:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [37][100/175]	Step 6612	lr 0.02478	Loss 1.1611 (0.9988)	Prec@(1,5) (70.1%, 93.3%)	
11/23 02:35:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [37][150/175]	Step 6662	lr 0.02478	Loss 1.0066 (1.0217)	Prec@(1,5) (69.6%, 93.1%)	
11/23 02:35:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [37][175/175]	Step 6687	lr 0.02478	Loss 1.1541 (1.0333)	Prec@(1,5) (69.2%, 92.9%)	
11/23 02:35:29AM evaluateStage_trainer.py:145 [INFO] Train: [ 37/599] Final Prec@1 69.2022%
11/23 02:35:31AM evaluateStage_trainer.py:180 [INFO] Valid: [ 37/599] Final Prec@1 52.5600%
11/23 02:35:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:35:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [38][50/175]	Step 6738	lr 0.02476	Loss 0.9014 (0.9569)	Prec@(1,5) (71.7%, 94.0%)	
11/23 02:35:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [38][100/175]	Step 6788	lr 0.02476	Loss 0.9141 (0.9850)	Prec@(1,5) (70.5%, 93.5%)	
11/23 02:36:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [38][150/175]	Step 6838	lr 0.02476	Loss 0.9786 (0.9975)	Prec@(1,5) (70.1%, 93.3%)	
11/23 02:36:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [38][175/175]	Step 6863	lr 0.02476	Loss 1.0218 (1.0052)	Prec@(1,5) (70.0%, 93.2%)	
11/23 02:36:14AM evaluateStage_trainer.py:145 [INFO] Train: [ 38/599] Final Prec@1 69.9822%
11/23 02:36:16AM evaluateStage_trainer.py:180 [INFO] Valid: [ 38/599] Final Prec@1 53.3800%
11/23 02:36:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:36:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [39][50/175]	Step 6914	lr 0.02475	Loss 0.8753 (0.9151)	Prec@(1,5) (72.9%, 94.4%)	
11/23 02:36:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [39][100/175]	Step 6964	lr 0.02475	Loss 0.9192 (0.9426)	Prec@(1,5) (71.7%, 94.1%)	
11/23 02:36:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [39][150/175]	Step 7014	lr 0.02475	Loss 1.0322 (0.9705)	Prec@(1,5) (71.0%, 93.7%)	
11/23 02:36:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [39][175/175]	Step 7039	lr 0.02475	Loss 1.0248 (0.9822)	Prec@(1,5) (70.6%, 93.6%)	
11/23 02:36:59AM evaluateStage_trainer.py:145 [INFO] Train: [ 39/599] Final Prec@1 70.5933%
11/23 02:37:02AM evaluateStage_trainer.py:180 [INFO] Valid: [ 39/599] Final Prec@1 53.1600%
11/23 02:37:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:37:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [40][50/175]	Step 7090	lr 0.02474	Loss 0.9881 (0.8898)	Prec@(1,5) (73.1%, 94.6%)	
11/23 02:37:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [40][100/175]	Step 7140	lr 0.02474	Loss 1.0568 (0.9233)	Prec@(1,5) (72.1%, 94.2%)	
11/23 02:37:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [40][150/175]	Step 7190	lr 0.02474	Loss 1.0371 (0.9507)	Prec@(1,5) (71.4%, 93.9%)	
11/23 02:37:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [40][175/175]	Step 7215	lr 0.02474	Loss 0.9192 (0.9616)	Prec@(1,5) (71.2%, 93.8%)	
11/23 02:37:44AM evaluateStage_trainer.py:145 [INFO] Train: [ 40/599] Final Prec@1 71.1711%
11/23 02:37:47AM evaluateStage_trainer.py:180 [INFO] Valid: [ 40/599] Final Prec@1 51.5200%
11/23 02:37:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 53.5600%
11/23 02:38:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [41][50/175]	Step 7266	lr 0.02472	Loss 0.7680 (0.8867)	Prec@(1,5) (73.4%, 94.6%)	
11/23 02:38:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [41][100/175]	Step 7316	lr 0.02472	Loss 1.0190 (0.9105)	Prec@(1,5) (72.4%, 94.4%)	
11/23 02:38:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [41][150/175]	Step 7366	lr 0.02472	Loss 0.8584 (0.9378)	Prec@(1,5) (71.7%, 94.1%)	
11/23 02:38:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [41][175/175]	Step 7391	lr 0.02472	Loss 0.9896 (0.9421)	Prec@(1,5) (71.6%, 94.1%)	
11/23 02:38:30AM evaluateStage_trainer.py:145 [INFO] Train: [ 41/599] Final Prec@1 71.6089%
11/23 02:38:33AM evaluateStage_trainer.py:180 [INFO] Valid: [ 41/599] Final Prec@1 54.4400%
11/23 02:38:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.4400%
11/23 02:38:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [42][50/175]	Step 7442	lr 0.02471	Loss 0.9363 (0.8589)	Prec@(1,5) (73.8%, 95.2%)	
11/23 02:38:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [42][100/175]	Step 7492	lr 0.02471	Loss 0.7709 (0.8944)	Prec@(1,5) (72.6%, 94.7%)	
11/23 02:39:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [42][150/175]	Step 7542	lr 0.02471	Loss 0.9149 (0.9267)	Prec@(1,5) (71.8%, 94.3%)	
11/23 02:39:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [42][175/175]	Step 7567	lr 0.02471	Loss 0.8363 (0.9304)	Prec@(1,5) (71.8%, 94.3%)	
11/23 02:39:16AM evaluateStage_trainer.py:145 [INFO] Train: [ 42/599] Final Prec@1 71.7311%
11/23 02:39:19AM evaluateStage_trainer.py:180 [INFO] Valid: [ 42/599] Final Prec@1 53.0000%
11/23 02:39:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.4400%
11/23 02:39:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [43][50/175]	Step 7618	lr 0.0247	Loss 0.9097 (0.8604)	Prec@(1,5) (74.3%, 95.0%)	
11/23 02:39:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [43][100/175]	Step 7668	lr 0.0247	Loss 0.7863 (0.8894)	Prec@(1,5) (73.5%, 94.6%)	
11/23 02:39:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [43][150/175]	Step 7718	lr 0.0247	Loss 0.8207 (0.9038)	Prec@(1,5) (72.8%, 94.6%)	
11/23 02:40:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [43][175/175]	Step 7743	lr 0.0247	Loss 0.9071 (0.9150)	Prec@(1,5) (72.6%, 94.4%)	
11/23 02:40:01AM evaluateStage_trainer.py:145 [INFO] Train: [ 43/599] Final Prec@1 72.5867%
11/23 02:40:04AM evaluateStage_trainer.py:180 [INFO] Valid: [ 43/599] Final Prec@1 52.7600%
11/23 02:40:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.4400%
11/23 02:40:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [44][50/175]	Step 7794	lr 0.02468	Loss 0.7895 (0.8376)	Prec@(1,5) (74.4%, 95.4%)	
11/23 02:40:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [44][100/175]	Step 7844	lr 0.02468	Loss 0.9835 (0.8638)	Prec@(1,5) (73.5%, 95.1%)	
11/23 02:40:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [44][150/175]	Step 7894	lr 0.02468	Loss 0.9353 (0.8838)	Prec@(1,5) (73.1%, 94.8%)	
11/23 02:40:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [44][175/175]	Step 7919	lr 0.02468	Loss 1.0580 (0.8949)	Prec@(1,5) (72.7%, 94.7%)	
11/23 02:40:47AM evaluateStage_trainer.py:145 [INFO] Train: [ 44/599] Final Prec@1 72.7511%
11/23 02:40:50AM evaluateStage_trainer.py:180 [INFO] Valid: [ 44/599] Final Prec@1 52.4000%
11/23 02:40:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.4400%
11/23 02:41:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [45][50/175]	Step 7970	lr 0.02467	Loss 0.8400 (0.8278)	Prec@(1,5) (74.6%, 95.3%)	
11/23 02:41:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [45][100/175]	Step 8020	lr 0.02467	Loss 0.8442 (0.8347)	Prec@(1,5) (74.4%, 95.2%)	
11/23 02:41:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [45][150/175]	Step 8070	lr 0.02467	Loss 0.7871 (0.8565)	Prec@(1,5) (73.9%, 94.9%)	
11/23 02:41:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [45][175/175]	Step 8095	lr 0.02467	Loss 1.0785 (0.8690)	Prec@(1,5) (73.6%, 94.8%)	
11/23 02:41:32AM evaluateStage_trainer.py:145 [INFO] Train: [ 45/599] Final Prec@1 73.5489%
11/23 02:41:35AM evaluateStage_trainer.py:180 [INFO] Valid: [ 45/599] Final Prec@1 52.0800%
11/23 02:41:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.4400%
11/23 02:41:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [46][50/175]	Step 8146	lr 0.02465	Loss 0.8546 (0.8067)	Prec@(1,5) (75.2%, 95.7%)	
11/23 02:42:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [46][100/175]	Step 8196	lr 0.02465	Loss 0.8416 (0.8369)	Prec@(1,5) (74.2%, 95.3%)	
11/23 02:42:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [46][150/175]	Step 8246	lr 0.02465	Loss 0.8867 (0.8568)	Prec@(1,5) (73.7%, 95.1%)	
11/23 02:42:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [46][175/175]	Step 8271	lr 0.02465	Loss 0.7645 (0.8650)	Prec@(1,5) (73.6%, 94.9%)	
11/23 02:42:18AM evaluateStage_trainer.py:145 [INFO] Train: [ 46/599] Final Prec@1 73.5556%
11/23 02:42:21AM evaluateStage_trainer.py:180 [INFO] Valid: [ 46/599] Final Prec@1 54.5600%
11/23 02:42:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:42:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [47][50/175]	Step 8322	lr 0.02464	Loss 0.8961 (0.7695)	Prec@(1,5) (76.5%, 96.1%)	
11/23 02:42:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [47][100/175]	Step 8372	lr 0.02464	Loss 0.8303 (0.8110)	Prec@(1,5) (75.1%, 95.7%)	
11/23 02:42:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [47][150/175]	Step 8422	lr 0.02464	Loss 0.8607 (0.8366)	Prec@(1,5) (74.4%, 95.3%)	
11/23 02:43:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [47][175/175]	Step 8447	lr 0.02464	Loss 0.9513 (0.8488)	Prec@(1,5) (73.9%, 95.2%)	
11/23 02:43:04AM evaluateStage_trainer.py:145 [INFO] Train: [ 47/599] Final Prec@1 73.9356%
11/23 02:43:06AM evaluateStage_trainer.py:180 [INFO] Valid: [ 47/599] Final Prec@1 51.9600%
11/23 02:43:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:43:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [48][50/175]	Step 8498	lr 0.02462	Loss 0.8013 (0.8123)	Prec@(1,5) (75.1%, 95.5%)	
11/23 02:43:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [48][100/175]	Step 8548	lr 0.02462	Loss 0.9401 (0.8288)	Prec@(1,5) (74.6%, 95.4%)	
11/23 02:43:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [48][150/175]	Step 8598	lr 0.02462	Loss 1.0059 (0.8405)	Prec@(1,5) (74.2%, 95.2%)	
11/23 02:43:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [48][175/175]	Step 8623	lr 0.02462	Loss 0.8928 (0.8446)	Prec@(1,5) (74.2%, 95.1%)	
11/23 02:43:49AM evaluateStage_trainer.py:145 [INFO] Train: [ 48/599] Final Prec@1 74.1644%
11/23 02:43:52AM evaluateStage_trainer.py:180 [INFO] Valid: [ 48/599] Final Prec@1 54.1200%
11/23 02:43:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:44:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [49][50/175]	Step 8674	lr 0.02461	Loss 0.7687 (0.7654)	Prec@(1,5) (76.2%, 96.1%)	
11/23 02:44:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [49][100/175]	Step 8724	lr 0.02461	Loss 0.7250 (0.7903)	Prec@(1,5) (75.5%, 95.8%)	
11/23 02:44:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [49][150/175]	Step 8774	lr 0.02461	Loss 0.9532 (0.8107)	Prec@(1,5) (75.1%, 95.5%)	
11/23 02:44:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [49][175/175]	Step 8799	lr 0.02461	Loss 0.8747 (0.8181)	Prec@(1,5) (75.0%, 95.4%)	
11/23 02:44:34AM evaluateStage_trainer.py:145 [INFO] Train: [ 49/599] Final Prec@1 74.9711%
11/23 02:44:37AM evaluateStage_trainer.py:180 [INFO] Valid: [ 49/599] Final Prec@1 54.1400%
11/23 02:44:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:44:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [50][50/175]	Step 8850	lr 0.02459	Loss 0.8923 (0.7387)	Prec@(1,5) (77.3%, 96.5%)	
11/23 02:45:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [50][100/175]	Step 8900	lr 0.02459	Loss 0.8983 (0.7714)	Prec@(1,5) (76.2%, 96.1%)	
11/23 02:45:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [50][150/175]	Step 8950	lr 0.02459	Loss 0.8248 (0.7969)	Prec@(1,5) (75.5%, 95.8%)	
11/23 02:45:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [50][175/175]	Step 8975	lr 0.02459	Loss 0.9299 (0.8059)	Prec@(1,5) (75.2%, 95.7%)	
11/23 02:45:19AM evaluateStage_trainer.py:145 [INFO] Train: [ 50/599] Final Prec@1 75.2444%
11/23 02:45:22AM evaluateStage_trainer.py:180 [INFO] Valid: [ 50/599] Final Prec@1 54.2800%
11/23 02:45:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:45:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [51][50/175]	Step 9026	lr 0.02457	Loss 0.7550 (0.7269)	Prec@(1,5) (77.3%, 96.8%)	
11/23 02:45:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [51][100/175]	Step 9076	lr 0.02457	Loss 0.8085 (0.7641)	Prec@(1,5) (76.3%, 96.2%)	
11/23 02:45:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [51][150/175]	Step 9126	lr 0.02457	Loss 0.7770 (0.7902)	Prec@(1,5) (75.7%, 95.8%)	
11/23 02:46:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [51][175/175]	Step 9151	lr 0.02457	Loss 0.8825 (0.8028)	Prec@(1,5) (75.3%, 95.6%)	
11/23 02:46:05AM evaluateStage_trainer.py:145 [INFO] Train: [ 51/599] Final Prec@1 75.2844%
11/23 02:46:08AM evaluateStage_trainer.py:180 [INFO] Valid: [ 51/599] Final Prec@1 53.0000%
11/23 02:46:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:46:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [52][50/175]	Step 9202	lr 0.02456	Loss 0.8889 (0.7414)	Prec@(1,5) (76.9%, 96.5%)	
11/23 02:46:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [52][100/175]	Step 9252	lr 0.02456	Loss 0.7845 (0.7543)	Prec@(1,5) (76.5%, 96.3%)	
11/23 02:46:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [52][150/175]	Step 9302	lr 0.02456	Loss 0.9061 (0.7730)	Prec@(1,5) (76.0%, 96.2%)	
11/23 02:46:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [52][175/175]	Step 9327	lr 0.02456	Loss 0.8670 (0.7841)	Prec@(1,5) (75.8%, 96.0%)	
11/23 02:46:50AM evaluateStage_trainer.py:145 [INFO] Train: [ 52/599] Final Prec@1 75.8089%
11/23 02:46:53AM evaluateStage_trainer.py:180 [INFO] Valid: [ 52/599] Final Prec@1 54.2400%
11/23 02:46:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:47:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [53][50/175]	Step 9378	lr 0.02454	Loss 0.7772 (0.7281)	Prec@(1,5) (77.2%, 96.5%)	
11/23 02:47:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [53][100/175]	Step 9428	lr 0.02454	Loss 0.6885 (0.7408)	Prec@(1,5) (77.0%, 96.3%)	
11/23 02:47:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [53][150/175]	Step 9478	lr 0.02454	Loss 0.8971 (0.7584)	Prec@(1,5) (76.4%, 96.1%)	
11/23 02:47:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [53][175/175]	Step 9503	lr 0.02454	Loss 0.8179 (0.7672)	Prec@(1,5) (76.1%, 96.0%)	
11/23 02:47:35AM evaluateStage_trainer.py:145 [INFO] Train: [ 53/599] Final Prec@1 76.1111%
11/23 02:47:38AM evaluateStage_trainer.py:180 [INFO] Valid: [ 53/599] Final Prec@1 53.3200%
11/23 02:47:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.5600%
11/23 02:47:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [54][50/175]	Step 9554	lr 0.02452	Loss 0.7621 (0.7242)	Prec@(1,5) (77.5%, 96.5%)	
11/23 02:48:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [54][100/175]	Step 9604	lr 0.02452	Loss 0.7399 (0.7304)	Prec@(1,5) (77.5%, 96.4%)	
11/23 02:48:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [54][150/175]	Step 9654	lr 0.02452	Loss 0.8921 (0.7486)	Prec@(1,5) (76.9%, 96.2%)	
11/23 02:48:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [54][175/175]	Step 9679	lr 0.02452	Loss 1.0357 (0.7590)	Prec@(1,5) (76.5%, 96.1%)	
11/23 02:48:21AM evaluateStage_trainer.py:145 [INFO] Train: [ 54/599] Final Prec@1 76.5378%
11/23 02:48:24AM evaluateStage_trainer.py:180 [INFO] Valid: [ 54/599] Final Prec@1 54.6800%
11/23 02:48:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.6800%
11/23 02:48:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [55][50/175]	Step 9730	lr 0.02451	Loss 0.6822 (0.6867)	Prec@(1,5) (78.9%, 96.9%)	
11/23 02:48:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [55][100/175]	Step 9780	lr 0.02451	Loss 0.6613 (0.7162)	Prec@(1,5) (77.8%, 96.6%)	
11/23 02:49:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [55][150/175]	Step 9830	lr 0.02451	Loss 0.7682 (0.7356)	Prec@(1,5) (77.2%, 96.5%)	
11/23 02:49:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [55][175/175]	Step 9855	lr 0.02451	Loss 0.8534 (0.7458)	Prec@(1,5) (76.9%, 96.4%)	
11/23 02:49:07AM evaluateStage_trainer.py:145 [INFO] Train: [ 55/599] Final Prec@1 76.9022%
11/23 02:49:09AM evaluateStage_trainer.py:180 [INFO] Valid: [ 55/599] Final Prec@1 54.9000%
11/23 02:49:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.9000%
11/23 02:49:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [56][50/175]	Step 9906	lr 0.02449	Loss 0.5362 (0.6738)	Prec@(1,5) (78.9%, 96.9%)	
11/23 02:49:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [56][100/175]	Step 9956	lr 0.02449	Loss 0.7900 (0.6890)	Prec@(1,5) (78.5%, 96.8%)	
11/23 02:49:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [56][150/175]	Step 10006	lr 0.02449	Loss 0.9402 (0.7128)	Prec@(1,5) (77.8%, 96.5%)	
11/23 02:49:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [56][175/175]	Step 10031	lr 0.02449	Loss 0.7449 (0.7238)	Prec@(1,5) (77.5%, 96.4%)	
11/23 02:49:53AM evaluateStage_trainer.py:145 [INFO] Train: [ 56/599] Final Prec@1 77.5111%
11/23 02:49:55AM evaluateStage_trainer.py:180 [INFO] Valid: [ 56/599] Final Prec@1 52.7200%
11/23 02:49:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.9000%
11/23 02:50:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [57][50/175]	Step 10082	lr 0.02447	Loss 0.8239 (0.6773)	Prec@(1,5) (78.8%, 97.0%)	
11/23 02:50:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [57][100/175]	Step 10132	lr 0.02447	Loss 0.7854 (0.6865)	Prec@(1,5) (78.3%, 96.9%)	
11/23 02:50:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [57][150/175]	Step 10182	lr 0.02447	Loss 0.7271 (0.7069)	Prec@(1,5) (77.9%, 96.7%)	
11/23 02:50:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [57][175/175]	Step 10207	lr 0.02447	Loss 0.7789 (0.7227)	Prec@(1,5) (77.5%, 96.5%)	
11/23 02:50:38AM evaluateStage_trainer.py:145 [INFO] Train: [ 57/599] Final Prec@1 77.4578%
11/23 02:50:41AM evaluateStage_trainer.py:180 [INFO] Valid: [ 57/599] Final Prec@1 51.6000%
11/23 02:50:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.9000%
11/23 02:50:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [58][50/175]	Step 10258	lr 0.02445	Loss 0.6264 (0.6806)	Prec@(1,5) (78.5%, 96.9%)	
11/23 02:51:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [58][100/175]	Step 10308	lr 0.02445	Loss 0.7657 (0.6884)	Prec@(1,5) (78.4%, 96.8%)	
11/23 02:51:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [58][150/175]	Step 10358	lr 0.02445	Loss 0.8532 (0.7015)	Prec@(1,5) (78.0%, 96.7%)	
11/23 02:51:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [58][175/175]	Step 10383	lr 0.02445	Loss 0.7564 (0.7119)	Prec@(1,5) (77.7%, 96.6%)	
11/23 02:51:24AM evaluateStage_trainer.py:145 [INFO] Train: [ 58/599] Final Prec@1 77.7244%
11/23 02:51:27AM evaluateStage_trainer.py:180 [INFO] Valid: [ 58/599] Final Prec@1 53.6000%
11/23 02:51:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.9000%
11/23 02:51:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [59][50/175]	Step 10434	lr 0.02443	Loss 0.6807 (0.6390)	Prec@(1,5) (79.9%, 97.2%)	
11/23 02:51:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [59][100/175]	Step 10484	lr 0.02443	Loss 0.8030 (0.6692)	Prec@(1,5) (78.9%, 97.1%)	
11/23 02:52:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [59][150/175]	Step 10534	lr 0.02443	Loss 0.8606 (0.6847)	Prec@(1,5) (78.5%, 96.9%)	
11/23 02:52:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [59][175/175]	Step 10559	lr 0.02443	Loss 0.7736 (0.6996)	Prec@(1,5) (78.1%, 96.8%)	
11/23 02:52:09AM evaluateStage_trainer.py:145 [INFO] Train: [ 59/599] Final Prec@1 78.0867%
11/23 02:52:12AM evaluateStage_trainer.py:180 [INFO] Valid: [ 59/599] Final Prec@1 54.3200%
11/23 02:52:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 54.9000%
11/23 02:52:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [60][50/175]	Step 10610	lr 0.02441	Loss 0.6760 (0.6486)	Prec@(1,5) (80.1%, 97.4%)	
11/23 02:52:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [60][100/175]	Step 10660	lr 0.02441	Loss 0.6696 (0.6558)	Prec@(1,5) (79.9%, 97.2%)	
11/23 02:52:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [60][150/175]	Step 10710	lr 0.02441	Loss 0.7367 (0.6758)	Prec@(1,5) (79.1%, 97.0%)	
11/23 02:52:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [60][175/175]	Step 10735	lr 0.02441	Loss 0.7814 (0.6839)	Prec@(1,5) (78.9%, 96.9%)	
11/23 02:52:55AM evaluateStage_trainer.py:145 [INFO] Train: [ 60/599] Final Prec@1 78.8911%
11/23 02:52:57AM evaluateStage_trainer.py:180 [INFO] Valid: [ 60/599] Final Prec@1 55.5200%
11/23 02:52:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5200%
11/23 02:53:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [61][50/175]	Step 10786	lr 0.02439	Loss 0.5429 (0.6226)	Prec@(1,5) (80.7%, 97.5%)	
11/23 02:53:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [61][100/175]	Step 10836	lr 0.02439	Loss 0.6598 (0.6428)	Prec@(1,5) (79.9%, 97.3%)	
11/23 02:53:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [61][150/175]	Step 10886	lr 0.02439	Loss 0.7168 (0.6665)	Prec@(1,5) (79.2%, 97.1%)	
11/23 02:53:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [61][175/175]	Step 10911	lr 0.02439	Loss 0.7906 (0.6806)	Prec@(1,5) (78.7%, 96.9%)	
11/23 02:53:40AM evaluateStage_trainer.py:145 [INFO] Train: [ 61/599] Final Prec@1 78.7000%
11/23 02:53:43AM evaluateStage_trainer.py:180 [INFO] Valid: [ 61/599] Final Prec@1 55.5400%
11/23 02:53:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:53:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [62][50/175]	Step 10962	lr 0.02437	Loss 0.5169 (0.6279)	Prec@(1,5) (80.7%, 97.2%)	
11/23 02:54:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [62][100/175]	Step 11012	lr 0.02437	Loss 0.7979 (0.6398)	Prec@(1,5) (80.2%, 97.2%)	
11/23 02:54:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [62][150/175]	Step 11062	lr 0.02437	Loss 0.7457 (0.6627)	Prec@(1,5) (79.4%, 97.0%)	
11/23 02:54:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [62][175/175]	Step 11087	lr 0.02437	Loss 0.7971 (0.6721)	Prec@(1,5) (79.0%, 97.0%)	
11/23 02:54:26AM evaluateStage_trainer.py:145 [INFO] Train: [ 62/599] Final Prec@1 79.0311%
11/23 02:54:30AM evaluateStage_trainer.py:180 [INFO] Valid: [ 62/599] Final Prec@1 54.8200%
11/23 02:54:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:54:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [63][50/175]	Step 11138	lr 0.02435	Loss 0.6258 (0.6006)	Prec@(1,5) (81.1%, 97.7%)	
11/23 02:54:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [63][100/175]	Step 11188	lr 0.02435	Loss 0.6322 (0.6206)	Prec@(1,5) (80.6%, 97.4%)	
11/23 02:55:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [63][150/175]	Step 11238	lr 0.02435	Loss 0.6457 (0.6474)	Prec@(1,5) (79.6%, 97.2%)	
11/23 02:55:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [63][175/175]	Step 11263	lr 0.02435	Loss 0.6270 (0.6581)	Prec@(1,5) (79.3%, 97.2%)	
11/23 02:55:12AM evaluateStage_trainer.py:145 [INFO] Train: [ 63/599] Final Prec@1 79.2800%
11/23 02:55:15AM evaluateStage_trainer.py:180 [INFO] Valid: [ 63/599] Final Prec@1 55.3800%
11/23 02:55:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:55:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [64][50/175]	Step 11314	lr 0.02433	Loss 0.6714 (0.6017)	Prec@(1,5) (81.1%, 97.5%)	
11/23 02:55:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [64][100/175]	Step 11364	lr 0.02433	Loss 0.6575 (0.6190)	Prec@(1,5) (80.7%, 97.4%)	
11/23 02:55:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [64][150/175]	Step 11414	lr 0.02433	Loss 0.6484 (0.6474)	Prec@(1,5) (79.7%, 97.3%)	
11/23 02:55:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [64][175/175]	Step 11439	lr 0.02433	Loss 0.6960 (0.6568)	Prec@(1,5) (79.3%, 97.2%)	
11/23 02:55:58AM evaluateStage_trainer.py:145 [INFO] Train: [ 64/599] Final Prec@1 79.2778%
11/23 02:56:01AM evaluateStage_trainer.py:180 [INFO] Valid: [ 64/599] Final Prec@1 54.0400%
11/23 02:56:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:56:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [65][50/175]	Step 11490	lr 0.02431	Loss 0.6294 (0.5768)	Prec@(1,5) (81.7%, 97.8%)	
11/23 02:56:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [65][100/175]	Step 11540	lr 0.02431	Loss 0.7320 (0.6034)	Prec@(1,5) (81.0%, 97.6%)	
11/23 02:56:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [65][150/175]	Step 11590	lr 0.02431	Loss 0.6515 (0.6214)	Prec@(1,5) (80.5%, 97.5%)	
11/23 02:56:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [65][175/175]	Step 11615	lr 0.02431	Loss 0.7039 (0.6362)	Prec@(1,5) (80.0%, 97.3%)	
11/23 02:56:43AM evaluateStage_trainer.py:145 [INFO] Train: [ 65/599] Final Prec@1 79.9689%
11/23 02:56:46AM evaluateStage_trainer.py:180 [INFO] Valid: [ 65/599] Final Prec@1 54.6600%
11/23 02:56:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:56:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [66][50/175]	Step 11666	lr 0.02429	Loss 0.6251 (0.5884)	Prec@(1,5) (81.4%, 97.8%)	
11/23 02:57:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [66][100/175]	Step 11716	lr 0.02429	Loss 0.6259 (0.6062)	Prec@(1,5) (80.8%, 97.7%)	
11/23 02:57:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [66][150/175]	Step 11766	lr 0.02429	Loss 0.6341 (0.6220)	Prec@(1,5) (80.3%, 97.5%)	
11/23 02:57:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [66][175/175]	Step 11791	lr 0.02429	Loss 0.6201 (0.6337)	Prec@(1,5) (80.1%, 97.5%)	
11/23 02:57:28AM evaluateStage_trainer.py:145 [INFO] Train: [ 66/599] Final Prec@1 80.0356%
11/23 02:57:32AM evaluateStage_trainer.py:180 [INFO] Valid: [ 66/599] Final Prec@1 54.6400%
11/23 02:57:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:57:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [67][50/175]	Step 11842	lr 0.02427	Loss 0.6496 (0.5905)	Prec@(1,5) (81.8%, 97.8%)	
11/23 02:57:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [67][100/175]	Step 11892	lr 0.02427	Loss 0.4564 (0.6019)	Prec@(1,5) (81.3%, 97.7%)	
11/23 02:58:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [67][150/175]	Step 11942	lr 0.02427	Loss 0.6714 (0.6238)	Prec@(1,5) (80.5%, 97.5%)	
11/23 02:58:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [67][175/175]	Step 11967	lr 0.02427	Loss 0.7095 (0.6316)	Prec@(1,5) (80.3%, 97.5%)	
11/23 02:58:14AM evaluateStage_trainer.py:145 [INFO] Train: [ 67/599] Final Prec@1 80.2933%
11/23 02:58:17AM evaluateStage_trainer.py:180 [INFO] Valid: [ 67/599] Final Prec@1 53.7200%
11/23 02:58:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:58:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [68][50/175]	Step 12018	lr 0.02425	Loss 0.5629 (0.5562)	Prec@(1,5) (82.3%, 98.1%)	
11/23 02:58:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [68][100/175]	Step 12068	lr 0.02425	Loss 0.5521 (0.5836)	Prec@(1,5) (81.6%, 97.8%)	
11/23 02:58:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [68][150/175]	Step 12118	lr 0.02425	Loss 0.5626 (0.6138)	Prec@(1,5) (80.8%, 97.6%)	
11/23 02:58:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [68][175/175]	Step 12143	lr 0.02425	Loss 0.5563 (0.6262)	Prec@(1,5) (80.3%, 97.5%)	
11/23 02:59:00AM evaluateStage_trainer.py:145 [INFO] Train: [ 68/599] Final Prec@1 80.2600%
11/23 02:59:02AM evaluateStage_trainer.py:180 [INFO] Valid: [ 68/599] Final Prec@1 54.1400%
11/23 02:59:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 02:59:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [69][50/175]	Step 12194	lr 0.02423	Loss 0.5648 (0.5518)	Prec@(1,5) (82.6%, 98.1%)	
11/23 02:59:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [69][100/175]	Step 12244	lr 0.02423	Loss 0.5789 (0.5670)	Prec@(1,5) (82.1%, 97.9%)	
11/23 02:59:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [69][150/175]	Step 12294	lr 0.02423	Loss 0.4935 (0.5859)	Prec@(1,5) (81.6%, 97.7%)	
11/23 02:59:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [69][175/175]	Step 12319	lr 0.02423	Loss 0.6149 (0.5949)	Prec@(1,5) (81.3%, 97.7%)	
11/23 02:59:45AM evaluateStage_trainer.py:145 [INFO] Train: [ 69/599] Final Prec@1 81.2178%
11/23 02:59:48AM evaluateStage_trainer.py:180 [INFO] Valid: [ 69/599] Final Prec@1 53.3400%
11/23 02:59:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:00:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [70][50/175]	Step 12370	lr 0.0242	Loss 0.5183 (0.5296)	Prec@(1,5) (83.4%, 98.2%)	
11/23 03:00:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [70][100/175]	Step 12420	lr 0.0242	Loss 0.6075 (0.5425)	Prec@(1,5) (83.0%, 98.1%)	
11/23 03:00:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [70][150/175]	Step 12470	lr 0.0242	Loss 0.6606 (0.5712)	Prec@(1,5) (82.1%, 97.8%)	
11/23 03:00:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [70][175/175]	Step 12495	lr 0.0242	Loss 0.7053 (0.5840)	Prec@(1,5) (81.6%, 97.8%)	
11/23 03:00:30AM evaluateStage_trainer.py:145 [INFO] Train: [ 70/599] Final Prec@1 81.6200%
11/23 03:00:33AM evaluateStage_trainer.py:180 [INFO] Valid: [ 70/599] Final Prec@1 53.8000%
11/23 03:00:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:00:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [71][50/175]	Step 12546	lr 0.02418	Loss 0.5179 (0.5617)	Prec@(1,5) (82.2%, 98.0%)	
11/23 03:00:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [71][100/175]	Step 12596	lr 0.02418	Loss 0.5544 (0.5644)	Prec@(1,5) (82.2%, 97.9%)	
11/23 03:01:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [71][150/175]	Step 12646	lr 0.02418	Loss 0.7307 (0.5802)	Prec@(1,5) (81.7%, 97.8%)	
11/23 03:01:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [71][175/175]	Step 12671	lr 0.02418	Loss 0.4741 (0.5882)	Prec@(1,5) (81.4%, 97.8%)	
11/23 03:01:16AM evaluateStage_trainer.py:145 [INFO] Train: [ 71/599] Final Prec@1 81.3644%
11/23 03:01:19AM evaluateStage_trainer.py:180 [INFO] Valid: [ 71/599] Final Prec@1 54.5800%
11/23 03:01:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:01:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [72][50/175]	Step 12722	lr 0.02416	Loss 0.6841 (0.5473)	Prec@(1,5) (82.5%, 98.1%)	
11/23 03:01:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [72][100/175]	Step 12772	lr 0.02416	Loss 0.5543 (0.5616)	Prec@(1,5) (82.2%, 98.0%)	
11/23 03:01:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [72][150/175]	Step 12822	lr 0.02416	Loss 0.7136 (0.5782)	Prec@(1,5) (81.6%, 97.9%)	
11/23 03:02:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [72][175/175]	Step 12847	lr 0.02416	Loss 0.6571 (0.5879)	Prec@(1,5) (81.2%, 97.9%)	
11/23 03:02:01AM evaluateStage_trainer.py:145 [INFO] Train: [ 72/599] Final Prec@1 81.2133%
11/23 03:02:04AM evaluateStage_trainer.py:180 [INFO] Valid: [ 72/599] Final Prec@1 53.8800%
11/23 03:02:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:02:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [73][50/175]	Step 12898	lr 0.02413	Loss 0.6077 (0.5250)	Prec@(1,5) (83.3%, 98.3%)	
11/23 03:02:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [73][100/175]	Step 12948	lr 0.02413	Loss 0.5447 (0.5403)	Prec@(1,5) (82.8%, 98.2%)	
11/23 03:02:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [73][150/175]	Step 12998	lr 0.02413	Loss 0.5832 (0.5655)	Prec@(1,5) (81.9%, 97.9%)	
11/23 03:02:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [73][175/175]	Step 13023	lr 0.02413	Loss 0.6482 (0.5753)	Prec@(1,5) (81.6%, 97.8%)	
11/23 03:02:47AM evaluateStage_trainer.py:145 [INFO] Train: [ 73/599] Final Prec@1 81.5889%
11/23 03:02:50AM evaluateStage_trainer.py:180 [INFO] Valid: [ 73/599] Final Prec@1 54.4400%
11/23 03:02:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:03:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [74][50/175]	Step 13074	lr 0.02411	Loss 0.5056 (0.5284)	Prec@(1,5) (83.3%, 98.3%)	
11/23 03:03:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [74][100/175]	Step 13124	lr 0.02411	Loss 0.5187 (0.5346)	Prec@(1,5) (83.0%, 98.2%)	
11/23 03:03:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [74][150/175]	Step 13174	lr 0.02411	Loss 0.6372 (0.5536)	Prec@(1,5) (82.4%, 98.0%)	
11/23 03:03:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [74][175/175]	Step 13199	lr 0.02411	Loss 0.5514 (0.5641)	Prec@(1,5) (82.1%, 98.0%)	
11/23 03:03:32AM evaluateStage_trainer.py:145 [INFO] Train: [ 74/599] Final Prec@1 82.1000%
11/23 03:03:35AM evaluateStage_trainer.py:180 [INFO] Valid: [ 74/599] Final Prec@1 55.1200%
11/23 03:03:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:03:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [75][50/175]	Step 13250	lr 0.02409	Loss 0.5499 (0.5202)	Prec@(1,5) (83.6%, 98.1%)	
11/23 03:04:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [75][100/175]	Step 13300	lr 0.02409	Loss 0.6136 (0.5325)	Prec@(1,5) (83.1%, 98.1%)	
11/23 03:04:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [75][150/175]	Step 13350	lr 0.02409	Loss 0.5086 (0.5567)	Prec@(1,5) (82.3%, 98.0%)	
11/23 03:04:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [75][175/175]	Step 13375	lr 0.02409	Loss 0.7056 (0.5707)	Prec@(1,5) (81.9%, 97.9%)	
11/23 03:04:18AM evaluateStage_trainer.py:145 [INFO] Train: [ 75/599] Final Prec@1 81.8622%
11/23 03:04:21AM evaluateStage_trainer.py:180 [INFO] Valid: [ 75/599] Final Prec@1 54.4200%
11/23 03:04:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:04:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [76][50/175]	Step 13426	lr 0.02406	Loss 0.5852 (0.5128)	Prec@(1,5) (83.8%, 98.3%)	
11/23 03:04:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [76][100/175]	Step 13476	lr 0.02406	Loss 0.5919 (0.5283)	Prec@(1,5) (83.1%, 98.2%)	
11/23 03:04:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [76][150/175]	Step 13526	lr 0.02406	Loss 0.5716 (0.5514)	Prec@(1,5) (82.4%, 98.1%)	
11/23 03:05:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [76][175/175]	Step 13551	lr 0.02406	Loss 0.6364 (0.5604)	Prec@(1,5) (82.1%, 98.0%)	
11/23 03:05:03AM evaluateStage_trainer.py:145 [INFO] Train: [ 76/599] Final Prec@1 82.0911%
11/23 03:05:06AM evaluateStage_trainer.py:180 [INFO] Valid: [ 76/599] Final Prec@1 53.9400%
11/23 03:05:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:05:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [77][50/175]	Step 13602	lr 0.02404	Loss 0.6160 (0.5215)	Prec@(1,5) (83.5%, 98.3%)	
11/23 03:05:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [77][100/175]	Step 13652	lr 0.02404	Loss 0.5383 (0.5339)	Prec@(1,5) (82.9%, 98.2%)	
11/23 03:05:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [77][150/175]	Step 13702	lr 0.02404	Loss 0.5793 (0.5544)	Prec@(1,5) (82.3%, 98.1%)	
11/23 03:05:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [77][175/175]	Step 13727	lr 0.02404	Loss 0.5891 (0.5625)	Prec@(1,5) (82.0%, 98.0%)	
11/23 03:05:49AM evaluateStage_trainer.py:145 [INFO] Train: [ 77/599] Final Prec@1 81.9800%
11/23 03:05:51AM evaluateStage_trainer.py:180 [INFO] Valid: [ 77/599] Final Prec@1 55.3400%
11/23 03:05:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:06:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [78][50/175]	Step 13778	lr 0.02401	Loss 0.5528 (0.5000)	Prec@(1,5) (84.1%, 98.3%)	
11/23 03:06:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [78][100/175]	Step 13828	lr 0.02401	Loss 0.4968 (0.5197)	Prec@(1,5) (83.6%, 98.2%)	
11/23 03:06:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [78][150/175]	Step 13878	lr 0.02401	Loss 0.5602 (0.5354)	Prec@(1,5) (83.0%, 98.1%)	
11/23 03:06:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [78][175/175]	Step 13903	lr 0.02401	Loss 0.5998 (0.5444)	Prec@(1,5) (82.6%, 98.1%)	
11/23 03:06:34AM evaluateStage_trainer.py:145 [INFO] Train: [ 78/599] Final Prec@1 82.5822%
11/23 03:06:37AM evaluateStage_trainer.py:180 [INFO] Valid: [ 78/599] Final Prec@1 53.5800%
11/23 03:06:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:06:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [79][50/175]	Step 13954	lr 0.02399	Loss 0.5405 (0.4941)	Prec@(1,5) (84.7%, 98.6%)	
11/23 03:07:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [79][100/175]	Step 14004	lr 0.02399	Loss 0.4778 (0.5045)	Prec@(1,5) (84.1%, 98.5%)	
11/23 03:07:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [79][150/175]	Step 14054	lr 0.02399	Loss 0.5122 (0.5231)	Prec@(1,5) (83.4%, 98.4%)	
11/23 03:07:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [79][175/175]	Step 14079	lr 0.02399	Loss 0.5781 (0.5361)	Prec@(1,5) (82.9%, 98.2%)	
11/23 03:07:19AM evaluateStage_trainer.py:145 [INFO] Train: [ 79/599] Final Prec@1 82.9089%
11/23 03:07:22AM evaluateStage_trainer.py:180 [INFO] Valid: [ 79/599] Final Prec@1 54.8800%
11/23 03:07:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:07:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [80][50/175]	Step 14130	lr 0.02396	Loss 0.4636 (0.4763)	Prec@(1,5) (84.9%, 98.7%)	
11/23 03:07:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [80][100/175]	Step 14180	lr 0.02396	Loss 0.5300 (0.4939)	Prec@(1,5) (84.3%, 98.5%)	
11/23 03:07:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [80][150/175]	Step 14230	lr 0.02396	Loss 0.5898 (0.5164)	Prec@(1,5) (83.4%, 98.4%)	
11/23 03:08:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [80][175/175]	Step 14255	lr 0.02396	Loss 0.5364 (0.5271)	Prec@(1,5) (83.1%, 98.3%)	
11/23 03:08:04AM evaluateStage_trainer.py:145 [INFO] Train: [ 80/599] Final Prec@1 83.1089%
11/23 03:08:07AM evaluateStage_trainer.py:180 [INFO] Valid: [ 80/599] Final Prec@1 54.6200%
11/23 03:08:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:08:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [81][50/175]	Step 14306	lr 0.02394	Loss 0.3905 (0.4694)	Prec@(1,5) (85.2%, 98.6%)	
11/23 03:08:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [81][100/175]	Step 14356	lr 0.02394	Loss 0.5306 (0.4865)	Prec@(1,5) (84.7%, 98.6%)	
11/23 03:08:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [81][150/175]	Step 14406	lr 0.02394	Loss 0.6057 (0.5113)	Prec@(1,5) (83.9%, 98.4%)	
11/23 03:08:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [81][175/175]	Step 14431	lr 0.02394	Loss 0.5498 (0.5195)	Prec@(1,5) (83.6%, 98.3%)	
11/23 03:08:50AM evaluateStage_trainer.py:145 [INFO] Train: [ 81/599] Final Prec@1 83.6200%
11/23 03:08:53AM evaluateStage_trainer.py:180 [INFO] Valid: [ 81/599] Final Prec@1 53.3200%
11/23 03:08:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:09:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [82][50/175]	Step 14482	lr 0.02391	Loss 0.4955 (0.4781)	Prec@(1,5) (84.4%, 98.7%)	
11/23 03:09:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [82][100/175]	Step 14532	lr 0.02391	Loss 0.3935 (0.4918)	Prec@(1,5) (84.1%, 98.6%)	
11/23 03:09:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [82][150/175]	Step 14582	lr 0.02391	Loss 0.4705 (0.5086)	Prec@(1,5) (83.7%, 98.4%)	
11/23 03:09:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [82][175/175]	Step 14607	lr 0.02391	Loss 0.6071 (0.5183)	Prec@(1,5) (83.3%, 98.4%)	
11/23 03:09:35AM evaluateStage_trainer.py:145 [INFO] Train: [ 82/599] Final Prec@1 83.3311%
11/23 03:09:38AM evaluateStage_trainer.py:180 [INFO] Valid: [ 82/599] Final Prec@1 55.1400%
11/23 03:09:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:09:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [83][50/175]	Step 14658	lr 0.02388	Loss 0.5743 (0.4547)	Prec@(1,5) (85.6%, 98.7%)	
11/23 03:10:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [83][100/175]	Step 14708	lr 0.02388	Loss 0.4854 (0.4642)	Prec@(1,5) (85.1%, 98.7%)	
11/23 03:10:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [83][150/175]	Step 14758	lr 0.02388	Loss 0.6155 (0.4862)	Prec@(1,5) (84.3%, 98.6%)	
11/23 03:10:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [83][175/175]	Step 14783	lr 0.02388	Loss 0.6414 (0.4975)	Prec@(1,5) (84.0%, 98.5%)	
11/23 03:10:21AM evaluateStage_trainer.py:145 [INFO] Train: [ 83/599] Final Prec@1 83.9600%
11/23 03:10:23AM evaluateStage_trainer.py:180 [INFO] Valid: [ 83/599] Final Prec@1 53.7400%
11/23 03:10:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 55.5400%
11/23 03:10:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [84][50/175]	Step 14834	lr 0.02386	Loss 0.4807 (0.4778)	Prec@(1,5) (84.9%, 98.5%)	
11/23 03:10:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [84][100/175]	Step 14884	lr 0.02386	Loss 0.5013 (0.4965)	Prec@(1,5) (84.2%, 98.5%)	
11/23 03:11:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [84][150/175]	Step 14934	lr 0.02386	Loss 0.6539 (0.5144)	Prec@(1,5) (83.4%, 98.5%)	
11/23 03:11:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [84][175/175]	Step 14959	lr 0.02386	Loss 0.6326 (0.5215)	Prec@(1,5) (83.2%, 98.4%)	
11/23 03:11:06AM evaluateStage_trainer.py:145 [INFO] Train: [ 84/599] Final Prec@1 83.1756%
11/23 03:11:09AM evaluateStage_trainer.py:180 [INFO] Valid: [ 84/599] Final Prec@1 56.3400%
11/23 03:11:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:11:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [85][50/175]	Step 15010	lr 0.02383	Loss 0.5186 (0.4627)	Prec@(1,5) (85.3%, 98.7%)	
11/23 03:11:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [85][100/175]	Step 15060	lr 0.02383	Loss 0.5691 (0.4682)	Prec@(1,5) (84.9%, 98.6%)	
11/23 03:11:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [85][150/175]	Step 15110	lr 0.02383	Loss 0.5691 (0.4857)	Prec@(1,5) (84.3%, 98.6%)	
11/23 03:11:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [85][175/175]	Step 15135	lr 0.02383	Loss 0.5833 (0.4942)	Prec@(1,5) (84.1%, 98.5%)	
11/23 03:11:52AM evaluateStage_trainer.py:145 [INFO] Train: [ 85/599] Final Prec@1 84.0244%
11/23 03:11:55AM evaluateStage_trainer.py:180 [INFO] Valid: [ 85/599] Final Prec@1 54.8200%
11/23 03:11:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:12:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [86][50/175]	Step 15186	lr 0.0238	Loss 0.3991 (0.4442)	Prec@(1,5) (86.0%, 98.7%)	
11/23 03:12:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [86][100/175]	Step 15236	lr 0.0238	Loss 0.5919 (0.4645)	Prec@(1,5) (85.4%, 98.7%)	
11/23 03:12:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [86][150/175]	Step 15286	lr 0.0238	Loss 0.5251 (0.4853)	Prec@(1,5) (84.6%, 98.6%)	
11/23 03:12:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [86][175/175]	Step 15311	lr 0.0238	Loss 0.5166 (0.4969)	Prec@(1,5) (84.2%, 98.5%)	
11/23 03:12:38AM evaluateStage_trainer.py:145 [INFO] Train: [ 86/599] Final Prec@1 84.2400%
11/23 03:12:41AM evaluateStage_trainer.py:180 [INFO] Valid: [ 86/599] Final Prec@1 53.0400%
11/23 03:12:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:12:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [87][50/175]	Step 15362	lr 0.02378	Loss 0.4476 (0.4446)	Prec@(1,5) (85.9%, 98.9%)	
11/23 03:13:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [87][100/175]	Step 15412	lr 0.02378	Loss 0.5167 (0.4546)	Prec@(1,5) (85.4%, 98.9%)	
11/23 03:13:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [87][150/175]	Step 15462	lr 0.02378	Loss 0.4858 (0.4732)	Prec@(1,5) (84.9%, 98.7%)	
11/23 03:13:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [87][175/175]	Step 15487	lr 0.02378	Loss 0.4313 (0.4827)	Prec@(1,5) (84.5%, 98.6%)	
11/23 03:13:23AM evaluateStage_trainer.py:145 [INFO] Train: [ 87/599] Final Prec@1 84.5444%
11/23 03:13:26AM evaluateStage_trainer.py:180 [INFO] Valid: [ 87/599] Final Prec@1 55.9400%
11/23 03:13:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:13:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [88][50/175]	Step 15538	lr 0.02375	Loss 0.4486 (0.4404)	Prec@(1,5) (86.1%, 98.8%)	
11/23 03:13:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [88][100/175]	Step 15588	lr 0.02375	Loss 0.4834 (0.4579)	Prec@(1,5) (85.3%, 98.7%)	
11/23 03:14:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [88][150/175]	Step 15638	lr 0.02375	Loss 0.6567 (0.4811)	Prec@(1,5) (84.5%, 98.6%)	
11/23 03:14:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [88][175/175]	Step 15663	lr 0.02375	Loss 0.5970 (0.4973)	Prec@(1,5) (83.9%, 98.5%)	
11/23 03:14:08AM evaluateStage_trainer.py:145 [INFO] Train: [ 88/599] Final Prec@1 83.9267%
11/23 03:14:11AM evaluateStage_trainer.py:180 [INFO] Valid: [ 88/599] Final Prec@1 55.3000%
11/23 03:14:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:14:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [89][50/175]	Step 15714	lr 0.02372	Loss 0.3226 (0.4418)	Prec@(1,5) (85.9%, 98.9%)	
11/23 03:14:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [89][100/175]	Step 15764	lr 0.02372	Loss 0.5440 (0.4459)	Prec@(1,5) (85.8%, 98.9%)	
11/23 03:14:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [89][150/175]	Step 15814	lr 0.02372	Loss 0.6150 (0.4649)	Prec@(1,5) (85.1%, 98.7%)	
11/23 03:14:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [89][175/175]	Step 15839	lr 0.02372	Loss 0.5163 (0.4737)	Prec@(1,5) (84.8%, 98.7%)	
11/23 03:14:54AM evaluateStage_trainer.py:145 [INFO] Train: [ 89/599] Final Prec@1 84.7844%
11/23 03:14:56AM evaluateStage_trainer.py:180 [INFO] Valid: [ 89/599] Final Prec@1 54.0000%
11/23 03:14:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:15:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [90][50/175]	Step 15890	lr 0.02369	Loss 0.5552 (0.4351)	Prec@(1,5) (86.0%, 98.8%)	
11/23 03:15:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [90][100/175]	Step 15940	lr 0.02369	Loss 0.4599 (0.4490)	Prec@(1,5) (85.7%, 98.7%)	
11/23 03:15:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [90][150/175]	Step 15990	lr 0.02369	Loss 0.5769 (0.4691)	Prec@(1,5) (85.1%, 98.6%)	
11/23 03:15:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [90][175/175]	Step 16015	lr 0.02369	Loss 0.5798 (0.4786)	Prec@(1,5) (84.7%, 98.5%)	
11/23 03:15:39AM evaluateStage_trainer.py:145 [INFO] Train: [ 90/599] Final Prec@1 84.6933%
11/23 03:15:42AM evaluateStage_trainer.py:180 [INFO] Valid: [ 90/599] Final Prec@1 55.2200%
11/23 03:15:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:15:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [91][50/175]	Step 16066	lr 0.02366	Loss 0.4505 (0.4437)	Prec@(1,5) (85.7%, 98.8%)	
11/23 03:16:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [91][100/175]	Step 16116	lr 0.02366	Loss 0.4205 (0.4585)	Prec@(1,5) (85.3%, 98.8%)	
11/23 03:16:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [91][150/175]	Step 16166	lr 0.02366	Loss 0.4995 (0.4751)	Prec@(1,5) (84.8%, 98.7%)	
11/23 03:16:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [91][175/175]	Step 16191	lr 0.02366	Loss 0.5180 (0.4839)	Prec@(1,5) (84.5%, 98.6%)	
11/23 03:16:25AM evaluateStage_trainer.py:145 [INFO] Train: [ 91/599] Final Prec@1 84.4911%
11/23 03:16:27AM evaluateStage_trainer.py:180 [INFO] Valid: [ 91/599] Final Prec@1 54.8800%
11/23 03:16:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:16:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [92][50/175]	Step 16242	lr 0.02363	Loss 0.5162 (0.4272)	Prec@(1,5) (86.2%, 99.0%)	
11/23 03:16:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [92][100/175]	Step 16292	lr 0.02363	Loss 0.4199 (0.4345)	Prec@(1,5) (85.9%, 98.9%)	
11/23 03:17:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [92][150/175]	Step 16342	lr 0.02363	Loss 0.4520 (0.4509)	Prec@(1,5) (85.4%, 98.9%)	
11/23 03:17:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [92][175/175]	Step 16367	lr 0.02363	Loss 0.5926 (0.4605)	Prec@(1,5) (85.1%, 98.8%)	
11/23 03:17:10AM evaluateStage_trainer.py:145 [INFO] Train: [ 92/599] Final Prec@1 85.1200%
11/23 03:17:13AM evaluateStage_trainer.py:180 [INFO] Valid: [ 92/599] Final Prec@1 55.0400%
11/23 03:17:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:17:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [93][50/175]	Step 16418	lr 0.02361	Loss 0.5140 (0.4473)	Prec@(1,5) (85.6%, 98.7%)	
11/23 03:17:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [93][100/175]	Step 16468	lr 0.02361	Loss 0.4824 (0.4562)	Prec@(1,5) (85.3%, 98.7%)	
11/23 03:17:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [93][150/175]	Step 16518	lr 0.02361	Loss 0.4702 (0.4707)	Prec@(1,5) (84.8%, 98.6%)	
11/23 03:17:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [93][175/175]	Step 16543	lr 0.02361	Loss 0.6127 (0.4774)	Prec@(1,5) (84.6%, 98.6%)	
11/23 03:17:55AM evaluateStage_trainer.py:145 [INFO] Train: [ 93/599] Final Prec@1 84.5867%
11/23 03:17:58AM evaluateStage_trainer.py:180 [INFO] Valid: [ 93/599] Final Prec@1 55.0000%
11/23 03:17:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:18:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [94][50/175]	Step 16594	lr 0.02358	Loss 0.3519 (0.4159)	Prec@(1,5) (86.4%, 98.9%)	
11/23 03:18:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [94][100/175]	Step 16644	lr 0.02358	Loss 0.4890 (0.4267)	Prec@(1,5) (86.1%, 98.9%)	
11/23 03:18:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [94][150/175]	Step 16694	lr 0.02358	Loss 0.5886 (0.4430)	Prec@(1,5) (85.5%, 98.8%)	
11/23 03:18:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [94][175/175]	Step 16719	lr 0.02358	Loss 0.4452 (0.4542)	Prec@(1,5) (85.1%, 98.8%)	
11/23 03:18:41AM evaluateStage_trainer.py:145 [INFO] Train: [ 94/599] Final Prec@1 85.1044%
11/23 03:18:43AM evaluateStage_trainer.py:180 [INFO] Valid: [ 94/599] Final Prec@1 54.7000%
11/23 03:18:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.3400%
11/23 03:18:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [95][50/175]	Step 16770	lr 0.02355	Loss 0.4673 (0.4253)	Prec@(1,5) (85.9%, 99.0%)	
11/23 03:19:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [95][100/175]	Step 16820	lr 0.02355	Loss 0.4288 (0.4271)	Prec@(1,5) (86.2%, 99.0%)	
11/23 03:19:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [95][150/175]	Step 16870	lr 0.02355	Loss 0.4278 (0.4390)	Prec@(1,5) (85.8%, 98.9%)	
11/23 03:19:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [95][175/175]	Step 16895	lr 0.02355	Loss 0.4651 (0.4497)	Prec@(1,5) (85.4%, 98.8%)	
11/23 03:19:26AM evaluateStage_trainer.py:145 [INFO] Train: [ 95/599] Final Prec@1 85.4333%
11/23 03:19:29AM evaluateStage_trainer.py:180 [INFO] Valid: [ 95/599] Final Prec@1 56.4400%
11/23 03:19:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:19:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [96][50/175]	Step 16946	lr 0.02352	Loss 0.5117 (0.4120)	Prec@(1,5) (86.8%, 99.1%)	
11/23 03:19:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [96][100/175]	Step 16996	lr 0.02352	Loss 0.5048 (0.4239)	Prec@(1,5) (86.4%, 99.0%)	
11/23 03:20:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [96][150/175]	Step 17046	lr 0.02352	Loss 0.4769 (0.4326)	Prec@(1,5) (85.9%, 98.9%)	
11/23 03:20:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [96][175/175]	Step 17071	lr 0.02352	Loss 0.5390 (0.4422)	Prec@(1,5) (85.7%, 98.9%)	
11/23 03:20:12AM evaluateStage_trainer.py:145 [INFO] Train: [ 96/599] Final Prec@1 85.6556%
11/23 03:20:15AM evaluateStage_trainer.py:180 [INFO] Valid: [ 96/599] Final Prec@1 54.4600%
11/23 03:20:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:20:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [97][50/175]	Step 17122	lr 0.02349	Loss 0.3052 (0.4087)	Prec@(1,5) (86.8%, 99.1%)	
11/23 03:20:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [97][100/175]	Step 17172	lr 0.02349	Loss 0.5261 (0.4159)	Prec@(1,5) (86.6%, 99.1%)	
11/23 03:20:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [97][150/175]	Step 17222	lr 0.02349	Loss 0.5327 (0.4337)	Prec@(1,5) (86.0%, 99.0%)	
11/23 03:20:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [97][175/175]	Step 17247	lr 0.02349	Loss 0.5455 (0.4435)	Prec@(1,5) (85.7%, 98.9%)	
11/23 03:20:57AM evaluateStage_trainer.py:145 [INFO] Train: [ 97/599] Final Prec@1 85.6978%
11/23 03:21:00AM evaluateStage_trainer.py:180 [INFO] Valid: [ 97/599] Final Prec@1 53.8200%
11/23 03:21:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:21:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [98][50/175]	Step 17298	lr 0.02345	Loss 0.5267 (0.4301)	Prec@(1,5) (86.1%, 98.9%)	
11/23 03:21:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [98][100/175]	Step 17348	lr 0.02345	Loss 0.4015 (0.4405)	Prec@(1,5) (85.8%, 98.9%)	
11/23 03:21:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [98][150/175]	Step 17398	lr 0.02345	Loss 0.5670 (0.4457)	Prec@(1,5) (85.7%, 98.9%)	
11/23 03:21:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [98][175/175]	Step 17423	lr 0.02345	Loss 0.5651 (0.4536)	Prec@(1,5) (85.5%, 98.8%)	
11/23 03:21:43AM evaluateStage_trainer.py:145 [INFO] Train: [ 98/599] Final Prec@1 85.4444%
11/23 03:21:45AM evaluateStage_trainer.py:180 [INFO] Valid: [ 98/599] Final Prec@1 54.8400%
11/23 03:21:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:21:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [99][50/175]	Step 17474	lr 0.02342	Loss 0.4034 (0.3881)	Prec@(1,5) (87.4%, 99.1%)	
11/23 03:22:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [99][100/175]	Step 17524	lr 0.02342	Loss 0.4641 (0.4055)	Prec@(1,5) (87.0%, 99.0%)	
11/23 03:22:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [99][150/175]	Step 17574	lr 0.02342	Loss 0.4469 (0.4200)	Prec@(1,5) (86.5%, 99.0%)	
11/23 03:22:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [99][175/175]	Step 17599	lr 0.02342	Loss 0.3882 (0.4293)	Prec@(1,5) (86.2%, 98.9%)	
11/23 03:22:28AM evaluateStage_trainer.py:145 [INFO] Train: [ 99/599] Final Prec@1 86.1889%
11/23 03:22:31AM evaluateStage_trainer.py:180 [INFO] Valid: [ 99/599] Final Prec@1 54.2200%
11/23 03:22:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:22:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [100][50/175]	Step 17650	lr 0.02339	Loss 0.3810 (0.3896)	Prec@(1,5) (87.6%, 99.1%)	
11/23 03:22:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [100][100/175]	Step 17700	lr 0.02339	Loss 0.4272 (0.3956)	Prec@(1,5) (87.4%, 99.1%)	
11/23 03:23:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [100][150/175]	Step 17750	lr 0.02339	Loss 0.5817 (0.4213)	Prec@(1,5) (86.5%, 98.9%)	
11/23 03:23:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [100][175/175]	Step 17775	lr 0.02339	Loss 0.4454 (0.4336)	Prec@(1,5) (86.1%, 98.9%)	
11/23 03:23:13AM evaluateStage_trainer.py:145 [INFO] Train: [100/599] Final Prec@1 86.1044%
11/23 03:23:16AM evaluateStage_trainer.py:180 [INFO] Valid: [100/599] Final Prec@1 54.1600%
11/23 03:23:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:23:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [101][50/175]	Step 17826	lr 0.02336	Loss 0.4382 (0.3954)	Prec@(1,5) (87.5%, 99.1%)	
11/23 03:23:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [101][100/175]	Step 17876	lr 0.02336	Loss 0.3263 (0.4026)	Prec@(1,5) (87.2%, 99.1%)	
11/23 03:23:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [101][150/175]	Step 17926	lr 0.02336	Loss 0.4196 (0.4158)	Prec@(1,5) (86.6%, 99.0%)	
11/23 03:23:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [101][175/175]	Step 17951	lr 0.02336	Loss 0.4480 (0.4215)	Prec@(1,5) (86.4%, 98.9%)	
11/23 03:23:59AM evaluateStage_trainer.py:145 [INFO] Train: [101/599] Final Prec@1 86.4178%
11/23 03:24:02AM evaluateStage_trainer.py:180 [INFO] Valid: [101/599] Final Prec@1 55.2200%
11/23 03:24:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:24:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [102][50/175]	Step 18002	lr 0.02333	Loss 0.3654 (0.3678)	Prec@(1,5) (87.8%, 99.2%)	
11/23 03:24:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [102][100/175]	Step 18052	lr 0.02333	Loss 0.3869 (0.3822)	Prec@(1,5) (87.6%, 99.1%)	
11/23 03:24:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [102][150/175]	Step 18102	lr 0.02333	Loss 0.5481 (0.4007)	Prec@(1,5) (86.9%, 99.0%)	
11/23 03:24:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [102][175/175]	Step 18127	lr 0.02333	Loss 0.4049 (0.4100)	Prec@(1,5) (86.6%, 99.0%)	
11/23 03:24:44AM evaluateStage_trainer.py:145 [INFO] Train: [102/599] Final Prec@1 86.6178%
11/23 03:24:47AM evaluateStage_trainer.py:180 [INFO] Valid: [102/599] Final Prec@1 54.0600%
11/23 03:24:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:25:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [103][50/175]	Step 18178	lr 0.0233	Loss 0.2950 (0.3831)	Prec@(1,5) (87.6%, 99.1%)	
11/23 03:25:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [103][100/175]	Step 18228	lr 0.0233	Loss 0.4557 (0.3946)	Prec@(1,5) (87.2%, 99.1%)	
11/23 03:25:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [103][150/175]	Step 18278	lr 0.0233	Loss 0.5026 (0.4097)	Prec@(1,5) (86.6%, 99.0%)	
11/23 03:25:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [103][175/175]	Step 18303	lr 0.0233	Loss 0.5307 (0.4193)	Prec@(1,5) (86.2%, 99.0%)	
11/23 03:25:30AM evaluateStage_trainer.py:145 [INFO] Train: [103/599] Final Prec@1 86.1644%
11/23 03:25:32AM evaluateStage_trainer.py:180 [INFO] Valid: [103/599] Final Prec@1 54.1200%
11/23 03:25:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:25:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [104][50/175]	Step 18354	lr 0.02326	Loss 0.5160 (0.3924)	Prec@(1,5) (87.4%, 99.1%)	
11/23 03:25:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [104][100/175]	Step 18404	lr 0.02326	Loss 0.4578 (0.4018)	Prec@(1,5) (87.1%, 99.0%)	
11/23 03:26:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [104][150/175]	Step 18454	lr 0.02326	Loss 0.4925 (0.4210)	Prec@(1,5) (86.4%, 98.9%)	
11/23 03:26:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [104][175/175]	Step 18479	lr 0.02326	Loss 0.4521 (0.4271)	Prec@(1,5) (86.2%, 98.9%)	
11/23 03:26:15AM evaluateStage_trainer.py:145 [INFO] Train: [104/599] Final Prec@1 86.2244%
11/23 03:26:18AM evaluateStage_trainer.py:180 [INFO] Valid: [104/599] Final Prec@1 53.1600%
11/23 03:26:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:26:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [105][50/175]	Step 18530	lr 0.02323	Loss 0.4016 (0.3827)	Prec@(1,5) (87.5%, 99.3%)	
11/23 03:26:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [105][100/175]	Step 18580	lr 0.02323	Loss 0.4820 (0.3957)	Prec@(1,5) (87.1%, 99.1%)	
11/23 03:26:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [105][150/175]	Step 18630	lr 0.02323	Loss 0.5169 (0.4077)	Prec@(1,5) (86.7%, 99.0%)	
11/23 03:27:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [105][175/175]	Step 18655	lr 0.02323	Loss 0.5631 (0.4160)	Prec@(1,5) (86.5%, 99.0%)	
11/23 03:27:00AM evaluateStage_trainer.py:145 [INFO] Train: [105/599] Final Prec@1 86.4622%
11/23 03:27:03AM evaluateStage_trainer.py:180 [INFO] Valid: [105/599] Final Prec@1 55.2200%
11/23 03:27:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:27:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [106][50/175]	Step 18706	lr 0.0232	Loss 0.3530 (0.3639)	Prec@(1,5) (88.5%, 99.2%)	
11/23 03:27:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [106][100/175]	Step 18756	lr 0.0232	Loss 0.4170 (0.3809)	Prec@(1,5) (87.9%, 99.2%)	
11/23 03:27:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [106][150/175]	Step 18806	lr 0.0232	Loss 0.4130 (0.3960)	Prec@(1,5) (87.3%, 99.1%)	
11/23 03:27:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [106][175/175]	Step 18831	lr 0.0232	Loss 0.5218 (0.4105)	Prec@(1,5) (86.8%, 99.1%)	
11/23 03:27:46AM evaluateStage_trainer.py:145 [INFO] Train: [106/599] Final Prec@1 86.7911%
11/23 03:27:48AM evaluateStage_trainer.py:180 [INFO] Valid: [106/599] Final Prec@1 54.6600%
11/23 03:27:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:28:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [107][50/175]	Step 18882	lr 0.02317	Loss 0.3673 (0.3712)	Prec@(1,5) (88.0%, 99.2%)	
11/23 03:28:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [107][100/175]	Step 18932	lr 0.02317	Loss 0.3490 (0.3856)	Prec@(1,5) (87.6%, 99.2%)	
11/23 03:28:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [107][150/175]	Step 18982	lr 0.02317	Loss 0.2634 (0.4082)	Prec@(1,5) (86.9%, 99.1%)	
11/23 03:28:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [107][175/175]	Step 19007	lr 0.02317	Loss 0.4786 (0.4161)	Prec@(1,5) (86.6%, 99.0%)	
11/23 03:28:31AM evaluateStage_trainer.py:145 [INFO] Train: [107/599] Final Prec@1 86.5733%
11/23 03:28:34AM evaluateStage_trainer.py:180 [INFO] Valid: [107/599] Final Prec@1 53.1200%
11/23 03:28:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:28:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [108][50/175]	Step 19058	lr 0.02313	Loss 0.4473 (0.3902)	Prec@(1,5) (87.7%, 99.2%)	
11/23 03:28:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [108][100/175]	Step 19108	lr 0.02313	Loss 0.3223 (0.3907)	Prec@(1,5) (87.4%, 99.1%)	
11/23 03:29:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [108][150/175]	Step 19158	lr 0.02313	Loss 0.4528 (0.4066)	Prec@(1,5) (86.9%, 99.1%)	
11/23 03:29:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [108][175/175]	Step 19183	lr 0.02313	Loss 0.4878 (0.4120)	Prec@(1,5) (86.7%, 99.0%)	
11/23 03:29:17AM evaluateStage_trainer.py:145 [INFO] Train: [108/599] Final Prec@1 86.7133%
11/23 03:29:19AM evaluateStage_trainer.py:180 [INFO] Valid: [108/599] Final Prec@1 56.0600%
11/23 03:29:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:29:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [109][50/175]	Step 19234	lr 0.0231	Loss 0.3930 (0.3530)	Prec@(1,5) (88.6%, 99.3%)	
11/23 03:29:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [109][100/175]	Step 19284	lr 0.0231	Loss 0.4064 (0.3662)	Prec@(1,5) (88.1%, 99.3%)	
11/23 03:29:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [109][150/175]	Step 19334	lr 0.0231	Loss 0.3808 (0.3864)	Prec@(1,5) (87.4%, 99.2%)	
11/23 03:30:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [109][175/175]	Step 19359	lr 0.0231	Loss 0.4235 (0.3939)	Prec@(1,5) (87.2%, 99.1%)	
11/23 03:30:02AM evaluateStage_trainer.py:145 [INFO] Train: [109/599] Final Prec@1 87.1733%
11/23 03:30:05AM evaluateStage_trainer.py:180 [INFO] Valid: [109/599] Final Prec@1 55.2800%
11/23 03:30:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:30:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [110][50/175]	Step 19410	lr 0.02306	Loss 0.4266 (0.3801)	Prec@(1,5) (87.9%, 99.1%)	
11/23 03:30:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [110][100/175]	Step 19460	lr 0.02306	Loss 0.3862 (0.3860)	Prec@(1,5) (87.6%, 99.1%)	
11/23 03:30:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [110][150/175]	Step 19510	lr 0.02306	Loss 0.3802 (0.3957)	Prec@(1,5) (87.3%, 99.0%)	
11/23 03:30:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [110][175/175]	Step 19535	lr 0.02306	Loss 0.5435 (0.4056)	Prec@(1,5) (87.0%, 99.0%)	
11/23 03:30:48AM evaluateStage_trainer.py:145 [INFO] Train: [110/599] Final Prec@1 86.9556%
11/23 03:30:50AM evaluateStage_trainer.py:180 [INFO] Valid: [110/599] Final Prec@1 54.4200%
11/23 03:30:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:31:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [111][50/175]	Step 19586	lr 0.02303	Loss 0.3723 (0.3648)	Prec@(1,5) (88.5%, 99.2%)	
11/23 03:31:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [111][100/175]	Step 19636	lr 0.02303	Loss 0.3599 (0.3676)	Prec@(1,5) (88.3%, 99.2%)	
11/23 03:31:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [111][150/175]	Step 19686	lr 0.02303	Loss 0.4823 (0.3804)	Prec@(1,5) (87.8%, 99.1%)	
11/23 03:31:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [111][175/175]	Step 19711	lr 0.02303	Loss 0.4623 (0.3890)	Prec@(1,5) (87.5%, 99.1%)	
11/23 03:31:33AM evaluateStage_trainer.py:145 [INFO] Train: [111/599] Final Prec@1 87.5133%
11/23 03:31:36AM evaluateStage_trainer.py:180 [INFO] Valid: [111/599] Final Prec@1 54.9000%
11/23 03:31:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:31:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [112][50/175]	Step 19762	lr 0.023	Loss 0.4230 (0.3488)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:32:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [112][100/175]	Step 19812	lr 0.023	Loss 0.3910 (0.3587)	Prec@(1,5) (88.4%, 99.3%)	
11/23 03:32:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [112][150/175]	Step 19862	lr 0.023	Loss 0.3861 (0.3806)	Prec@(1,5) (87.5%, 99.2%)	
11/23 03:32:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [112][175/175]	Step 19887	lr 0.023	Loss 0.5014 (0.3886)	Prec@(1,5) (87.3%, 99.1%)	
11/23 03:32:19AM evaluateStage_trainer.py:145 [INFO] Train: [112/599] Final Prec@1 87.2667%
11/23 03:32:21AM evaluateStage_trainer.py:180 [INFO] Valid: [112/599] Final Prec@1 55.4400%
11/23 03:32:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:32:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [113][50/175]	Step 19938	lr 0.02296	Loss 0.2807 (0.3404)	Prec@(1,5) (89.0%, 99.4%)	
11/23 03:32:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [113][100/175]	Step 19988	lr 0.02296	Loss 0.4943 (0.3620)	Prec@(1,5) (88.3%, 99.3%)	
11/23 03:32:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [113][150/175]	Step 20038	lr 0.02296	Loss 0.5596 (0.3758)	Prec@(1,5) (87.7%, 99.2%)	
11/23 03:33:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [113][175/175]	Step 20063	lr 0.02296	Loss 0.6051 (0.3891)	Prec@(1,5) (87.3%, 99.1%)	
11/23 03:33:04AM evaluateStage_trainer.py:145 [INFO] Train: [113/599] Final Prec@1 87.3089%
11/23 03:33:07AM evaluateStage_trainer.py:180 [INFO] Valid: [113/599] Final Prec@1 53.7400%
11/23 03:33:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:33:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [114][50/175]	Step 20114	lr 0.02292	Loss 0.3514 (0.3740)	Prec@(1,5) (87.9%, 99.2%)	
11/23 03:33:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [114][100/175]	Step 20164	lr 0.02292	Loss 0.3387 (0.3779)	Prec@(1,5) (88.0%, 99.2%)	
11/23 03:33:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [114][150/175]	Step 20214	lr 0.02292	Loss 0.3579 (0.3814)	Prec@(1,5) (87.8%, 99.2%)	
11/23 03:33:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [114][175/175]	Step 20239	lr 0.02292	Loss 0.4481 (0.3873)	Prec@(1,5) (87.7%, 99.1%)	
11/23 03:33:49AM evaluateStage_trainer.py:145 [INFO] Train: [114/599] Final Prec@1 87.6600%
11/23 03:33:52AM evaluateStage_trainer.py:180 [INFO] Valid: [114/599] Final Prec@1 55.1400%
11/23 03:33:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:34:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [115][50/175]	Step 20290	lr 0.02289	Loss 0.2469 (0.3508)	Prec@(1,5) (88.8%, 99.4%)	
11/23 03:34:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [115][100/175]	Step 20340	lr 0.02289	Loss 0.3864 (0.3652)	Prec@(1,5) (88.2%, 99.3%)	
11/23 03:34:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [115][150/175]	Step 20390	lr 0.02289	Loss 0.4778 (0.3852)	Prec@(1,5) (87.5%, 99.2%)	
11/23 03:34:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [115][175/175]	Step 20415	lr 0.02289	Loss 0.4123 (0.3945)	Prec@(1,5) (87.2%, 99.1%)	
11/23 03:34:35AM evaluateStage_trainer.py:145 [INFO] Train: [115/599] Final Prec@1 87.1867%
11/23 03:34:38AM evaluateStage_trainer.py:180 [INFO] Valid: [115/599] Final Prec@1 54.0400%
11/23 03:34:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:34:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [116][50/175]	Step 20466	lr 0.02285	Loss 0.3676 (0.3492)	Prec@(1,5) (88.8%, 99.3%)	
11/23 03:35:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [116][100/175]	Step 20516	lr 0.02285	Loss 0.4080 (0.3586)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:35:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [116][150/175]	Step 20566	lr 0.02285	Loss 0.4351 (0.3663)	Prec@(1,5) (88.2%, 99.2%)	
11/23 03:35:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [116][175/175]	Step 20591	lr 0.02285	Loss 0.3618 (0.3760)	Prec@(1,5) (87.9%, 99.1%)	
11/23 03:35:20AM evaluateStage_trainer.py:145 [INFO] Train: [116/599] Final Prec@1 87.9489%
11/23 03:35:23AM evaluateStage_trainer.py:180 [INFO] Valid: [116/599] Final Prec@1 54.9400%
11/23 03:35:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:35:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [117][50/175]	Step 20642	lr 0.02282	Loss 0.4283 (0.3444)	Prec@(1,5) (89.0%, 99.2%)	
11/23 03:35:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [117][100/175]	Step 20692	lr 0.02282	Loss 0.3373 (0.3537)	Prec@(1,5) (88.6%, 99.3%)	
11/23 03:36:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [117][150/175]	Step 20742	lr 0.02282	Loss 0.4128 (0.3605)	Prec@(1,5) (88.4%, 99.2%)	
11/23 03:36:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [117][175/175]	Step 20767	lr 0.02282	Loss 0.4022 (0.3689)	Prec@(1,5) (88.0%, 99.2%)	
11/23 03:36:06AM evaluateStage_trainer.py:145 [INFO] Train: [117/599] Final Prec@1 88.0378%
11/23 03:36:09AM evaluateStage_trainer.py:180 [INFO] Valid: [117/599] Final Prec@1 54.8600%
11/23 03:36:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:36:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [118][50/175]	Step 20818	lr 0.02278	Loss 0.2564 (0.3425)	Prec@(1,5) (88.7%, 99.4%)	
11/23 03:36:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [118][100/175]	Step 20868	lr 0.02278	Loss 0.3708 (0.3521)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:36:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [118][150/175]	Step 20918	lr 0.02278	Loss 0.4334 (0.3643)	Prec@(1,5) (88.1%, 99.3%)	
11/23 03:36:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [118][175/175]	Step 20943	lr 0.02278	Loss 0.3553 (0.3719)	Prec@(1,5) (87.9%, 99.2%)	
11/23 03:36:52AM evaluateStage_trainer.py:145 [INFO] Train: [118/599] Final Prec@1 87.8511%
11/23 03:36:54AM evaluateStage_trainer.py:180 [INFO] Valid: [118/599] Final Prec@1 53.3600%
11/23 03:36:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:37:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [119][50/175]	Step 20994	lr 0.02275	Loss 0.3354 (0.3230)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:37:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [119][100/175]	Step 21044	lr 0.02275	Loss 0.3535 (0.3395)	Prec@(1,5) (89.0%, 99.4%)	
11/23 03:37:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [119][150/175]	Step 21094	lr 0.02275	Loss 0.4192 (0.3621)	Prec@(1,5) (88.3%, 99.3%)	
11/23 03:37:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [119][175/175]	Step 21119	lr 0.02275	Loss 0.3487 (0.3689)	Prec@(1,5) (88.0%, 99.3%)	
11/23 03:37:37AM evaluateStage_trainer.py:145 [INFO] Train: [119/599] Final Prec@1 87.9800%
11/23 03:37:40AM evaluateStage_trainer.py:180 [INFO] Valid: [119/599] Final Prec@1 53.3400%
11/23 03:37:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:37:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [120][50/175]	Step 21170	lr 0.02271	Loss 0.3476 (0.3250)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:38:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [120][100/175]	Step 21220	lr 0.02271	Loss 0.3652 (0.3377)	Prec@(1,5) (89.2%, 99.3%)	
11/23 03:38:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [120][150/175]	Step 21270	lr 0.02271	Loss 0.4164 (0.3535)	Prec@(1,5) (88.6%, 99.3%)	
11/23 03:38:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [120][175/175]	Step 21295	lr 0.02271	Loss 0.4240 (0.3613)	Prec@(1,5) (88.3%, 99.3%)	
11/23 03:38:23AM evaluateStage_trainer.py:145 [INFO] Train: [120/599] Final Prec@1 88.3111%
11/23 03:38:25AM evaluateStage_trainer.py:180 [INFO] Valid: [120/599] Final Prec@1 55.4800%
11/23 03:38:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:38:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [121][50/175]	Step 21346	lr 0.02267	Loss 0.3662 (0.3208)	Prec@(1,5) (89.5%, 99.5%)	
11/23 03:38:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [121][100/175]	Step 21396	lr 0.02267	Loss 0.3530 (0.3372)	Prec@(1,5) (88.9%, 99.4%)	
11/23 03:39:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [121][150/175]	Step 21446	lr 0.02267	Loss 0.4774 (0.3530)	Prec@(1,5) (88.5%, 99.4%)	
11/23 03:39:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [121][175/175]	Step 21471	lr 0.02267	Loss 0.3979 (0.3600)	Prec@(1,5) (88.2%, 99.3%)	
11/23 03:39:08AM evaluateStage_trainer.py:145 [INFO] Train: [121/599] Final Prec@1 88.2311%
11/23 03:39:11AM evaluateStage_trainer.py:180 [INFO] Valid: [121/599] Final Prec@1 54.5200%
11/23 03:39:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:39:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [122][50/175]	Step 21522	lr 0.02263	Loss 0.3323 (0.3145)	Prec@(1,5) (89.9%, 99.5%)	
11/23 03:39:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [122][100/175]	Step 21572	lr 0.02263	Loss 0.4309 (0.3231)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:39:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [122][150/175]	Step 21622	lr 0.02263	Loss 0.3252 (0.3420)	Prec@(1,5) (88.9%, 99.3%)	
11/23 03:39:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [122][175/175]	Step 21647	lr 0.02263	Loss 0.4461 (0.3498)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:39:53AM evaluateStage_trainer.py:145 [INFO] Train: [122/599] Final Prec@1 88.7333%
11/23 03:39:56AM evaluateStage_trainer.py:180 [INFO] Valid: [122/599] Final Prec@1 55.8000%
11/23 03:39:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.4400%
11/23 03:40:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [123][50/175]	Step 21698	lr 0.0226	Loss 0.3259 (0.3281)	Prec@(1,5) (89.3%, 99.4%)	
11/23 03:40:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [123][100/175]	Step 21748	lr 0.0226	Loss 0.3163 (0.3437)	Prec@(1,5) (88.8%, 99.4%)	
11/23 03:40:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [123][150/175]	Step 21798	lr 0.0226	Loss 0.2920 (0.3497)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:40:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [123][175/175]	Step 21823	lr 0.0226	Loss 0.3554 (0.3561)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:40:39AM evaluateStage_trainer.py:145 [INFO] Train: [123/599] Final Prec@1 88.4711%
11/23 03:40:41AM evaluateStage_trainer.py:180 [INFO] Valid: [123/599] Final Prec@1 56.5400%
11/23 03:40:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:40:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [124][50/175]	Step 21874	lr 0.02256	Loss 0.2909 (0.3243)	Prec@(1,5) (89.5%, 99.4%)	
11/23 03:41:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [124][100/175]	Step 21924	lr 0.02256	Loss 0.4608 (0.3385)	Prec@(1,5) (89.0%, 99.4%)	
11/23 03:41:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [124][150/175]	Step 21974	lr 0.02256	Loss 0.3116 (0.3453)	Prec@(1,5) (88.7%, 99.4%)	
11/23 03:41:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [124][175/175]	Step 21999	lr 0.02256	Loss 0.5033 (0.3552)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:41:25AM evaluateStage_trainer.py:145 [INFO] Train: [124/599] Final Prec@1 88.4667%
11/23 03:41:27AM evaluateStage_trainer.py:180 [INFO] Valid: [124/599] Final Prec@1 55.2000%
11/23 03:41:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:41:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [125][50/175]	Step 22050	lr 0.02252	Loss 0.3192 (0.3364)	Prec@(1,5) (89.1%, 99.4%)	
11/23 03:41:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [125][100/175]	Step 22100	lr 0.02252	Loss 0.4192 (0.3473)	Prec@(1,5) (88.8%, 99.4%)	
11/23 03:42:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [125][150/175]	Step 22150	lr 0.02252	Loss 0.4280 (0.3589)	Prec@(1,5) (88.4%, 99.3%)	
11/23 03:42:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [125][175/175]	Step 22175	lr 0.02252	Loss 0.4644 (0.3676)	Prec@(1,5) (88.1%, 99.3%)	
11/23 03:42:10AM evaluateStage_trainer.py:145 [INFO] Train: [125/599] Final Prec@1 88.0844%
11/23 03:42:13AM evaluateStage_trainer.py:180 [INFO] Valid: [125/599] Final Prec@1 54.8400%
11/23 03:42:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:42:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [126][50/175]	Step 22226	lr 0.02248	Loss 0.3058 (0.3222)	Prec@(1,5) (89.5%, 99.4%)	
11/23 03:42:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [126][100/175]	Step 22276	lr 0.02248	Loss 0.3634 (0.3332)	Prec@(1,5) (89.1%, 99.4%)	
11/23 03:42:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [126][150/175]	Step 22326	lr 0.02248	Loss 0.4133 (0.3461)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:42:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [126][175/175]	Step 22351	lr 0.02248	Loss 0.4011 (0.3571)	Prec@(1,5) (88.3%, 99.3%)	
11/23 03:42:56AM evaluateStage_trainer.py:145 [INFO] Train: [126/599] Final Prec@1 88.2911%
11/23 03:42:59AM evaluateStage_trainer.py:180 [INFO] Valid: [126/599] Final Prec@1 53.6000%
11/23 03:42:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:43:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [127][50/175]	Step 22402	lr 0.02244	Loss 0.2522 (0.3077)	Prec@(1,5) (89.9%, 99.7%)	
11/23 03:43:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [127][100/175]	Step 22452	lr 0.02244	Loss 0.3698 (0.3176)	Prec@(1,5) (89.6%, 99.6%)	
11/23 03:43:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [127][150/175]	Step 22502	lr 0.02244	Loss 0.3866 (0.3327)	Prec@(1,5) (89.1%, 99.5%)	
11/23 03:43:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [127][175/175]	Step 22527	lr 0.02244	Loss 0.4767 (0.3437)	Prec@(1,5) (88.7%, 99.4%)	
11/23 03:43:42AM evaluateStage_trainer.py:145 [INFO] Train: [127/599] Final Prec@1 88.7178%
11/23 03:43:44AM evaluateStage_trainer.py:180 [INFO] Valid: [127/599] Final Prec@1 54.2800%
11/23 03:43:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:43:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [128][50/175]	Step 22578	lr 0.0224	Loss 0.3243 (0.3143)	Prec@(1,5) (90.0%, 99.5%)	
11/23 03:44:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [128][100/175]	Step 22628	lr 0.0224	Loss 0.3010 (0.3227)	Prec@(1,5) (89.7%, 99.5%)	
11/23 03:44:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [128][150/175]	Step 22678	lr 0.0224	Loss 0.5414 (0.3487)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:44:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [128][175/175]	Step 22703	lr 0.0224	Loss 0.3921 (0.3567)	Prec@(1,5) (88.4%, 99.3%)	
11/23 03:44:27AM evaluateStage_trainer.py:145 [INFO] Train: [128/599] Final Prec@1 88.4111%
11/23 03:44:30AM evaluateStage_trainer.py:180 [INFO] Valid: [128/599] Final Prec@1 54.2200%
11/23 03:44:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.5400%
11/23 03:44:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [129][50/175]	Step 22754	lr 0.02237	Loss 0.3856 (0.3349)	Prec@(1,5) (89.5%, 99.4%)	
11/23 03:44:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [129][100/175]	Step 22804	lr 0.02237	Loss 0.3020 (0.3281)	Prec@(1,5) (89.7%, 99.4%)	
11/23 03:45:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [129][150/175]	Step 22854	lr 0.02237	Loss 0.3672 (0.3405)	Prec@(1,5) (89.2%, 99.3%)	
11/23 03:45:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [129][175/175]	Step 22879	lr 0.02237	Loss 0.4706 (0.3499)	Prec@(1,5) (88.9%, 99.3%)	
11/23 03:45:13AM evaluateStage_trainer.py:145 [INFO] Train: [129/599] Final Prec@1 88.8511%
11/23 03:45:16AM evaluateStage_trainer.py:180 [INFO] Valid: [129/599] Final Prec@1 56.9800%
11/23 03:45:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:45:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [130][50/175]	Step 22930	lr 0.02233	Loss 0.3760 (0.3021)	Prec@(1,5) (90.4%, 99.5%)	
11/23 03:45:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [130][100/175]	Step 22980	lr 0.02233	Loss 0.3659 (0.3178)	Prec@(1,5) (89.7%, 99.5%)	
11/23 03:45:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [130][150/175]	Step 23030	lr 0.02233	Loss 0.4098 (0.3298)	Prec@(1,5) (89.3%, 99.4%)	
11/23 03:45:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [130][175/175]	Step 23055	lr 0.02233	Loss 0.3936 (0.3389)	Prec@(1,5) (89.0%, 99.4%)	
11/23 03:45:59AM evaluateStage_trainer.py:145 [INFO] Train: [130/599] Final Prec@1 88.9467%
11/23 03:46:01AM evaluateStage_trainer.py:180 [INFO] Valid: [130/599] Final Prec@1 56.1000%
11/23 03:46:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:46:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [131][50/175]	Step 23106	lr 0.02229	Loss 0.2955 (0.3025)	Prec@(1,5) (90.3%, 99.4%)	
11/23 03:46:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [131][100/175]	Step 23156	lr 0.02229	Loss 0.3043 (0.3038)	Prec@(1,5) (90.2%, 99.5%)	
11/23 03:46:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [131][150/175]	Step 23206	lr 0.02229	Loss 0.3644 (0.3116)	Prec@(1,5) (90.0%, 99.5%)	
11/23 03:46:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [131][175/175]	Step 23231	lr 0.02229	Loss 0.4992 (0.3205)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:46:44AM evaluateStage_trainer.py:145 [INFO] Train: [131/599] Final Prec@1 89.6244%
11/23 03:46:47AM evaluateStage_trainer.py:180 [INFO] Valid: [131/599] Final Prec@1 54.4400%
11/23 03:46:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:47:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [132][50/175]	Step 23282	lr 0.02225	Loss 0.3877 (0.3141)	Prec@(1,5) (89.7%, 99.5%)	
11/23 03:47:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [132][100/175]	Step 23332	lr 0.02225	Loss 0.3310 (0.3269)	Prec@(1,5) (89.5%, 99.4%)	
11/23 03:47:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [132][150/175]	Step 23382	lr 0.02225	Loss 0.3363 (0.3482)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:47:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [132][175/175]	Step 23407	lr 0.02225	Loss 0.4974 (0.3562)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:47:30AM evaluateStage_trainer.py:145 [INFO] Train: [132/599] Final Prec@1 88.4533%
11/23 03:47:32AM evaluateStage_trainer.py:180 [INFO] Valid: [132/599] Final Prec@1 54.0400%
11/23 03:47:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:47:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [133][50/175]	Step 23458	lr 0.02221	Loss 0.3285 (0.3424)	Prec@(1,5) (89.1%, 99.3%)	
11/23 03:47:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [133][100/175]	Step 23508	lr 0.02221	Loss 0.3388 (0.3407)	Prec@(1,5) (88.9%, 99.3%)	
11/23 03:48:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [133][150/175]	Step 23558	lr 0.02221	Loss 0.4163 (0.3450)	Prec@(1,5) (88.7%, 99.3%)	
11/23 03:48:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [133][175/175]	Step 23583	lr 0.02221	Loss 0.3630 (0.3503)	Prec@(1,5) (88.5%, 99.3%)	
11/23 03:48:15AM evaluateStage_trainer.py:145 [INFO] Train: [133/599] Final Prec@1 88.5333%
11/23 03:48:18AM evaluateStage_trainer.py:180 [INFO] Valid: [133/599] Final Prec@1 55.3400%
11/23 03:48:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:48:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [134][50/175]	Step 23634	lr 0.02217	Loss 0.3302 (0.3045)	Prec@(1,5) (90.1%, 99.4%)	
11/23 03:48:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [134][100/175]	Step 23684	lr 0.02217	Loss 0.2169 (0.3080)	Prec@(1,5) (90.0%, 99.5%)	
11/23 03:48:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [134][150/175]	Step 23734	lr 0.02217	Loss 0.2881 (0.3226)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:49:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [134][175/175]	Step 23759	lr 0.02217	Loss 0.3409 (0.3276)	Prec@(1,5) (89.4%, 99.4%)	
11/23 03:49:00AM evaluateStage_trainer.py:145 [INFO] Train: [134/599] Final Prec@1 89.3711%
11/23 03:49:03AM evaluateStage_trainer.py:180 [INFO] Valid: [134/599] Final Prec@1 55.8000%
11/23 03:49:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:49:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [135][50/175]	Step 23810	lr 0.02212	Loss 0.4290 (0.2990)	Prec@(1,5) (90.4%, 99.6%)	
11/23 03:49:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [135][100/175]	Step 23860	lr 0.02212	Loss 0.3015 (0.3091)	Prec@(1,5) (90.2%, 99.5%)	
11/23 03:49:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [135][150/175]	Step 23910	lr 0.02212	Loss 0.2979 (0.3190)	Prec@(1,5) (89.8%, 99.4%)	
11/23 03:49:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [135][175/175]	Step 23935	lr 0.02212	Loss 0.3753 (0.3263)	Prec@(1,5) (89.5%, 99.4%)	
11/23 03:49:46AM evaluateStage_trainer.py:145 [INFO] Train: [135/599] Final Prec@1 89.4844%
11/23 03:49:48AM evaluateStage_trainer.py:180 [INFO] Valid: [135/599] Final Prec@1 54.3600%
11/23 03:49:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:50:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [136][50/175]	Step 23986	lr 0.02208	Loss 0.2736 (0.2923)	Prec@(1,5) (90.6%, 99.6%)	
11/23 03:50:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [136][100/175]	Step 24036	lr 0.02208	Loss 0.3646 (0.2982)	Prec@(1,5) (90.4%, 99.6%)	
11/23 03:50:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [136][150/175]	Step 24086	lr 0.02208	Loss 0.3785 (0.3148)	Prec@(1,5) (89.9%, 99.5%)	
11/23 03:50:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [136][175/175]	Step 24111	lr 0.02208	Loss 0.3671 (0.3195)	Prec@(1,5) (89.7%, 99.5%)	
11/23 03:50:31AM evaluateStage_trainer.py:145 [INFO] Train: [136/599] Final Prec@1 89.7244%
11/23 03:50:34AM evaluateStage_trainer.py:180 [INFO] Valid: [136/599] Final Prec@1 55.8800%
11/23 03:50:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:50:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [137][50/175]	Step 24162	lr 0.02204	Loss 0.3493 (0.2983)	Prec@(1,5) (90.5%, 99.6%)	
11/23 03:50:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [137][100/175]	Step 24212	lr 0.02204	Loss 0.3341 (0.3009)	Prec@(1,5) (90.3%, 99.6%)	
11/23 03:51:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [137][150/175]	Step 24262	lr 0.02204	Loss 0.3713 (0.3140)	Prec@(1,5) (89.8%, 99.5%)	
11/23 03:51:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [137][175/175]	Step 24287	lr 0.02204	Loss 0.3564 (0.3204)	Prec@(1,5) (89.6%, 99.5%)	
11/23 03:51:16AM evaluateStage_trainer.py:145 [INFO] Train: [137/599] Final Prec@1 89.6289%
11/23 03:51:19AM evaluateStage_trainer.py:180 [INFO] Valid: [137/599] Final Prec@1 52.5200%
11/23 03:51:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:51:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [138][50/175]	Step 24338	lr 0.022	Loss 0.3347 (0.2831)	Prec@(1,5) (90.7%, 99.5%)	
11/23 03:51:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [138][100/175]	Step 24388	lr 0.022	Loss 0.4162 (0.2908)	Prec@(1,5) (90.7%, 99.5%)	
11/23 03:51:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [138][150/175]	Step 24438	lr 0.022	Loss 0.3756 (0.3025)	Prec@(1,5) (90.1%, 99.5%)	
11/23 03:52:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [138][175/175]	Step 24463	lr 0.022	Loss 0.3689 (0.3091)	Prec@(1,5) (89.9%, 99.5%)	
11/23 03:52:02AM evaluateStage_trainer.py:145 [INFO] Train: [138/599] Final Prec@1 89.9067%
11/23 03:52:05AM evaluateStage_trainer.py:180 [INFO] Valid: [138/599] Final Prec@1 55.7800%
11/23 03:52:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:52:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [139][50/175]	Step 24514	lr 0.02196	Loss 0.2270 (0.2789)	Prec@(1,5) (91.2%, 99.6%)	
11/23 03:52:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [139][100/175]	Step 24564	lr 0.02196	Loss 0.3583 (0.2887)	Prec@(1,5) (90.9%, 99.5%)	
11/23 03:52:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [139][150/175]	Step 24614	lr 0.02196	Loss 0.2850 (0.3044)	Prec@(1,5) (90.3%, 99.5%)	
11/23 03:52:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [139][175/175]	Step 24639	lr 0.02196	Loss 0.3827 (0.3134)	Prec@(1,5) (90.0%, 99.5%)	
11/23 03:52:47AM evaluateStage_trainer.py:145 [INFO] Train: [139/599] Final Prec@1 90.0133%
11/23 03:52:50AM evaluateStage_trainer.py:180 [INFO] Valid: [139/599] Final Prec@1 54.5200%
11/23 03:52:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:53:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [140][50/175]	Step 24690	lr 0.02192	Loss 0.2879 (0.2933)	Prec@(1,5) (90.4%, 99.5%)	
11/23 03:53:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [140][100/175]	Step 24740	lr 0.02192	Loss 0.3966 (0.2991)	Prec@(1,5) (90.3%, 99.5%)	
11/23 03:53:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [140][150/175]	Step 24790	lr 0.02192	Loss 0.3239 (0.3137)	Prec@(1,5) (89.8%, 99.5%)	
11/23 03:53:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [140][175/175]	Step 24815	lr 0.02192	Loss 0.3815 (0.3193)	Prec@(1,5) (89.6%, 99.4%)	
11/23 03:53:33AM evaluateStage_trainer.py:145 [INFO] Train: [140/599] Final Prec@1 89.6244%
11/23 03:53:36AM evaluateStage_trainer.py:180 [INFO] Valid: [140/599] Final Prec@1 53.1000%
11/23 03:53:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:53:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [141][50/175]	Step 24866	lr 0.02188	Loss 0.2644 (0.2752)	Prec@(1,5) (91.4%, 99.7%)	
11/23 03:54:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [141][100/175]	Step 24916	lr 0.02188	Loss 0.3739 (0.2847)	Prec@(1,5) (91.0%, 99.6%)	
11/23 03:54:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [141][150/175]	Step 24966	lr 0.02188	Loss 0.3252 (0.2975)	Prec@(1,5) (90.5%, 99.6%)	
11/23 03:54:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [141][175/175]	Step 24991	lr 0.02188	Loss 0.3775 (0.3045)	Prec@(1,5) (90.3%, 99.5%)	
11/23 03:54:18AM evaluateStage_trainer.py:145 [INFO] Train: [141/599] Final Prec@1 90.2756%
11/23 03:54:21AM evaluateStage_trainer.py:180 [INFO] Valid: [141/599] Final Prec@1 55.8200%
11/23 03:54:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:54:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [142][50/175]	Step 25042	lr 0.02183	Loss 0.2449 (0.2908)	Prec@(1,5) (90.5%, 99.6%)	
11/23 03:54:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [142][100/175]	Step 25092	lr 0.02183	Loss 0.2797 (0.2992)	Prec@(1,5) (90.3%, 99.6%)	
11/23 03:54:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [142][150/175]	Step 25142	lr 0.02183	Loss 0.2970 (0.3124)	Prec@(1,5) (89.9%, 99.6%)	
11/23 03:55:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [142][175/175]	Step 25167	lr 0.02183	Loss 0.3242 (0.3190)	Prec@(1,5) (89.6%, 99.5%)	
11/23 03:55:04AM evaluateStage_trainer.py:145 [INFO] Train: [142/599] Final Prec@1 89.6533%
11/23 03:55:06AM evaluateStage_trainer.py:180 [INFO] Valid: [142/599] Final Prec@1 55.7400%
11/23 03:55:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:55:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [143][50/175]	Step 25218	lr 0.02179	Loss 0.3179 (0.3000)	Prec@(1,5) (90.1%, 99.6%)	
11/23 03:55:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [143][100/175]	Step 25268	lr 0.02179	Loss 0.3430 (0.3058)	Prec@(1,5) (89.9%, 99.6%)	
11/23 03:55:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [143][150/175]	Step 25318	lr 0.02179	Loss 0.3505 (0.3146)	Prec@(1,5) (89.7%, 99.5%)	
11/23 03:55:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [143][175/175]	Step 25343	lr 0.02179	Loss 0.3408 (0.3200)	Prec@(1,5) (89.6%, 99.5%)	
11/23 03:55:49AM evaluateStage_trainer.py:145 [INFO] Train: [143/599] Final Prec@1 89.5800%
11/23 03:55:52AM evaluateStage_trainer.py:180 [INFO] Valid: [143/599] Final Prec@1 56.1800%
11/23 03:55:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:56:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [144][50/175]	Step 25394	lr 0.02175	Loss 0.3314 (0.2943)	Prec@(1,5) (90.7%, 99.5%)	
11/23 03:56:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [144][100/175]	Step 25444	lr 0.02175	Loss 0.2493 (0.3051)	Prec@(1,5) (90.3%, 99.5%)	
11/23 03:56:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [144][150/175]	Step 25494	lr 0.02175	Loss 0.3051 (0.3098)	Prec@(1,5) (90.1%, 99.4%)	
11/23 03:56:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [144][175/175]	Step 25519	lr 0.02175	Loss 0.3935 (0.3150)	Prec@(1,5) (89.8%, 99.4%)	
11/23 03:56:34AM evaluateStage_trainer.py:145 [INFO] Train: [144/599] Final Prec@1 89.8267%
11/23 03:56:37AM evaluateStage_trainer.py:180 [INFO] Valid: [144/599] Final Prec@1 55.1200%
11/23 03:56:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:56:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [145][50/175]	Step 25570	lr 0.0217	Loss 0.2803 (0.2817)	Prec@(1,5) (91.0%, 99.6%)	
11/23 03:57:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [145][100/175]	Step 25620	lr 0.0217	Loss 0.3576 (0.2884)	Prec@(1,5) (90.7%, 99.6%)	
11/23 03:57:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [145][150/175]	Step 25670	lr 0.0217	Loss 0.3910 (0.3008)	Prec@(1,5) (90.2%, 99.6%)	
11/23 03:57:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [145][175/175]	Step 25695	lr 0.0217	Loss 0.3136 (0.3063)	Prec@(1,5) (90.0%, 99.5%)	
11/23 03:57:20AM evaluateStage_trainer.py:145 [INFO] Train: [145/599] Final Prec@1 90.0133%
11/23 03:57:22AM evaluateStage_trainer.py:180 [INFO] Valid: [145/599] Final Prec@1 55.5000%
11/23 03:57:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:57:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [146][50/175]	Step 25746	lr 0.02166	Loss 0.2551 (0.2694)	Prec@(1,5) (91.4%, 99.6%)	
11/23 03:57:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [146][100/175]	Step 25796	lr 0.02166	Loss 0.2906 (0.2819)	Prec@(1,5) (90.9%, 99.6%)	
11/23 03:57:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [146][150/175]	Step 25846	lr 0.02166	Loss 0.3231 (0.2927)	Prec@(1,5) (90.6%, 99.6%)	
11/23 03:58:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [146][175/175]	Step 25871	lr 0.02166	Loss 0.4728 (0.3019)	Prec@(1,5) (90.3%, 99.5%)	
11/23 03:58:05AM evaluateStage_trainer.py:145 [INFO] Train: [146/599] Final Prec@1 90.2622%
11/23 03:58:08AM evaluateStage_trainer.py:180 [INFO] Valid: [146/599] Final Prec@1 56.4600%
11/23 03:58:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:58:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [147][50/175]	Step 25922	lr 0.02162	Loss 0.2083 (0.2714)	Prec@(1,5) (91.5%, 99.7%)	
11/23 03:58:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [147][100/175]	Step 25972	lr 0.02162	Loss 0.3535 (0.2797)	Prec@(1,5) (91.0%, 99.6%)	
11/23 03:58:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [147][150/175]	Step 26022	lr 0.02162	Loss 0.3589 (0.2947)	Prec@(1,5) (90.5%, 99.6%)	
11/23 03:58:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [147][175/175]	Step 26047	lr 0.02162	Loss 0.4190 (0.3013)	Prec@(1,5) (90.2%, 99.5%)	
11/23 03:58:51AM evaluateStage_trainer.py:145 [INFO] Train: [147/599] Final Prec@1 90.2400%
11/23 03:58:54AM evaluateStage_trainer.py:180 [INFO] Valid: [147/599] Final Prec@1 55.1200%
11/23 03:58:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:59:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [148][50/175]	Step 26098	lr 0.02157	Loss 0.2940 (0.2700)	Prec@(1,5) (91.4%, 99.6%)	
11/23 03:59:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [148][100/175]	Step 26148	lr 0.02157	Loss 0.3097 (0.2828)	Prec@(1,5) (91.1%, 99.6%)	
11/23 03:59:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [148][150/175]	Step 26198	lr 0.02157	Loss 0.3341 (0.2980)	Prec@(1,5) (90.6%, 99.5%)	
11/23 03:59:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [148][175/175]	Step 26223	lr 0.02157	Loss 0.2963 (0.3065)	Prec@(1,5) (90.2%, 99.5%)	
11/23 03:59:37AM evaluateStage_trainer.py:145 [INFO] Train: [148/599] Final Prec@1 90.2089%
11/23 03:59:40AM evaluateStage_trainer.py:180 [INFO] Valid: [148/599] Final Prec@1 54.4000%
11/23 03:59:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 03:59:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [149][50/175]	Step 26274	lr 0.02153	Loss 0.2257 (0.2829)	Prec@(1,5) (90.9%, 99.6%)	
11/23 04:00:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [149][100/175]	Step 26324	lr 0.02153	Loss 0.2500 (0.2864)	Prec@(1,5) (90.7%, 99.6%)	
11/23 04:00:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [149][150/175]	Step 26374	lr 0.02153	Loss 0.3302 (0.2977)	Prec@(1,5) (90.4%, 99.6%)	
11/23 04:00:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [149][175/175]	Step 26399	lr 0.02153	Loss 0.5051 (0.3026)	Prec@(1,5) (90.2%, 99.6%)	
11/23 04:00:23AM evaluateStage_trainer.py:145 [INFO] Train: [149/599] Final Prec@1 90.2022%
11/23 04:00:26AM evaluateStage_trainer.py:180 [INFO] Valid: [149/599] Final Prec@1 55.1800%
11/23 04:00:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:00:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [150][50/175]	Step 26450	lr 0.02149	Loss 0.2545 (0.2766)	Prec@(1,5) (91.1%, 99.6%)	
11/23 04:00:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [150][100/175]	Step 26500	lr 0.02149	Loss 0.2589 (0.2786)	Prec@(1,5) (91.1%, 99.6%)	
11/23 04:01:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [150][150/175]	Step 26550	lr 0.02149	Loss 0.2969 (0.2877)	Prec@(1,5) (90.8%, 99.6%)	
11/23 04:01:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [150][175/175]	Step 26575	lr 0.02149	Loss 0.4024 (0.2946)	Prec@(1,5) (90.6%, 99.5%)	
11/23 04:01:09AM evaluateStage_trainer.py:145 [INFO] Train: [150/599] Final Prec@1 90.5444%
11/23 04:01:12AM evaluateStage_trainer.py:180 [INFO] Valid: [150/599] Final Prec@1 56.0600%
11/23 04:01:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:01:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [151][50/175]	Step 26626	lr 0.02144	Loss 0.3240 (0.2769)	Prec@(1,5) (91.3%, 99.6%)	
11/23 04:01:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [151][100/175]	Step 26676	lr 0.02144	Loss 0.2628 (0.2827)	Prec@(1,5) (91.0%, 99.5%)	
11/23 04:01:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [151][150/175]	Step 26726	lr 0.02144	Loss 0.2754 (0.2984)	Prec@(1,5) (90.4%, 99.5%)	
11/23 04:01:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [151][175/175]	Step 26751	lr 0.02144	Loss 0.3815 (0.3057)	Prec@(1,5) (90.1%, 99.5%)	
11/23 04:01:55AM evaluateStage_trainer.py:145 [INFO] Train: [151/599] Final Prec@1 90.1067%
11/23 04:01:58AM evaluateStage_trainer.py:180 [INFO] Valid: [151/599] Final Prec@1 53.2000%
11/23 04:01:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:02:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [152][50/175]	Step 26802	lr 0.0214	Loss 0.3429 (0.2765)	Prec@(1,5) (90.8%, 99.6%)	
11/23 04:02:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [152][100/175]	Step 26852	lr 0.0214	Loss 0.3236 (0.2761)	Prec@(1,5) (90.9%, 99.6%)	
11/23 04:02:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [152][150/175]	Step 26902	lr 0.0214	Loss 0.2746 (0.2846)	Prec@(1,5) (90.6%, 99.6%)	
11/23 04:02:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [152][175/175]	Step 26927	lr 0.0214	Loss 0.3324 (0.2905)	Prec@(1,5) (90.5%, 99.6%)	
11/23 04:02:41AM evaluateStage_trainer.py:145 [INFO] Train: [152/599] Final Prec@1 90.4978%
11/23 04:02:44AM evaluateStage_trainer.py:180 [INFO] Valid: [152/599] Final Prec@1 55.2000%
11/23 04:02:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:02:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [153][50/175]	Step 26978	lr 0.02135	Loss 0.2376 (0.2663)	Prec@(1,5) (91.3%, 99.7%)	
11/23 04:03:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [153][100/175]	Step 27028	lr 0.02135	Loss 0.2385 (0.2659)	Prec@(1,5) (91.5%, 99.7%)	
11/23 04:03:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [153][150/175]	Step 27078	lr 0.02135	Loss 0.2631 (0.2766)	Prec@(1,5) (91.1%, 99.6%)	
11/23 04:03:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [153][175/175]	Step 27103	lr 0.02135	Loss 0.3439 (0.2848)	Prec@(1,5) (90.8%, 99.6%)	
11/23 04:03:27AM evaluateStage_trainer.py:145 [INFO] Train: [153/599] Final Prec@1 90.8267%
11/23 04:03:30AM evaluateStage_trainer.py:180 [INFO] Valid: [153/599] Final Prec@1 54.0400%
11/23 04:03:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:03:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [154][50/175]	Step 27154	lr 0.02131	Loss 0.2264 (0.2641)	Prec@(1,5) (91.8%, 99.5%)	
11/23 04:03:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [154][100/175]	Step 27204	lr 0.02131	Loss 0.3595 (0.2637)	Prec@(1,5) (91.7%, 99.6%)	
11/23 04:04:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [154][150/175]	Step 27254	lr 0.02131	Loss 0.2317 (0.2831)	Prec@(1,5) (90.9%, 99.5%)	
11/23 04:04:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [154][175/175]	Step 27279	lr 0.02131	Loss 0.3292 (0.2930)	Prec@(1,5) (90.5%, 99.5%)	
11/23 04:04:13AM evaluateStage_trainer.py:145 [INFO] Train: [154/599] Final Prec@1 90.4867%
11/23 04:04:15AM evaluateStage_trainer.py:180 [INFO] Valid: [154/599] Final Prec@1 53.7200%
11/23 04:04:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:04:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [155][50/175]	Step 27330	lr 0.02126	Loss 0.3304 (0.2814)	Prec@(1,5) (90.8%, 99.6%)	
11/23 04:04:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [155][100/175]	Step 27380	lr 0.02126	Loss 0.2516 (0.2796)	Prec@(1,5) (91.1%, 99.5%)	
11/23 04:04:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [155][150/175]	Step 27430	lr 0.02126	Loss 0.3361 (0.2860)	Prec@(1,5) (90.9%, 99.5%)	
11/23 04:04:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [155][175/175]	Step 27455	lr 0.02126	Loss 0.3340 (0.2913)	Prec@(1,5) (90.7%, 99.5%)	
11/23 04:04:58AM evaluateStage_trainer.py:145 [INFO] Train: [155/599] Final Prec@1 90.6800%
11/23 04:05:01AM evaluateStage_trainer.py:180 [INFO] Valid: [155/599] Final Prec@1 53.3200%
11/23 04:05:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:05:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [156][50/175]	Step 27506	lr 0.02121	Loss 0.2771 (0.2509)	Prec@(1,5) (92.1%, 99.6%)	
11/23 04:05:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [156][100/175]	Step 27556	lr 0.02121	Loss 0.2213 (0.2571)	Prec@(1,5) (91.9%, 99.7%)	
11/23 04:05:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [156][150/175]	Step 27606	lr 0.02121	Loss 0.2705 (0.2708)	Prec@(1,5) (91.3%, 99.7%)	
11/23 04:05:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [156][175/175]	Step 27631	lr 0.02121	Loss 0.3317 (0.2797)	Prec@(1,5) (91.0%, 99.6%)	
11/23 04:05:44AM evaluateStage_trainer.py:145 [INFO] Train: [156/599] Final Prec@1 91.0067%
11/23 04:05:47AM evaluateStage_trainer.py:180 [INFO] Valid: [156/599] Final Prec@1 53.4200%
11/23 04:05:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:06:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [157][50/175]	Step 27682	lr 0.02117	Loss 0.2407 (0.2608)	Prec@(1,5) (91.8%, 99.6%)	
11/23 04:06:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [157][100/175]	Step 27732	lr 0.02117	Loss 0.3223 (0.2606)	Prec@(1,5) (91.7%, 99.6%)	
11/23 04:06:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [157][150/175]	Step 27782	lr 0.02117	Loss 0.3132 (0.2693)	Prec@(1,5) (91.4%, 99.6%)	
11/23 04:06:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [157][175/175]	Step 27807	lr 0.02117	Loss 0.3364 (0.2760)	Prec@(1,5) (91.2%, 99.6%)	
11/23 04:06:30AM evaluateStage_trainer.py:145 [INFO] Train: [157/599] Final Prec@1 91.1578%
11/23 04:06:33AM evaluateStage_trainer.py:180 [INFO] Valid: [157/599] Final Prec@1 55.9200%
11/23 04:06:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:06:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [158][50/175]	Step 27858	lr 0.02112	Loss 0.2135 (0.2513)	Prec@(1,5) (92.1%, 99.6%)	
11/23 04:06:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [158][100/175]	Step 27908	lr 0.02112	Loss 0.3457 (0.2586)	Prec@(1,5) (91.6%, 99.7%)	
11/23 04:07:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [158][150/175]	Step 27958	lr 0.02112	Loss 0.3014 (0.2706)	Prec@(1,5) (91.3%, 99.6%)	
11/23 04:07:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [158][175/175]	Step 27983	lr 0.02112	Loss 0.3237 (0.2773)	Prec@(1,5) (91.0%, 99.6%)	
11/23 04:07:15AM evaluateStage_trainer.py:145 [INFO] Train: [158/599] Final Prec@1 91.0022%
11/23 04:07:18AM evaluateStage_trainer.py:180 [INFO] Valid: [158/599] Final Prec@1 56.2400%
11/23 04:07:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:07:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [159][50/175]	Step 28034	lr 0.02108	Loss 0.3083 (0.2500)	Prec@(1,5) (92.1%, 99.7%)	
11/23 04:07:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [159][100/175]	Step 28084	lr 0.02108	Loss 0.3186 (0.2651)	Prec@(1,5) (91.5%, 99.6%)	
11/23 04:07:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [159][150/175]	Step 28134	lr 0.02108	Loss 0.3243 (0.2783)	Prec@(1,5) (91.1%, 99.6%)	
11/23 04:08:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [159][175/175]	Step 28159	lr 0.02108	Loss 0.3956 (0.2856)	Prec@(1,5) (90.8%, 99.6%)	
11/23 04:08:01AM evaluateStage_trainer.py:145 [INFO] Train: [159/599] Final Prec@1 90.8444%
11/23 04:08:04AM evaluateStage_trainer.py:180 [INFO] Valid: [159/599] Final Prec@1 55.9200%
11/23 04:08:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:08:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [160][50/175]	Step 28210	lr 0.02103	Loss 0.2675 (0.2608)	Prec@(1,5) (91.8%, 99.6%)	
11/23 04:08:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [160][100/175]	Step 28260	lr 0.02103	Loss 0.2537 (0.2587)	Prec@(1,5) (91.9%, 99.6%)	
11/23 04:08:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [160][150/175]	Step 28310	lr 0.02103	Loss 0.3135 (0.2662)	Prec@(1,5) (91.7%, 99.6%)	
11/23 04:08:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [160][175/175]	Step 28335	lr 0.02103	Loss 0.3405 (0.2722)	Prec@(1,5) (91.5%, 99.6%)	
11/23 04:08:46AM evaluateStage_trainer.py:145 [INFO] Train: [160/599] Final Prec@1 91.4578%
11/23 04:08:50AM evaluateStage_trainer.py:180 [INFO] Valid: [160/599] Final Prec@1 54.7800%
11/23 04:08:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:09:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [161][50/175]	Step 28386	lr 0.02098	Loss 0.2774 (0.2668)	Prec@(1,5) (91.2%, 99.6%)	
11/23 04:09:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [161][100/175]	Step 28436	lr 0.02098	Loss 0.3807 (0.2666)	Prec@(1,5) (91.3%, 99.7%)	
11/23 04:09:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [161][150/175]	Step 28486	lr 0.02098	Loss 0.3105 (0.2805)	Prec@(1,5) (90.9%, 99.6%)	
11/23 04:09:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [161][175/175]	Step 28511	lr 0.02098	Loss 0.3454 (0.2844)	Prec@(1,5) (90.7%, 99.6%)	
11/23 04:09:32AM evaluateStage_trainer.py:145 [INFO] Train: [161/599] Final Prec@1 90.6956%
11/23 04:09:35AM evaluateStage_trainer.py:180 [INFO] Valid: [161/599] Final Prec@1 53.1600%
11/23 04:09:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:09:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [162][50/175]	Step 28562	lr 0.02094	Loss 0.3115 (0.2557)	Prec@(1,5) (91.7%, 99.7%)	
11/23 04:10:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [162][100/175]	Step 28612	lr 0.02094	Loss 0.3335 (0.2583)	Prec@(1,5) (91.6%, 99.7%)	
11/23 04:10:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [162][150/175]	Step 28662	lr 0.02094	Loss 0.3164 (0.2731)	Prec@(1,5) (91.2%, 99.6%)	
11/23 04:10:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [162][175/175]	Step 28687	lr 0.02094	Loss 0.3214 (0.2819)	Prec@(1,5) (90.9%, 99.6%)	
11/23 04:10:18AM evaluateStage_trainer.py:145 [INFO] Train: [162/599] Final Prec@1 90.8600%
11/23 04:10:20AM evaluateStage_trainer.py:180 [INFO] Valid: [162/599] Final Prec@1 53.5800%
11/23 04:10:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:10:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [163][50/175]	Step 28738	lr 0.02089	Loss 0.2430 (0.2575)	Prec@(1,5) (92.1%, 99.7%)	
11/23 04:10:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [163][100/175]	Step 28788	lr 0.02089	Loss 0.3345 (0.2584)	Prec@(1,5) (91.9%, 99.7%)	
11/23 04:10:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [163][150/175]	Step 28838	lr 0.02089	Loss 0.2764 (0.2667)	Prec@(1,5) (91.5%, 99.6%)	
11/23 04:11:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [163][175/175]	Step 28863	lr 0.02089	Loss 0.3387 (0.2698)	Prec@(1,5) (91.4%, 99.6%)	
11/23 04:11:03AM evaluateStage_trainer.py:145 [INFO] Train: [163/599] Final Prec@1 91.3489%
11/23 04:11:06AM evaluateStage_trainer.py:180 [INFO] Valid: [163/599] Final Prec@1 54.6200%
11/23 04:11:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 56.9800%
11/23 04:11:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [164][50/175]	Step 28914	lr 0.02084	Loss 0.2582 (0.2287)	Prec@(1,5) (92.6%, 99.8%)	
11/23 04:11:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [164][100/175]	Step 28964	lr 0.02084	Loss 0.2358 (0.2494)	Prec@(1,5) (91.9%, 99.7%)	
11/23 04:11:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [164][150/175]	Step 29014	lr 0.02084	Loss 0.2776 (0.2565)	Prec@(1,5) (91.7%, 99.7%)	
11/23 04:11:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [164][175/175]	Step 29039	lr 0.02084	Loss 0.2999 (0.2613)	Prec@(1,5) (91.5%, 99.7%)	
11/23 04:11:49AM evaluateStage_trainer.py:145 [INFO] Train: [164/599] Final Prec@1 91.5178%
11/23 04:11:52AM evaluateStage_trainer.py:180 [INFO] Valid: [164/599] Final Prec@1 57.6000%
11/23 04:11:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:12:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [165][50/175]	Step 29090	lr 0.02079	Loss 0.2189 (0.2315)	Prec@(1,5) (92.7%, 99.8%)	
11/23 04:12:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [165][100/175]	Step 29140	lr 0.02079	Loss 0.2313 (0.2336)	Prec@(1,5) (92.7%, 99.7%)	
11/23 04:12:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [165][150/175]	Step 29190	lr 0.02079	Loss 0.2220 (0.2436)	Prec@(1,5) (92.3%, 99.7%)	
11/23 04:12:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [165][175/175]	Step 29215	lr 0.02079	Loss 0.3037 (0.2520)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:12:35AM evaluateStage_trainer.py:145 [INFO] Train: [165/599] Final Prec@1 92.0178%
11/23 04:12:39AM evaluateStage_trainer.py:180 [INFO] Valid: [165/599] Final Prec@1 54.8200%
11/23 04:12:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:12:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [166][50/175]	Step 29266	lr 0.02075	Loss 0.1790 (0.2341)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:13:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [166][100/175]	Step 29316	lr 0.02075	Loss 0.3076 (0.2407)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:13:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [166][150/175]	Step 29366	lr 0.02075	Loss 0.2691 (0.2504)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:13:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [166][175/175]	Step 29391	lr 0.02075	Loss 0.2418 (0.2538)	Prec@(1,5) (91.9%, 99.7%)	
11/23 04:13:21AM evaluateStage_trainer.py:145 [INFO] Train: [166/599] Final Prec@1 91.9022%
11/23 04:13:24AM evaluateStage_trainer.py:180 [INFO] Valid: [166/599] Final Prec@1 55.7000%
11/23 04:13:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:13:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [167][50/175]	Step 29442	lr 0.0207	Loss 0.2289 (0.2429)	Prec@(1,5) (92.1%, 99.7%)	
11/23 04:13:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [167][100/175]	Step 29492	lr 0.0207	Loss 0.2494 (0.2491)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:14:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [167][150/175]	Step 29542	lr 0.0207	Loss 0.3910 (0.2642)	Prec@(1,5) (91.5%, 99.6%)	
11/23 04:14:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [167][175/175]	Step 29567	lr 0.0207	Loss 0.3784 (0.2745)	Prec@(1,5) (91.1%, 99.6%)	
11/23 04:14:07AM evaluateStage_trainer.py:145 [INFO] Train: [167/599] Final Prec@1 91.1044%
11/23 04:14:10AM evaluateStage_trainer.py:180 [INFO] Valid: [167/599] Final Prec@1 53.7400%
11/23 04:14:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:14:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [168][50/175]	Step 29618	lr 0.02065	Loss 0.2685 (0.2531)	Prec@(1,5) (92.1%, 99.6%)	
11/23 04:14:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [168][100/175]	Step 29668	lr 0.02065	Loss 0.2971 (0.2532)	Prec@(1,5) (92.1%, 99.6%)	
11/23 04:14:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [168][150/175]	Step 29718	lr 0.02065	Loss 0.3165 (0.2682)	Prec@(1,5) (91.4%, 99.6%)	
11/23 04:14:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [168][175/175]	Step 29743	lr 0.02065	Loss 0.3159 (0.2746)	Prec@(1,5) (91.2%, 99.6%)	
11/23 04:14:52AM evaluateStage_trainer.py:145 [INFO] Train: [168/599] Final Prec@1 91.2022%
11/23 04:14:56AM evaluateStage_trainer.py:180 [INFO] Valid: [168/599] Final Prec@1 55.9000%
11/23 04:14:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:15:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [169][50/175]	Step 29794	lr 0.0206	Loss 0.2900 (0.2427)	Prec@(1,5) (92.4%, 99.6%)	
11/23 04:15:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [169][100/175]	Step 29844	lr 0.0206	Loss 0.2028 (0.2547)	Prec@(1,5) (91.8%, 99.6%)	
11/23 04:15:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [169][150/175]	Step 29894	lr 0.0206	Loss 0.3277 (0.2633)	Prec@(1,5) (91.5%, 99.6%)	
11/23 04:15:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [169][175/175]	Step 29919	lr 0.0206	Loss 0.4240 (0.2685)	Prec@(1,5) (91.4%, 99.6%)	
11/23 04:15:38AM evaluateStage_trainer.py:145 [INFO] Train: [169/599] Final Prec@1 91.4067%
11/23 04:15:41AM evaluateStage_trainer.py:180 [INFO] Valid: [169/599] Final Prec@1 56.6600%
11/23 04:15:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:15:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [170][50/175]	Step 29970	lr 0.02055	Loss 0.2891 (0.2395)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:16:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [170][100/175]	Step 30020	lr 0.02055	Loss 0.1991 (0.2464)	Prec@(1,5) (92.2%, 99.6%)	
11/23 04:16:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [170][150/175]	Step 30070	lr 0.02055	Loss 0.2262 (0.2524)	Prec@(1,5) (92.0%, 99.6%)	
11/23 04:16:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [170][175/175]	Step 30095	lr 0.02055	Loss 0.3306 (0.2565)	Prec@(1,5) (91.8%, 99.6%)	
11/23 04:16:24AM evaluateStage_trainer.py:145 [INFO] Train: [170/599] Final Prec@1 91.7911%
11/23 04:16:28AM evaluateStage_trainer.py:180 [INFO] Valid: [170/599] Final Prec@1 56.9000%
11/23 04:16:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:16:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [171][50/175]	Step 30146	lr 0.0205	Loss 0.2495 (0.2204)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:16:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [171][100/175]	Step 30196	lr 0.0205	Loss 0.2734 (0.2262)	Prec@(1,5) (92.9%, 99.8%)	
11/23 04:17:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [171][150/175]	Step 30246	lr 0.0205	Loss 0.2834 (0.2358)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:17:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [171][175/175]	Step 30271	lr 0.0205	Loss 0.3032 (0.2440)	Prec@(1,5) (92.2%, 99.7%)	
11/23 04:17:10AM evaluateStage_trainer.py:145 [INFO] Train: [171/599] Final Prec@1 92.1422%
11/23 04:17:13AM evaluateStage_trainer.py:180 [INFO] Valid: [171/599] Final Prec@1 55.8800%
11/23 04:17:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:17:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [172][50/175]	Step 30322	lr 0.02045	Loss 0.1610 (0.2388)	Prec@(1,5) (92.3%, 99.6%)	
11/23 04:17:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [172][100/175]	Step 30372	lr 0.02045	Loss 0.2731 (0.2402)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:17:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [172][150/175]	Step 30422	lr 0.02045	Loss 0.2334 (0.2445)	Prec@(1,5) (92.2%, 99.7%)	
11/23 04:17:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [172][175/175]	Step 30447	lr 0.02045	Loss 0.3101 (0.2487)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:17:56AM evaluateStage_trainer.py:145 [INFO] Train: [172/599] Final Prec@1 91.9956%
11/23 04:17:59AM evaluateStage_trainer.py:180 [INFO] Valid: [172/599] Final Prec@1 57.2400%
11/23 04:17:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:18:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [173][50/175]	Step 30498	lr 0.0204	Loss 0.1749 (0.2314)	Prec@(1,5) (92.6%, 99.8%)	
11/23 04:18:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [173][100/175]	Step 30548	lr 0.0204	Loss 0.2455 (0.2411)	Prec@(1,5) (92.3%, 99.7%)	
11/23 04:18:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [173][150/175]	Step 30598	lr 0.0204	Loss 0.3410 (0.2497)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:18:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [173][175/175]	Step 30623	lr 0.0204	Loss 0.2758 (0.2560)	Prec@(1,5) (91.8%, 99.6%)	
11/23 04:18:41AM evaluateStage_trainer.py:145 [INFO] Train: [173/599] Final Prec@1 91.8089%
11/23 04:18:44AM evaluateStage_trainer.py:180 [INFO] Valid: [173/599] Final Prec@1 55.8200%
11/23 04:18:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:18:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [174][50/175]	Step 30674	lr 0.02035	Loss 0.2043 (0.2373)	Prec@(1,5) (92.5%, 99.7%)	
11/23 04:19:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [174][100/175]	Step 30724	lr 0.02035	Loss 0.1923 (0.2443)	Prec@(1,5) (92.2%, 99.7%)	
11/23 04:19:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [174][150/175]	Step 30774	lr 0.02035	Loss 0.4068 (0.2547)	Prec@(1,5) (91.8%, 99.7%)	
11/23 04:19:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [174][175/175]	Step 30799	lr 0.02035	Loss 0.2314 (0.2592)	Prec@(1,5) (91.6%, 99.7%)	
11/23 04:19:27AM evaluateStage_trainer.py:145 [INFO] Train: [174/599] Final Prec@1 91.6200%
11/23 04:19:30AM evaluateStage_trainer.py:180 [INFO] Valid: [174/599] Final Prec@1 54.4600%
11/23 04:19:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:19:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [175][50/175]	Step 30850	lr 0.02031	Loss 0.1797 (0.2400)	Prec@(1,5) (92.4%, 99.8%)	
11/23 04:19:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [175][100/175]	Step 30900	lr 0.02031	Loss 0.2317 (0.2424)	Prec@(1,5) (92.3%, 99.7%)	
11/23 04:20:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [175][150/175]	Step 30950	lr 0.02031	Loss 0.3648 (0.2511)	Prec@(1,5) (92.0%, 99.7%)	
11/23 04:20:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [175][175/175]	Step 30975	lr 0.02031	Loss 0.2688 (0.2538)	Prec@(1,5) (91.8%, 99.7%)	
11/23 04:20:13AM evaluateStage_trainer.py:145 [INFO] Train: [175/599] Final Prec@1 91.8178%
11/23 04:20:16AM evaluateStage_trainer.py:180 [INFO] Valid: [175/599] Final Prec@1 54.1600%
11/23 04:20:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:20:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [176][50/175]	Step 31026	lr 0.02026	Loss 0.2286 (0.2150)	Prec@(1,5) (93.2%, 99.7%)	
11/23 04:20:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [176][100/175]	Step 31076	lr 0.02026	Loss 0.2099 (0.2262)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:20:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [176][150/175]	Step 31126	lr 0.02026	Loss 0.3194 (0.2378)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:20:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [176][175/175]	Step 31151	lr 0.02026	Loss 0.2867 (0.2440)	Prec@(1,5) (92.1%, 99.7%)	
11/23 04:20:58AM evaluateStage_trainer.py:145 [INFO] Train: [176/599] Final Prec@1 92.1222%
11/23 04:21:01AM evaluateStage_trainer.py:180 [INFO] Valid: [176/599] Final Prec@1 56.5000%
11/23 04:21:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:21:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [177][50/175]	Step 31202	lr 0.02021	Loss 0.1740 (0.2227)	Prec@(1,5) (93.1%, 99.7%)	
11/23 04:21:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [177][100/175]	Step 31252	lr 0.02021	Loss 0.2335 (0.2269)	Prec@(1,5) (93.0%, 99.7%)	
11/23 04:21:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [177][150/175]	Step 31302	lr 0.02021	Loss 0.2712 (0.2381)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:21:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [177][175/175]	Step 31327	lr 0.02021	Loss 0.2680 (0.2432)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:21:44AM evaluateStage_trainer.py:145 [INFO] Train: [177/599] Final Prec@1 92.3667%
11/23 04:21:46AM evaluateStage_trainer.py:180 [INFO] Valid: [177/599] Final Prec@1 57.0000%
11/23 04:21:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:21:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [178][50/175]	Step 31378	lr 0.02015	Loss 0.2389 (0.2240)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:22:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [178][100/175]	Step 31428	lr 0.02015	Loss 0.1900 (0.2207)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:22:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [178][150/175]	Step 31478	lr 0.02015	Loss 0.2609 (0.2287)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:22:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [178][175/175]	Step 31503	lr 0.02015	Loss 0.2698 (0.2344)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:22:29AM evaluateStage_trainer.py:145 [INFO] Train: [178/599] Final Prec@1 92.6356%
11/23 04:22:32AM evaluateStage_trainer.py:180 [INFO] Valid: [178/599] Final Prec@1 57.1000%
11/23 04:22:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:22:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [179][50/175]	Step 31554	lr 0.0201	Loss 0.2075 (0.2201)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:22:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [179][100/175]	Step 31604	lr 0.0201	Loss 0.2834 (0.2282)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:23:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [179][150/175]	Step 31654	lr 0.0201	Loss 0.2925 (0.2323)	Prec@(1,5) (92.7%, 99.7%)	
11/23 04:23:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [179][175/175]	Step 31679	lr 0.0201	Loss 0.2863 (0.2350)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:23:15AM evaluateStage_trainer.py:145 [INFO] Train: [179/599] Final Prec@1 92.5578%
11/23 04:23:18AM evaluateStage_trainer.py:180 [INFO] Valid: [179/599] Final Prec@1 57.0000%
11/23 04:23:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:23:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [180][50/175]	Step 31730	lr 0.02005	Loss 0.2053 (0.2230)	Prec@(1,5) (93.0%, 99.7%)	
11/23 04:23:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [180][100/175]	Step 31780	lr 0.02005	Loss 0.1649 (0.2311)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:23:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [180][150/175]	Step 31830	lr 0.02005	Loss 0.2727 (0.2393)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:24:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [180][175/175]	Step 31855	lr 0.02005	Loss 0.3225 (0.2458)	Prec@(1,5) (92.2%, 99.7%)	
11/23 04:24:00AM evaluateStage_trainer.py:145 [INFO] Train: [180/599] Final Prec@1 92.1822%
11/23 04:24:04AM evaluateStage_trainer.py:180 [INFO] Valid: [180/599] Final Prec@1 57.4800%
11/23 04:24:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:24:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [181][50/175]	Step 31906	lr 0.02	Loss 0.2257 (0.2292)	Prec@(1,5) (92.6%, 99.8%)	
11/23 04:24:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [181][100/175]	Step 31956	lr 0.02	Loss 0.2544 (0.2311)	Prec@(1,5) (92.6%, 99.8%)	
11/23 04:24:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [181][150/175]	Step 32006	lr 0.02	Loss 0.2543 (0.2335)	Prec@(1,5) (92.5%, 99.8%)	
11/23 04:24:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [181][175/175]	Step 32031	lr 0.02	Loss 0.2969 (0.2404)	Prec@(1,5) (92.3%, 99.7%)	
11/23 04:24:46AM evaluateStage_trainer.py:145 [INFO] Train: [181/599] Final Prec@1 92.2467%
11/23 04:24:49AM evaluateStage_trainer.py:180 [INFO] Valid: [181/599] Final Prec@1 57.2200%
11/23 04:24:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:25:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [182][50/175]	Step 32082	lr 0.01995	Loss 0.1466 (0.2244)	Prec@(1,5) (93.2%, 99.7%)	
11/23 04:25:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [182][100/175]	Step 32132	lr 0.01995	Loss 0.2212 (0.2249)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:25:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [182][150/175]	Step 32182	lr 0.01995	Loss 0.2364 (0.2275)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:25:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [182][175/175]	Step 32207	lr 0.01995	Loss 0.1851 (0.2301)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:25:32AM evaluateStage_trainer.py:145 [INFO] Train: [182/599] Final Prec@1 92.7600%
11/23 04:25:35AM evaluateStage_trainer.py:180 [INFO] Valid: [182/599] Final Prec@1 56.5000%
11/23 04:25:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:25:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [183][50/175]	Step 32258	lr 0.0199	Loss 0.2197 (0.2079)	Prec@(1,5) (93.5%, 99.9%)	
11/23 04:25:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [183][100/175]	Step 32308	lr 0.0199	Loss 0.2692 (0.2176)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:26:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [183][150/175]	Step 32358	lr 0.0199	Loss 0.2075 (0.2226)	Prec@(1,5) (92.9%, 99.8%)	
11/23 04:26:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [183][175/175]	Step 32383	lr 0.0199	Loss 0.2011 (0.2263)	Prec@(1,5) (92.8%, 99.8%)	
11/23 04:26:17AM evaluateStage_trainer.py:145 [INFO] Train: [183/599] Final Prec@1 92.7778%
11/23 04:26:20AM evaluateStage_trainer.py:180 [INFO] Valid: [183/599] Final Prec@1 56.2200%
11/23 04:26:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:26:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [184][50/175]	Step 32434	lr 0.01985	Loss 0.2156 (0.2179)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:26:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [184][100/175]	Step 32484	lr 0.01985	Loss 0.2505 (0.2257)	Prec@(1,5) (92.8%, 99.8%)	
11/23 04:26:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [184][150/175]	Step 32534	lr 0.01985	Loss 0.2231 (0.2346)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:27:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [184][175/175]	Step 32559	lr 0.01985	Loss 0.3868 (0.2410)	Prec@(1,5) (92.3%, 99.7%)	
11/23 04:27:03AM evaluateStage_trainer.py:145 [INFO] Train: [184/599] Final Prec@1 92.3000%
11/23 04:27:05AM evaluateStage_trainer.py:180 [INFO] Valid: [184/599] Final Prec@1 57.0000%
11/23 04:27:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:27:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [185][50/175]	Step 32610	lr 0.0198	Loss 0.2268 (0.2275)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:27:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [185][100/175]	Step 32660	lr 0.0198	Loss 0.2297 (0.2347)	Prec@(1,5) (92.5%, 99.7%)	
11/23 04:27:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [185][150/175]	Step 32710	lr 0.0198	Loss 0.2435 (0.2384)	Prec@(1,5) (92.4%, 99.7%)	
11/23 04:27:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [185][175/175]	Step 32735	lr 0.0198	Loss 0.2604 (0.2430)	Prec@(1,5) (92.2%, 99.7%)	
11/23 04:27:48AM evaluateStage_trainer.py:145 [INFO] Train: [185/599] Final Prec@1 92.2067%
11/23 04:27:51AM evaluateStage_trainer.py:180 [INFO] Valid: [185/599] Final Prec@1 55.4200%
11/23 04:27:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:28:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [186][50/175]	Step 32786	lr 0.01975	Loss 0.1709 (0.2030)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:28:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [186][100/175]	Step 32836	lr 0.01975	Loss 0.2709 (0.2142)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:28:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [186][150/175]	Step 32886	lr 0.01975	Loss 0.2258 (0.2261)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:28:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [186][175/175]	Step 32911	lr 0.01975	Loss 0.2672 (0.2302)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:28:34AM evaluateStage_trainer.py:145 [INFO] Train: [186/599] Final Prec@1 92.6356%
11/23 04:28:37AM evaluateStage_trainer.py:180 [INFO] Valid: [186/599] Final Prec@1 55.4200%
11/23 04:28:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:28:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [187][50/175]	Step 32962	lr 0.01969	Loss 0.1686 (0.2012)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:29:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [187][100/175]	Step 33012	lr 0.01969	Loss 0.2065 (0.2077)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:29:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [187][150/175]	Step 33062	lr 0.01969	Loss 0.3551 (0.2213)	Prec@(1,5) (93.0%, 99.7%)	
11/23 04:29:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [187][175/175]	Step 33087	lr 0.01969	Loss 0.2925 (0.2304)	Prec@(1,5) (92.7%, 99.7%)	
11/23 04:29:20AM evaluateStage_trainer.py:145 [INFO] Train: [187/599] Final Prec@1 92.6489%
11/23 04:29:23AM evaluateStage_trainer.py:180 [INFO] Valid: [187/599] Final Prec@1 55.0000%
11/23 04:29:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:29:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [188][50/175]	Step 33138	lr 0.01964	Loss 0.2251 (0.2186)	Prec@(1,5) (93.1%, 99.7%)	
11/23 04:29:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [188][100/175]	Step 33188	lr 0.01964	Loss 0.1868 (0.2146)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:30:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [188][150/175]	Step 33238	lr 0.01964	Loss 0.2555 (0.2174)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:30:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [188][175/175]	Step 33263	lr 0.01964	Loss 0.2011 (0.2193)	Prec@(1,5) (93.0%, 99.8%)	
11/23 04:30:06AM evaluateStage_trainer.py:145 [INFO] Train: [188/599] Final Prec@1 93.0444%
11/23 04:30:08AM evaluateStage_trainer.py:180 [INFO] Valid: [188/599] Final Prec@1 55.9600%
11/23 04:30:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:30:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [189][50/175]	Step 33314	lr 0.01959	Loss 0.1981 (0.1849)	Prec@(1,5) (94.1%, 99.9%)	
11/23 04:30:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [189][100/175]	Step 33364	lr 0.01959	Loss 0.2381 (0.1962)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:30:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [189][150/175]	Step 33414	lr 0.01959	Loss 0.1797 (0.2152)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:30:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [189][175/175]	Step 33439	lr 0.01959	Loss 0.2789 (0.2220)	Prec@(1,5) (92.9%, 99.8%)	
11/23 04:30:51AM evaluateStage_trainer.py:145 [INFO] Train: [189/599] Final Prec@1 92.8889%
11/23 04:30:54AM evaluateStage_trainer.py:180 [INFO] Valid: [189/599] Final Prec@1 56.5600%
11/23 04:30:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:31:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [190][50/175]	Step 33490	lr 0.01954	Loss 0.1853 (0.2063)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:31:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [190][100/175]	Step 33540	lr 0.01954	Loss 0.1958 (0.2120)	Prec@(1,5) (93.3%, 99.7%)	
11/23 04:31:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [190][150/175]	Step 33590	lr 0.01954	Loss 0.2132 (0.2246)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:31:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [190][175/175]	Step 33615	lr 0.01954	Loss 0.2500 (0.2316)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:31:37AM evaluateStage_trainer.py:145 [INFO] Train: [190/599] Final Prec@1 92.6311%
11/23 04:31:39AM evaluateStage_trainer.py:180 [INFO] Valid: [190/599] Final Prec@1 55.8800%
11/23 04:31:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:31:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [191][50/175]	Step 33666	lr 0.01948	Loss 0.1790 (0.2018)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:32:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [191][100/175]	Step 33716	lr 0.01948	Loss 0.2301 (0.2108)	Prec@(1,5) (93.3%, 99.7%)	
11/23 04:32:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [191][150/175]	Step 33766	lr 0.01948	Loss 0.2475 (0.2228)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:32:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [191][175/175]	Step 33791	lr 0.01948	Loss 0.2867 (0.2282)	Prec@(1,5) (92.7%, 99.7%)	
11/23 04:32:22AM evaluateStage_trainer.py:145 [INFO] Train: [191/599] Final Prec@1 92.6844%
11/23 04:32:25AM evaluateStage_trainer.py:180 [INFO] Valid: [191/599] Final Prec@1 55.2800%
11/23 04:32:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:32:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [192][50/175]	Step 33842	lr 0.01943	Loss 0.1957 (0.2009)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:32:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [192][100/175]	Step 33892	lr 0.01943	Loss 0.2259 (0.2104)	Prec@(1,5) (93.3%, 99.7%)	
11/23 04:33:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [192][150/175]	Step 33942	lr 0.01943	Loss 0.2610 (0.2247)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:33:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [192][175/175]	Step 33967	lr 0.01943	Loss 0.2470 (0.2317)	Prec@(1,5) (92.6%, 99.7%)	
11/23 04:33:07AM evaluateStage_trainer.py:145 [INFO] Train: [192/599] Final Prec@1 92.5733%
11/23 04:33:11AM evaluateStage_trainer.py:180 [INFO] Valid: [192/599] Final Prec@1 55.2400%
11/23 04:33:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:33:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [193][50/175]	Step 34018	lr 0.01938	Loss 0.2340 (0.2106)	Prec@(1,5) (93.7%, 99.7%)	
11/23 04:33:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [193][100/175]	Step 34068	lr 0.01938	Loss 0.1905 (0.2182)	Prec@(1,5) (93.3%, 99.7%)	
11/23 04:33:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [193][150/175]	Step 34118	lr 0.01938	Loss 0.2251 (0.2253)	Prec@(1,5) (93.0%, 99.7%)	
11/23 04:33:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [193][175/175]	Step 34143	lr 0.01938	Loss 0.3785 (0.2301)	Prec@(1,5) (92.8%, 99.7%)	
11/23 04:33:53AM evaluateStage_trainer.py:145 [INFO] Train: [193/599] Final Prec@1 92.7689%
11/23 04:33:57AM evaluateStage_trainer.py:180 [INFO] Valid: [193/599] Final Prec@1 56.1000%
11/23 04:33:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:34:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [194][50/175]	Step 34194	lr 0.01932	Loss 0.1784 (0.2193)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:34:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [194][100/175]	Step 34244	lr 0.01932	Loss 0.1517 (0.2189)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:34:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [194][150/175]	Step 34294	lr 0.01932	Loss 0.2802 (0.2227)	Prec@(1,5) (93.0%, 99.8%)	
11/23 04:34:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [194][175/175]	Step 34319	lr 0.01932	Loss 0.2710 (0.2297)	Prec@(1,5) (92.7%, 99.7%)	
11/23 04:34:39AM evaluateStage_trainer.py:145 [INFO] Train: [194/599] Final Prec@1 92.7133%
11/23 04:34:42AM evaluateStage_trainer.py:180 [INFO] Valid: [194/599] Final Prec@1 56.5600%
11/23 04:34:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:34:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [195][50/175]	Step 34370	lr 0.01927	Loss 0.2319 (0.1940)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:35:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [195][100/175]	Step 34420	lr 0.01927	Loss 0.2017 (0.2043)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:35:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [195][150/175]	Step 34470	lr 0.01927	Loss 0.2416 (0.2128)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:35:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [195][175/175]	Step 34495	lr 0.01927	Loss 0.3358 (0.2165)	Prec@(1,5) (93.2%, 99.8%)	
11/23 04:35:25AM evaluateStage_trainer.py:145 [INFO] Train: [195/599] Final Prec@1 93.2200%
11/23 04:35:28AM evaluateStage_trainer.py:180 [INFO] Valid: [195/599] Final Prec@1 56.3200%
11/23 04:35:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:35:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [196][50/175]	Step 34546	lr 0.01922	Loss 0.2110 (0.2029)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:35:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [196][100/175]	Step 34596	lr 0.01922	Loss 0.2368 (0.2112)	Prec@(1,5) (93.2%, 99.8%)	
11/23 04:36:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [196][150/175]	Step 34646	lr 0.01922	Loss 0.2547 (0.2198)	Prec@(1,5) (92.9%, 99.8%)	
11/23 04:36:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [196][175/175]	Step 34671	lr 0.01922	Loss 0.2943 (0.2259)	Prec@(1,5) (92.7%, 99.8%)	
11/23 04:36:11AM evaluateStage_trainer.py:145 [INFO] Train: [196/599] Final Prec@1 92.7089%
11/23 04:36:14AM evaluateStage_trainer.py:180 [INFO] Valid: [196/599] Final Prec@1 55.6200%
11/23 04:36:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6000%
11/23 04:36:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [197][50/175]	Step 34722	lr 0.01916	Loss 0.1857 (0.2040)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:36:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [197][100/175]	Step 34772	lr 0.01916	Loss 0.2515 (0.2043)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:36:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [197][150/175]	Step 34822	lr 0.01916	Loss 0.1935 (0.2144)	Prec@(1,5) (93.2%, 99.7%)	
11/23 04:36:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [197][175/175]	Step 34847	lr 0.01916	Loss 0.3055 (0.2207)	Prec@(1,5) (92.9%, 99.7%)	
11/23 04:36:56AM evaluateStage_trainer.py:145 [INFO] Train: [197/599] Final Prec@1 92.9200%
11/23 04:36:59AM evaluateStage_trainer.py:180 [INFO] Valid: [197/599] Final Prec@1 57.6600%
11/23 04:37:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:37:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [198][50/175]	Step 34898	lr 0.01911	Loss 0.1886 (0.2055)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:37:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [198][100/175]	Step 34948	lr 0.01911	Loss 0.1874 (0.2098)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:37:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [198][150/175]	Step 34998	lr 0.01911	Loss 0.2218 (0.2129)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:37:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [198][175/175]	Step 35023	lr 0.01911	Loss 0.2529 (0.2157)	Prec@(1,5) (93.2%, 99.8%)	
11/23 04:37:42AM evaluateStage_trainer.py:145 [INFO] Train: [198/599] Final Prec@1 93.2156%
11/23 04:37:45AM evaluateStage_trainer.py:180 [INFO] Valid: [198/599] Final Prec@1 55.5800%
11/23 04:37:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:37:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [199][50/175]	Step 35074	lr 0.01905	Loss 0.2010 (0.2048)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:38:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [199][100/175]	Step 35124	lr 0.01905	Loss 0.1649 (0.2077)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:38:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [199][150/175]	Step 35174	lr 0.01905	Loss 0.1663 (0.2150)	Prec@(1,5) (93.3%, 99.7%)	
11/23 04:38:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [199][175/175]	Step 35199	lr 0.01905	Loss 0.2654 (0.2170)	Prec@(1,5) (93.1%, 99.7%)	
11/23 04:38:28AM evaluateStage_trainer.py:145 [INFO] Train: [199/599] Final Prec@1 93.1067%
11/23 04:38:31AM evaluateStage_trainer.py:180 [INFO] Valid: [199/599] Final Prec@1 56.1800%
11/23 04:38:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:38:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [200][50/175]	Step 35250	lr 0.019	Loss 0.2133 (0.1865)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:38:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [200][100/175]	Step 35300	lr 0.019	Loss 0.2687 (0.1973)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:39:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [200][150/175]	Step 35350	lr 0.019	Loss 0.2181 (0.2031)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:39:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [200][175/175]	Step 35375	lr 0.019	Loss 0.3286 (0.2077)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:39:14AM evaluateStage_trainer.py:145 [INFO] Train: [200/599] Final Prec@1 93.4244%
11/23 04:39:17AM evaluateStage_trainer.py:180 [INFO] Valid: [200/599] Final Prec@1 55.6000%
11/23 04:39:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:39:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [201][50/175]	Step 35426	lr 0.01895	Loss 0.1924 (0.1906)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:39:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [201][100/175]	Step 35476	lr 0.01895	Loss 0.2025 (0.1983)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:39:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [201][150/175]	Step 35526	lr 0.01895	Loss 0.2449 (0.2022)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:39:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [201][175/175]	Step 35551	lr 0.01895	Loss 0.2379 (0.2075)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:40:00AM evaluateStage_trainer.py:145 [INFO] Train: [201/599] Final Prec@1 93.3933%
11/23 04:40:03AM evaluateStage_trainer.py:180 [INFO] Valid: [201/599] Final Prec@1 56.2200%
11/23 04:40:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:40:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [202][50/175]	Step 35602	lr 0.01889	Loss 0.1642 (0.1944)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:40:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [202][100/175]	Step 35652	lr 0.01889	Loss 0.1989 (0.2053)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:40:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [202][150/175]	Step 35702	lr 0.01889	Loss 0.2221 (0.2112)	Prec@(1,5) (93.2%, 99.8%)	
11/23 04:40:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [202][175/175]	Step 35727	lr 0.01889	Loss 0.3266 (0.2176)	Prec@(1,5) (93.0%, 99.8%)	
11/23 04:40:46AM evaluateStage_trainer.py:145 [INFO] Train: [202/599] Final Prec@1 92.9489%
11/23 04:40:48AM evaluateStage_trainer.py:180 [INFO] Valid: [202/599] Final Prec@1 53.6400%
11/23 04:40:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:41:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [203][50/175]	Step 35778	lr 0.01884	Loss 0.1640 (0.2038)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:41:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [203][100/175]	Step 35828	lr 0.01884	Loss 0.1943 (0.2069)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:41:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [203][150/175]	Step 35878	lr 0.01884	Loss 0.3176 (0.2135)	Prec@(1,5) (93.2%, 99.8%)	
11/23 04:41:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [203][175/175]	Step 35903	lr 0.01884	Loss 0.2565 (0.2168)	Prec@(1,5) (93.1%, 99.8%)	
11/23 04:41:31AM evaluateStage_trainer.py:145 [INFO] Train: [203/599] Final Prec@1 93.0467%
11/23 04:41:34AM evaluateStage_trainer.py:180 [INFO] Valid: [203/599] Final Prec@1 56.8400%
11/23 04:41:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.6600%
11/23 04:41:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [204][50/175]	Step 35954	lr 0.01878	Loss 0.1906 (0.1886)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:41:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [204][100/175]	Step 36004	lr 0.01878	Loss 0.2094 (0.1940)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:42:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [204][150/175]	Step 36054	lr 0.01878	Loss 0.2316 (0.2006)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:42:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [204][175/175]	Step 36079	lr 0.01878	Loss 0.2066 (0.2037)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:42:16AM evaluateStage_trainer.py:145 [INFO] Train: [204/599] Final Prec@1 93.6978%
11/23 04:42:20AM evaluateStage_trainer.py:180 [INFO] Valid: [204/599] Final Prec@1 57.8200%
11/23 04:42:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.8200%
11/23 04:42:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [205][50/175]	Step 36130	lr 0.01873	Loss 0.2193 (0.1731)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:42:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [205][100/175]	Step 36180	lr 0.01873	Loss 0.1569 (0.1778)	Prec@(1,5) (94.4%, 99.9%)	
11/23 04:42:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [205][150/175]	Step 36230	lr 0.01873	Loss 0.2172 (0.1848)	Prec@(1,5) (94.1%, 99.9%)	
11/23 04:43:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [205][175/175]	Step 36255	lr 0.01873	Loss 0.2407 (0.1899)	Prec@(1,5) (93.9%, 99.9%)	
11/23 04:43:03AM evaluateStage_trainer.py:145 [INFO] Train: [205/599] Final Prec@1 93.9467%
11/23 04:43:06AM evaluateStage_trainer.py:180 [INFO] Valid: [205/599] Final Prec@1 57.1600%
11/23 04:43:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.8200%
11/23 04:43:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [206][50/175]	Step 36306	lr 0.01867	Loss 0.1954 (0.2004)	Prec@(1,5) (93.5%, 99.9%)	
11/23 04:43:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [206][100/175]	Step 36356	lr 0.01867	Loss 0.1804 (0.2065)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:43:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [206][150/175]	Step 36406	lr 0.01867	Loss 0.1923 (0.2078)	Prec@(1,5) (93.5%, 99.8%)	
11/23 04:43:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [206][175/175]	Step 36431	lr 0.01867	Loss 0.1765 (0.2099)	Prec@(1,5) (93.4%, 99.8%)	
11/23 04:43:48AM evaluateStage_trainer.py:145 [INFO] Train: [206/599] Final Prec@1 93.3956%
11/23 04:43:52AM evaluateStage_trainer.py:180 [INFO] Valid: [206/599] Final Prec@1 57.4600%
11/23 04:43:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 57.8200%
11/23 04:44:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [207][50/175]	Step 36482	lr 0.01862	Loss 0.1499 (0.1819)	Prec@(1,5) (94.3%, 99.8%)	
11/23 04:44:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [207][100/175]	Step 36532	lr 0.01862	Loss 0.2032 (0.1792)	Prec@(1,5) (94.5%, 99.8%)	
11/23 04:44:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [207][150/175]	Step 36582	lr 0.01862	Loss 0.2578 (0.1876)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:44:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [207][175/175]	Step 36607	lr 0.01862	Loss 0.2383 (0.1944)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:44:34AM evaluateStage_trainer.py:145 [INFO] Train: [207/599] Final Prec@1 93.8378%
11/23 04:44:37AM evaluateStage_trainer.py:180 [INFO] Valid: [207/599] Final Prec@1 58.1400%
11/23 04:44:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:44:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [208][50/175]	Step 36658	lr 0.01856	Loss 0.1859 (0.1941)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:45:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [208][100/175]	Step 36708	lr 0.01856	Loss 0.2202 (0.1957)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:45:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [208][150/175]	Step 36758	lr 0.01856	Loss 0.2256 (0.1973)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:45:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [208][175/175]	Step 36783	lr 0.01856	Loss 0.2600 (0.2003)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:45:21AM evaluateStage_trainer.py:145 [INFO] Train: [208/599] Final Prec@1 93.6822%
11/23 04:45:24AM evaluateStage_trainer.py:180 [INFO] Valid: [208/599] Final Prec@1 56.7400%
11/23 04:45:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:45:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [209][50/175]	Step 36834	lr 0.0185	Loss 0.1935 (0.1700)	Prec@(1,5) (94.8%, 99.9%)	
11/23 04:45:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [209][100/175]	Step 36884	lr 0.0185	Loss 0.2051 (0.1772)	Prec@(1,5) (94.6%, 99.8%)	
11/23 04:46:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [209][150/175]	Step 36934	lr 0.0185	Loss 0.3039 (0.1899)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:46:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [209][175/175]	Step 36959	lr 0.0185	Loss 0.2296 (0.1977)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:46:07AM evaluateStage_trainer.py:145 [INFO] Train: [209/599] Final Prec@1 93.7800%
11/23 04:46:10AM evaluateStage_trainer.py:180 [INFO] Valid: [209/599] Final Prec@1 55.3000%
11/23 04:46:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:46:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [210][50/175]	Step 37010	lr 0.01845	Loss 0.1592 (0.1947)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:46:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [210][100/175]	Step 37060	lr 0.01845	Loss 0.2794 (0.1968)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:46:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [210][150/175]	Step 37110	lr 0.01845	Loss 0.3509 (0.2026)	Prec@(1,5) (93.6%, 99.8%)	
11/23 04:46:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [210][175/175]	Step 37135	lr 0.01845	Loss 0.2606 (0.2093)	Prec@(1,5) (93.3%, 99.8%)	
11/23 04:46:53AM evaluateStage_trainer.py:145 [INFO] Train: [210/599] Final Prec@1 93.3400%
11/23 04:46:56AM evaluateStage_trainer.py:180 [INFO] Valid: [210/599] Final Prec@1 56.0400%
11/23 04:46:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:47:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [211][50/175]	Step 37186	lr 0.01839	Loss 0.1629 (0.1817)	Prec@(1,5) (94.4%, 99.8%)	
11/23 04:47:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [211][100/175]	Step 37236	lr 0.01839	Loss 0.1885 (0.1799)	Prec@(1,5) (94.4%, 99.9%)	
11/23 04:47:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [211][150/175]	Step 37286	lr 0.01839	Loss 0.1829 (0.1853)	Prec@(1,5) (94.2%, 99.9%)	
11/23 04:47:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [211][175/175]	Step 37311	lr 0.01839	Loss 0.2032 (0.1902)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:47:39AM evaluateStage_trainer.py:145 [INFO] Train: [211/599] Final Prec@1 93.9800%
11/23 04:47:42AM evaluateStage_trainer.py:180 [INFO] Valid: [211/599] Final Prec@1 57.2200%
11/23 04:47:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:47:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [212][50/175]	Step 37362	lr 0.01834	Loss 0.2191 (0.1844)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:48:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [212][100/175]	Step 37412	lr 0.01834	Loss 0.1963 (0.1899)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:48:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [212][150/175]	Step 37462	lr 0.01834	Loss 0.1975 (0.1887)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:48:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [212][175/175]	Step 37487	lr 0.01834	Loss 0.2978 (0.1937)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:48:25AM evaluateStage_trainer.py:145 [INFO] Train: [212/599] Final Prec@1 93.8911%
11/23 04:48:27AM evaluateStage_trainer.py:180 [INFO] Valid: [212/599] Final Prec@1 56.5200%
11/23 04:48:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:48:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [213][50/175]	Step 37538	lr 0.01828	Loss 0.1265 (0.1812)	Prec@(1,5) (94.3%, 99.9%)	
11/23 04:48:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [213][100/175]	Step 37588	lr 0.01828	Loss 0.1731 (0.1757)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:49:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [213][150/175]	Step 37638	lr 0.01828	Loss 0.1657 (0.1826)	Prec@(1,5) (94.3%, 99.9%)	
11/23 04:49:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [213][175/175]	Step 37663	lr 0.01828	Loss 0.1993 (0.1874)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:49:10AM evaluateStage_trainer.py:145 [INFO] Train: [213/599] Final Prec@1 94.0489%
11/23 04:49:13AM evaluateStage_trainer.py:180 [INFO] Valid: [213/599] Final Prec@1 57.0800%
11/23 04:49:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:49:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [214][50/175]	Step 37714	lr 0.01822	Loss 0.1393 (0.1752)	Prec@(1,5) (94.6%, 99.8%)	
11/23 04:49:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [214][100/175]	Step 37764	lr 0.01822	Loss 0.2016 (0.1779)	Prec@(1,5) (94.6%, 99.8%)	
11/23 04:49:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [214][150/175]	Step 37814	lr 0.01822	Loss 0.1769 (0.1874)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:49:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [214][175/175]	Step 37839	lr 0.01822	Loss 0.2726 (0.1937)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:49:55AM evaluateStage_trainer.py:145 [INFO] Train: [214/599] Final Prec@1 93.9956%
11/23 04:49:59AM evaluateStage_trainer.py:180 [INFO] Valid: [214/599] Final Prec@1 56.4200%
11/23 04:49:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:50:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [215][50/175]	Step 37890	lr 0.01817	Loss 0.1536 (0.1840)	Prec@(1,5) (94.3%, 99.8%)	
11/23 04:50:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [215][100/175]	Step 37940	lr 0.01817	Loss 0.1794 (0.1909)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:50:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [215][150/175]	Step 37990	lr 0.01817	Loss 0.2397 (0.1969)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:50:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [215][175/175]	Step 38015	lr 0.01817	Loss 0.2789 (0.2015)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:50:41AM evaluateStage_trainer.py:145 [INFO] Train: [215/599] Final Prec@1 93.6644%
11/23 04:50:45AM evaluateStage_trainer.py:180 [INFO] Valid: [215/599] Final Prec@1 57.8600%
11/23 04:50:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:50:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [216][50/175]	Step 38066	lr 0.01811	Loss 0.1490 (0.1780)	Prec@(1,5) (94.5%, 99.8%)	
11/23 04:51:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [216][100/175]	Step 38116	lr 0.01811	Loss 0.1858 (0.1853)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:51:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [216][150/175]	Step 38166	lr 0.01811	Loss 0.2404 (0.1940)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:51:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [216][175/175]	Step 38191	lr 0.01811	Loss 0.1987 (0.1982)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:51:27AM evaluateStage_trainer.py:145 [INFO] Train: [216/599] Final Prec@1 93.7289%
11/23 04:51:30AM evaluateStage_trainer.py:180 [INFO] Valid: [216/599] Final Prec@1 56.4400%
11/23 04:51:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:51:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [217][50/175]	Step 38242	lr 0.01805	Loss 0.2146 (0.1866)	Prec@(1,5) (94.3%, 99.7%)	
11/23 04:51:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [217][100/175]	Step 38292	lr 0.01805	Loss 0.1323 (0.1895)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:52:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [217][150/175]	Step 38342	lr 0.01805	Loss 0.2107 (0.1950)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:52:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [217][175/175]	Step 38367	lr 0.01805	Loss 0.2414 (0.1996)	Prec@(1,5) (93.7%, 99.8%)	
11/23 04:52:13AM evaluateStage_trainer.py:145 [INFO] Train: [217/599] Final Prec@1 93.6956%
11/23 04:52:16AM evaluateStage_trainer.py:180 [INFO] Valid: [217/599] Final Prec@1 56.8200%
11/23 04:52:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:52:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [218][50/175]	Step 38418	lr 0.018	Loss 0.1990 (0.1743)	Prec@(1,5) (94.5%, 99.9%)	
11/23 04:52:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [218][100/175]	Step 38468	lr 0.018	Loss 0.1351 (0.1741)	Prec@(1,5) (94.7%, 99.8%)	
11/23 04:52:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [218][150/175]	Step 38518	lr 0.018	Loss 0.2258 (0.1810)	Prec@(1,5) (94.5%, 99.8%)	
11/23 04:52:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [218][175/175]	Step 38543	lr 0.018	Loss 0.2317 (0.1856)	Prec@(1,5) (94.3%, 99.8%)	
11/23 04:52:58AM evaluateStage_trainer.py:145 [INFO] Train: [218/599] Final Prec@1 94.2756%
11/23 04:53:01AM evaluateStage_trainer.py:180 [INFO] Valid: [218/599] Final Prec@1 54.8600%
11/23 04:53:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:53:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [219][50/175]	Step 38594	lr 0.01794	Loss 0.1851 (0.1780)	Prec@(1,5) (94.5%, 99.8%)	
11/23 04:53:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [219][100/175]	Step 38644	lr 0.01794	Loss 0.2556 (0.1829)	Prec@(1,5) (94.3%, 99.8%)	
11/23 04:53:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [219][150/175]	Step 38694	lr 0.01794	Loss 0.1880 (0.1916)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:53:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [219][175/175]	Step 38719	lr 0.01794	Loss 0.2384 (0.1972)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:53:44AM evaluateStage_trainer.py:145 [INFO] Train: [219/599] Final Prec@1 93.7622%
11/23 04:53:47AM evaluateStage_trainer.py:180 [INFO] Valid: [219/599] Final Prec@1 54.6800%
11/23 04:53:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:54:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [220][50/175]	Step 38770	lr 0.01788	Loss 0.1696 (0.1729)	Prec@(1,5) (94.6%, 99.8%)	
11/23 04:54:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [220][100/175]	Step 38820	lr 0.01788	Loss 0.2046 (0.1765)	Prec@(1,5) (94.5%, 99.8%)	
11/23 04:54:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [220][150/175]	Step 38870	lr 0.01788	Loss 0.2128 (0.1901)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:54:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [220][175/175]	Step 38895	lr 0.01788	Loss 0.2115 (0.1951)	Prec@(1,5) (93.8%, 99.8%)	
11/23 04:54:29AM evaluateStage_trainer.py:145 [INFO] Train: [220/599] Final Prec@1 93.7711%
11/23 04:54:33AM evaluateStage_trainer.py:180 [INFO] Valid: [220/599] Final Prec@1 56.6800%
11/23 04:54:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:54:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [221][50/175]	Step 38946	lr 0.01782	Loss 0.2285 (0.1799)	Prec@(1,5) (94.3%, 99.8%)	
11/23 04:54:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [221][100/175]	Step 38996	lr 0.01782	Loss 0.1923 (0.1817)	Prec@(1,5) (94.4%, 99.8%)	
11/23 04:55:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [221][150/175]	Step 39046	lr 0.01782	Loss 0.1616 (0.1881)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:55:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [221][175/175]	Step 39071	lr 0.01782	Loss 0.1806 (0.1918)	Prec@(1,5) (93.9%, 99.8%)	
11/23 04:55:15AM evaluateStage_trainer.py:145 [INFO] Train: [221/599] Final Prec@1 93.9222%
11/23 04:55:18AM evaluateStage_trainer.py:180 [INFO] Valid: [221/599] Final Prec@1 56.6600%
11/23 04:55:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:55:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [222][50/175]	Step 39122	lr 0.01777	Loss 0.1693 (0.1661)	Prec@(1,5) (94.9%, 99.9%)	
11/23 04:55:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [222][100/175]	Step 39172	lr 0.01777	Loss 0.1665 (0.1708)	Prec@(1,5) (94.7%, 99.9%)	
11/23 04:55:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [222][150/175]	Step 39222	lr 0.01777	Loss 0.2810 (0.1762)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:56:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [222][175/175]	Step 39247	lr 0.01777	Loss 0.1947 (0.1792)	Prec@(1,5) (94.4%, 99.9%)	
11/23 04:56:01AM evaluateStage_trainer.py:145 [INFO] Train: [222/599] Final Prec@1 94.3778%
11/23 04:56:03AM evaluateStage_trainer.py:180 [INFO] Valid: [222/599] Final Prec@1 57.0800%
11/23 04:56:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.1400%
11/23 04:56:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [223][50/175]	Step 39298	lr 0.01771	Loss 0.1548 (0.1691)	Prec@(1,5) (94.7%, 99.9%)	
11/23 04:56:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [223][100/175]	Step 39348	lr 0.01771	Loss 0.1630 (0.1698)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:56:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [223][150/175]	Step 39398	lr 0.01771	Loss 0.1650 (0.1715)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:56:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [223][175/175]	Step 39423	lr 0.01771	Loss 0.2470 (0.1746)	Prec@(1,5) (94.5%, 99.9%)	
11/23 04:56:46AM evaluateStage_trainer.py:145 [INFO] Train: [223/599] Final Prec@1 94.5000%
11/23 04:56:49AM evaluateStage_trainer.py:180 [INFO] Valid: [223/599] Final Prec@1 58.8800%
11/23 04:56:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.8800%
11/23 04:57:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [224][50/175]	Step 39474	lr 0.01765	Loss 0.1722 (0.1637)	Prec@(1,5) (94.9%, 99.9%)	
11/23 04:57:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [224][100/175]	Step 39524	lr 0.01765	Loss 0.1932 (0.1679)	Prec@(1,5) (94.7%, 99.9%)	
11/23 04:57:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [224][150/175]	Step 39574	lr 0.01765	Loss 0.2169 (0.1728)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:57:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [224][175/175]	Step 39599	lr 0.01765	Loss 0.2040 (0.1782)	Prec@(1,5) (94.4%, 99.9%)	
11/23 04:57:33AM evaluateStage_trainer.py:145 [INFO] Train: [224/599] Final Prec@1 94.4000%
11/23 04:57:35AM evaluateStage_trainer.py:180 [INFO] Valid: [224/599] Final Prec@1 57.2000%
11/23 04:57:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.8800%
11/23 04:57:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [225][50/175]	Step 39650	lr 0.01759	Loss 0.1756 (0.1836)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:58:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [225][100/175]	Step 39700	lr 0.01759	Loss 0.1652 (0.1842)	Prec@(1,5) (94.2%, 99.8%)	
11/23 04:58:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [225][150/175]	Step 39750	lr 0.01759	Loss 0.1767 (0.1870)	Prec@(1,5) (94.1%, 99.8%)	
11/23 04:58:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [225][175/175]	Step 39775	lr 0.01759	Loss 0.2664 (0.1913)	Prec@(1,5) (94.0%, 99.8%)	
11/23 04:58:18AM evaluateStage_trainer.py:145 [INFO] Train: [225/599] Final Prec@1 93.9889%
11/23 04:58:21AM evaluateStage_trainer.py:180 [INFO] Valid: [225/599] Final Prec@1 57.2000%
11/23 04:58:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.8800%
11/23 04:58:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [226][50/175]	Step 39826	lr 0.01753	Loss 0.1419 (0.1648)	Prec@(1,5) (94.8%, 99.9%)	
11/23 04:58:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [226][100/175]	Step 39876	lr 0.01753	Loss 0.2052 (0.1671)	Prec@(1,5) (94.8%, 99.9%)	
11/23 04:58:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [226][150/175]	Step 39926	lr 0.01753	Loss 0.1715 (0.1732)	Prec@(1,5) (94.5%, 99.9%)	
11/23 04:59:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [226][175/175]	Step 39951	lr 0.01753	Loss 0.1735 (0.1798)	Prec@(1,5) (94.2%, 99.9%)	
11/23 04:59:04AM evaluateStage_trainer.py:145 [INFO] Train: [226/599] Final Prec@1 94.2378%
11/23 04:59:07AM evaluateStage_trainer.py:180 [INFO] Valid: [226/599] Final Prec@1 56.9800%
11/23 04:59:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.8800%
11/23 04:59:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [227][50/175]	Step 40002	lr 0.01748	Loss 0.1531 (0.1654)	Prec@(1,5) (95.0%, 99.8%)	
11/23 04:59:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [227][100/175]	Step 40052	lr 0.01748	Loss 0.1323 (0.1651)	Prec@(1,5) (95.0%, 99.9%)	
11/23 04:59:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [227][150/175]	Step 40102	lr 0.01748	Loss 0.1760 (0.1713)	Prec@(1,5) (94.8%, 99.8%)	
11/23 04:59:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [227][175/175]	Step 40127	lr 0.01748	Loss 0.2152 (0.1738)	Prec@(1,5) (94.6%, 99.9%)	
11/23 04:59:50AM evaluateStage_trainer.py:145 [INFO] Train: [227/599] Final Prec@1 94.6511%
11/23 04:59:52AM evaluateStage_trainer.py:180 [INFO] Valid: [227/599] Final Prec@1 56.2600%
11/23 04:59:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 58.8800%
11/23 05:00:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [228][50/175]	Step 40178	lr 0.01742	Loss 0.1565 (0.1504)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:00:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [228][100/175]	Step 40228	lr 0.01742	Loss 0.1556 (0.1524)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:00:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [228][150/175]	Step 40278	lr 0.01742	Loss 0.1509 (0.1581)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:00:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [228][175/175]	Step 40303	lr 0.01742	Loss 0.1838 (0.1608)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:00:35AM evaluateStage_trainer.py:145 [INFO] Train: [228/599] Final Prec@1 95.0267%
11/23 05:00:38AM evaluateStage_trainer.py:180 [INFO] Valid: [228/599] Final Prec@1 59.8600%
11/23 05:00:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:00:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [229][50/175]	Step 40354	lr 0.01736	Loss 0.1634 (0.1460)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:01:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [229][100/175]	Step 40404	lr 0.01736	Loss 0.1704 (0.1537)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:01:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [229][150/175]	Step 40454	lr 0.01736	Loss 0.1476 (0.1590)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:01:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [229][175/175]	Step 40479	lr 0.01736	Loss 0.2068 (0.1636)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:01:21AM evaluateStage_trainer.py:145 [INFO] Train: [229/599] Final Prec@1 94.9956%
11/23 05:01:24AM evaluateStage_trainer.py:180 [INFO] Valid: [229/599] Final Prec@1 55.3800%
11/23 05:01:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:01:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [230][50/175]	Step 40530	lr 0.0173	Loss 0.1536 (0.1576)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:01:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [230][100/175]	Step 40580	lr 0.0173	Loss 0.1662 (0.1629)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:02:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [230][150/175]	Step 40630	lr 0.0173	Loss 0.1996 (0.1690)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:02:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [230][175/175]	Step 40655	lr 0.0173	Loss 0.1784 (0.1715)	Prec@(1,5) (94.7%, 99.9%)	
11/23 05:02:07AM evaluateStage_trainer.py:145 [INFO] Train: [230/599] Final Prec@1 94.6778%
11/23 05:02:11AM evaluateStage_trainer.py:180 [INFO] Valid: [230/599] Final Prec@1 56.9600%
11/23 05:02:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:02:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [231][50/175]	Step 40706	lr 0.01724	Loss 0.1771 (0.1559)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:02:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [231][100/175]	Step 40756	lr 0.01724	Loss 0.1531 (0.1598)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:02:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [231][150/175]	Step 40806	lr 0.01724	Loss 0.1816 (0.1633)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:02:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [231][175/175]	Step 40831	lr 0.01724	Loss 0.2259 (0.1676)	Prec@(1,5) (94.9%, 99.9%)	
11/23 05:02:53AM evaluateStage_trainer.py:145 [INFO] Train: [231/599] Final Prec@1 94.8689%
11/23 05:02:57AM evaluateStage_trainer.py:180 [INFO] Valid: [231/599] Final Prec@1 57.4600%
11/23 05:02:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:03:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [232][50/175]	Step 40882	lr 0.01718	Loss 0.2107 (0.1414)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:03:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [232][100/175]	Step 40932	lr 0.01718	Loss 0.2182 (0.1507)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:03:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [232][150/175]	Step 40982	lr 0.01718	Loss 0.2814 (0.1651)	Prec@(1,5) (94.9%, 99.9%)	
11/23 05:03:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [232][175/175]	Step 41007	lr 0.01718	Loss 0.1743 (0.1684)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:03:39AM evaluateStage_trainer.py:145 [INFO] Train: [232/599] Final Prec@1 94.8089%
11/23 05:03:42AM evaluateStage_trainer.py:180 [INFO] Valid: [232/599] Final Prec@1 56.6200%
11/23 05:03:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:03:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [233][50/175]	Step 41058	lr 0.01712	Loss 0.1310 (0.1469)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:04:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [233][100/175]	Step 41108	lr 0.01712	Loss 0.1670 (0.1491)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:04:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [233][150/175]	Step 41158	lr 0.01712	Loss 0.1820 (0.1483)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:04:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [233][175/175]	Step 41183	lr 0.01712	Loss 0.2394 (0.1521)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:04:25AM evaluateStage_trainer.py:145 [INFO] Train: [233/599] Final Prec@1 95.5133%
11/23 05:04:28AM evaluateStage_trainer.py:180 [INFO] Valid: [233/599] Final Prec@1 57.4000%
11/23 05:04:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:04:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [234][50/175]	Step 41234	lr 0.01706	Loss 0.1516 (0.1449)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:04:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [234][100/175]	Step 41284	lr 0.01706	Loss 0.1335 (0.1469)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:05:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [234][150/175]	Step 41334	lr 0.01706	Loss 0.1708 (0.1511)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:05:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [234][175/175]	Step 41359	lr 0.01706	Loss 0.1645 (0.1557)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:05:10AM evaluateStage_trainer.py:145 [INFO] Train: [234/599] Final Prec@1 95.1911%
11/23 05:05:14AM evaluateStage_trainer.py:180 [INFO] Valid: [234/599] Final Prec@1 56.9000%
11/23 05:05:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:05:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [235][50/175]	Step 41410	lr 0.01701	Loss 0.1226 (0.1535)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:05:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [235][100/175]	Step 41460	lr 0.01701	Loss 0.2109 (0.1587)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:05:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [235][150/175]	Step 41510	lr 0.01701	Loss 0.1425 (0.1651)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:05:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [235][175/175]	Step 41535	lr 0.01701	Loss 0.2617 (0.1693)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:05:56AM evaluateStage_trainer.py:145 [INFO] Train: [235/599] Final Prec@1 94.8222%
11/23 05:06:00AM evaluateStage_trainer.py:180 [INFO] Valid: [235/599] Final Prec@1 57.1600%
11/23 05:06:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:06:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [236][50/175]	Step 41586	lr 0.01695	Loss 0.1516 (0.1573)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:06:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [236][100/175]	Step 41636	lr 0.01695	Loss 0.1819 (0.1574)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:06:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [236][150/175]	Step 41686	lr 0.01695	Loss 0.2713 (0.1652)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:06:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [236][175/175]	Step 41711	lr 0.01695	Loss 0.2545 (0.1712)	Prec@(1,5) (94.6%, 99.9%)	
11/23 05:06:42AM evaluateStage_trainer.py:145 [INFO] Train: [236/599] Final Prec@1 94.6222%
11/23 05:06:46AM evaluateStage_trainer.py:180 [INFO] Valid: [236/599] Final Prec@1 56.0400%
11/23 05:06:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:06:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [237][50/175]	Step 41762	lr 0.01689	Loss 0.1490 (0.1711)	Prec@(1,5) (94.8%, 99.8%)	
11/23 05:07:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [237][100/175]	Step 41812	lr 0.01689	Loss 0.2331 (0.1742)	Prec@(1,5) (94.6%, 99.9%)	
11/23 05:07:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [237][150/175]	Step 41862	lr 0.01689	Loss 0.1587 (0.1775)	Prec@(1,5) (94.4%, 99.9%)	
11/23 05:07:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [237][175/175]	Step 41887	lr 0.01689	Loss 0.1203 (0.1784)	Prec@(1,5) (94.4%, 99.9%)	
11/23 05:07:28AM evaluateStage_trainer.py:145 [INFO] Train: [237/599] Final Prec@1 94.4333%
11/23 05:07:32AM evaluateStage_trainer.py:180 [INFO] Valid: [237/599] Final Prec@1 58.0800%
11/23 05:07:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:07:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [238][50/175]	Step 41938	lr 0.01683	Loss 0.1477 (0.1396)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:07:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [238][100/175]	Step 41988	lr 0.01683	Loss 0.1309 (0.1442)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:08:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [238][150/175]	Step 42038	lr 0.01683	Loss 0.1492 (0.1479)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:08:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [238][175/175]	Step 42063	lr 0.01683	Loss 0.1413 (0.1505)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:08:14AM evaluateStage_trainer.py:145 [INFO] Train: [238/599] Final Prec@1 95.5267%
11/23 05:08:18AM evaluateStage_trainer.py:180 [INFO] Valid: [238/599] Final Prec@1 57.2000%
11/23 05:08:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:08:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [239][50/175]	Step 42114	lr 0.01677	Loss 0.1488 (0.1411)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:08:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [239][100/175]	Step 42164	lr 0.01677	Loss 0.2214 (0.1470)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:08:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [239][150/175]	Step 42214	lr 0.01677	Loss 0.1598 (0.1574)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:09:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [239][175/175]	Step 42239	lr 0.01677	Loss 0.1537 (0.1611)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:09:00AM evaluateStage_trainer.py:145 [INFO] Train: [239/599] Final Prec@1 95.0067%
11/23 05:09:04AM evaluateStage_trainer.py:180 [INFO] Valid: [239/599] Final Prec@1 57.8600%
11/23 05:09:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:09:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [240][50/175]	Step 42290	lr 0.01671	Loss 0.1303 (0.1442)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:09:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [240][100/175]	Step 42340	lr 0.01671	Loss 0.1502 (0.1464)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:09:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [240][150/175]	Step 42390	lr 0.01671	Loss 0.1972 (0.1515)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:09:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [240][175/175]	Step 42415	lr 0.01671	Loss 0.1600 (0.1556)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:09:46AM evaluateStage_trainer.py:145 [INFO] Train: [240/599] Final Prec@1 95.1756%
11/23 05:09:50AM evaluateStage_trainer.py:180 [INFO] Valid: [240/599] Final Prec@1 55.6400%
11/23 05:09:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:10:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [241][50/175]	Step 42466	lr 0.01665	Loss 0.1579 (0.1463)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:10:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [241][100/175]	Step 42516	lr 0.01665	Loss 0.1122 (0.1489)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:10:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [241][150/175]	Step 42566	lr 0.01665	Loss 0.1773 (0.1511)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:10:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [241][175/175]	Step 42591	lr 0.01665	Loss 0.1650 (0.1526)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:10:32AM evaluateStage_trainer.py:145 [INFO] Train: [241/599] Final Prec@1 95.2267%
11/23 05:10:36AM evaluateStage_trainer.py:180 [INFO] Valid: [241/599] Final Prec@1 57.5800%
11/23 05:10:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:10:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [242][50/175]	Step 42642	lr 0.01659	Loss 0.1279 (0.1484)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:11:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [242][100/175]	Step 42692	lr 0.01659	Loss 0.1623 (0.1415)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:11:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [242][150/175]	Step 42742	lr 0.01659	Loss 0.2145 (0.1486)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:11:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [242][175/175]	Step 42767	lr 0.01659	Loss 0.1368 (0.1522)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:11:18AM evaluateStage_trainer.py:145 [INFO] Train: [242/599] Final Prec@1 95.3778%
11/23 05:11:21AM evaluateStage_trainer.py:180 [INFO] Valid: [242/599] Final Prec@1 57.3000%
11/23 05:11:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:11:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [243][50/175]	Step 42818	lr 0.01653	Loss 0.1424 (0.1439)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:11:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [243][100/175]	Step 42868	lr 0.01653	Loss 0.1744 (0.1481)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:11:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [243][150/175]	Step 42918	lr 0.01653	Loss 0.1710 (0.1557)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:12:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [243][175/175]	Step 42943	lr 0.01653	Loss 0.1446 (0.1593)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:12:04AM evaluateStage_trainer.py:145 [INFO] Train: [243/599] Final Prec@1 95.1222%
11/23 05:12:08AM evaluateStage_trainer.py:180 [INFO] Valid: [243/599] Final Prec@1 56.3600%
11/23 05:12:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:12:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [244][50/175]	Step 42994	lr 0.01647	Loss 0.1359 (0.1472)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:12:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [244][100/175]	Step 43044	lr 0.01647	Loss 0.1874 (0.1442)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:12:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [244][150/175]	Step 43094	lr 0.01647	Loss 0.1666 (0.1503)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:12:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [244][175/175]	Step 43119	lr 0.01647	Loss 0.1334 (0.1537)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:12:50AM evaluateStage_trainer.py:145 [INFO] Train: [244/599] Final Prec@1 95.2200%
11/23 05:12:53AM evaluateStage_trainer.py:180 [INFO] Valid: [244/599] Final Prec@1 58.5400%
11/23 05:12:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:13:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [245][50/175]	Step 43170	lr 0.01641	Loss 0.1275 (0.1501)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:13:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [245][100/175]	Step 43220	lr 0.01641	Loss 0.1537 (0.1485)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:13:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [245][150/175]	Step 43270	lr 0.01641	Loss 0.1950 (0.1503)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:13:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [245][175/175]	Step 43295	lr 0.01641	Loss 0.1629 (0.1530)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:13:36AM evaluateStage_trainer.py:145 [INFO] Train: [245/599] Final Prec@1 95.2956%
11/23 05:13:39AM evaluateStage_trainer.py:180 [INFO] Valid: [245/599] Final Prec@1 58.2000%
11/23 05:13:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:13:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [246][50/175]	Step 43346	lr 0.01635	Loss 0.1148 (0.1243)	Prec@(1,5) (96.3%, 100.0%)	
11/23 05:14:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [246][100/175]	Step 43396	lr 0.01635	Loss 0.1291 (0.1290)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:14:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [246][150/175]	Step 43446	lr 0.01635	Loss 0.1545 (0.1368)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:14:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [246][175/175]	Step 43471	lr 0.01635	Loss 0.1811 (0.1414)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:14:22AM evaluateStage_trainer.py:145 [INFO] Train: [246/599] Final Prec@1 95.7400%
11/23 05:14:25AM evaluateStage_trainer.py:180 [INFO] Valid: [246/599] Final Prec@1 57.4000%
11/23 05:14:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:14:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [247][50/175]	Step 43522	lr 0.01629	Loss 0.1219 (0.1370)	Prec@(1,5) (95.8%, 100.0%)	
11/23 05:14:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [247][100/175]	Step 43572	lr 0.01629	Loss 0.1531 (0.1372)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:15:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [247][150/175]	Step 43622	lr 0.01629	Loss 0.2112 (0.1393)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:15:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [247][175/175]	Step 43647	lr 0.01629	Loss 0.1449 (0.1430)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:15:08AM evaluateStage_trainer.py:145 [INFO] Train: [247/599] Final Prec@1 95.6956%
11/23 05:15:11AM evaluateStage_trainer.py:180 [INFO] Valid: [247/599] Final Prec@1 58.7400%
11/23 05:15:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:15:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [248][50/175]	Step 43698	lr 0.01623	Loss 0.1162 (0.1445)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:15:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [248][100/175]	Step 43748	lr 0.01623	Loss 0.1379 (0.1424)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:15:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [248][150/175]	Step 43798	lr 0.01623	Loss 0.1408 (0.1466)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:15:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [248][175/175]	Step 43823	lr 0.01623	Loss 0.1209 (0.1495)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:15:54AM evaluateStage_trainer.py:145 [INFO] Train: [248/599] Final Prec@1 95.4200%
11/23 05:15:57AM evaluateStage_trainer.py:180 [INFO] Valid: [248/599] Final Prec@1 57.3800%
11/23 05:15:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:16:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [249][50/175]	Step 43874	lr 0.01617	Loss 0.0951 (0.1254)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:16:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [249][100/175]	Step 43924	lr 0.01617	Loss 0.1437 (0.1303)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:16:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [249][150/175]	Step 43974	lr 0.01617	Loss 0.1652 (0.1356)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:16:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [249][175/175]	Step 43999	lr 0.01617	Loss 0.1529 (0.1386)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:16:40AM evaluateStage_trainer.py:145 [INFO] Train: [249/599] Final Prec@1 95.7644%
11/23 05:16:43AM evaluateStage_trainer.py:180 [INFO] Valid: [249/599] Final Prec@1 59.0000%
11/23 05:16:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:16:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [250][50/175]	Step 44050	lr 0.01611	Loss 0.1344 (0.1349)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:17:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [250][100/175]	Step 44100	lr 0.01611	Loss 0.1353 (0.1367)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:17:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [250][150/175]	Step 44150	lr 0.01611	Loss 0.1298 (0.1423)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:17:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [250][175/175]	Step 44175	lr 0.01611	Loss 0.1351 (0.1444)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:17:26AM evaluateStage_trainer.py:145 [INFO] Train: [250/599] Final Prec@1 95.6644%
11/23 05:17:29AM evaluateStage_trainer.py:180 [INFO] Valid: [250/599] Final Prec@1 59.4000%
11/23 05:17:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:17:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [251][50/175]	Step 44226	lr 0.01605	Loss 0.0833 (0.1357)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:17:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [251][100/175]	Step 44276	lr 0.01605	Loss 0.1167 (0.1366)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:18:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [251][150/175]	Step 44326	lr 0.01605	Loss 0.1815 (0.1446)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:18:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [251][175/175]	Step 44351	lr 0.01605	Loss 0.2308 (0.1482)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:18:11AM evaluateStage_trainer.py:145 [INFO] Train: [251/599] Final Prec@1 95.4956%
11/23 05:18:15AM evaluateStage_trainer.py:180 [INFO] Valid: [251/599] Final Prec@1 58.2400%
11/23 05:18:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:18:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [252][50/175]	Step 44402	lr 0.01598	Loss 0.1281 (0.1484)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:18:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [252][100/175]	Step 44452	lr 0.01598	Loss 0.1567 (0.1398)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:18:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [252][150/175]	Step 44502	lr 0.01598	Loss 0.1628 (0.1468)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:18:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [252][175/175]	Step 44527	lr 0.01598	Loss 0.1410 (0.1520)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:18:57AM evaluateStage_trainer.py:145 [INFO] Train: [252/599] Final Prec@1 95.3444%
11/23 05:19:01AM evaluateStage_trainer.py:180 [INFO] Valid: [252/599] Final Prec@1 58.3800%
11/23 05:19:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:19:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [253][50/175]	Step 44578	lr 0.01592	Loss 0.1483 (0.1558)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:19:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [253][100/175]	Step 44628	lr 0.01592	Loss 0.1959 (0.1579)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:19:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [253][150/175]	Step 44678	lr 0.01592	Loss 0.2279 (0.1601)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:19:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [253][175/175]	Step 44703	lr 0.01592	Loss 0.2090 (0.1620)	Prec@(1,5) (94.9%, 99.9%)	
11/23 05:19:43AM evaluateStage_trainer.py:145 [INFO] Train: [253/599] Final Prec@1 94.9044%
11/23 05:19:47AM evaluateStage_trainer.py:180 [INFO] Valid: [253/599] Final Prec@1 56.0200%
11/23 05:19:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:20:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [254][50/175]	Step 44754	lr 0.01586	Loss 0.1911 (0.1398)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:20:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [254][100/175]	Step 44804	lr 0.01586	Loss 0.1168 (0.1472)	Prec@(1,5) (95.5%, 99.9%)	
11/23 05:20:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [254][150/175]	Step 44854	lr 0.01586	Loss 0.1453 (0.1531)	Prec@(1,5) (95.3%, 99.9%)	
11/23 05:20:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [254][175/175]	Step 44879	lr 0.01586	Loss 0.2429 (0.1592)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:20:29AM evaluateStage_trainer.py:145 [INFO] Train: [254/599] Final Prec@1 95.0467%
11/23 05:20:33AM evaluateStage_trainer.py:180 [INFO] Valid: [254/599] Final Prec@1 56.1400%
11/23 05:20:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:20:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [255][50/175]	Step 44930	lr 0.0158	Loss 0.1346 (0.1529)	Prec@(1,5) (95.4%, 99.9%)	
11/23 05:20:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [255][100/175]	Step 44980	lr 0.0158	Loss 0.1541 (0.1578)	Prec@(1,5) (95.1%, 99.9%)	
11/23 05:21:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [255][150/175]	Step 45030	lr 0.0158	Loss 0.2115 (0.1643)	Prec@(1,5) (95.0%, 99.9%)	
11/23 05:21:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [255][175/175]	Step 45055	lr 0.0158	Loss 0.1608 (0.1675)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:21:15AM evaluateStage_trainer.py:145 [INFO] Train: [255/599] Final Prec@1 94.7978%
11/23 05:21:18AM evaluateStage_trainer.py:180 [INFO] Valid: [255/599] Final Prec@1 56.8200%
11/23 05:21:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:21:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [256][50/175]	Step 45106	lr 0.01574	Loss 0.1583 (0.1595)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:21:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [256][100/175]	Step 45156	lr 0.01574	Loss 0.1578 (0.1602)	Prec@(1,5) (95.2%, 99.9%)	
11/23 05:21:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [256][150/175]	Step 45206	lr 0.01574	Loss 0.2880 (0.1666)	Prec@(1,5) (94.9%, 99.9%)	
11/23 05:22:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [256][175/175]	Step 45231	lr 0.01574	Loss 0.1842 (0.1689)	Prec@(1,5) (94.8%, 99.9%)	
11/23 05:22:01AM evaluateStage_trainer.py:145 [INFO] Train: [256/599] Final Prec@1 94.8244%
11/23 05:22:04AM evaluateStage_trainer.py:180 [INFO] Valid: [256/599] Final Prec@1 58.5400%
11/23 05:22:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:22:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [257][50/175]	Step 45282	lr 0.01568	Loss 0.1431 (0.1381)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:22:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [257][100/175]	Step 45332	lr 0.01568	Loss 0.1411 (0.1389)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:22:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [257][150/175]	Step 45382	lr 0.01568	Loss 0.1465 (0.1422)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:22:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [257][175/175]	Step 45407	lr 0.01568	Loss 0.1612 (0.1445)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:22:47AM evaluateStage_trainer.py:145 [INFO] Train: [257/599] Final Prec@1 95.6356%
11/23 05:22:50AM evaluateStage_trainer.py:180 [INFO] Valid: [257/599] Final Prec@1 58.9000%
11/23 05:22:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:23:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [258][50/175]	Step 45458	lr 0.01562	Loss 0.0838 (0.1318)	Prec@(1,5) (96.0%, 100.0%)	
11/23 05:23:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [258][100/175]	Step 45508	lr 0.01562	Loss 0.1712 (0.1349)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:23:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [258][150/175]	Step 45558	lr 0.01562	Loss 0.1405 (0.1379)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:23:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [258][175/175]	Step 45583	lr 0.01562	Loss 0.1030 (0.1388)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:23:33AM evaluateStage_trainer.py:145 [INFO] Train: [258/599] Final Prec@1 95.7689%
11/23 05:23:36AM evaluateStage_trainer.py:180 [INFO] Valid: [258/599] Final Prec@1 57.6000%
11/23 05:23:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:23:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [259][50/175]	Step 45634	lr 0.01556	Loss 0.0846 (0.1150)	Prec@(1,5) (96.6%, 100.0%)	
11/23 05:24:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [259][100/175]	Step 45684	lr 0.01556	Loss 0.0995 (0.1145)	Prec@(1,5) (96.7%, 100.0%)	
11/23 05:24:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [259][150/175]	Step 45734	lr 0.01556	Loss 0.1242 (0.1164)	Prec@(1,5) (96.6%, 100.0%)	
11/23 05:24:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [259][175/175]	Step 45759	lr 0.01556	Loss 0.1672 (0.1212)	Prec@(1,5) (96.5%, 100.0%)	
11/23 05:24:19AM evaluateStage_trainer.py:145 [INFO] Train: [259/599] Final Prec@1 96.4467%
11/23 05:24:22AM evaluateStage_trainer.py:180 [INFO] Valid: [259/599] Final Prec@1 57.7800%
11/23 05:24:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:24:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [260][50/175]	Step 45810	lr 0.01549	Loss 0.1687 (0.1282)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:24:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [260][100/175]	Step 45860	lr 0.01549	Loss 0.1286 (0.1309)	Prec@(1,5) (96.2%, 99.9%)	
11/23 05:24:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [260][150/175]	Step 45910	lr 0.01549	Loss 0.1141 (0.1347)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:25:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [260][175/175]	Step 45935	lr 0.01549	Loss 0.1928 (0.1360)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:25:05AM evaluateStage_trainer.py:145 [INFO] Train: [260/599] Final Prec@1 95.9289%
11/23 05:25:08AM evaluateStage_trainer.py:180 [INFO] Valid: [260/599] Final Prec@1 59.7400%
11/23 05:25:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:25:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [261][50/175]	Step 45986	lr 0.01543	Loss 0.1147 (0.1322)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:25:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [261][100/175]	Step 46036	lr 0.01543	Loss 0.1711 (0.1343)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:25:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [261][150/175]	Step 46086	lr 0.01543	Loss 0.1290 (0.1380)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:25:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [261][175/175]	Step 46111	lr 0.01543	Loss 0.2219 (0.1396)	Prec@(1,5) (95.7%, 99.9%)	
11/23 05:25:51AM evaluateStage_trainer.py:145 [INFO] Train: [261/599] Final Prec@1 95.6778%
11/23 05:25:54AM evaluateStage_trainer.py:180 [INFO] Valid: [261/599] Final Prec@1 58.5800%
11/23 05:25:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:26:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [262][50/175]	Step 46162	lr 0.01537	Loss 0.1151 (0.1256)	Prec@(1,5) (96.4%, 99.9%)	
11/23 05:26:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [262][100/175]	Step 46212	lr 0.01537	Loss 0.1416 (0.1341)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:26:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [262][150/175]	Step 46262	lr 0.01537	Loss 0.2114 (0.1390)	Prec@(1,5) (95.8%, 99.9%)	
11/23 05:26:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [262][175/175]	Step 46287	lr 0.01537	Loss 0.2188 (0.1438)	Prec@(1,5) (95.6%, 99.9%)	
11/23 05:26:37AM evaluateStage_trainer.py:145 [INFO] Train: [262/599] Final Prec@1 95.5956%
11/23 05:26:40AM evaluateStage_trainer.py:180 [INFO] Valid: [262/599] Final Prec@1 58.5800%
11/23 05:26:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:26:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [263][50/175]	Step 46338	lr 0.01531	Loss 0.1379 (0.1363)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:27:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [263][100/175]	Step 46388	lr 0.01531	Loss 0.1639 (0.1301)	Prec@(1,5) (96.2%, 99.9%)	
11/23 05:27:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [263][150/175]	Step 46438	lr 0.01531	Loss 0.1611 (0.1344)	Prec@(1,5) (96.0%, 99.9%)	
11/23 05:27:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [263][175/175]	Step 46463	lr 0.01531	Loss 0.1118 (0.1363)	Prec@(1,5) (95.9%, 99.9%)	
11/23 05:27:23AM evaluateStage_trainer.py:145 [INFO] Train: [263/599] Final Prec@1 95.9044%
11/23 05:27:26AM evaluateStage_trainer.py:180 [INFO] Valid: [263/599] Final Prec@1 58.0200%
11/23 05:27:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:27:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [264][50/175]	Step 46514	lr 0.01525	Loss 0.1274 (0.1239)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:27:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [264][100/175]	Step 46564	lr 0.01525	Loss 0.1019 (0.1258)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:28:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [264][150/175]	Step 46614	lr 0.01525	Loss 0.1320 (0.1267)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:28:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [264][175/175]	Step 46639	lr 0.01525	Loss 0.1509 (0.1282)	Prec@(1,5) (96.1%, 100.0%)	
11/23 05:28:09AM evaluateStage_trainer.py:145 [INFO] Train: [264/599] Final Prec@1 96.0756%
11/23 05:28:12AM evaluateStage_trainer.py:180 [INFO] Valid: [264/599] Final Prec@1 56.4600%
11/23 05:28:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:28:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [265][50/175]	Step 46690	lr 0.01519	Loss 0.1308 (0.1263)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:28:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [265][100/175]	Step 46740	lr 0.01519	Loss 0.1112 (0.1282)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:28:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [265][150/175]	Step 46790	lr 0.01519	Loss 0.0759 (0.1267)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:28:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [265][175/175]	Step 46815	lr 0.01519	Loss 0.1351 (0.1278)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:28:55AM evaluateStage_trainer.py:145 [INFO] Train: [265/599] Final Prec@1 96.2578%
11/23 05:28:58AM evaluateStage_trainer.py:180 [INFO] Valid: [265/599] Final Prec@1 59.3600%
11/23 05:28:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:29:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [266][50/175]	Step 46866	lr 0.01513	Loss 0.0751 (0.1087)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:29:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [266][100/175]	Step 46916	lr 0.01513	Loss 0.1134 (0.1131)	Prec@(1,5) (96.7%, 100.0%)	
11/23 05:29:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [266][150/175]	Step 46966	lr 0.01513	Loss 0.2422 (0.1228)	Prec@(1,5) (96.4%, 99.9%)	
11/23 05:29:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [266][175/175]	Step 46991	lr 0.01513	Loss 0.1588 (0.1258)	Prec@(1,5) (96.2%, 99.9%)	
11/23 05:29:41AM evaluateStage_trainer.py:145 [INFO] Train: [266/599] Final Prec@1 96.2467%
11/23 05:29:44AM evaluateStage_trainer.py:180 [INFO] Valid: [266/599] Final Prec@1 57.8000%
11/23 05:29:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:29:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [267][50/175]	Step 47042	lr 0.01506	Loss 0.1370 (0.1274)	Prec@(1,5) (96.2%, 100.0%)	
11/23 05:30:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [267][100/175]	Step 47092	lr 0.01506	Loss 0.1331 (0.1225)	Prec@(1,5) (96.5%, 99.9%)	
11/23 05:30:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [267][150/175]	Step 47142	lr 0.01506	Loss 0.1382 (0.1270)	Prec@(1,5) (96.2%, 99.9%)	
11/23 05:30:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [267][175/175]	Step 47167	lr 0.01506	Loss 0.1371 (0.1309)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:30:27AM evaluateStage_trainer.py:145 [INFO] Train: [267/599] Final Prec@1 96.1178%
11/23 05:30:30AM evaluateStage_trainer.py:180 [INFO] Valid: [267/599] Final Prec@1 55.6000%
11/23 05:30:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:30:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [268][50/175]	Step 47218	lr 0.015	Loss 0.1018 (0.1234)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:30:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [268][100/175]	Step 47268	lr 0.015	Loss 0.1733 (0.1249)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:31:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [268][150/175]	Step 47318	lr 0.015	Loss 0.1275 (0.1294)	Prec@(1,5) (96.2%, 99.9%)	
11/23 05:31:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [268][175/175]	Step 47343	lr 0.015	Loss 0.1568 (0.1298)	Prec@(1,5) (96.1%, 99.9%)	
11/23 05:31:13AM evaluateStage_trainer.py:145 [INFO] Train: [268/599] Final Prec@1 96.1200%
11/23 05:31:16AM evaluateStage_trainer.py:180 [INFO] Valid: [268/599] Final Prec@1 56.9600%
11/23 05:31:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:31:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [269][50/175]	Step 47394	lr 0.01494	Loss 0.1903 (0.1086)	Prec@(1,5) (97.1%, 99.9%)	
11/23 05:31:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [269][100/175]	Step 47444	lr 0.01494	Loss 0.0907 (0.1105)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:31:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [269][150/175]	Step 47494	lr 0.01494	Loss 0.0979 (0.1112)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:31:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [269][175/175]	Step 47519	lr 0.01494	Loss 0.1480 (0.1135)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:31:59AM evaluateStage_trainer.py:145 [INFO] Train: [269/599] Final Prec@1 96.7600%
11/23 05:32:02AM evaluateStage_trainer.py:180 [INFO] Valid: [269/599] Final Prec@1 59.5600%
11/23 05:32:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:32:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [270][50/175]	Step 47570	lr 0.01488	Loss 0.1461 (0.1066)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:32:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [270][100/175]	Step 47620	lr 0.01488	Loss 0.1285 (0.1089)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:32:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [270][150/175]	Step 47670	lr 0.01488	Loss 0.1189 (0.1084)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:32:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [270][175/175]	Step 47695	lr 0.01488	Loss 0.0868 (0.1102)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:32:45AM evaluateStage_trainer.py:145 [INFO] Train: [270/599] Final Prec@1 96.8311%
11/23 05:32:48AM evaluateStage_trainer.py:180 [INFO] Valid: [270/599] Final Prec@1 58.4200%
11/23 05:32:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:33:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [271][50/175]	Step 47746	lr 0.01482	Loss 0.0686 (0.1005)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:33:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [271][100/175]	Step 47796	lr 0.01482	Loss 0.0729 (0.1019)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:33:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [271][150/175]	Step 47846	lr 0.01482	Loss 0.0921 (0.1026)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:33:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [271][175/175]	Step 47871	lr 0.01482	Loss 0.1166 (0.1051)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:33:31AM evaluateStage_trainer.py:145 [INFO] Train: [271/599] Final Prec@1 96.9444%
11/23 05:33:34AM evaluateStage_trainer.py:180 [INFO] Valid: [271/599] Final Prec@1 58.2200%
11/23 05:33:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:33:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [272][50/175]	Step 47922	lr 0.01475	Loss 0.0903 (0.1004)	Prec@(1,5) (97.2%, 100.0%)	
11/23 05:33:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [272][100/175]	Step 47972	lr 0.01475	Loss 0.1387 (0.1041)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:34:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [272][150/175]	Step 48022	lr 0.01475	Loss 0.1277 (0.1076)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:34:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [272][175/175]	Step 48047	lr 0.01475	Loss 0.0866 (0.1108)	Prec@(1,5) (96.7%, 100.0%)	
11/23 05:34:17AM evaluateStage_trainer.py:145 [INFO] Train: [272/599] Final Prec@1 96.7422%
11/23 05:34:20AM evaluateStage_trainer.py:180 [INFO] Valid: [272/599] Final Prec@1 58.3200%
11/23 05:34:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:34:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [273][50/175]	Step 48098	lr 0.01469	Loss 0.1016 (0.0996)	Prec@(1,5) (97.2%, 100.0%)	
11/23 05:34:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [273][100/175]	Step 48148	lr 0.01469	Loss 0.1312 (0.0989)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:34:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [273][150/175]	Step 48198	lr 0.01469	Loss 0.0888 (0.0971)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:35:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [273][175/175]	Step 48223	lr 0.01469	Loss 0.1003 (0.0974)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:35:03AM evaluateStage_trainer.py:145 [INFO] Train: [273/599] Final Prec@1 97.3178%
11/23 05:35:06AM evaluateStage_trainer.py:180 [INFO] Valid: [273/599] Final Prec@1 59.2800%
11/23 05:35:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:35:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [274][50/175]	Step 48274	lr 0.01463	Loss 0.0416 (0.0877)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:35:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [274][100/175]	Step 48324	lr 0.01463	Loss 0.1166 (0.0909)	Prec@(1,5) (97.5%, 100.0%)	
11/23 05:35:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [274][150/175]	Step 48374	lr 0.01463	Loss 0.1143 (0.0983)	Prec@(1,5) (97.2%, 100.0%)	
11/23 05:35:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [274][175/175]	Step 48399	lr 0.01463	Loss 0.1454 (0.1012)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:35:49AM evaluateStage_trainer.py:145 [INFO] Train: [274/599] Final Prec@1 97.1111%
11/23 05:35:52AM evaluateStage_trainer.py:180 [INFO] Valid: [274/599] Final Prec@1 58.7000%
11/23 05:35:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:36:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [275][50/175]	Step 48450	lr 0.01457	Loss 0.1160 (0.0988)	Prec@(1,5) (97.2%, 99.9%)	
11/23 05:36:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [275][100/175]	Step 48500	lr 0.01457	Loss 0.0649 (0.1036)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:36:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [275][150/175]	Step 48550	lr 0.01457	Loss 0.0917 (0.1066)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:36:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [275][175/175]	Step 48575	lr 0.01457	Loss 0.1699 (0.1092)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:36:34AM evaluateStage_trainer.py:145 [INFO] Train: [275/599] Final Prec@1 96.8289%
11/23 05:36:38AM evaluateStage_trainer.py:180 [INFO] Valid: [275/599] Final Prec@1 58.1400%
11/23 05:36:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:36:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [276][50/175]	Step 48626	lr 0.0145	Loss 0.0944 (0.0999)	Prec@(1,5) (97.1%, 99.9%)	
11/23 05:37:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [276][100/175]	Step 48676	lr 0.0145	Loss 0.1108 (0.1012)	Prec@(1,5) (97.1%, 99.9%)	
11/23 05:37:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [276][150/175]	Step 48726	lr 0.0145	Loss 0.1677 (0.1094)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:37:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [276][175/175]	Step 48751	lr 0.0145	Loss 0.0888 (0.1136)	Prec@(1,5) (96.7%, 99.9%)	
11/23 05:37:21AM evaluateStage_trainer.py:145 [INFO] Train: [276/599] Final Prec@1 96.6844%
11/23 05:37:24AM evaluateStage_trainer.py:180 [INFO] Valid: [276/599] Final Prec@1 58.3800%
11/23 05:37:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 59.8600%
11/23 05:37:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [277][50/175]	Step 48802	lr 0.01444	Loss 0.1419 (0.1144)	Prec@(1,5) (96.5%, 99.9%)	
11/23 05:37:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [277][100/175]	Step 48852	lr 0.01444	Loss 0.0974 (0.1172)	Prec@(1,5) (96.4%, 99.9%)	
11/23 05:38:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [277][150/175]	Step 48902	lr 0.01444	Loss 0.1232 (0.1188)	Prec@(1,5) (96.5%, 99.9%)	
11/23 05:38:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [277][175/175]	Step 48927	lr 0.01444	Loss 0.1541 (0.1198)	Prec@(1,5) (96.5%, 99.9%)	
11/23 05:38:07AM evaluateStage_trainer.py:145 [INFO] Train: [277/599] Final Prec@1 96.4778%
11/23 05:38:10AM evaluateStage_trainer.py:180 [INFO] Valid: [277/599] Final Prec@1 60.3000%
11/23 05:38:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:38:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [278][50/175]	Step 48978	lr 0.01438	Loss 0.1476 (0.0987)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:38:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [278][100/175]	Step 49028	lr 0.01438	Loss 0.1143 (0.1049)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:38:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [278][150/175]	Step 49078	lr 0.01438	Loss 0.1699 (0.1103)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:38:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [278][175/175]	Step 49103	lr 0.01438	Loss 0.1248 (0.1152)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:38:53AM evaluateStage_trainer.py:145 [INFO] Train: [278/599] Final Prec@1 96.6156%
11/23 05:38:56AM evaluateStage_trainer.py:180 [INFO] Valid: [278/599] Final Prec@1 58.2200%
11/23 05:38:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:39:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [279][50/175]	Step 49154	lr 0.01432	Loss 0.1060 (0.1083)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:39:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [279][100/175]	Step 49204	lr 0.01432	Loss 0.0957 (0.1101)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:39:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [279][150/175]	Step 49254	lr 0.01432	Loss 0.1099 (0.1124)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:39:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [279][175/175]	Step 49279	lr 0.01432	Loss 0.1711 (0.1134)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:39:39AM evaluateStage_trainer.py:145 [INFO] Train: [279/599] Final Prec@1 96.7578%
11/23 05:39:42AM evaluateStage_trainer.py:180 [INFO] Valid: [279/599] Final Prec@1 58.2600%
11/23 05:39:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:39:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [280][50/175]	Step 49330	lr 0.01425	Loss 0.1330 (0.1021)	Prec@(1,5) (97.2%, 99.9%)	
11/23 05:40:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [280][100/175]	Step 49380	lr 0.01425	Loss 0.0807 (0.1051)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:40:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [280][150/175]	Step 49430	lr 0.01425	Loss 0.0629 (0.1067)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:40:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [280][175/175]	Step 49455	lr 0.01425	Loss 0.1244 (0.1086)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:40:24AM evaluateStage_trainer.py:145 [INFO] Train: [280/599] Final Prec@1 96.8756%
11/23 05:40:28AM evaluateStage_trainer.py:180 [INFO] Valid: [280/599] Final Prec@1 58.7800%
11/23 05:40:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:40:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [281][50/175]	Step 49506	lr 0.01419	Loss 0.1124 (0.0963)	Prec@(1,5) (97.2%, 100.0%)	
11/23 05:40:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [281][100/175]	Step 49556	lr 0.01419	Loss 0.1403 (0.1024)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:41:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [281][150/175]	Step 49606	lr 0.01419	Loss 0.1300 (0.1100)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:41:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [281][175/175]	Step 49631	lr 0.01419	Loss 0.0991 (0.1141)	Prec@(1,5) (96.6%, 100.0%)	
11/23 05:41:10AM evaluateStage_trainer.py:145 [INFO] Train: [281/599] Final Prec@1 96.6444%
11/23 05:41:14AM evaluateStage_trainer.py:180 [INFO] Valid: [281/599] Final Prec@1 58.2600%
11/23 05:41:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:41:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [282][50/175]	Step 49682	lr 0.01413	Loss 0.1334 (0.1083)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:41:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [282][100/175]	Step 49732	lr 0.01413	Loss 0.1157 (0.1120)	Prec@(1,5) (96.7%, 100.0%)	
11/23 05:41:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [282][150/175]	Step 49782	lr 0.01413	Loss 0.1402 (0.1169)	Prec@(1,5) (96.5%, 100.0%)	
11/23 05:41:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [282][175/175]	Step 49807	lr 0.01413	Loss 0.1572 (0.1210)	Prec@(1,5) (96.4%, 99.9%)	
11/23 05:41:56AM evaluateStage_trainer.py:145 [INFO] Train: [282/599] Final Prec@1 96.3822%
11/23 05:41:59AM evaluateStage_trainer.py:180 [INFO] Valid: [282/599] Final Prec@1 59.3400%
11/23 05:42:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:42:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [283][50/175]	Step 49858	lr 0.01407	Loss 0.1417 (0.1152)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:42:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [283][100/175]	Step 49908	lr 0.01407	Loss 0.1292 (0.1153)	Prec@(1,5) (96.7%, 99.9%)	
11/23 05:42:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [283][150/175]	Step 49958	lr 0.01407	Loss 0.1301 (0.1160)	Prec@(1,5) (96.7%, 99.9%)	
11/23 05:42:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [283][175/175]	Step 49983	lr 0.01407	Loss 0.0938 (0.1173)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:42:42AM evaluateStage_trainer.py:145 [INFO] Train: [283/599] Final Prec@1 96.6156%
11/23 05:42:45AM evaluateStage_trainer.py:180 [INFO] Valid: [283/599] Final Prec@1 59.4200%
11/23 05:42:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:42:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [284][50/175]	Step 50034	lr 0.014	Loss 0.1634 (0.1175)	Prec@(1,5) (96.7%, 99.9%)	
11/23 05:43:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [284][100/175]	Step 50084	lr 0.014	Loss 0.1082 (0.1169)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:43:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [284][150/175]	Step 50134	lr 0.014	Loss 0.1395 (0.1204)	Prec@(1,5) (96.5%, 99.9%)	
11/23 05:43:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [284][175/175]	Step 50159	lr 0.014	Loss 0.1560 (0.1243)	Prec@(1,5) (96.3%, 99.9%)	
11/23 05:43:28AM evaluateStage_trainer.py:145 [INFO] Train: [284/599] Final Prec@1 96.3467%
11/23 05:43:31AM evaluateStage_trainer.py:180 [INFO] Valid: [284/599] Final Prec@1 57.7400%
11/23 05:43:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:43:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [285][50/175]	Step 50210	lr 0.01394	Loss 0.0846 (0.1082)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:43:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [285][100/175]	Step 50260	lr 0.01394	Loss 0.1108 (0.1122)	Prec@(1,5) (96.7%, 100.0%)	
11/23 05:44:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [285][150/175]	Step 50310	lr 0.01394	Loss 0.1167 (0.1155)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:44:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [285][175/175]	Step 50335	lr 0.01394	Loss 0.1697 (0.1177)	Prec@(1,5) (96.6%, 99.9%)	
11/23 05:44:14AM evaluateStage_trainer.py:145 [INFO] Train: [285/599] Final Prec@1 96.5378%
11/23 05:44:17AM evaluateStage_trainer.py:180 [INFO] Valid: [285/599] Final Prec@1 56.9600%
11/23 05:44:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:44:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [286][50/175]	Step 50386	lr 0.01388	Loss 0.1027 (0.1072)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:44:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [286][100/175]	Step 50436	lr 0.01388	Loss 0.1254 (0.1059)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:44:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [286][150/175]	Step 50486	lr 0.01388	Loss 0.1066 (0.1084)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:44:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [286][175/175]	Step 50511	lr 0.01388	Loss 0.1440 (0.1101)	Prec@(1,5) (96.8%, 100.0%)	
11/23 05:44:59AM evaluateStage_trainer.py:145 [INFO] Train: [286/599] Final Prec@1 96.8244%
11/23 05:45:03AM evaluateStage_trainer.py:180 [INFO] Valid: [286/599] Final Prec@1 59.0000%
11/23 05:45:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:45:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [287][50/175]	Step 50562	lr 0.01382	Loss 0.1074 (0.1002)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:45:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [287][100/175]	Step 50612	lr 0.01382	Loss 0.0931 (0.1004)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:45:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [287][150/175]	Step 50662	lr 0.01382	Loss 0.1157 (0.1058)	Prec@(1,5) (96.9%, 99.9%)	
11/23 05:45:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [287][175/175]	Step 50687	lr 0.01382	Loss 0.1477 (0.1092)	Prec@(1,5) (96.8%, 99.9%)	
11/23 05:45:46AM evaluateStage_trainer.py:145 [INFO] Train: [287/599] Final Prec@1 96.7956%
11/23 05:45:49AM evaluateStage_trainer.py:180 [INFO] Valid: [287/599] Final Prec@1 58.2200%
11/23 05:45:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:46:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [288][50/175]	Step 50738	lr 0.01375	Loss 0.1221 (0.0988)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:46:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [288][100/175]	Step 50788	lr 0.01375	Loss 0.1117 (0.1041)	Prec@(1,5) (97.0%, 100.0%)	
11/23 05:46:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [288][150/175]	Step 50838	lr 0.01375	Loss 0.1062 (0.1062)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:46:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [288][175/175]	Step 50863	lr 0.01375	Loss 0.0776 (0.1077)	Prec@(1,5) (96.9%, 100.0%)	
11/23 05:46:31AM evaluateStage_trainer.py:145 [INFO] Train: [288/599] Final Prec@1 96.8622%
11/23 05:46:35AM evaluateStage_trainer.py:180 [INFO] Valid: [288/599] Final Prec@1 57.3000%
11/23 05:46:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:46:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [289][50/175]	Step 50914	lr 0.01369	Loss 0.0829 (0.0975)	Prec@(1,5) (97.2%, 99.9%)	
11/23 05:46:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [289][100/175]	Step 50964	lr 0.01369	Loss 0.1217 (0.0993)	Prec@(1,5) (97.2%, 99.9%)	
11/23 05:47:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [289][150/175]	Step 51014	lr 0.01369	Loss 0.0895 (0.1012)	Prec@(1,5) (97.1%, 100.0%)	
11/23 05:47:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [289][175/175]	Step 51039	lr 0.01369	Loss 0.1311 (0.1037)	Prec@(1,5) (97.0%, 99.9%)	
11/23 05:47:17AM evaluateStage_trainer.py:145 [INFO] Train: [289/599] Final Prec@1 97.0444%
11/23 05:47:20AM evaluateStage_trainer.py:180 [INFO] Valid: [289/599] Final Prec@1 59.2200%
11/23 05:47:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:47:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [290][50/175]	Step 51090	lr 0.01363	Loss 0.0958 (0.0913)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:47:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [290][100/175]	Step 51140	lr 0.01363	Loss 0.0894 (0.0923)	Prec@(1,5) (97.4%, 100.0%)	
11/23 05:47:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [290][150/175]	Step 51190	lr 0.01363	Loss 0.1094 (0.0961)	Prec@(1,5) (97.3%, 99.9%)	
11/23 05:48:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [290][175/175]	Step 51215	lr 0.01363	Loss 0.1097 (0.0977)	Prec@(1,5) (97.3%, 99.9%)	
11/23 05:48:03AM evaluateStage_trainer.py:145 [INFO] Train: [290/599] Final Prec@1 97.2733%
11/23 05:48:06AM evaluateStage_trainer.py:180 [INFO] Valid: [290/599] Final Prec@1 59.3000%
11/23 05:48:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:48:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [291][50/175]	Step 51266	lr 0.01357	Loss 0.0731 (0.0826)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:48:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [291][100/175]	Step 51316	lr 0.01357	Loss 0.0940 (0.0796)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:48:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [291][150/175]	Step 51366	lr 0.01357	Loss 0.1574 (0.0830)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:48:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [291][175/175]	Step 51391	lr 0.01357	Loss 0.1506 (0.0840)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:48:49AM evaluateStage_trainer.py:145 [INFO] Train: [291/599] Final Prec@1 97.7822%
11/23 05:48:52AM evaluateStage_trainer.py:180 [INFO] Valid: [291/599] Final Prec@1 59.1200%
11/23 05:48:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.3000%
11/23 05:49:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [292][50/175]	Step 51442	lr 0.0135	Loss 0.0922 (0.0785)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:49:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [292][100/175]	Step 51492	lr 0.0135	Loss 0.0722 (0.0774)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:49:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [292][150/175]	Step 51542	lr 0.0135	Loss 0.1022 (0.0801)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:49:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [292][175/175]	Step 51567	lr 0.0135	Loss 0.0704 (0.0822)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:49:35AM evaluateStage_trainer.py:145 [INFO] Train: [292/599] Final Prec@1 97.8111%
11/23 05:49:38AM evaluateStage_trainer.py:180 [INFO] Valid: [292/599] Final Prec@1 60.7200%
11/23 05:49:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:49:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [293][50/175]	Step 51618	lr 0.01344	Loss 0.0862 (0.0784)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:50:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [293][100/175]	Step 51668	lr 0.01344	Loss 0.1078 (0.0762)	Prec@(1,5) (98.0%, 100.0%)	
11/23 05:50:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [293][150/175]	Step 51718	lr 0.01344	Loss 0.1008 (0.0785)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:50:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [293][175/175]	Step 51743	lr 0.01344	Loss 0.0677 (0.0792)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:50:21AM evaluateStage_trainer.py:145 [INFO] Train: [293/599] Final Prec@1 97.8533%
11/23 05:50:25AM evaluateStage_trainer.py:180 [INFO] Valid: [293/599] Final Prec@1 58.3800%
11/23 05:50:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:50:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [294][50/175]	Step 51794	lr 0.01338	Loss 0.1115 (0.0840)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:50:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [294][100/175]	Step 51844	lr 0.01338	Loss 0.1039 (0.0840)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:51:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [294][150/175]	Step 51894	lr 0.01338	Loss 0.1010 (0.0871)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:51:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [294][175/175]	Step 51919	lr 0.01338	Loss 0.0732 (0.0886)	Prec@(1,5) (97.5%, 100.0%)	
11/23 05:51:07AM evaluateStage_trainer.py:145 [INFO] Train: [294/599] Final Prec@1 97.5333%
11/23 05:51:11AM evaluateStage_trainer.py:180 [INFO] Valid: [294/599] Final Prec@1 58.3800%
11/23 05:51:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:51:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [295][50/175]	Step 51970	lr 0.01331	Loss 0.0808 (0.0816)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:51:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [295][100/175]	Step 52020	lr 0.01331	Loss 0.0846 (0.0824)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:51:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [295][150/175]	Step 52070	lr 0.01331	Loss 0.0693 (0.0857)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:51:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [295][175/175]	Step 52095	lr 0.01331	Loss 0.1162 (0.0885)	Prec@(1,5) (97.5%, 100.0%)	
11/23 05:51:53AM evaluateStage_trainer.py:145 [INFO] Train: [295/599] Final Prec@1 97.5422%
11/23 05:51:57AM evaluateStage_trainer.py:180 [INFO] Valid: [295/599] Final Prec@1 60.2800%
11/23 05:51:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:52:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [296][50/175]	Step 52146	lr 0.01325	Loss 0.0612 (0.0858)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:52:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [296][100/175]	Step 52196	lr 0.01325	Loss 0.0946 (0.0893)	Prec@(1,5) (97.4%, 100.0%)	
11/23 05:52:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [296][150/175]	Step 52246	lr 0.01325	Loss 0.1002 (0.0946)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:52:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [296][175/175]	Step 52271	lr 0.01325	Loss 0.1005 (0.0959)	Prec@(1,5) (97.2%, 100.0%)	
11/23 05:52:39AM evaluateStage_trainer.py:145 [INFO] Train: [296/599] Final Prec@1 97.2489%
11/23 05:52:43AM evaluateStage_trainer.py:180 [INFO] Valid: [296/599] Final Prec@1 60.1200%
11/23 05:52:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:52:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [297][50/175]	Step 52322	lr 0.01319	Loss 0.0845 (0.0855)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:53:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [297][100/175]	Step 52372	lr 0.01319	Loss 0.1289 (0.0929)	Prec@(1,5) (97.4%, 100.0%)	
11/23 05:53:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [297][150/175]	Step 52422	lr 0.01319	Loss 0.0958 (0.0958)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:53:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [297][175/175]	Step 52447	lr 0.01319	Loss 0.1307 (0.0965)	Prec@(1,5) (97.3%, 100.0%)	
11/23 05:53:25AM evaluateStage_trainer.py:145 [INFO] Train: [297/599] Final Prec@1 97.3178%
11/23 05:53:28AM evaluateStage_trainer.py:180 [INFO] Valid: [297/599] Final Prec@1 58.9000%
11/23 05:53:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:53:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [298][50/175]	Step 52498	lr 0.01313	Loss 0.0808 (0.0791)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:53:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [298][100/175]	Step 52548	lr 0.01313	Loss 0.0922 (0.0786)	Prec@(1,5) (97.9%, 100.0%)	
11/23 05:54:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [298][150/175]	Step 52598	lr 0.01313	Loss 0.1496 (0.0830)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:54:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [298][175/175]	Step 52623	lr 0.01313	Loss 0.0996 (0.0851)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:54:11AM evaluateStage_trainer.py:145 [INFO] Train: [298/599] Final Prec@1 97.6800%
11/23 05:54:14AM evaluateStage_trainer.py:180 [INFO] Valid: [298/599] Final Prec@1 58.0400%
11/23 05:54:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:54:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [299][50/175]	Step 52674	lr 0.01306	Loss 0.0835 (0.0810)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:54:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [299][100/175]	Step 52724	lr 0.01306	Loss 0.0574 (0.0853)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:54:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [299][150/175]	Step 52774	lr 0.01306	Loss 0.1099 (0.0882)	Prec@(1,5) (97.5%, 100.0%)	
11/23 05:54:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [299][175/175]	Step 52799	lr 0.01306	Loss 0.1140 (0.0906)	Prec@(1,5) (97.4%, 100.0%)	
11/23 05:54:57AM evaluateStage_trainer.py:145 [INFO] Train: [299/599] Final Prec@1 97.4244%
11/23 05:55:00AM evaluateStage_trainer.py:180 [INFO] Valid: [299/599] Final Prec@1 60.2400%
11/23 05:55:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:55:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [300][50/175]	Step 52850	lr 0.013	Loss 0.0761 (0.0828)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:55:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [300][100/175]	Step 52900	lr 0.013	Loss 0.0783 (0.0839)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:55:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [300][150/175]	Step 52950	lr 0.013	Loss 0.0862 (0.0856)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:55:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [300][175/175]	Step 52975	lr 0.013	Loss 0.0918 (0.0872)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:55:43AM evaluateStage_trainer.py:145 [INFO] Train: [300/599] Final Prec@1 97.5733%
11/23 05:55:46AM evaluateStage_trainer.py:180 [INFO] Valid: [300/599] Final Prec@1 60.5200%
11/23 05:55:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:55:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [301][50/175]	Step 53026	lr 0.01294	Loss 0.0870 (0.0841)	Prec@(1,5) (97.8%, 100.0%)	
11/23 05:56:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [301][100/175]	Step 53076	lr 0.01294	Loss 0.0868 (0.0864)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:56:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [301][150/175]	Step 53126	lr 0.01294	Loss 0.0705 (0.0898)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:56:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [301][175/175]	Step 53151	lr 0.01294	Loss 0.0871 (0.0916)	Prec@(1,5) (97.6%, 100.0%)	
11/23 05:56:29AM evaluateStage_trainer.py:145 [INFO] Train: [301/599] Final Prec@1 97.5978%
11/23 05:56:32AM evaluateStage_trainer.py:180 [INFO] Valid: [301/599] Final Prec@1 59.7400%
11/23 05:56:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:56:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [302][50/175]	Step 53202	lr 0.01287	Loss 0.0740 (0.0792)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:56:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [302][100/175]	Step 53252	lr 0.01287	Loss 0.0710 (0.0815)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:57:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [302][150/175]	Step 53302	lr 0.01287	Loss 0.1050 (0.0821)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:57:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [302][175/175]	Step 53327	lr 0.01287	Loss 0.0670 (0.0838)	Prec@(1,5) (97.7%, 100.0%)	
11/23 05:57:15AM evaluateStage_trainer.py:145 [INFO] Train: [302/599] Final Prec@1 97.6800%
11/23 05:57:18AM evaluateStage_trainer.py:180 [INFO] Valid: [302/599] Final Prec@1 60.0600%
11/23 05:57:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:57:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [303][50/175]	Step 53378	lr 0.01281	Loss 0.0439 (0.0695)	Prec@(1,5) (98.3%, 100.0%)	
11/23 05:57:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [303][100/175]	Step 53428	lr 0.01281	Loss 0.0729 (0.0676)	Prec@(1,5) (98.4%, 100.0%)	
11/23 05:57:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [303][150/175]	Step 53478	lr 0.01281	Loss 0.0738 (0.0713)	Prec@(1,5) (98.2%, 100.0%)	
11/23 05:58:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [303][175/175]	Step 53503	lr 0.01281	Loss 0.0930 (0.0740)	Prec@(1,5) (98.1%, 100.0%)	
11/23 05:58:01AM evaluateStage_trainer.py:145 [INFO] Train: [303/599] Final Prec@1 98.1022%
11/23 05:58:04AM evaluateStage_trainer.py:180 [INFO] Valid: [303/599] Final Prec@1 60.0600%
11/23 05:58:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:58:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [304][50/175]	Step 53554	lr 0.01275	Loss 0.0502 (0.0720)	Prec@(1,5) (98.2%, 100.0%)	
11/23 05:58:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [304][100/175]	Step 53604	lr 0.01275	Loss 0.0666 (0.0706)	Prec@(1,5) (98.2%, 100.0%)	
11/23 05:58:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [304][150/175]	Step 53654	lr 0.01275	Loss 0.0869 (0.0723)	Prec@(1,5) (98.1%, 100.0%)	
11/23 05:58:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [304][175/175]	Step 53679	lr 0.01275	Loss 0.0613 (0.0744)	Prec@(1,5) (98.1%, 100.0%)	
11/23 05:58:47AM evaluateStage_trainer.py:145 [INFO] Train: [304/599] Final Prec@1 98.0467%
11/23 05:58:50AM evaluateStage_trainer.py:180 [INFO] Valid: [304/599] Final Prec@1 60.0000%
11/23 05:58:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:59:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [305][50/175]	Step 53730	lr 0.01269	Loss 0.0672 (0.0681)	Prec@(1,5) (98.3%, 100.0%)	
11/23 05:59:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [305][100/175]	Step 53780	lr 0.01269	Loss 0.0620 (0.0691)	Prec@(1,5) (98.2%, 100.0%)	
11/23 05:59:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [305][150/175]	Step 53830	lr 0.01269	Loss 0.1117 (0.0724)	Prec@(1,5) (98.1%, 100.0%)	
11/23 05:59:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [305][175/175]	Step 53855	lr 0.01269	Loss 0.0965 (0.0744)	Prec@(1,5) (98.0%, 100.0%)	
11/23 05:59:33AM evaluateStage_trainer.py:145 [INFO] Train: [305/599] Final Prec@1 98.0089%
11/23 05:59:36AM evaluateStage_trainer.py:180 [INFO] Valid: [305/599] Final Prec@1 60.6800%
11/23 05:59:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 05:59:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [306][50/175]	Step 53906	lr 0.01262	Loss 0.0564 (0.0759)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:00:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [306][100/175]	Step 53956	lr 0.01262	Loss 0.0735 (0.0792)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:00:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [306][150/175]	Step 54006	lr 0.01262	Loss 0.0696 (0.0813)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:00:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [306][175/175]	Step 54031	lr 0.01262	Loss 0.0760 (0.0814)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:00:19AM evaluateStage_trainer.py:145 [INFO] Train: [306/599] Final Prec@1 97.8644%
11/23 06:00:22AM evaluateStage_trainer.py:180 [INFO] Valid: [306/599] Final Prec@1 60.0800%
11/23 06:00:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7200%
11/23 06:00:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [307][50/175]	Step 54082	lr 0.01256	Loss 0.1062 (0.0694)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:00:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [307][100/175]	Step 54132	lr 0.01256	Loss 0.0704 (0.0729)	Prec@(1,5) (98.0%, 100.0%)	
11/23 06:00:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [307][150/175]	Step 54182	lr 0.01256	Loss 0.0636 (0.0730)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:01:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [307][175/175]	Step 54207	lr 0.01256	Loss 0.0703 (0.0730)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:01:05AM evaluateStage_trainer.py:145 [INFO] Train: [307/599] Final Prec@1 98.0756%
11/23 06:01:08AM evaluateStage_trainer.py:180 [INFO] Valid: [307/599] Final Prec@1 60.7600%
11/23 06:01:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.7600%
11/23 06:01:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [308][50/175]	Step 54258	lr 0.0125	Loss 0.1109 (0.0690)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:01:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [308][100/175]	Step 54308	lr 0.0125	Loss 0.0461 (0.0673)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:01:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [308][150/175]	Step 54358	lr 0.0125	Loss 0.0638 (0.0691)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:01:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [308][175/175]	Step 54383	lr 0.0125	Loss 0.0719 (0.0702)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:01:51AM evaluateStage_trainer.py:145 [INFO] Train: [308/599] Final Prec@1 98.1933%
11/23 06:01:54AM evaluateStage_trainer.py:180 [INFO] Valid: [308/599] Final Prec@1 60.8200%
11/23 06:01:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8200%
11/23 06:02:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [309][50/175]	Step 54434	lr 0.01243	Loss 0.0742 (0.0610)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:02:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [309][100/175]	Step 54484	lr 0.01243	Loss 0.0508 (0.0658)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:02:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [309][150/175]	Step 54534	lr 0.01243	Loss 0.0749 (0.0698)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:02:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [309][175/175]	Step 54559	lr 0.01243	Loss 0.0851 (0.0718)	Prec@(1,5) (98.0%, 100.0%)	
11/23 06:02:37AM evaluateStage_trainer.py:145 [INFO] Train: [309/599] Final Prec@1 98.0356%
11/23 06:02:41AM evaluateStage_trainer.py:180 [INFO] Valid: [309/599] Final Prec@1 60.8800%
11/23 06:02:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:02:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [310][50/175]	Step 54610	lr 0.01237	Loss 0.0852 (0.0709)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:03:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [310][100/175]	Step 54660	lr 0.01237	Loss 0.0467 (0.0709)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:03:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [310][150/175]	Step 54710	lr 0.01237	Loss 0.0741 (0.0711)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:03:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [310][175/175]	Step 54735	lr 0.01237	Loss 0.0864 (0.0723)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:03:24AM evaluateStage_trainer.py:145 [INFO] Train: [310/599] Final Prec@1 98.0822%
11/23 06:03:27AM evaluateStage_trainer.py:180 [INFO] Valid: [310/599] Final Prec@1 59.7400%
11/23 06:03:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:03:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [311][50/175]	Step 54786	lr 0.01231	Loss 0.0683 (0.0641)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:03:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [311][100/175]	Step 54836	lr 0.01231	Loss 0.0980 (0.0658)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:04:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [311][150/175]	Step 54886	lr 0.01231	Loss 0.0995 (0.0708)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:04:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [311][175/175]	Step 54911	lr 0.01231	Loss 0.0627 (0.0724)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:04:10AM evaluateStage_trainer.py:145 [INFO] Train: [311/599] Final Prec@1 98.1356%
11/23 06:04:13AM evaluateStage_trainer.py:180 [INFO] Valid: [311/599] Final Prec@1 59.5800%
11/23 06:04:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:04:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [312][50/175]	Step 54962	lr 0.01225	Loss 0.1096 (0.0706)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:04:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [312][100/175]	Step 55012	lr 0.01225	Loss 0.0854 (0.0710)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:04:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [312][150/175]	Step 55062	lr 0.01225	Loss 0.0816 (0.0722)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:04:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [312][175/175]	Step 55087	lr 0.01225	Loss 0.0814 (0.0729)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:04:56AM evaluateStage_trainer.py:145 [INFO] Train: [312/599] Final Prec@1 98.0933%
11/23 06:04:59AM evaluateStage_trainer.py:180 [INFO] Valid: [312/599] Final Prec@1 60.2400%
11/23 06:05:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:05:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [313][50/175]	Step 55138	lr 0.01218	Loss 0.1120 (0.0691)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:05:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [313][100/175]	Step 55188	lr 0.01218	Loss 0.1114 (0.0712)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:05:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [313][150/175]	Step 55238	lr 0.01218	Loss 0.0804 (0.0735)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:05:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [313][175/175]	Step 55263	lr 0.01218	Loss 0.0992 (0.0754)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:05:42AM evaluateStage_trainer.py:145 [INFO] Train: [313/599] Final Prec@1 98.0711%
11/23 06:05:45AM evaluateStage_trainer.py:180 [INFO] Valid: [313/599] Final Prec@1 60.4000%
11/23 06:05:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:05:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [314][50/175]	Step 55314	lr 0.01212	Loss 0.0956 (0.0722)	Prec@(1,5) (98.0%, 100.0%)	
11/23 06:06:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [314][100/175]	Step 55364	lr 0.01212	Loss 0.1014 (0.0722)	Prec@(1,5) (98.0%, 100.0%)	
11/23 06:06:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [314][150/175]	Step 55414	lr 0.01212	Loss 0.0753 (0.0751)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:06:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [314][175/175]	Step 55439	lr 0.01212	Loss 0.0971 (0.0770)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:06:28AM evaluateStage_trainer.py:145 [INFO] Train: [314/599] Final Prec@1 97.8933%
11/23 06:06:31AM evaluateStage_trainer.py:180 [INFO] Valid: [314/599] Final Prec@1 59.5600%
11/23 06:06:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:06:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [315][50/175]	Step 55490	lr 0.01206	Loss 0.0763 (0.0716)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:06:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [315][100/175]	Step 55540	lr 0.01206	Loss 0.0525 (0.0707)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:07:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [315][150/175]	Step 55590	lr 0.01206	Loss 0.0779 (0.0711)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:07:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [315][175/175]	Step 55615	lr 0.01206	Loss 0.1080 (0.0729)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:07:14AM evaluateStage_trainer.py:145 [INFO] Train: [315/599] Final Prec@1 98.0933%
11/23 06:07:17AM evaluateStage_trainer.py:180 [INFO] Valid: [315/599] Final Prec@1 58.3000%
11/23 06:07:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:07:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [316][50/175]	Step 55666	lr 0.012	Loss 0.0627 (0.0635)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:07:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [316][100/175]	Step 55716	lr 0.012	Loss 0.1546 (0.0678)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:07:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [316][150/175]	Step 55766	lr 0.012	Loss 0.0794 (0.0697)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:07:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [316][175/175]	Step 55791	lr 0.012	Loss 0.0351 (0.0706)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:08:00AM evaluateStage_trainer.py:145 [INFO] Train: [316/599] Final Prec@1 98.1889%
11/23 06:08:03AM evaluateStage_trainer.py:180 [INFO] Valid: [316/599] Final Prec@1 59.7600%
11/23 06:08:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:08:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [317][50/175]	Step 55842	lr 0.01193	Loss 0.0477 (0.0624)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:08:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [317][100/175]	Step 55892	lr 0.01193	Loss 0.0850 (0.0656)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:08:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [317][150/175]	Step 55942	lr 0.01193	Loss 0.0802 (0.0687)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:08:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [317][175/175]	Step 55967	lr 0.01193	Loss 0.0647 (0.0702)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:08:45AM evaluateStage_trainer.py:145 [INFO] Train: [317/599] Final Prec@1 98.2267%
11/23 06:08:49AM evaluateStage_trainer.py:180 [INFO] Valid: [317/599] Final Prec@1 58.7400%
11/23 06:08:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:09:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [318][50/175]	Step 56018	lr 0.01187	Loss 0.0378 (0.0611)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:09:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [318][100/175]	Step 56068	lr 0.01187	Loss 0.0784 (0.0648)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:09:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [318][150/175]	Step 56118	lr 0.01187	Loss 0.0912 (0.0673)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:09:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [318][175/175]	Step 56143	lr 0.01187	Loss 0.1068 (0.0693)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:09:31AM evaluateStage_trainer.py:145 [INFO] Train: [318/599] Final Prec@1 98.1911%
11/23 06:09:35AM evaluateStage_trainer.py:180 [INFO] Valid: [318/599] Final Prec@1 59.4400%
11/23 06:09:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:09:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [319][50/175]	Step 56194	lr 0.01181	Loss 0.0831 (0.0671)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:09:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [319][100/175]	Step 56244	lr 0.01181	Loss 0.0556 (0.0688)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:10:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [319][150/175]	Step 56294	lr 0.01181	Loss 0.0918 (0.0732)	Prec@(1,5) (98.0%, 100.0%)	
11/23 06:10:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [319][175/175]	Step 56319	lr 0.01181	Loss 0.0802 (0.0762)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:10:17AM evaluateStage_trainer.py:145 [INFO] Train: [319/599] Final Prec@1 97.9222%
11/23 06:10:21AM evaluateStage_trainer.py:180 [INFO] Valid: [319/599] Final Prec@1 59.2400%
11/23 06:10:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:10:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [320][50/175]	Step 56370	lr 0.01175	Loss 0.0707 (0.0768)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:10:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [320][100/175]	Step 56420	lr 0.01175	Loss 0.0521 (0.0807)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:10:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [320][150/175]	Step 56470	lr 0.01175	Loss 0.0887 (0.0813)	Prec@(1,5) (97.9%, 100.0%)	
11/23 06:11:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [320][175/175]	Step 56495	lr 0.01175	Loss 0.0536 (0.0816)	Prec@(1,5) (97.8%, 100.0%)	
11/23 06:11:03AM evaluateStage_trainer.py:145 [INFO] Train: [320/599] Final Prec@1 97.8444%
11/23 06:11:07AM evaluateStage_trainer.py:180 [INFO] Valid: [320/599] Final Prec@1 59.6400%
11/23 06:11:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:11:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [321][50/175]	Step 56546	lr 0.01168	Loss 0.0610 (0.0673)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:11:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [321][100/175]	Step 56596	lr 0.01168	Loss 0.0405 (0.0660)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:11:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [321][150/175]	Step 56646	lr 0.01168	Loss 0.0583 (0.0656)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:11:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [321][175/175]	Step 56671	lr 0.01168	Loss 0.0983 (0.0673)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:11:49AM evaluateStage_trainer.py:145 [INFO] Train: [321/599] Final Prec@1 98.2911%
11/23 06:11:53AM evaluateStage_trainer.py:180 [INFO] Valid: [321/599] Final Prec@1 60.7200%
11/23 06:11:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:12:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [322][50/175]	Step 56722	lr 0.01162	Loss 0.0539 (0.0680)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:12:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [322][100/175]	Step 56772	lr 0.01162	Loss 0.0427 (0.0652)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:12:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [322][150/175]	Step 56822	lr 0.01162	Loss 0.0541 (0.0638)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:12:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [322][175/175]	Step 56847	lr 0.01162	Loss 0.0464 (0.0634)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:12:35AM evaluateStage_trainer.py:145 [INFO] Train: [322/599] Final Prec@1 98.4956%
11/23 06:12:38AM evaluateStage_trainer.py:180 [INFO] Valid: [322/599] Final Prec@1 60.2400%
11/23 06:12:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 60.8800%
11/23 06:12:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [323][50/175]	Step 56898	lr 0.01156	Loss 0.0582 (0.0510)	Prec@(1,5) (98.8%, 100.0%)	
11/23 06:13:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [323][100/175]	Step 56948	lr 0.01156	Loss 0.0432 (0.0549)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:13:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [323][150/175]	Step 56998	lr 0.01156	Loss 0.0615 (0.0585)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:13:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [323][175/175]	Step 57023	lr 0.01156	Loss 0.0949 (0.0606)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:13:21AM evaluateStage_trainer.py:145 [INFO] Train: [323/599] Final Prec@1 98.4667%
11/23 06:13:25AM evaluateStage_trainer.py:180 [INFO] Valid: [323/599] Final Prec@1 61.2800%
11/23 06:13:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:13:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [324][50/175]	Step 57074	lr 0.0115	Loss 0.0620 (0.0607)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:13:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [324][100/175]	Step 57124	lr 0.0115	Loss 0.0635 (0.0633)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:14:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [324][150/175]	Step 57174	lr 0.0115	Loss 0.0741 (0.0686)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:14:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [324][175/175]	Step 57199	lr 0.0115	Loss 0.1205 (0.0714)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:14:08AM evaluateStage_trainer.py:145 [INFO] Train: [324/599] Final Prec@1 98.1089%
11/23 06:14:11AM evaluateStage_trainer.py:180 [INFO] Valid: [324/599] Final Prec@1 60.3200%
11/23 06:14:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:14:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [325][50/175]	Step 57250	lr 0.01143	Loss 0.0468 (0.0669)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:14:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [325][100/175]	Step 57300	lr 0.01143	Loss 0.0489 (0.0658)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:14:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [325][150/175]	Step 57350	lr 0.01143	Loss 0.0914 (0.0674)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:14:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [325][175/175]	Step 57375	lr 0.01143	Loss 0.0695 (0.0685)	Prec@(1,5) (98.3%, 100.0%)	
11/23 06:14:53AM evaluateStage_trainer.py:145 [INFO] Train: [325/599] Final Prec@1 98.2778%
11/23 06:14:57AM evaluateStage_trainer.py:180 [INFO] Valid: [325/599] Final Prec@1 59.8800%
11/23 06:14:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:15:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [326][50/175]	Step 57426	lr 0.01137	Loss 0.0471 (0.0693)	Prec@(1,5) (98.1%, 100.0%)	
11/23 06:15:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [326][100/175]	Step 57476	lr 0.01137	Loss 0.0544 (0.0681)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:15:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [326][150/175]	Step 57526	lr 0.01137	Loss 0.0530 (0.0674)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:15:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [326][175/175]	Step 57551	lr 0.01137	Loss 0.0503 (0.0686)	Prec@(1,5) (98.2%, 100.0%)	
11/23 06:15:40AM evaluateStage_trainer.py:145 [INFO] Train: [326/599] Final Prec@1 98.2244%
11/23 06:15:43AM evaluateStage_trainer.py:180 [INFO] Valid: [326/599] Final Prec@1 60.6800%
11/23 06:15:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:15:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [327][50/175]	Step 57602	lr 0.01131	Loss 0.0629 (0.0549)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:16:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [327][100/175]	Step 57652	lr 0.01131	Loss 0.0501 (0.0576)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:16:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [327][150/175]	Step 57702	lr 0.01131	Loss 0.0409 (0.0602)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:16:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [327][175/175]	Step 57727	lr 0.01131	Loss 0.0927 (0.0615)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:16:25AM evaluateStage_trainer.py:145 [INFO] Train: [327/599] Final Prec@1 98.4844%
11/23 06:16:29AM evaluateStage_trainer.py:180 [INFO] Valid: [327/599] Final Prec@1 59.9800%
11/23 06:16:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:16:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [328][50/175]	Step 57778	lr 0.01125	Loss 0.0489 (0.0600)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:16:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [328][100/175]	Step 57828	lr 0.01125	Loss 0.0515 (0.0566)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:17:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [328][150/175]	Step 57878	lr 0.01125	Loss 0.0570 (0.0558)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:17:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [328][175/175]	Step 57903	lr 0.01125	Loss 0.0540 (0.0578)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:17:11AM evaluateStage_trainer.py:145 [INFO] Train: [328/599] Final Prec@1 98.5956%
11/23 06:17:14AM evaluateStage_trainer.py:180 [INFO] Valid: [328/599] Final Prec@1 60.8000%
11/23 06:17:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:17:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [329][50/175]	Step 57954	lr 0.01118	Loss 0.0474 (0.0560)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:17:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [329][100/175]	Step 58004	lr 0.01118	Loss 0.0515 (0.0569)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:17:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [329][150/175]	Step 58054	lr 0.01118	Loss 0.0387 (0.0579)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:17:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [329][175/175]	Step 58079	lr 0.01118	Loss 0.0842 (0.0589)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:17:57AM evaluateStage_trainer.py:145 [INFO] Train: [329/599] Final Prec@1 98.5267%
11/23 06:18:00AM evaluateStage_trainer.py:180 [INFO] Valid: [329/599] Final Prec@1 60.0600%
11/23 06:18:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:18:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [330][50/175]	Step 58130	lr 0.01112	Loss 0.0634 (0.0520)	Prec@(1,5) (98.8%, 100.0%)	
11/23 06:18:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [330][100/175]	Step 58180	lr 0.01112	Loss 0.0696 (0.0548)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:18:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [330][150/175]	Step 58230	lr 0.01112	Loss 0.0731 (0.0589)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:18:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [330][175/175]	Step 58255	lr 0.01112	Loss 0.1020 (0.0610)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:18:43AM evaluateStage_trainer.py:145 [INFO] Train: [330/599] Final Prec@1 98.4556%
11/23 06:18:46AM evaluateStage_trainer.py:180 [INFO] Valid: [330/599] Final Prec@1 59.0400%
11/23 06:18:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.2800%
11/23 06:18:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [331][50/175]	Step 58306	lr 0.01106	Loss 0.0730 (0.0606)	Prec@(1,5) (98.5%, 100.0%)	
11/23 06:19:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [331][100/175]	Step 58356	lr 0.01106	Loss 0.0443 (0.0585)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:19:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [331][150/175]	Step 58406	lr 0.01106	Loss 0.0766 (0.0574)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:19:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [331][175/175]	Step 58431	lr 0.01106	Loss 0.0570 (0.0570)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:19:29AM evaluateStage_trainer.py:145 [INFO] Train: [331/599] Final Prec@1 98.6600%
11/23 06:19:32AM evaluateStage_trainer.py:180 [INFO] Valid: [331/599] Final Prec@1 61.7600%
11/23 06:19:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.7600%
11/23 06:19:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [332][50/175]	Step 58482	lr 0.011	Loss 0.0351 (0.0505)	Prec@(1,5) (98.8%, 100.0%)	
11/23 06:19:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [332][100/175]	Step 58532	lr 0.011	Loss 0.0572 (0.0513)	Prec@(1,5) (98.8%, 100.0%)	
11/23 06:20:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [332][150/175]	Step 58582	lr 0.011	Loss 0.0494 (0.0523)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:20:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [332][175/175]	Step 58607	lr 0.011	Loss 0.0534 (0.0534)	Prec@(1,5) (98.7%, 100.0%)	
11/23 06:20:15AM evaluateStage_trainer.py:145 [INFO] Train: [332/599] Final Prec@1 98.7000%
11/23 06:20:19AM evaluateStage_trainer.py:180 [INFO] Valid: [332/599] Final Prec@1 60.1400%
11/23 06:20:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.7600%
11/23 06:20:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [333][50/175]	Step 58658	lr 0.01094	Loss 0.0572 (0.0574)	Prec@(1,5) (98.4%, 100.0%)	
11/23 06:20:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [333][100/175]	Step 58708	lr 0.01094	Loss 0.0651 (0.0566)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:20:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [333][150/175]	Step 58758	lr 0.01094	Loss 0.0871 (0.0561)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:21:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [333][175/175]	Step 58783	lr 0.01094	Loss 0.0681 (0.0566)	Prec@(1,5) (98.6%, 100.0%)	
11/23 06:21:01AM evaluateStage_trainer.py:145 [INFO] Train: [333/599] Final Prec@1 98.5733%
11/23 06:21:05AM evaluateStage_trainer.py:180 [INFO] Valid: [333/599] Final Prec@1 59.7000%
11/23 06:21:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 61.7600%
11/23 06:21:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [334][50/175]	Step 58834	lr 0.01087	Loss 0.0557 (0.0473)	Prec@(1,5) (99.0%, 100.0%)	
11/23 06:21:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [334][100/175]	Step 58884	lr 0.01087	Loss 0.0582 (0.0454)	Prec@(1,5) (99.1%, 100.0%)	
11/23 06:21:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [334][150/175]	Step 58934	lr 0.01087	Loss 0.0419 (0.0463)	Prec@(1,5) (99.0%, 100.0%)	
11/23 06:21:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [334][175/175]	Step 58959	lr 0.01087	Loss 0.0398 (0.0467)	Prec@(1,5) (99.0%, 100.0%)	
11/23 06:21:47AM evaluateStage_trainer.py:145 [INFO] Train: [334/599] Final Prec@1 98.9756%
11/23 06:21:50AM evaluateStage_trainer.py:180 [INFO] Valid: [334/599] Final Prec@1 62.3400%
11/23 06:21:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.3400%
11/23 06:22:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [335][50/175]	Step 59010	lr 0.01081	Loss 0.0312 (0.0408)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:22:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [335][100/175]	Step 59060	lr 0.01081	Loss 0.0410 (0.0412)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:22:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [335][150/175]	Step 59110	lr 0.01081	Loss 0.0386 (0.0404)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:22:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [335][175/175]	Step 59135	lr 0.01081	Loss 0.0428 (0.0405)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:22:33AM evaluateStage_trainer.py:145 [INFO] Train: [335/599] Final Prec@1 99.1933%
11/23 06:22:37AM evaluateStage_trainer.py:180 [INFO] Valid: [335/599] Final Prec@1 62.1800%
11/23 06:22:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.3400%
11/23 06:22:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [336][50/175]	Step 59186	lr 0.01075	Loss 0.0494 (0.0380)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:23:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [336][100/175]	Step 59236	lr 0.01075	Loss 0.0322 (0.0359)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:23:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [336][150/175]	Step 59286	lr 0.01075	Loss 0.0777 (0.0372)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:23:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [336][175/175]	Step 59311	lr 0.01075	Loss 0.0354 (0.0377)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:23:20AM evaluateStage_trainer.py:145 [INFO] Train: [336/599] Final Prec@1 99.2400%
11/23 06:23:23AM evaluateStage_trainer.py:180 [INFO] Valid: [336/599] Final Prec@1 62.6800%
11/23 06:23:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:23:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [337][50/175]	Step 59362	lr 0.01069	Loss 0.0477 (0.0377)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:23:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [337][100/175]	Step 59412	lr 0.01069	Loss 0.0288 (0.0370)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:24:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [337][150/175]	Step 59462	lr 0.01069	Loss 0.0285 (0.0371)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:24:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [337][175/175]	Step 59487	lr 0.01069	Loss 0.0337 (0.0376)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:24:06AM evaluateStage_trainer.py:145 [INFO] Train: [337/599] Final Prec@1 99.2422%
11/23 06:24:09AM evaluateStage_trainer.py:180 [INFO] Valid: [337/599] Final Prec@1 62.5600%
11/23 06:24:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:24:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [338][50/175]	Step 59538	lr 0.01063	Loss 0.0359 (0.0343)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:24:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [338][100/175]	Step 59588	lr 0.01063	Loss 0.0200 (0.0344)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:24:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [338][150/175]	Step 59638	lr 0.01063	Loss 0.0225 (0.0359)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:24:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [338][175/175]	Step 59663	lr 0.01063	Loss 0.0286 (0.0369)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:24:52AM evaluateStage_trainer.py:145 [INFO] Train: [338/599] Final Prec@1 99.2733%
11/23 06:24:55AM evaluateStage_trainer.py:180 [INFO] Valid: [338/599] Final Prec@1 62.5200%
11/23 06:24:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:25:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [339][50/175]	Step 59714	lr 0.01057	Loss 0.0327 (0.0331)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:25:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [339][100/175]	Step 59764	lr 0.01057	Loss 0.0392 (0.0340)	Prec@(1,5) (99.3%, 100.0%)	
11/23 06:25:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [339][150/175]	Step 59814	lr 0.01057	Loss 0.0482 (0.0362)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:25:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [339][175/175]	Step 59839	lr 0.01057	Loss 0.0370 (0.0377)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:25:38AM evaluateStage_trainer.py:145 [INFO] Train: [339/599] Final Prec@1 99.1600%
11/23 06:25:41AM evaluateStage_trainer.py:180 [INFO] Valid: [339/599] Final Prec@1 61.5600%
11/23 06:25:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:25:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [340][50/175]	Step 59890	lr 0.01051	Loss 0.0490 (0.0364)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:26:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [340][100/175]	Step 59940	lr 0.01051	Loss 0.0411 (0.0373)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:26:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [340][150/175]	Step 59990	lr 0.01051	Loss 0.0374 (0.0381)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:26:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [340][175/175]	Step 60015	lr 0.01051	Loss 0.0292 (0.0386)	Prec@(1,5) (99.2%, 100.0%)	
11/23 06:26:24AM evaluateStage_trainer.py:145 [INFO] Train: [340/599] Final Prec@1 99.2178%
11/23 06:26:27AM evaluateStage_trainer.py:180 [INFO] Valid: [340/599] Final Prec@1 60.2800%
11/23 06:26:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:26:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [341][50/175]	Step 60066	lr 0.01044	Loss 0.0357 (0.0316)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:26:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [341][100/175]	Step 60116	lr 0.01044	Loss 0.0300 (0.0317)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:27:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [341][150/175]	Step 60166	lr 0.01044	Loss 0.0488 (0.0328)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:27:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [341][175/175]	Step 60191	lr 0.01044	Loss 0.0299 (0.0332)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:27:09AM evaluateStage_trainer.py:145 [INFO] Train: [341/599] Final Prec@1 99.4178%
11/23 06:27:13AM evaluateStage_trainer.py:180 [INFO] Valid: [341/599] Final Prec@1 61.7600%
11/23 06:27:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:27:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [342][50/175]	Step 60242	lr 0.01038	Loss 0.0235 (0.0263)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:27:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [342][100/175]	Step 60292	lr 0.01038	Loss 0.0230 (0.0285)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:27:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [342][150/175]	Step 60342	lr 0.01038	Loss 0.0444 (0.0300)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:27:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [342][175/175]	Step 60367	lr 0.01038	Loss 0.0360 (0.0310)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:27:56AM evaluateStage_trainer.py:145 [INFO] Train: [342/599] Final Prec@1 99.4733%
11/23 06:27:59AM evaluateStage_trainer.py:180 [INFO] Valid: [342/599] Final Prec@1 62.0400%
11/23 06:27:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:28:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [343][50/175]	Step 60418	lr 0.01032	Loss 0.0252 (0.0300)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:28:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [343][100/175]	Step 60468	lr 0.01032	Loss 0.0212 (0.0298)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:28:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [343][150/175]	Step 60518	lr 0.01032	Loss 0.0342 (0.0301)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:28:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [343][175/175]	Step 60543	lr 0.01032	Loss 0.0379 (0.0305)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:28:42AM evaluateStage_trainer.py:145 [INFO] Train: [343/599] Final Prec@1 99.4333%
11/23 06:28:45AM evaluateStage_trainer.py:180 [INFO] Valid: [343/599] Final Prec@1 62.5600%
11/23 06:28:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:28:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [344][50/175]	Step 60594	lr 0.01026	Loss 0.0152 (0.0310)	Prec@(1,5) (99.4%, 100.0%)	
11/23 06:29:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [344][100/175]	Step 60644	lr 0.01026	Loss 0.0288 (0.0300)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:29:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [344][150/175]	Step 60694	lr 0.01026	Loss 0.0290 (0.0289)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:29:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [344][175/175]	Step 60719	lr 0.01026	Loss 0.0251 (0.0291)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:29:27AM evaluateStage_trainer.py:145 [INFO] Train: [344/599] Final Prec@1 99.5111%
11/23 06:29:31AM evaluateStage_trainer.py:180 [INFO] Valid: [344/599] Final Prec@1 62.5200%
11/23 06:29:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.6800%
11/23 06:29:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [345][50/175]	Step 60770	lr 0.0102	Loss 0.0177 (0.0250)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:29:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [345][100/175]	Step 60820	lr 0.0102	Loss 0.0346 (0.0256)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [345][150/175]	Step 60870	lr 0.0102	Loss 0.0157 (0.0255)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [345][175/175]	Step 60895	lr 0.0102	Loss 0.0324 (0.0259)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:13AM evaluateStage_trainer.py:145 [INFO] Train: [345/599] Final Prec@1 99.5978%
11/23 06:30:17AM evaluateStage_trainer.py:180 [INFO] Valid: [345/599] Final Prec@1 62.7800%
11/23 06:30:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.7800%
11/23 06:30:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [346][50/175]	Step 60946	lr 0.01014	Loss 0.0279 (0.0232)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [346][100/175]	Step 60996	lr 0.01014	Loss 0.0338 (0.0247)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [346][150/175]	Step 61046	lr 0.01014	Loss 0.0202 (0.0249)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:30:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [346][175/175]	Step 61071	lr 0.01014	Loss 0.0307 (0.0253)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:31:00AM evaluateStage_trainer.py:145 [INFO] Train: [346/599] Final Prec@1 99.6000%
11/23 06:31:03AM evaluateStage_trainer.py:180 [INFO] Valid: [346/599] Final Prec@1 62.6000%
11/23 06:31:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 62.7800%
11/23 06:31:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [347][50/175]	Step 61122	lr 0.01008	Loss 0.0269 (0.0260)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:31:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [347][100/175]	Step 61172	lr 0.01008	Loss 0.0295 (0.0252)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:31:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [347][150/175]	Step 61222	lr 0.01008	Loss 0.0269 (0.0259)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:31:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [347][175/175]	Step 61247	lr 0.01008	Loss 0.0197 (0.0257)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:31:46AM evaluateStage_trainer.py:145 [INFO] Train: [347/599] Final Prec@1 99.5778%
11/23 06:31:48AM evaluateStage_trainer.py:180 [INFO] Valid: [347/599] Final Prec@1 64.0400%
11/23 06:31:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:32:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [348][50/175]	Step 61298	lr 0.01002	Loss 0.0175 (0.0236)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:32:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [348][100/175]	Step 61348	lr 0.01002	Loss 0.0358 (0.0246)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:32:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [348][150/175]	Step 61398	lr 0.01002	Loss 0.0316 (0.0252)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:32:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [348][175/175]	Step 61423	lr 0.01002	Loss 0.0307 (0.0260)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:32:31AM evaluateStage_trainer.py:145 [INFO] Train: [348/599] Final Prec@1 99.5756%
11/23 06:32:34AM evaluateStage_trainer.py:180 [INFO] Valid: [348/599] Final Prec@1 62.6200%
11/23 06:32:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:32:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [349][50/175]	Step 61474	lr 0.00995	Loss 0.0182 (0.0235)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:32:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [349][100/175]	Step 61524	lr 0.00995	Loss 0.0275 (0.0243)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:33:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [349][150/175]	Step 61574	lr 0.00995	Loss 0.0344 (0.0241)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:33:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [349][175/175]	Step 61599	lr 0.00995	Loss 0.0214 (0.0242)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:33:16AM evaluateStage_trainer.py:145 [INFO] Train: [349/599] Final Prec@1 99.6289%
11/23 06:33:19AM evaluateStage_trainer.py:180 [INFO] Valid: [349/599] Final Prec@1 62.1800%
11/23 06:33:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:33:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [350][50/175]	Step 61650	lr 0.00989	Loss 0.0173 (0.0215)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:33:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [350][100/175]	Step 61700	lr 0.00989	Loss 0.0219 (0.0221)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:33:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [350][150/175]	Step 61750	lr 0.00989	Loss 0.0225 (0.0221)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:34:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [350][175/175]	Step 61775	lr 0.00989	Loss 0.0142 (0.0222)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:34:02AM evaluateStage_trainer.py:145 [INFO] Train: [350/599] Final Prec@1 99.6956%
11/23 06:34:05AM evaluateStage_trainer.py:180 [INFO] Valid: [350/599] Final Prec@1 62.9400%
11/23 06:34:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:34:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [351][50/175]	Step 61826	lr 0.00983	Loss 0.0528 (0.0242)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:34:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [351][100/175]	Step 61876	lr 0.00983	Loss 0.0138 (0.0251)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:34:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [351][150/175]	Step 61926	lr 0.00983	Loss 0.0252 (0.0258)	Prec@(1,5) (99.5%, 100.0%)	
11/23 06:34:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [351][175/175]	Step 61951	lr 0.00983	Loss 0.0320 (0.0258)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:34:48AM evaluateStage_trainer.py:145 [INFO] Train: [351/599] Final Prec@1 99.5533%
11/23 06:34:51AM evaluateStage_trainer.py:180 [INFO] Valid: [351/599] Final Prec@1 63.1600%
11/23 06:34:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:35:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [352][50/175]	Step 62002	lr 0.00977	Loss 0.0262 (0.0220)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:35:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [352][100/175]	Step 62052	lr 0.00977	Loss 0.0535 (0.0222)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:35:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [352][150/175]	Step 62102	lr 0.00977	Loss 0.0318 (0.0229)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:35:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [352][175/175]	Step 62127	lr 0.00977	Loss 0.0162 (0.0232)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:35:34AM evaluateStage_trainer.py:145 [INFO] Train: [352/599] Final Prec@1 99.6556%
11/23 06:35:37AM evaluateStage_trainer.py:180 [INFO] Valid: [352/599] Final Prec@1 62.2800%
11/23 06:35:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:35:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [353][50/175]	Step 62178	lr 0.00971	Loss 0.0152 (0.0241)	Prec@(1,5) (99.6%, 100.0%)	
11/23 06:36:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [353][100/175]	Step 62228	lr 0.00971	Loss 0.0119 (0.0230)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:36:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [353][150/175]	Step 62278	lr 0.00971	Loss 0.0157 (0.0224)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:36:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [353][175/175]	Step 62303	lr 0.00971	Loss 0.0425 (0.0225)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:36:20AM evaluateStage_trainer.py:145 [INFO] Train: [353/599] Final Prec@1 99.6844%
11/23 06:36:23AM evaluateStage_trainer.py:180 [INFO] Valid: [353/599] Final Prec@1 63.5400%
11/23 06:36:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:36:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [354][50/175]	Step 62354	lr 0.00965	Loss 0.0189 (0.0174)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:36:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [354][100/175]	Step 62404	lr 0.00965	Loss 0.0159 (0.0192)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:36:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [354][150/175]	Step 62454	lr 0.00965	Loss 0.0249 (0.0202)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:37:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [354][175/175]	Step 62479	lr 0.00965	Loss 0.0176 (0.0206)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:37:05AM evaluateStage_trainer.py:145 [INFO] Train: [354/599] Final Prec@1 99.7022%
11/23 06:37:09AM evaluateStage_trainer.py:180 [INFO] Valid: [354/599] Final Prec@1 63.4400%
11/23 06:37:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:37:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [355][50/175]	Step 62530	lr 0.00959	Loss 0.0327 (0.0201)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:37:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [355][100/175]	Step 62580	lr 0.00959	Loss 0.0179 (0.0201)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:37:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [355][150/175]	Step 62630	lr 0.00959	Loss 0.0211 (0.0210)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:37:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [355][175/175]	Step 62655	lr 0.00959	Loss 0.0117 (0.0209)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:37:51AM evaluateStage_trainer.py:145 [INFO] Train: [355/599] Final Prec@1 99.7222%
11/23 06:37:55AM evaluateStage_trainer.py:180 [INFO] Valid: [355/599] Final Prec@1 63.5000%
11/23 06:37:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.0400%
11/23 06:38:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [356][50/175]	Step 62706	lr 0.00953	Loss 0.0156 (0.0190)	Prec@(1,5) (99.7%, 100.0%)	
11/23 06:38:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [356][100/175]	Step 62756	lr 0.00953	Loss 0.0120 (0.0188)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:38:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [356][150/175]	Step 62806	lr 0.00953	Loss 0.0174 (0.0183)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:38:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [356][175/175]	Step 62831	lr 0.00953	Loss 0.0269 (0.0180)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:38:37AM evaluateStage_trainer.py:145 [INFO] Train: [356/599] Final Prec@1 99.7689%
11/23 06:38:40AM evaluateStage_trainer.py:180 [INFO] Valid: [356/599] Final Prec@1 64.5400%
11/23 06:38:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:38:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [357][50/175]	Step 62882	lr 0.00947	Loss 0.0327 (0.0174)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:39:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [357][100/175]	Step 62932	lr 0.00947	Loss 0.0154 (0.0163)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:39:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [357][150/175]	Step 62982	lr 0.00947	Loss 0.0103 (0.0169)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:39:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [357][175/175]	Step 63007	lr 0.00947	Loss 0.0189 (0.0176)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:39:23AM evaluateStage_trainer.py:145 [INFO] Train: [357/599] Final Prec@1 99.8067%
11/23 06:39:26AM evaluateStage_trainer.py:180 [INFO] Valid: [357/599] Final Prec@1 63.1000%
11/23 06:39:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:39:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [358][50/175]	Step 63058	lr 0.00941	Loss 0.0171 (0.0159)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:39:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [358][100/175]	Step 63108	lr 0.00941	Loss 0.0154 (0.0164)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [358][150/175]	Step 63158	lr 0.00941	Loss 0.0103 (0.0165)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [358][175/175]	Step 63183	lr 0.00941	Loss 0.0108 (0.0167)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:09AM evaluateStage_trainer.py:145 [INFO] Train: [358/599] Final Prec@1 99.8111%
11/23 06:40:12AM evaluateStage_trainer.py:180 [INFO] Valid: [358/599] Final Prec@1 64.1800%
11/23 06:40:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:40:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [359][50/175]	Step 63234	lr 0.00935	Loss 0.0134 (0.0158)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [359][100/175]	Step 63284	lr 0.00935	Loss 0.0104 (0.0163)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [359][150/175]	Step 63334	lr 0.00935	Loss 0.0168 (0.0171)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [359][175/175]	Step 63359	lr 0.00935	Loss 0.0139 (0.0171)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:40:55AM evaluateStage_trainer.py:145 [INFO] Train: [359/599] Final Prec@1 99.7711%
11/23 06:40:58AM evaluateStage_trainer.py:180 [INFO] Valid: [359/599] Final Prec@1 63.9800%
11/23 06:40:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:41:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [360][50/175]	Step 63410	lr 0.00929	Loss 0.0247 (0.0159)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:41:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [360][100/175]	Step 63460	lr 0.00929	Loss 0.0115 (0.0158)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:41:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [360][150/175]	Step 63510	lr 0.00929	Loss 0.0135 (0.0161)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:41:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [360][175/175]	Step 63535	lr 0.00929	Loss 0.0151 (0.0160)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:41:41AM evaluateStage_trainer.py:145 [INFO] Train: [360/599] Final Prec@1 99.8511%
11/23 06:41:44AM evaluateStage_trainer.py:180 [INFO] Valid: [360/599] Final Prec@1 63.8000%
11/23 06:41:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:41:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [361][50/175]	Step 63586	lr 0.00923	Loss 0.0112 (0.0136)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:42:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [361][100/175]	Step 63636	lr 0.00923	Loss 0.0123 (0.0143)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:42:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [361][150/175]	Step 63686	lr 0.00923	Loss 0.0192 (0.0150)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:42:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [361][175/175]	Step 63711	lr 0.00923	Loss 0.0075 (0.0150)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:42:27AM evaluateStage_trainer.py:145 [INFO] Train: [361/599] Final Prec@1 99.8400%
11/23 06:42:30AM evaluateStage_trainer.py:180 [INFO] Valid: [361/599] Final Prec@1 64.0200%
11/23 06:42:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:42:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [362][50/175]	Step 63762	lr 0.00917	Loss 0.0092 (0.0153)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:42:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [362][100/175]	Step 63812	lr 0.00917	Loss 0.0166 (0.0150)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:43:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [362][150/175]	Step 63862	lr 0.00917	Loss 0.0099 (0.0149)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:43:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [362][175/175]	Step 63887	lr 0.00917	Loss 0.0099 (0.0148)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:43:13AM evaluateStage_trainer.py:145 [INFO] Train: [362/599] Final Prec@1 99.8311%
11/23 06:43:16AM evaluateStage_trainer.py:180 [INFO] Valid: [362/599] Final Prec@1 63.7800%
11/23 06:43:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:43:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [363][50/175]	Step 63938	lr 0.00911	Loss 0.0121 (0.0134)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:43:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [363][100/175]	Step 63988	lr 0.00911	Loss 0.0152 (0.0136)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:43:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [363][150/175]	Step 64038	lr 0.00911	Loss 0.0148 (0.0139)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:43:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [363][175/175]	Step 64063	lr 0.00911	Loss 0.0111 (0.0138)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:43:59AM evaluateStage_trainer.py:145 [INFO] Train: [363/599] Final Prec@1 99.8689%
11/23 06:44:02AM evaluateStage_trainer.py:180 [INFO] Valid: [363/599] Final Prec@1 63.8000%
11/23 06:44:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:44:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [364][50/175]	Step 64114	lr 0.00905	Loss 0.0219 (0.0131)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:44:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [364][100/175]	Step 64164	lr 0.00905	Loss 0.0065 (0.0126)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:44:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [364][150/175]	Step 64214	lr 0.00905	Loss 0.0147 (0.0126)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:44:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [364][175/175]	Step 64239	lr 0.00905	Loss 0.0133 (0.0128)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:44:44AM evaluateStage_trainer.py:145 [INFO] Train: [364/599] Final Prec@1 99.8911%
11/23 06:44:47AM evaluateStage_trainer.py:180 [INFO] Valid: [364/599] Final Prec@1 64.1200%
11/23 06:44:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.5400%
11/23 06:45:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [365][50/175]	Step 64290	lr 0.00899	Loss 0.0075 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:45:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [365][100/175]	Step 64340	lr 0.00899	Loss 0.0094 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:45:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [365][150/175]	Step 64390	lr 0.00899	Loss 0.0072 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:45:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [365][175/175]	Step 64415	lr 0.00899	Loss 0.0074 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:45:29AM evaluateStage_trainer.py:145 [INFO] Train: [365/599] Final Prec@1 99.9267%
11/23 06:45:32AM evaluateStage_trainer.py:180 [INFO] Valid: [365/599] Final Prec@1 64.7000%
11/23 06:45:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:45:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [366][50/175]	Step 64466	lr 0.00894	Loss 0.0090 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:45:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [366][100/175]	Step 64516	lr 0.00894	Loss 0.0073 (0.0109)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:46:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [366][150/175]	Step 64566	lr 0.00894	Loss 0.0094 (0.0120)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:46:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [366][175/175]	Step 64591	lr 0.00894	Loss 0.0081 (0.0124)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:46:15AM evaluateStage_trainer.py:145 [INFO] Train: [366/599] Final Prec@1 99.8689%
11/23 06:46:18AM evaluateStage_trainer.py:180 [INFO] Valid: [366/599] Final Prec@1 64.6200%
11/23 06:46:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:46:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [367][50/175]	Step 64642	lr 0.00888	Loss 0.0131 (0.0146)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:46:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [367][100/175]	Step 64692	lr 0.00888	Loss 0.0155 (0.0138)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:46:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [367][150/175]	Step 64742	lr 0.00888	Loss 0.0123 (0.0134)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:47:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [367][175/175]	Step 64767	lr 0.00888	Loss 0.0095 (0.0138)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:47:00AM evaluateStage_trainer.py:145 [INFO] Train: [367/599] Final Prec@1 99.8556%
11/23 06:47:03AM evaluateStage_trainer.py:180 [INFO] Valid: [367/599] Final Prec@1 63.7000%
11/23 06:47:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:47:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [368][50/175]	Step 64818	lr 0.00882	Loss 0.0175 (0.0159)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:47:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [368][100/175]	Step 64868	lr 0.00882	Loss 0.0246 (0.0156)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:47:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [368][150/175]	Step 64918	lr 0.00882	Loss 0.0141 (0.0147)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:47:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [368][175/175]	Step 64943	lr 0.00882	Loss 0.0155 (0.0150)	Prec@(1,5) (99.8%, 100.0%)	
11/23 06:47:46AM evaluateStage_trainer.py:145 [INFO] Train: [368/599] Final Prec@1 99.8400%
11/23 06:47:48AM evaluateStage_trainer.py:180 [INFO] Valid: [368/599] Final Prec@1 63.9600%
11/23 06:47:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:48:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [369][50/175]	Step 64994	lr 0.00876	Loss 0.0073 (0.0134)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:48:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [369][100/175]	Step 65044	lr 0.00876	Loss 0.0086 (0.0125)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:48:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [369][150/175]	Step 65094	lr 0.00876	Loss 0.0150 (0.0127)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:48:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [369][175/175]	Step 65119	lr 0.00876	Loss 0.0120 (0.0126)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:48:31AM evaluateStage_trainer.py:145 [INFO] Train: [369/599] Final Prec@1 99.8889%
11/23 06:48:33AM evaluateStage_trainer.py:180 [INFO] Valid: [369/599] Final Prec@1 63.3800%
11/23 06:48:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:48:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [370][50/175]	Step 65170	lr 0.0087	Loss 0.0105 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:48:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [370][100/175]	Step 65220	lr 0.0087	Loss 0.0111 (0.0116)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:49:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [370][150/175]	Step 65270	lr 0.0087	Loss 0.0073 (0.0114)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:49:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [370][175/175]	Step 65295	lr 0.0087	Loss 0.0218 (0.0119)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:49:16AM evaluateStage_trainer.py:145 [INFO] Train: [370/599] Final Prec@1 99.9022%
11/23 06:49:18AM evaluateStage_trainer.py:180 [INFO] Valid: [370/599] Final Prec@1 64.2600%
11/23 06:49:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:49:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [371][50/175]	Step 65346	lr 0.00864	Loss 0.0086 (0.0099)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:49:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [371][100/175]	Step 65396	lr 0.00864	Loss 0.0121 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:49:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [371][150/175]	Step 65446	lr 0.00864	Loss 0.0102 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [371][175/175]	Step 65471	lr 0.00864	Loss 0.0085 (0.0101)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:01AM evaluateStage_trainer.py:145 [INFO] Train: [371/599] Final Prec@1 99.9333%
11/23 06:50:04AM evaluateStage_trainer.py:180 [INFO] Valid: [371/599] Final Prec@1 64.5000%
11/23 06:50:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:50:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [372][50/175]	Step 65522	lr 0.00858	Loss 0.0082 (0.0101)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [372][100/175]	Step 65572	lr 0.00858	Loss 0.0068 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [372][150/175]	Step 65622	lr 0.00858	Loss 0.0223 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [372][175/175]	Step 65647	lr 0.00858	Loss 0.0082 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:50:46AM evaluateStage_trainer.py:145 [INFO] Train: [372/599] Final Prec@1 99.9333%
11/23 06:50:49AM evaluateStage_trainer.py:180 [INFO] Valid: [372/599] Final Prec@1 63.8800%
11/23 06:50:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:51:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [373][50/175]	Step 65698	lr 0.00852	Loss 0.0055 (0.0110)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:51:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [373][100/175]	Step 65748	lr 0.00852	Loss 0.0165 (0.0108)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:51:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [373][150/175]	Step 65798	lr 0.00852	Loss 0.0052 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:51:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [373][175/175]	Step 65823	lr 0.00852	Loss 0.0065 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:51:31AM evaluateStage_trainer.py:145 [INFO] Train: [373/599] Final Prec@1 99.9333%
11/23 06:51:34AM evaluateStage_trainer.py:180 [INFO] Valid: [373/599] Final Prec@1 64.0400%
11/23 06:51:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:51:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [374][50/175]	Step 65874	lr 0.00847	Loss 0.0096 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:51:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [374][100/175]	Step 65924	lr 0.00847	Loss 0.0094 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:52:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [374][150/175]	Step 65974	lr 0.00847	Loss 0.0101 (0.0091)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:52:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [374][175/175]	Step 65999	lr 0.00847	Loss 0.0133 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:52:17AM evaluateStage_trainer.py:145 [INFO] Train: [374/599] Final Prec@1 99.9267%
11/23 06:52:19AM evaluateStage_trainer.py:180 [INFO] Valid: [374/599] Final Prec@1 64.1000%
11/23 06:52:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7000%
11/23 06:52:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [375][50/175]	Step 66050	lr 0.00841	Loss 0.0091 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:52:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [375][100/175]	Step 66100	lr 0.00841	Loss 0.0144 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:52:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [375][150/175]	Step 66150	lr 0.00841	Loss 0.0075 (0.0109)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [375][175/175]	Step 66175	lr 0.00841	Loss 0.0150 (0.0111)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:02AM evaluateStage_trainer.py:145 [INFO] Train: [375/599] Final Prec@1 99.9311%
11/23 06:53:05AM evaluateStage_trainer.py:180 [INFO] Valid: [375/599] Final Prec@1 64.7200%
11/23 06:53:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:53:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [376][50/175]	Step 66226	lr 0.00835	Loss 0.0104 (0.0101)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [376][100/175]	Step 66276	lr 0.00835	Loss 0.0093 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [376][150/175]	Step 66326	lr 0.00835	Loss 0.0158 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [376][175/175]	Step 66351	lr 0.00835	Loss 0.0108 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:53:48AM evaluateStage_trainer.py:145 [INFO] Train: [376/599] Final Prec@1 99.9289%
11/23 06:53:51AM evaluateStage_trainer.py:180 [INFO] Valid: [376/599] Final Prec@1 64.1200%
11/23 06:53:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:54:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [377][50/175]	Step 66402	lr 0.00829	Loss 0.0069 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:54:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [377][100/175]	Step 66452	lr 0.00829	Loss 0.0068 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:54:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [377][150/175]	Step 66502	lr 0.00829	Loss 0.0206 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:54:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [377][175/175]	Step 66527	lr 0.00829	Loss 0.0098 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:54:34AM evaluateStage_trainer.py:145 [INFO] Train: [377/599] Final Prec@1 99.9311%
11/23 06:54:37AM evaluateStage_trainer.py:180 [INFO] Valid: [377/599] Final Prec@1 64.4800%
11/23 06:54:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:54:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [378][50/175]	Step 66578	lr 0.00823	Loss 0.0119 (0.0116)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:55:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [378][100/175]	Step 66628	lr 0.00823	Loss 0.0078 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:55:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [378][150/175]	Step 66678	lr 0.00823	Loss 0.0119 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:55:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [378][175/175]	Step 66703	lr 0.00823	Loss 0.0148 (0.0117)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:55:20AM evaluateStage_trainer.py:145 [INFO] Train: [378/599] Final Prec@1 99.9111%
11/23 06:55:23AM evaluateStage_trainer.py:180 [INFO] Valid: [378/599] Final Prec@1 64.3000%
11/23 06:55:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:55:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [379][50/175]	Step 66754	lr 0.00818	Loss 0.0158 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:55:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [379][100/175]	Step 66804	lr 0.00818	Loss 0.0061 (0.0095)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:56:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [379][150/175]	Step 66854	lr 0.00818	Loss 0.0228 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [379][175/175]	Step 66879	lr 0.00818	Loss 0.0080 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:06AM evaluateStage_trainer.py:145 [INFO] Train: [379/599] Final Prec@1 99.9422%
11/23 06:56:09AM evaluateStage_trainer.py:180 [INFO] Valid: [379/599] Final Prec@1 64.4200%
11/23 06:56:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:56:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [380][50/175]	Step 66930	lr 0.00812	Loss 0.0111 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [380][100/175]	Step 66980	lr 0.00812	Loss 0.0118 (0.0116)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [380][150/175]	Step 67030	lr 0.00812	Loss 0.0098 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [380][175/175]	Step 67055	lr 0.00812	Loss 0.0133 (0.0114)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:56:51AM evaluateStage_trainer.py:145 [INFO] Train: [380/599] Final Prec@1 99.9133%
11/23 06:56:54AM evaluateStage_trainer.py:180 [INFO] Valid: [380/599] Final Prec@1 64.6400%
11/23 06:56:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:57:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [381][50/175]	Step 67106	lr 0.00806	Loss 0.0105 (0.0108)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:57:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [381][100/175]	Step 67156	lr 0.00806	Loss 0.0101 (0.0111)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:57:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [381][150/175]	Step 67206	lr 0.00806	Loss 0.0106 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:57:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [381][175/175]	Step 67231	lr 0.00806	Loss 0.0073 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:57:37AM evaluateStage_trainer.py:145 [INFO] Train: [381/599] Final Prec@1 99.9200%
11/23 06:57:39AM evaluateStage_trainer.py:180 [INFO] Valid: [381/599] Final Prec@1 64.2400%
11/23 06:57:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:57:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [382][50/175]	Step 67282	lr 0.008	Loss 0.0076 (0.0101)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:58:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [382][100/175]	Step 67332	lr 0.008	Loss 0.0105 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:58:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [382][150/175]	Step 67382	lr 0.008	Loss 0.0062 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:58:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [382][175/175]	Step 67407	lr 0.008	Loss 0.0087 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:58:22AM evaluateStage_trainer.py:145 [INFO] Train: [382/599] Final Prec@1 99.9156%
11/23 06:58:24AM evaluateStage_trainer.py:180 [INFO] Valid: [382/599] Final Prec@1 63.9000%
11/23 06:58:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:58:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [383][50/175]	Step 67458	lr 0.00795	Loss 0.0075 (0.0107)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:58:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [383][100/175]	Step 67508	lr 0.00795	Loss 0.0103 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:59:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [383][150/175]	Step 67558	lr 0.00795	Loss 0.0065 (0.0110)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:59:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [383][175/175]	Step 67583	lr 0.00795	Loss 0.0105 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:59:07AM evaluateStage_trainer.py:145 [INFO] Train: [383/599] Final Prec@1 99.9378%
11/23 06:59:09AM evaluateStage_trainer.py:180 [INFO] Valid: [383/599] Final Prec@1 64.3400%
11/23 06:59:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 06:59:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [384][50/175]	Step 67634	lr 0.00789	Loss 0.0084 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:59:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [384][100/175]	Step 67684	lr 0.00789	Loss 0.0089 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:59:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [384][150/175]	Step 67734	lr 0.00789	Loss 0.0123 (0.0098)	Prec@(1,5) (100.0%, 100.0%)	
11/23 06:59:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [384][175/175]	Step 67759	lr 0.00789	Loss 0.0057 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 06:59:52AM evaluateStage_trainer.py:145 [INFO] Train: [384/599] Final Prec@1 99.9422%
11/23 06:59:55AM evaluateStage_trainer.py:180 [INFO] Valid: [384/599] Final Prec@1 63.9200%
11/23 06:59:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 64.7200%
11/23 07:00:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [385][50/175]	Step 67810	lr 0.00783	Loss 0.0080 (0.0110)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:00:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [385][100/175]	Step 67860	lr 0.00783	Loss 0.0089 (0.0109)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:00:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [385][150/175]	Step 67910	lr 0.00783	Loss 0.0095 (0.0112)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:00:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [385][175/175]	Step 67935	lr 0.00783	Loss 0.0097 (0.0113)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:00:38AM evaluateStage_trainer.py:145 [INFO] Train: [385/599] Final Prec@1 99.9222%
11/23 07:00:41AM evaluateStage_trainer.py:180 [INFO] Valid: [385/599] Final Prec@1 65.0600%
11/23 07:00:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:00:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [386][50/175]	Step 67986	lr 0.00778	Loss 0.0099 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:01:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [386][100/175]	Step 68036	lr 0.00778	Loss 0.0089 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:01:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [386][150/175]	Step 68086	lr 0.00778	Loss 0.0089 (0.0114)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:01:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [386][175/175]	Step 68111	lr 0.00778	Loss 0.0109 (0.0114)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:01:24AM evaluateStage_trainer.py:145 [INFO] Train: [386/599] Final Prec@1 99.9044%
11/23 07:01:27AM evaluateStage_trainer.py:180 [INFO] Valid: [386/599] Final Prec@1 64.8600%
11/23 07:01:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:01:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [387][50/175]	Step 68162	lr 0.00772	Loss 0.0058 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:01:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [387][100/175]	Step 68212	lr 0.00772	Loss 0.0075 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:02:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [387][150/175]	Step 68262	lr 0.00772	Loss 0.0065 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:02:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [387][175/175]	Step 68287	lr 0.00772	Loss 0.0090 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:02:10AM evaluateStage_trainer.py:145 [INFO] Train: [387/599] Final Prec@1 99.9600%
11/23 07:02:13AM evaluateStage_trainer.py:180 [INFO] Valid: [387/599] Final Prec@1 64.7800%
11/23 07:02:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:02:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [388][50/175]	Step 68338	lr 0.00766	Loss 0.0070 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:02:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [388][100/175]	Step 68388	lr 0.00766	Loss 0.0074 (0.0096)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:02:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [388][150/175]	Step 68438	lr 0.00766	Loss 0.0078 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:02:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [388][175/175]	Step 68463	lr 0.00766	Loss 0.0118 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:02:56AM evaluateStage_trainer.py:145 [INFO] Train: [388/599] Final Prec@1 99.9422%
11/23 07:02:59AM evaluateStage_trainer.py:180 [INFO] Valid: [388/599] Final Prec@1 64.3800%
11/23 07:02:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:03:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [389][50/175]	Step 68514	lr 0.00761	Loss 0.0081 (0.0098)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:03:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [389][100/175]	Step 68564	lr 0.00761	Loss 0.0081 (0.0098)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:03:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [389][150/175]	Step 68614	lr 0.00761	Loss 0.0077 (0.0096)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:03:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [389][175/175]	Step 68639	lr 0.00761	Loss 0.0199 (0.0096)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:03:42AM evaluateStage_trainer.py:145 [INFO] Train: [389/599] Final Prec@1 99.9511%
11/23 07:03:45AM evaluateStage_trainer.py:180 [INFO] Valid: [389/599] Final Prec@1 64.6200%
11/23 07:03:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:03:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [390][50/175]	Step 68690	lr 0.00755	Loss 0.0072 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:04:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [390][100/175]	Step 68740	lr 0.00755	Loss 0.0087 (0.0096)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:04:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [390][150/175]	Step 68790	lr 0.00755	Loss 0.0090 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:04:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [390][175/175]	Step 68815	lr 0.00755	Loss 0.0127 (0.0108)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:04:28AM evaluateStage_trainer.py:145 [INFO] Train: [390/599] Final Prec@1 99.9244%
11/23 07:04:32AM evaluateStage_trainer.py:180 [INFO] Valid: [390/599] Final Prec@1 63.9600%
11/23 07:04:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:04:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [391][50/175]	Step 68866	lr 0.0075	Loss 0.0064 (0.0091)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:04:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [391][100/175]	Step 68916	lr 0.0075	Loss 0.0085 (0.0097)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:05:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [391][150/175]	Step 68966	lr 0.0075	Loss 0.0105 (0.0097)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:05:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [391][175/175]	Step 68991	lr 0.0075	Loss 0.0090 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:05:14AM evaluateStage_trainer.py:145 [INFO] Train: [391/599] Final Prec@1 99.9333%
11/23 07:05:18AM evaluateStage_trainer.py:180 [INFO] Valid: [391/599] Final Prec@1 64.0000%
11/23 07:05:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:05:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [392][50/175]	Step 69042	lr 0.00744	Loss 0.0144 (0.0109)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:05:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [392][100/175]	Step 69092	lr 0.00744	Loss 0.0098 (0.0113)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:05:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [392][150/175]	Step 69142	lr 0.00744	Loss 0.0149 (0.0115)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:06:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [392][175/175]	Step 69167	lr 0.00744	Loss 0.0114 (0.0117)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:06:00AM evaluateStage_trainer.py:145 [INFO] Train: [392/599] Final Prec@1 99.9067%
11/23 07:06:04AM evaluateStage_trainer.py:180 [INFO] Valid: [392/599] Final Prec@1 64.4200%
11/23 07:06:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:06:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [393][50/175]	Step 69218	lr 0.00738	Loss 0.0092 (0.0103)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:06:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [393][100/175]	Step 69268	lr 0.00738	Loss 0.0070 (0.0101)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:06:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [393][150/175]	Step 69318	lr 0.00738	Loss 0.0088 (0.0098)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:06:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [393][175/175]	Step 69343	lr 0.00738	Loss 0.0086 (0.0100)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:06:46AM evaluateStage_trainer.py:145 [INFO] Train: [393/599] Final Prec@1 99.9578%
11/23 07:06:50AM evaluateStage_trainer.py:180 [INFO] Valid: [393/599] Final Prec@1 64.8000%
11/23 07:06:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:07:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [394][50/175]	Step 69394	lr 0.00733	Loss 0.0068 (0.0097)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:07:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [394][100/175]	Step 69444	lr 0.00733	Loss 0.0087 (0.0092)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:07:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [394][150/175]	Step 69494	lr 0.00733	Loss 0.0082 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:07:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [394][175/175]	Step 69519	lr 0.00733	Loss 0.0136 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:07:32AM evaluateStage_trainer.py:145 [INFO] Train: [394/599] Final Prec@1 99.9244%
11/23 07:07:35AM evaluateStage_trainer.py:180 [INFO] Valid: [394/599] Final Prec@1 64.6200%
11/23 07:07:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:07:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [395][50/175]	Step 69570	lr 0.00727	Loss 0.0077 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:08:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [395][100/175]	Step 69620	lr 0.00727	Loss 0.0119 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:08:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [395][150/175]	Step 69670	lr 0.00727	Loss 0.0076 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:08:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [395][175/175]	Step 69695	lr 0.00727	Loss 0.0094 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:08:18AM evaluateStage_trainer.py:145 [INFO] Train: [395/599] Final Prec@1 99.9578%
11/23 07:08:22AM evaluateStage_trainer.py:180 [INFO] Valid: [395/599] Final Prec@1 65.0000%
11/23 07:08:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:08:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [396][50/175]	Step 69746	lr 0.00722	Loss 0.0073 (0.0110)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:08:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [396][100/175]	Step 69796	lr 0.00722	Loss 0.0104 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:08:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [396][150/175]	Step 69846	lr 0.00722	Loss 0.0083 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:09:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [396][175/175]	Step 69871	lr 0.00722	Loss 0.0113 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:09:04AM evaluateStage_trainer.py:145 [INFO] Train: [396/599] Final Prec@1 99.9356%
11/23 07:09:07AM evaluateStage_trainer.py:180 [INFO] Valid: [396/599] Final Prec@1 64.9000%
11/23 07:09:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:09:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [397][50/175]	Step 69922	lr 0.00716	Loss 0.0080 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:09:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [397][100/175]	Step 69972	lr 0.00716	Loss 0.0102 (0.0094)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:09:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [397][150/175]	Step 70022	lr 0.00716	Loss 0.0131 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:09:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [397][175/175]	Step 70047	lr 0.00716	Loss 0.0072 (0.0101)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:09:50AM evaluateStage_trainer.py:145 [INFO] Train: [397/599] Final Prec@1 99.9467%
11/23 07:09:54AM evaluateStage_trainer.py:180 [INFO] Valid: [397/599] Final Prec@1 64.3200%
11/23 07:09:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:10:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [398][50/175]	Step 70098	lr 0.00711	Loss 0.0069 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:10:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [398][100/175]	Step 70148	lr 0.00711	Loss 0.0114 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:10:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [398][150/175]	Step 70198	lr 0.00711	Loss 0.0087 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:10:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [398][175/175]	Step 70223	lr 0.00711	Loss 0.0071 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:10:36AM evaluateStage_trainer.py:145 [INFO] Train: [398/599] Final Prec@1 99.9578%
11/23 07:10:40AM evaluateStage_trainer.py:180 [INFO] Valid: [398/599] Final Prec@1 64.6800%
11/23 07:10:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:10:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [399][50/175]	Step 70274	lr 0.00705	Loss 0.0091 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:11:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [399][100/175]	Step 70324	lr 0.00705	Loss 0.0063 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:11:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [399][150/175]	Step 70374	lr 0.00705	Loss 0.0101 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:11:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [399][175/175]	Step 70399	lr 0.00705	Loss 0.0079 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:11:22AM evaluateStage_trainer.py:145 [INFO] Train: [399/599] Final Prec@1 99.9667%
11/23 07:11:26AM evaluateStage_trainer.py:180 [INFO] Valid: [399/599] Final Prec@1 63.8800%
11/23 07:11:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:11:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [400][50/175]	Step 70450	lr 0.007	Loss 0.0084 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:11:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [400][100/175]	Step 70500	lr 0.007	Loss 0.0091 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:12:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [400][150/175]	Step 70550	lr 0.007	Loss 0.0095 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:12:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [400][175/175]	Step 70575	lr 0.007	Loss 0.0085 (0.0107)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:12:08AM evaluateStage_trainer.py:145 [INFO] Train: [400/599] Final Prec@1 99.9333%
11/23 07:12:12AM evaluateStage_trainer.py:180 [INFO] Valid: [400/599] Final Prec@1 64.2000%
11/23 07:12:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:12:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [401][50/175]	Step 70626	lr 0.00695	Loss 0.0080 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:12:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [401][100/175]	Step 70676	lr 0.00695	Loss 0.0090 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:12:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [401][150/175]	Step 70726	lr 0.00695	Loss 0.0118 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:12:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [401][175/175]	Step 70751	lr 0.00695	Loss 0.0075 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:12:54AM evaluateStage_trainer.py:145 [INFO] Train: [401/599] Final Prec@1 99.9644%
11/23 07:12:58AM evaluateStage_trainer.py:180 [INFO] Valid: [401/599] Final Prec@1 63.9800%
11/23 07:12:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:13:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [402][50/175]	Step 70802	lr 0.00689	Loss 0.0095 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:13:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [402][100/175]	Step 70852	lr 0.00689	Loss 0.0091 (0.0094)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:13:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [402][150/175]	Step 70902	lr 0.00689	Loss 0.0064 (0.0099)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:13:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [402][175/175]	Step 70927	lr 0.00689	Loss 0.0100 (0.0098)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:13:40AM evaluateStage_trainer.py:145 [INFO] Train: [402/599] Final Prec@1 99.9511%
11/23 07:13:44AM evaluateStage_trainer.py:180 [INFO] Valid: [402/599] Final Prec@1 64.6600%
11/23 07:13:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:13:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [403][50/175]	Step 70978	lr 0.00684	Loss 0.0080 (0.0087)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:14:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [403][100/175]	Step 71028	lr 0.00684	Loss 0.0066 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:14:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [403][150/175]	Step 71078	lr 0.00684	Loss 0.0136 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:14:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [403][175/175]	Step 71103	lr 0.00684	Loss 0.0075 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:14:26AM evaluateStage_trainer.py:145 [INFO] Train: [403/599] Final Prec@1 99.9578%
11/23 07:14:30AM evaluateStage_trainer.py:180 [INFO] Valid: [403/599] Final Prec@1 64.6000%
11/23 07:14:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:14:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [404][50/175]	Step 71154	lr 0.00678	Loss 0.0124 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:14:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [404][100/175]	Step 71204	lr 0.00678	Loss 0.0059 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:15:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [404][150/175]	Step 71254	lr 0.00678	Loss 0.0097 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:15:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [404][175/175]	Step 71279	lr 0.00678	Loss 0.0174 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:15:12AM evaluateStage_trainer.py:145 [INFO] Train: [404/599] Final Prec@1 99.9667%
11/23 07:15:16AM evaluateStage_trainer.py:180 [INFO] Valid: [404/599] Final Prec@1 64.4000%
11/23 07:15:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:15:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [405][50/175]	Step 71330	lr 0.00673	Loss 0.0048 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:15:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [405][100/175]	Step 71380	lr 0.00673	Loss 0.0070 (0.0089)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:15:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [405][150/175]	Step 71430	lr 0.00673	Loss 0.0119 (0.0088)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:15:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [405][175/175]	Step 71455	lr 0.00673	Loss 0.0087 (0.0089)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:15:59AM evaluateStage_trainer.py:145 [INFO] Train: [405/599] Final Prec@1 99.9467%
11/23 07:16:02AM evaluateStage_trainer.py:180 [INFO] Valid: [405/599] Final Prec@1 64.5200%
11/23 07:16:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.0600%
11/23 07:16:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [406][50/175]	Step 71506	lr 0.00668	Loss 0.0111 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:16:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [406][100/175]	Step 71556	lr 0.00668	Loss 0.0068 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:16:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [406][150/175]	Step 71606	lr 0.00668	Loss 0.0080 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:16:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [406][175/175]	Step 71631	lr 0.00668	Loss 0.0078 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:16:45AM evaluateStage_trainer.py:145 [INFO] Train: [406/599] Final Prec@1 99.9533%
11/23 07:16:48AM evaluateStage_trainer.py:180 [INFO] Valid: [406/599] Final Prec@1 65.1400%
11/23 07:16:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:17:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [407][50/175]	Step 71682	lr 0.00662	Loss 0.0084 (0.0095)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:17:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [407][100/175]	Step 71732	lr 0.00662	Loss 0.0088 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:17:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [407][150/175]	Step 71782	lr 0.00662	Loss 0.0076 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:17:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [407][175/175]	Step 71807	lr 0.00662	Loss 0.0081 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:17:31AM evaluateStage_trainer.py:145 [INFO] Train: [407/599] Final Prec@1 99.9489%
11/23 07:17:35AM evaluateStage_trainer.py:180 [INFO] Valid: [407/599] Final Prec@1 64.3800%
11/23 07:17:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:17:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [408][50/175]	Step 71858	lr 0.00657	Loss 0.0100 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:17:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [408][100/175]	Step 71908	lr 0.00657	Loss 0.0085 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:18:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [408][150/175]	Step 71958	lr 0.00657	Loss 0.0067 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:18:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [408][175/175]	Step 71983	lr 0.00657	Loss 0.0090 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:18:17AM evaluateStage_trainer.py:145 [INFO] Train: [408/599] Final Prec@1 99.9511%
11/23 07:18:21AM evaluateStage_trainer.py:180 [INFO] Valid: [408/599] Final Prec@1 64.6000%
11/23 07:18:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:18:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [409][50/175]	Step 72034	lr 0.00652	Loss 0.0066 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:18:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [409][100/175]	Step 72084	lr 0.00652	Loss 0.0126 (0.0090)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:18:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [409][150/175]	Step 72134	lr 0.00652	Loss 0.0083 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:19:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [409][175/175]	Step 72159	lr 0.00652	Loss 0.0082 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:19:03AM evaluateStage_trainer.py:145 [INFO] Train: [409/599] Final Prec@1 99.9378%
11/23 07:19:07AM evaluateStage_trainer.py:180 [INFO] Valid: [409/599] Final Prec@1 63.5000%
11/23 07:19:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:19:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [410][50/175]	Step 72210	lr 0.00646	Loss 0.0058 (0.0095)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:19:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [410][100/175]	Step 72260	lr 0.00646	Loss 0.0102 (0.0096)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:19:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [410][150/175]	Step 72310	lr 0.00646	Loss 0.0103 (0.0097)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:19:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [410][175/175]	Step 72335	lr 0.00646	Loss 0.0094 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:19:49AM evaluateStage_trainer.py:145 [INFO] Train: [410/599] Final Prec@1 99.9400%
11/23 07:19:53AM evaluateStage_trainer.py:180 [INFO] Valid: [410/599] Final Prec@1 63.9000%
11/23 07:19:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:20:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [411][50/175]	Step 72386	lr 0.00641	Loss 0.0081 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:20:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [411][100/175]	Step 72436	lr 0.00641	Loss 0.0062 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:20:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [411][150/175]	Step 72486	lr 0.00641	Loss 0.0088 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:20:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [411][175/175]	Step 72511	lr 0.00641	Loss 0.0071 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:20:35AM evaluateStage_trainer.py:145 [INFO] Train: [411/599] Final Prec@1 99.9644%
11/23 07:20:39AM evaluateStage_trainer.py:180 [INFO] Valid: [411/599] Final Prec@1 64.8400%
11/23 07:20:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:20:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [412][50/175]	Step 72562	lr 0.00636	Loss 0.0070 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:21:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [412][100/175]	Step 72612	lr 0.00636	Loss 0.0090 (0.0106)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:21:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [412][150/175]	Step 72662	lr 0.00636	Loss 0.0059 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:21:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [412][175/175]	Step 72687	lr 0.00636	Loss 0.0094 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:21:21AM evaluateStage_trainer.py:145 [INFO] Train: [412/599] Final Prec@1 99.9422%
11/23 07:21:25AM evaluateStage_trainer.py:180 [INFO] Valid: [412/599] Final Prec@1 64.7600%
11/23 07:21:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:21:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [413][50/175]	Step 72738	lr 0.00631	Loss 0.0082 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:21:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [413][100/175]	Step 72788	lr 0.00631	Loss 0.0082 (0.0097)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:22:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [413][150/175]	Step 72838	lr 0.00631	Loss 0.0099 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [413][175/175]	Step 72863	lr 0.00631	Loss 0.0090 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:07AM evaluateStage_trainer.py:145 [INFO] Train: [413/599] Final Prec@1 99.9444%
11/23 07:22:11AM evaluateStage_trainer.py:180 [INFO] Valid: [413/599] Final Prec@1 64.1800%
11/23 07:22:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:22:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [414][50/175]	Step 72914	lr 0.00625	Loss 0.0050 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [414][100/175]	Step 72964	lr 0.00625	Loss 0.0078 (0.0103)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [414][150/175]	Step 73014	lr 0.00625	Loss 0.0092 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [414][175/175]	Step 73039	lr 0.00625	Loss 0.0104 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:22:53AM evaluateStage_trainer.py:145 [INFO] Train: [414/599] Final Prec@1 99.9333%
11/23 07:22:57AM evaluateStage_trainer.py:180 [INFO] Valid: [414/599] Final Prec@1 64.3200%
11/23 07:22:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:23:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [415][50/175]	Step 73090	lr 0.0062	Loss 0.0084 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:23:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [415][100/175]	Step 73140	lr 0.0062	Loss 0.0104 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:23:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [415][150/175]	Step 73190	lr 0.0062	Loss 0.0103 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:23:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [415][175/175]	Step 73215	lr 0.0062	Loss 0.0116 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:23:39AM evaluateStage_trainer.py:145 [INFO] Train: [415/599] Final Prec@1 99.9711%
11/23 07:23:43AM evaluateStage_trainer.py:180 [INFO] Valid: [415/599] Final Prec@1 64.3200%
11/23 07:23:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:23:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [416][50/175]	Step 73266	lr 0.00615	Loss 0.0061 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:24:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [416][100/175]	Step 73316	lr 0.00615	Loss 0.0073 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:24:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [416][150/175]	Step 73366	lr 0.00615	Loss 0.0092 (0.0095)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:24:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [416][175/175]	Step 73391	lr 0.00615	Loss 0.0171 (0.0097)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:24:25AM evaluateStage_trainer.py:145 [INFO] Train: [416/599] Final Prec@1 99.9578%
11/23 07:24:29AM evaluateStage_trainer.py:180 [INFO] Valid: [416/599] Final Prec@1 64.8600%
11/23 07:24:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:24:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [417][50/175]	Step 73442	lr 0.0061	Loss 0.0086 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:24:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [417][100/175]	Step 73492	lr 0.0061	Loss 0.0117 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [417][150/175]	Step 73542	lr 0.0061	Loss 0.0054 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [417][175/175]	Step 73567	lr 0.0061	Loss 0.0123 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:11AM evaluateStage_trainer.py:145 [INFO] Train: [417/599] Final Prec@1 99.9533%
11/23 07:25:15AM evaluateStage_trainer.py:180 [INFO] Valid: [417/599] Final Prec@1 64.4200%
11/23 07:25:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1400%
11/23 07:25:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [418][50/175]	Step 73618	lr 0.00605	Loss 0.0083 (0.0091)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:25:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [418][100/175]	Step 73668	lr 0.00605	Loss 0.0123 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [418][150/175]	Step 73718	lr 0.00605	Loss 0.0175 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [418][175/175]	Step 73743	lr 0.00605	Loss 0.0089 (0.0096)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:25:57AM evaluateStage_trainer.py:145 [INFO] Train: [418/599] Final Prec@1 99.9533%
11/23 07:26:01AM evaluateStage_trainer.py:180 [INFO] Valid: [418/599] Final Prec@1 65.1600%
11/23 07:26:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:26:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [419][50/175]	Step 73794	lr 0.006	Loss 0.0105 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:26:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [419][100/175]	Step 73844	lr 0.006	Loss 0.0092 (0.0092)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:26:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [419][150/175]	Step 73894	lr 0.006	Loss 0.0072 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:26:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [419][175/175]	Step 73919	lr 0.006	Loss 0.0083 (0.0095)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:26:44AM evaluateStage_trainer.py:145 [INFO] Train: [419/599] Final Prec@1 99.9444%
11/23 07:26:47AM evaluateStage_trainer.py:180 [INFO] Valid: [419/599] Final Prec@1 65.0200%
11/23 07:26:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:27:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [420][50/175]	Step 73970	lr 0.00595	Loss 0.0074 (0.0096)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:27:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [420][100/175]	Step 74020	lr 0.00595	Loss 0.0133 (0.0100)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:27:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [420][150/175]	Step 74070	lr 0.00595	Loss 0.0083 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:27:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [420][175/175]	Step 74095	lr 0.00595	Loss 0.0076 (0.0105)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:27:29AM evaluateStage_trainer.py:145 [INFO] Train: [420/599] Final Prec@1 99.9467%
11/23 07:27:33AM evaluateStage_trainer.py:180 [INFO] Valid: [420/599] Final Prec@1 64.0600%
11/23 07:27:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:27:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [421][50/175]	Step 74146	lr 0.0059	Loss 0.0102 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:27:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [421][100/175]	Step 74196	lr 0.0059	Loss 0.0126 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:28:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [421][150/175]	Step 74246	lr 0.0059	Loss 0.0118 (0.0102)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:28:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [421][175/175]	Step 74271	lr 0.0059	Loss 0.0100 (0.0104)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:28:16AM evaluateStage_trainer.py:145 [INFO] Train: [421/599] Final Prec@1 99.9311%
11/23 07:28:19AM evaluateStage_trainer.py:180 [INFO] Valid: [421/599] Final Prec@1 64.9000%
11/23 07:28:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:28:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [422][50/175]	Step 74322	lr 0.00585	Loss 0.0086 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:28:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [422][100/175]	Step 74372	lr 0.00585	Loss 0.0150 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:28:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [422][150/175]	Step 74422	lr 0.00585	Loss 0.0067 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [422][175/175]	Step 74447	lr 0.00585	Loss 0.0079 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:01AM evaluateStage_trainer.py:145 [INFO] Train: [422/599] Final Prec@1 99.9644%
11/23 07:29:05AM evaluateStage_trainer.py:180 [INFO] Valid: [422/599] Final Prec@1 65.1200%
11/23 07:29:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:29:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [423][50/175]	Step 74498	lr 0.00579	Loss 0.0095 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [423][100/175]	Step 74548	lr 0.00579	Loss 0.0107 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [423][150/175]	Step 74598	lr 0.00579	Loss 0.0086 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [423][175/175]	Step 74623	lr 0.00579	Loss 0.0068 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:29:48AM evaluateStage_trainer.py:145 [INFO] Train: [423/599] Final Prec@1 99.9600%
11/23 07:29:51AM evaluateStage_trainer.py:180 [INFO] Valid: [423/599] Final Prec@1 64.8600%
11/23 07:29:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:30:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [424][50/175]	Step 74674	lr 0.00574	Loss 0.0094 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:30:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [424][100/175]	Step 74724	lr 0.00574	Loss 0.0057 (0.0086)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:30:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [424][150/175]	Step 74774	lr 0.00574	Loss 0.0097 (0.0091)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:30:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [424][175/175]	Step 74799	lr 0.00574	Loss 0.0169 (0.0092)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:30:34AM evaluateStage_trainer.py:145 [INFO] Train: [424/599] Final Prec@1 99.9400%
11/23 07:30:37AM evaluateStage_trainer.py:180 [INFO] Valid: [424/599] Final Prec@1 64.4000%
11/23 07:30:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:30:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [425][50/175]	Step 74850	lr 0.00569	Loss 0.0088 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [425][100/175]	Step 74900	lr 0.00569	Loss 0.0113 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [425][150/175]	Step 74950	lr 0.00569	Loss 0.0067 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [425][175/175]	Step 74975	lr 0.00569	Loss 0.0082 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:19AM evaluateStage_trainer.py:145 [INFO] Train: [425/599] Final Prec@1 99.9533%
11/23 07:31:23AM evaluateStage_trainer.py:180 [INFO] Valid: [425/599] Final Prec@1 64.1000%
11/23 07:31:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:31:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [426][50/175]	Step 75026	lr 0.00565	Loss 0.0104 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [426][100/175]	Step 75076	lr 0.00565	Loss 0.0065 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:31:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [426][150/175]	Step 75126	lr 0.00565	Loss 0.0056 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:32:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [426][175/175]	Step 75151	lr 0.00565	Loss 0.0079 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:32:05AM evaluateStage_trainer.py:145 [INFO] Train: [426/599] Final Prec@1 99.9756%
11/23 07:32:09AM evaluateStage_trainer.py:180 [INFO] Valid: [426/599] Final Prec@1 64.0600%
11/23 07:32:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:32:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [427][50/175]	Step 75202	lr 0.0056	Loss 0.0075 (0.0090)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:32:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [427][100/175]	Step 75252	lr 0.0056	Loss 0.0110 (0.0094)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:32:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [427][150/175]	Step 75302	lr 0.0056	Loss 0.0065 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:32:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [427][175/175]	Step 75327	lr 0.0056	Loss 0.0086 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:32:51AM evaluateStage_trainer.py:145 [INFO] Train: [427/599] Final Prec@1 99.9533%
11/23 07:32:55AM evaluateStage_trainer.py:180 [INFO] Valid: [427/599] Final Prec@1 64.7000%
11/23 07:32:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:33:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [428][50/175]	Step 75378	lr 0.00555	Loss 0.0082 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:33:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [428][100/175]	Step 75428	lr 0.00555	Loss 0.0082 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:33:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [428][150/175]	Step 75478	lr 0.00555	Loss 0.0080 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:33:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [428][175/175]	Step 75503	lr 0.00555	Loss 0.0060 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:33:37AM evaluateStage_trainer.py:145 [INFO] Train: [428/599] Final Prec@1 99.9556%
11/23 07:33:41AM evaluateStage_trainer.py:180 [INFO] Valid: [428/599] Final Prec@1 64.9400%
11/23 07:33:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:33:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [429][50/175]	Step 75554	lr 0.0055	Loss 0.0074 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:34:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [429][100/175]	Step 75604	lr 0.0055	Loss 0.0079 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:34:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [429][150/175]	Step 75654	lr 0.0055	Loss 0.0077 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:34:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [429][175/175]	Step 75679	lr 0.0055	Loss 0.0097 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:34:23AM evaluateStage_trainer.py:145 [INFO] Train: [429/599] Final Prec@1 99.9689%
11/23 07:34:27AM evaluateStage_trainer.py:180 [INFO] Valid: [429/599] Final Prec@1 64.9000%
11/23 07:34:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:34:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [430][50/175]	Step 75730	lr 0.00545	Loss 0.0049 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:34:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [430][100/175]	Step 75780	lr 0.00545	Loss 0.0089 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:35:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [430][150/175]	Step 75830	lr 0.00545	Loss 0.0072 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:35:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [430][175/175]	Step 75855	lr 0.00545	Loss 0.0080 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:35:09AM evaluateStage_trainer.py:145 [INFO] Train: [430/599] Final Prec@1 99.9600%
11/23 07:35:13AM evaluateStage_trainer.py:180 [INFO] Valid: [430/599] Final Prec@1 64.4800%
11/23 07:35:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:35:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [431][50/175]	Step 75906	lr 0.0054	Loss 0.0060 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:35:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [431][100/175]	Step 75956	lr 0.0054	Loss 0.0066 (0.0093)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:35:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [431][150/175]	Step 76006	lr 0.0054	Loss 0.0089 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:35:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [431][175/175]	Step 76031	lr 0.0054	Loss 0.0091 (0.0094)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:35:55AM evaluateStage_trainer.py:145 [INFO] Train: [431/599] Final Prec@1 99.9444%
11/23 07:35:58AM evaluateStage_trainer.py:180 [INFO] Valid: [431/599] Final Prec@1 64.3400%
11/23 07:35:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:36:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [432][50/175]	Step 76082	lr 0.00535	Loss 0.0069 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:36:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [432][100/175]	Step 76132	lr 0.00535	Loss 0.0070 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:36:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [432][150/175]	Step 76182	lr 0.00535	Loss 0.0070 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:36:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [432][175/175]	Step 76207	lr 0.00535	Loss 0.0167 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:36:41AM evaluateStage_trainer.py:145 [INFO] Train: [432/599] Final Prec@1 99.9644%
11/23 07:36:44AM evaluateStage_trainer.py:180 [INFO] Valid: [432/599] Final Prec@1 64.8800%
11/23 07:36:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:36:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [433][50/175]	Step 76258	lr 0.0053	Loss 0.0093 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:37:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [433][100/175]	Step 76308	lr 0.0053	Loss 0.0079 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:37:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [433][150/175]	Step 76358	lr 0.0053	Loss 0.0088 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:37:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [433][175/175]	Step 76383	lr 0.0053	Loss 0.0124 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:37:27AM evaluateStage_trainer.py:145 [INFO] Train: [433/599] Final Prec@1 99.9556%
11/23 07:37:31AM evaluateStage_trainer.py:180 [INFO] Valid: [433/599] Final Prec@1 64.5200%
11/23 07:37:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:37:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [434][50/175]	Step 76434	lr 0.00525	Loss 0.0087 (0.0096)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:37:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [434][100/175]	Step 76484	lr 0.00525	Loss 0.0067 (0.0098)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:38:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [434][150/175]	Step 76534	lr 0.00525	Loss 0.0215 (0.0093)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [434][175/175]	Step 76559	lr 0.00525	Loss 0.0077 (0.0092)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:13AM evaluateStage_trainer.py:145 [INFO] Train: [434/599] Final Prec@1 99.9644%
11/23 07:38:16AM evaluateStage_trainer.py:180 [INFO] Valid: [434/599] Final Prec@1 65.1000%
11/23 07:38:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:38:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [435][50/175]	Step 76610	lr 0.00521	Loss 0.0076 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [435][100/175]	Step 76660	lr 0.00521	Loss 0.0089 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [435][150/175]	Step 76710	lr 0.00521	Loss 0.0080 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [435][175/175]	Step 76735	lr 0.00521	Loss 0.0115 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:38:59AM evaluateStage_trainer.py:145 [INFO] Train: [435/599] Final Prec@1 99.9644%
11/23 07:39:02AM evaluateStage_trainer.py:180 [INFO] Valid: [435/599] Final Prec@1 65.1000%
11/23 07:39:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:39:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [436][50/175]	Step 76786	lr 0.00516	Loss 0.0069 (0.0081)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:39:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [436][100/175]	Step 76836	lr 0.00516	Loss 0.0067 (0.0085)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:39:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [436][150/175]	Step 76886	lr 0.00516	Loss 0.0118 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:39:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [436][175/175]	Step 76911	lr 0.00516	Loss 0.0082 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:39:45AM evaluateStage_trainer.py:145 [INFO] Train: [436/599] Final Prec@1 99.9600%
11/23 07:39:49AM evaluateStage_trainer.py:180 [INFO] Valid: [436/599] Final Prec@1 64.7200%
11/23 07:39:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:40:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [437][50/175]	Step 76962	lr 0.00511	Loss 0.0047 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:40:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [437][100/175]	Step 77012	lr 0.00511	Loss 0.0141 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:40:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [437][150/175]	Step 77062	lr 0.00511	Loss 0.0451 (0.0091)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:40:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [437][175/175]	Step 77087	lr 0.00511	Loss 0.0068 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:40:31AM evaluateStage_trainer.py:145 [INFO] Train: [437/599] Final Prec@1 99.9556%
11/23 07:40:35AM evaluateStage_trainer.py:180 [INFO] Valid: [437/599] Final Prec@1 65.0200%
11/23 07:40:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:40:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [438][50/175]	Step 77138	lr 0.00506	Loss 0.0091 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:40:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [438][100/175]	Step 77188	lr 0.00506	Loss 0.0143 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:41:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [438][150/175]	Step 77238	lr 0.00506	Loss 0.0088 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:41:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [438][175/175]	Step 77263	lr 0.00506	Loss 0.0216 (0.0086)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:41:17AM evaluateStage_trainer.py:145 [INFO] Train: [438/599] Final Prec@1 99.9689%
11/23 07:41:20AM evaluateStage_trainer.py:180 [INFO] Valid: [438/599] Final Prec@1 64.6200%
11/23 07:41:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:41:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [439][50/175]	Step 77314	lr 0.00502	Loss 0.0077 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:41:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [439][100/175]	Step 77364	lr 0.00502	Loss 0.0071 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:41:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [439][150/175]	Step 77414	lr 0.00502	Loss 0.0052 (0.0090)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [439][175/175]	Step 77439	lr 0.00502	Loss 0.0156 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:03AM evaluateStage_trainer.py:145 [INFO] Train: [439/599] Final Prec@1 99.9644%
11/23 07:42:07AM evaluateStage_trainer.py:180 [INFO] Valid: [439/599] Final Prec@1 65.0200%
11/23 07:42:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:42:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [440][50/175]	Step 77490	lr 0.00497	Loss 0.0061 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [440][100/175]	Step 77540	lr 0.00497	Loss 0.0079 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [440][150/175]	Step 77590	lr 0.00497	Loss 0.0062 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [440][175/175]	Step 77615	lr 0.00497	Loss 0.0074 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:42:49AM evaluateStage_trainer.py:145 [INFO] Train: [440/599] Final Prec@1 99.9711%
11/23 07:42:52AM evaluateStage_trainer.py:180 [INFO] Valid: [440/599] Final Prec@1 64.4600%
11/23 07:42:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:43:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [441][50/175]	Step 77666	lr 0.00492	Loss 0.0055 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:43:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [441][100/175]	Step 77716	lr 0.00492	Loss 0.0056 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:43:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [441][150/175]	Step 77766	lr 0.00492	Loss 0.0071 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:43:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [441][175/175]	Step 77791	lr 0.00492	Loss 0.0105 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:43:35AM evaluateStage_trainer.py:145 [INFO] Train: [441/599] Final Prec@1 99.9667%
11/23 07:43:39AM evaluateStage_trainer.py:180 [INFO] Valid: [441/599] Final Prec@1 64.6800%
11/23 07:43:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:43:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [442][50/175]	Step 77842	lr 0.00488	Loss 0.0078 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:44:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [442][100/175]	Step 77892	lr 0.00488	Loss 0.0051 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:44:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [442][150/175]	Step 77942	lr 0.00488	Loss 0.0084 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:44:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [442][175/175]	Step 77967	lr 0.00488	Loss 0.0087 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:44:21AM evaluateStage_trainer.py:145 [INFO] Train: [442/599] Final Prec@1 99.9600%
11/23 07:44:25AM evaluateStage_trainer.py:180 [INFO] Valid: [442/599] Final Prec@1 64.7000%
11/23 07:44:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:44:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [443][50/175]	Step 78018	lr 0.00483	Loss 0.0073 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:44:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [443][100/175]	Step 78068	lr 0.00483	Loss 0.0097 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [443][150/175]	Step 78118	lr 0.00483	Loss 0.0063 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [443][175/175]	Step 78143	lr 0.00483	Loss 0.0095 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:07AM evaluateStage_trainer.py:145 [INFO] Train: [443/599] Final Prec@1 99.9644%
11/23 07:45:11AM evaluateStage_trainer.py:180 [INFO] Valid: [443/599] Final Prec@1 64.2400%
11/23 07:45:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:45:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [444][50/175]	Step 78194	lr 0.00479	Loss 0.0069 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [444][100/175]	Step 78244	lr 0.00479	Loss 0.0046 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [444][150/175]	Step 78294	lr 0.00479	Loss 0.0069 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [444][175/175]	Step 78319	lr 0.00479	Loss 0.0082 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:45:53AM evaluateStage_trainer.py:145 [INFO] Train: [444/599] Final Prec@1 99.9644%
11/23 07:45:57AM evaluateStage_trainer.py:180 [INFO] Valid: [444/599] Final Prec@1 65.1000%
11/23 07:45:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:46:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [445][50/175]	Step 78370	lr 0.00474	Loss 0.0094 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:46:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [445][100/175]	Step 78420	lr 0.00474	Loss 0.0138 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:46:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [445][150/175]	Step 78470	lr 0.00474	Loss 0.0071 (0.0095)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:46:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [445][175/175]	Step 78495	lr 0.00474	Loss 0.0090 (0.0094)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:46:39AM evaluateStage_trainer.py:145 [INFO] Train: [445/599] Final Prec@1 99.9511%
11/23 07:46:43AM evaluateStage_trainer.py:180 [INFO] Valid: [445/599] Final Prec@1 64.4000%
11/23 07:46:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:46:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [446][50/175]	Step 78546	lr 0.00469	Loss 0.0071 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:47:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [446][100/175]	Step 78596	lr 0.00469	Loss 0.0081 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:47:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [446][150/175]	Step 78646	lr 0.00469	Loss 0.0078 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:47:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [446][175/175]	Step 78671	lr 0.00469	Loss 0.0068 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:47:25AM evaluateStage_trainer.py:145 [INFO] Train: [446/599] Final Prec@1 99.9733%
11/23 07:47:29AM evaluateStage_trainer.py:180 [INFO] Valid: [446/599] Final Prec@1 64.8800%
11/23 07:47:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:47:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [447][50/175]	Step 78722	lr 0.00465	Loss 0.0070 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:47:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [447][100/175]	Step 78772	lr 0.00465	Loss 0.0075 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [447][150/175]	Step 78822	lr 0.00465	Loss 0.0080 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [447][175/175]	Step 78847	lr 0.00465	Loss 0.0065 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:11AM evaluateStage_trainer.py:145 [INFO] Train: [447/599] Final Prec@1 99.9556%
11/23 07:48:15AM evaluateStage_trainer.py:180 [INFO] Valid: [447/599] Final Prec@1 64.2200%
11/23 07:48:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:48:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [448][50/175]	Step 78898	lr 0.0046	Loss 0.0056 (0.0088)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [448][100/175]	Step 78948	lr 0.0046	Loss 0.0076 (0.0087)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [448][150/175]	Step 78998	lr 0.0046	Loss 0.0065 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [448][175/175]	Step 79023	lr 0.0046	Loss 0.0062 (0.0089)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:48:57AM evaluateStage_trainer.py:145 [INFO] Train: [448/599] Final Prec@1 99.9622%
11/23 07:49:01AM evaluateStage_trainer.py:180 [INFO] Valid: [448/599] Final Prec@1 64.4200%
11/23 07:49:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:49:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [449][50/175]	Step 79074	lr 0.00456	Loss 0.0099 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:49:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [449][100/175]	Step 79124	lr 0.00456	Loss 0.0071 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:49:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [449][150/175]	Step 79174	lr 0.00456	Loss 0.0333 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:49:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [449][175/175]	Step 79199	lr 0.00456	Loss 0.0077 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:49:43AM evaluateStage_trainer.py:145 [INFO] Train: [449/599] Final Prec@1 99.9689%
11/23 07:49:46AM evaluateStage_trainer.py:180 [INFO] Valid: [449/599] Final Prec@1 64.3400%
11/23 07:49:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:50:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [450][50/175]	Step 79250	lr 0.00451	Loss 0.0065 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:50:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [450][100/175]	Step 79300	lr 0.00451	Loss 0.0111 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:50:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [450][150/175]	Step 79350	lr 0.00451	Loss 0.0069 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:50:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [450][175/175]	Step 79375	lr 0.00451	Loss 0.0210 (0.0085)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:50:29AM evaluateStage_trainer.py:145 [INFO] Train: [450/599] Final Prec@1 99.9667%
11/23 07:50:32AM evaluateStage_trainer.py:180 [INFO] Valid: [450/599] Final Prec@1 64.6000%
11/23 07:50:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:50:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [451][50/175]	Step 79426	lr 0.00447	Loss 0.0094 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:50:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [451][100/175]	Step 79476	lr 0.00447	Loss 0.0085 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:51:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [451][150/175]	Step 79526	lr 0.00447	Loss 0.0066 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:51:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [451][175/175]	Step 79551	lr 0.00447	Loss 0.0057 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:51:15AM evaluateStage_trainer.py:145 [INFO] Train: [451/599] Final Prec@1 99.9667%
11/23 07:51:19AM evaluateStage_trainer.py:180 [INFO] Valid: [451/599] Final Prec@1 65.1400%
11/23 07:51:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.1600%
11/23 07:51:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [452][50/175]	Step 79602	lr 0.00443	Loss 0.0107 (0.0081)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:51:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [452][100/175]	Step 79652	lr 0.00443	Loss 0.0074 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:51:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [452][150/175]	Step 79702	lr 0.00443	Loss 0.0122 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [452][175/175]	Step 79727	lr 0.00443	Loss 0.0061 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:01AM evaluateStage_trainer.py:145 [INFO] Train: [452/599] Final Prec@1 99.9689%
11/23 07:52:05AM evaluateStage_trainer.py:180 [INFO] Valid: [452/599] Final Prec@1 65.2800%
11/23 07:52:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.2800%
11/23 07:52:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [453][50/175]	Step 79778	lr 0.00438	Loss 0.0084 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [453][100/175]	Step 79828	lr 0.00438	Loss 0.0084 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [453][150/175]	Step 79878	lr 0.00438	Loss 0.0069 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [453][175/175]	Step 79903	lr 0.00438	Loss 0.0064 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:52:48AM evaluateStage_trainer.py:145 [INFO] Train: [453/599] Final Prec@1 99.9733%
11/23 07:52:51AM evaluateStage_trainer.py:180 [INFO] Valid: [453/599] Final Prec@1 64.7000%
11/23 07:52:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.2800%
11/23 07:53:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [454][50/175]	Step 79954	lr 0.00434	Loss 0.0065 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:53:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [454][100/175]	Step 80004	lr 0.00434	Loss 0.0097 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:53:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [454][150/175]	Step 80054	lr 0.00434	Loss 0.0078 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:53:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [454][175/175]	Step 80079	lr 0.00434	Loss 0.0062 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:53:34AM evaluateStage_trainer.py:145 [INFO] Train: [454/599] Final Prec@1 99.9689%
11/23 07:53:37AM evaluateStage_trainer.py:180 [INFO] Valid: [454/599] Final Prec@1 65.1600%
11/23 07:53:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.2800%
11/23 07:53:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [455][50/175]	Step 80130	lr 0.0043	Loss 0.0061 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:54:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [455][100/175]	Step 80180	lr 0.0043	Loss 0.0095 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:54:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [455][150/175]	Step 80230	lr 0.0043	Loss 0.0072 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:54:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [455][175/175]	Step 80255	lr 0.0043	Loss 0.0099 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:54:20AM evaluateStage_trainer.py:145 [INFO] Train: [455/599] Final Prec@1 99.9733%
11/23 07:54:23AM evaluateStage_trainer.py:180 [INFO] Valid: [455/599] Final Prec@1 64.6200%
11/23 07:54:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.2800%
11/23 07:54:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [456][50/175]	Step 80306	lr 0.00425	Loss 0.0053 (0.0085)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:54:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [456][100/175]	Step 80356	lr 0.00425	Loss 0.0058 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [456][150/175]	Step 80406	lr 0.00425	Loss 0.0102 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [456][175/175]	Step 80431	lr 0.00425	Loss 0.0115 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:06AM evaluateStage_trainer.py:145 [INFO] Train: [456/599] Final Prec@1 99.9644%
11/23 07:55:09AM evaluateStage_trainer.py:180 [INFO] Valid: [456/599] Final Prec@1 64.8600%
11/23 07:55:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.2800%
11/23 07:55:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [457][50/175]	Step 80482	lr 0.00421	Loss 0.0091 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [457][100/175]	Step 80532	lr 0.00421	Loss 0.0093 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [457][150/175]	Step 80582	lr 0.00421	Loss 0.0047 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [457][175/175]	Step 80607	lr 0.00421	Loss 0.0056 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:55:52AM evaluateStage_trainer.py:145 [INFO] Train: [457/599] Final Prec@1 99.9800%
11/23 07:55:55AM evaluateStage_trainer.py:180 [INFO] Valid: [457/599] Final Prec@1 65.3400%
11/23 07:55:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:56:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [458][50/175]	Step 80658	lr 0.00417	Loss 0.0069 (0.0083)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:56:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [458][100/175]	Step 80708	lr 0.00417	Loss 0.0062 (0.0083)	Prec@(1,5) (99.9%, 100.0%)	
11/23 07:56:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [458][150/175]	Step 80758	lr 0.00417	Loss 0.0075 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:56:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [458][175/175]	Step 80783	lr 0.00417	Loss 0.0066 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:56:38AM evaluateStage_trainer.py:145 [INFO] Train: [458/599] Final Prec@1 99.9556%
11/23 07:56:41AM evaluateStage_trainer.py:180 [INFO] Valid: [458/599] Final Prec@1 64.7600%
11/23 07:56:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:56:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [459][50/175]	Step 80834	lr 0.00412	Loss 0.0063 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:57:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [459][100/175]	Step 80884	lr 0.00412	Loss 0.0148 (0.0083)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:57:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [459][150/175]	Step 80934	lr 0.00412	Loss 0.0069 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:57:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [459][175/175]	Step 80959	lr 0.00412	Loss 0.0057 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:57:24AM evaluateStage_trainer.py:145 [INFO] Train: [459/599] Final Prec@1 99.9644%
11/23 07:57:27AM evaluateStage_trainer.py:180 [INFO] Valid: [459/599] Final Prec@1 64.7800%
11/23 07:57:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:57:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [460][50/175]	Step 81010	lr 0.00408	Loss 0.0075 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:57:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [460][100/175]	Step 81060	lr 0.00408	Loss 0.0086 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [460][150/175]	Step 81110	lr 0.00408	Loss 0.0068 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [460][175/175]	Step 81135	lr 0.00408	Loss 0.0088 (0.0081)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:10AM evaluateStage_trainer.py:145 [INFO] Train: [460/599] Final Prec@1 99.9667%
11/23 07:58:13AM evaluateStage_trainer.py:180 [INFO] Valid: [460/599] Final Prec@1 64.8600%
11/23 07:58:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:58:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [461][50/175]	Step 81186	lr 0.00404	Loss 0.0057 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [461][100/175]	Step 81236	lr 0.00404	Loss 0.0058 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [461][150/175]	Step 81286	lr 0.00404	Loss 0.0084 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [461][175/175]	Step 81311	lr 0.00404	Loss 0.0080 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:58:56AM evaluateStage_trainer.py:145 [INFO] Train: [461/599] Final Prec@1 99.9800%
11/23 07:58:59AM evaluateStage_trainer.py:180 [INFO] Valid: [461/599] Final Prec@1 64.3000%
11/23 07:58:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:59:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [462][50/175]	Step 81362	lr 0.004	Loss 0.0055 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:59:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [462][100/175]	Step 81412	lr 0.004	Loss 0.0065 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:59:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [462][150/175]	Step 81462	lr 0.004	Loss 0.0056 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:59:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [462][175/175]	Step 81487	lr 0.004	Loss 0.0083 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 07:59:42AM evaluateStage_trainer.py:145 [INFO] Train: [462/599] Final Prec@1 99.9778%
11/23 07:59:45AM evaluateStage_trainer.py:180 [INFO] Valid: [462/599] Final Prec@1 64.7400%
11/23 07:59:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 07:59:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [463][50/175]	Step 81538	lr 0.00396	Loss 0.0081 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:00:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [463][100/175]	Step 81588	lr 0.00396	Loss 0.0078 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:00:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [463][150/175]	Step 81638	lr 0.00396	Loss 0.0078 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:00:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [463][175/175]	Step 81663	lr 0.00396	Loss 0.0061 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:00:28AM evaluateStage_trainer.py:145 [INFO] Train: [463/599] Final Prec@1 99.9756%
11/23 08:00:31AM evaluateStage_trainer.py:180 [INFO] Valid: [463/599] Final Prec@1 65.1000%
11/23 08:00:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:00:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [464][50/175]	Step 81714	lr 0.00392	Loss 0.0088 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:00:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [464][100/175]	Step 81764	lr 0.00392	Loss 0.0061 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:01:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [464][150/175]	Step 81814	lr 0.00392	Loss 0.0093 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:01:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [464][175/175]	Step 81839	lr 0.00392	Loss 0.0088 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:01:14AM evaluateStage_trainer.py:145 [INFO] Train: [464/599] Final Prec@1 99.9622%
11/23 08:01:18AM evaluateStage_trainer.py:180 [INFO] Valid: [464/599] Final Prec@1 65.0000%
11/23 08:01:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:01:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [465][50/175]	Step 81890	lr 0.00388	Loss 0.0061 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:01:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [465][100/175]	Step 81940	lr 0.00388	Loss 0.0047 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:01:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [465][150/175]	Step 81990	lr 0.00388	Loss 0.0067 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [465][175/175]	Step 82015	lr 0.00388	Loss 0.0077 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:00AM evaluateStage_trainer.py:145 [INFO] Train: [465/599] Final Prec@1 99.9711%
11/23 08:02:04AM evaluateStage_trainer.py:180 [INFO] Valid: [465/599] Final Prec@1 64.2600%
11/23 08:02:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:02:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [466][50/175]	Step 82066	lr 0.00383	Loss 0.0077 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [466][100/175]	Step 82116	lr 0.00383	Loss 0.0069 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [466][150/175]	Step 82166	lr 0.00383	Loss 0.0123 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [466][175/175]	Step 82191	lr 0.00383	Loss 0.0065 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:02:46AM evaluateStage_trainer.py:145 [INFO] Train: [466/599] Final Prec@1 99.9711%
11/23 08:02:49AM evaluateStage_trainer.py:180 [INFO] Valid: [466/599] Final Prec@1 64.9800%
11/23 08:02:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:03:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [467][50/175]	Step 82242	lr 0.00379	Loss 0.0142 (0.0077)	Prec@(1,5) (99.9%, 100.0%)	
11/23 08:03:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [467][100/175]	Step 82292	lr 0.00379	Loss 0.0058 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:03:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [467][150/175]	Step 82342	lr 0.00379	Loss 0.0086 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:03:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [467][175/175]	Step 82367	lr 0.00379	Loss 0.0078 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:03:32AM evaluateStage_trainer.py:145 [INFO] Train: [467/599] Final Prec@1 99.9622%
11/23 08:03:36AM evaluateStage_trainer.py:180 [INFO] Valid: [467/599] Final Prec@1 64.0000%
11/23 08:03:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:03:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [468][50/175]	Step 82418	lr 0.00375	Loss 0.0088 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [468][100/175]	Step 82468	lr 0.00375	Loss 0.0084 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [468][150/175]	Step 82518	lr 0.00375	Loss 0.0080 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [468][175/175]	Step 82543	lr 0.00375	Loss 0.0057 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:18AM evaluateStage_trainer.py:145 [INFO] Train: [468/599] Final Prec@1 99.9711%
11/23 08:04:22AM evaluateStage_trainer.py:180 [INFO] Valid: [468/599] Final Prec@1 64.5800%
11/23 08:04:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:04:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [469][50/175]	Step 82594	lr 0.00371	Loss 0.0082 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [469][100/175]	Step 82644	lr 0.00371	Loss 0.0072 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:04:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [469][150/175]	Step 82694	lr 0.00371	Loss 0.0078 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:05:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [469][175/175]	Step 82719	lr 0.00371	Loss 0.0078 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:05:04AM evaluateStage_trainer.py:145 [INFO] Train: [469/599] Final Prec@1 99.9756%
11/23 08:05:07AM evaluateStage_trainer.py:180 [INFO] Valid: [469/599] Final Prec@1 64.6600%
11/23 08:05:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:05:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [470][50/175]	Step 82770	lr 0.00367	Loss 0.0111 (0.0080)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:05:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [470][100/175]	Step 82820	lr 0.00367	Loss 0.0090 (0.0082)	Prec@(1,5) (99.9%, 100.0%)	
11/23 08:05:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [470][150/175]	Step 82870	lr 0.00367	Loss 0.0065 (0.0082)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:05:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [470][175/175]	Step 82895	lr 0.00367	Loss 0.0068 (0.0084)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:05:50AM evaluateStage_trainer.py:145 [INFO] Train: [470/599] Final Prec@1 99.9533%
11/23 08:05:53AM evaluateStage_trainer.py:180 [INFO] Valid: [470/599] Final Prec@1 64.8600%
11/23 08:05:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:06:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [471][50/175]	Step 82946	lr 0.00363	Loss 0.0061 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:06:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [471][100/175]	Step 82996	lr 0.00363	Loss 0.0065 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:06:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [471][150/175]	Step 83046	lr 0.00363	Loss 0.0138 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:06:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [471][175/175]	Step 83071	lr 0.00363	Loss 0.0089 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:06:36AM evaluateStage_trainer.py:145 [INFO] Train: [471/599] Final Prec@1 99.9800%
11/23 08:06:39AM evaluateStage_trainer.py:180 [INFO] Valid: [471/599] Final Prec@1 65.0800%
11/23 08:06:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:06:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [472][50/175]	Step 83122	lr 0.0036	Loss 0.0049 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:07:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [472][100/175]	Step 83172	lr 0.0036	Loss 0.0059 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:07:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [472][150/175]	Step 83222	lr 0.0036	Loss 0.0073 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:07:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [472][175/175]	Step 83247	lr 0.0036	Loss 0.0088 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:07:22AM evaluateStage_trainer.py:145 [INFO] Train: [472/599] Final Prec@1 99.9733%
11/23 08:07:25AM evaluateStage_trainer.py:180 [INFO] Valid: [472/599] Final Prec@1 65.2400%
11/23 08:07:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:07:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [473][50/175]	Step 83298	lr 0.00356	Loss 0.0076 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:07:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [473][100/175]	Step 83348	lr 0.00356	Loss 0.0060 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [473][150/175]	Step 83398	lr 0.00356	Loss 0.0074 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [473][175/175]	Step 83423	lr 0.00356	Loss 0.0058 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:08AM evaluateStage_trainer.py:145 [INFO] Train: [473/599] Final Prec@1 99.9733%
11/23 08:08:12AM evaluateStage_trainer.py:180 [INFO] Valid: [473/599] Final Prec@1 64.8600%
11/23 08:08:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:08:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [474][50/175]	Step 83474	lr 0.00352	Loss 0.0066 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [474][100/175]	Step 83524	lr 0.00352	Loss 0.0097 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [474][150/175]	Step 83574	lr 0.00352	Loss 0.0058 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [474][175/175]	Step 83599	lr 0.00352	Loss 0.0071 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:08:54AM evaluateStage_trainer.py:145 [INFO] Train: [474/599] Final Prec@1 99.9711%
11/23 08:08:58AM evaluateStage_trainer.py:180 [INFO] Valid: [474/599] Final Prec@1 64.7800%
11/23 08:08:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:09:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [475][50/175]	Step 83650	lr 0.00348	Loss 0.0084 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:09:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [475][100/175]	Step 83700	lr 0.00348	Loss 0.0041 (0.0077)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:09:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [475][150/175]	Step 83750	lr 0.00348	Loss 0.0071 (0.0078)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:09:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [475][175/175]	Step 83775	lr 0.00348	Loss 0.0062 (0.0079)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:09:40AM evaluateStage_trainer.py:145 [INFO] Train: [475/599] Final Prec@1 99.9733%
11/23 08:09:44AM evaluateStage_trainer.py:180 [INFO] Valid: [475/599] Final Prec@1 64.7800%
11/23 08:09:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:09:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [476][50/175]	Step 83826	lr 0.00344	Loss 0.0059 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:10:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [476][100/175]	Step 83876	lr 0.00344	Loss 0.0073 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:10:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [476][150/175]	Step 83926	lr 0.00344	Loss 0.0067 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:10:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [476][175/175]	Step 83951	lr 0.00344	Loss 0.0065 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:10:26AM evaluateStage_trainer.py:145 [INFO] Train: [476/599] Final Prec@1 99.9822%
11/23 08:10:30AM evaluateStage_trainer.py:180 [INFO] Valid: [476/599] Final Prec@1 65.1200%
11/23 08:10:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:10:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [477][50/175]	Step 84002	lr 0.0034	Loss 0.0088 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:10:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [477][100/175]	Step 84052	lr 0.0034	Loss 0.0062 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [477][150/175]	Step 84102	lr 0.0034	Loss 0.0071 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [477][175/175]	Step 84127	lr 0.0034	Loss 0.0147 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:12AM evaluateStage_trainer.py:145 [INFO] Train: [477/599] Final Prec@1 99.9756%
11/23 08:11:15AM evaluateStage_trainer.py:180 [INFO] Valid: [477/599] Final Prec@1 64.7400%
11/23 08:11:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:11:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [478][50/175]	Step 84178	lr 0.00337	Loss 0.0063 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [478][100/175]	Step 84228	lr 0.00337	Loss 0.0071 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [478][150/175]	Step 84278	lr 0.00337	Loss 0.0092 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [478][175/175]	Step 84303	lr 0.00337	Loss 0.0054 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:11:58AM evaluateStage_trainer.py:145 [INFO] Train: [478/599] Final Prec@1 99.9733%
11/23 08:12:01AM evaluateStage_trainer.py:180 [INFO] Valid: [478/599] Final Prec@1 64.8400%
11/23 08:12:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:12:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [479][50/175]	Step 84354	lr 0.00333	Loss 0.0062 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:12:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [479][100/175]	Step 84404	lr 0.00333	Loss 0.0051 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:12:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [479][150/175]	Step 84454	lr 0.00333	Loss 0.0079 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:12:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [479][175/175]	Step 84479	lr 0.00333	Loss 0.0062 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:12:44AM evaluateStage_trainer.py:145 [INFO] Train: [479/599] Final Prec@1 99.9689%
11/23 08:12:47AM evaluateStage_trainer.py:180 [INFO] Valid: [479/599] Final Prec@1 64.8800%
11/23 08:12:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:13:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [480][50/175]	Step 84530	lr 0.00329	Loss 0.0057 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:13:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [480][100/175]	Step 84580	lr 0.00329	Loss 0.0101 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:13:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [480][150/175]	Step 84630	lr 0.00329	Loss 0.0059 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:13:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [480][175/175]	Step 84655	lr 0.00329	Loss 0.0071 (0.0076)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:13:30AM evaluateStage_trainer.py:145 [INFO] Train: [480/599] Final Prec@1 99.9667%
11/23 08:13:33AM evaluateStage_trainer.py:180 [INFO] Valid: [480/599] Final Prec@1 64.4800%
11/23 08:13:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:13:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [481][50/175]	Step 84706	lr 0.00325	Loss 0.0061 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:13:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [481][100/175]	Step 84756	lr 0.00325	Loss 0.0081 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:14:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [481][150/175]	Step 84806	lr 0.00325	Loss 0.0073 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:14:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [481][175/175]	Step 84831	lr 0.00325	Loss 0.0077 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:14:16AM evaluateStage_trainer.py:145 [INFO] Train: [481/599] Final Prec@1 99.9778%
11/23 08:14:19AM evaluateStage_trainer.py:180 [INFO] Valid: [481/599] Final Prec@1 64.6800%
11/23 08:14:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:14:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [482][50/175]	Step 84882	lr 0.00322	Loss 0.0072 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:14:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [482][100/175]	Step 84932	lr 0.00322	Loss 0.0067 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:14:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [482][150/175]	Step 84982	lr 0.00322	Loss 0.0062 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [482][175/175]	Step 85007	lr 0.00322	Loss 0.0062 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:02AM evaluateStage_trainer.py:145 [INFO] Train: [482/599] Final Prec@1 99.9778%
11/23 08:15:05AM evaluateStage_trainer.py:180 [INFO] Valid: [482/599] Final Prec@1 64.4200%
11/23 08:15:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:15:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [483][50/175]	Step 85058	lr 0.00318	Loss 0.0074 (0.0074)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [483][100/175]	Step 85108	lr 0.00318	Loss 0.0084 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [483][150/175]	Step 85158	lr 0.00318	Loss 0.0138 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [483][175/175]	Step 85183	lr 0.00318	Loss 0.0052 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:15:48AM evaluateStage_trainer.py:145 [INFO] Train: [483/599] Final Prec@1 99.9733%
11/23 08:15:51AM evaluateStage_trainer.py:180 [INFO] Valid: [483/599] Final Prec@1 65.3400%
11/23 08:15:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:16:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [484][50/175]	Step 85234	lr 0.00315	Loss 0.0048 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:16:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [484][100/175]	Step 85284	lr 0.00315	Loss 0.0066 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:16:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [484][150/175]	Step 85334	lr 0.00315	Loss 0.0051 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:16:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [484][175/175]	Step 85359	lr 0.00315	Loss 0.0069 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:16:34AM evaluateStage_trainer.py:145 [INFO] Train: [484/599] Final Prec@1 99.9778%
11/23 08:16:37AM evaluateStage_trainer.py:180 [INFO] Valid: [484/599] Final Prec@1 65.1000%
11/23 08:16:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:16:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [485][50/175]	Step 85410	lr 0.00311	Loss 0.0055 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [485][100/175]	Step 85460	lr 0.00311	Loss 0.0056 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [485][150/175]	Step 85510	lr 0.00311	Loss 0.0055 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [485][175/175]	Step 85535	lr 0.00311	Loss 0.0090 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:19AM evaluateStage_trainer.py:145 [INFO] Train: [485/599] Final Prec@1 99.9800%
11/23 08:17:23AM evaluateStage_trainer.py:180 [INFO] Valid: [485/599] Final Prec@1 64.5400%
11/23 08:17:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:17:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [486][50/175]	Step 85586	lr 0.00308	Loss 0.0065 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [486][100/175]	Step 85636	lr 0.00308	Loss 0.0049 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:17:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [486][150/175]	Step 85686	lr 0.00308	Loss 0.0059 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [486][175/175]	Step 85711	lr 0.00308	Loss 0.0080 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:05AM evaluateStage_trainer.py:145 [INFO] Train: [486/599] Final Prec@1 99.9689%
11/23 08:18:09AM evaluateStage_trainer.py:180 [INFO] Valid: [486/599] Final Prec@1 64.4200%
11/23 08:18:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:18:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [487][50/175]	Step 85762	lr 0.00304	Loss 0.0171 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [487][100/175]	Step 85812	lr 0.00304	Loss 0.0059 (0.0075)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [487][150/175]	Step 85862	lr 0.00304	Loss 0.0051 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [487][175/175]	Step 85887	lr 0.00304	Loss 0.0067 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:18:52AM evaluateStage_trainer.py:145 [INFO] Train: [487/599] Final Prec@1 99.9733%
11/23 08:18:55AM evaluateStage_trainer.py:180 [INFO] Valid: [487/599] Final Prec@1 63.8600%
11/23 08:18:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:19:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [488][50/175]	Step 85938	lr 0.003	Loss 0.0060 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:19:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [488][100/175]	Step 85988	lr 0.003	Loss 0.0086 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:19:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [488][150/175]	Step 86038	lr 0.003	Loss 0.0065 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:19:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [488][175/175]	Step 86063	lr 0.003	Loss 0.0067 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:19:38AM evaluateStage_trainer.py:145 [INFO] Train: [488/599] Final Prec@1 99.9711%
11/23 08:19:41AM evaluateStage_trainer.py:180 [INFO] Valid: [488/599] Final Prec@1 65.0800%
11/23 08:19:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:19:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [489][50/175]	Step 86114	lr 0.00297	Loss 0.0056 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:20:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [489][100/175]	Step 86164	lr 0.00297	Loss 0.0082 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:20:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [489][150/175]	Step 86214	lr 0.00297	Loss 0.0081 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:20:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [489][175/175]	Step 86239	lr 0.00297	Loss 0.0051 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:20:24AM evaluateStage_trainer.py:145 [INFO] Train: [489/599] Final Prec@1 99.9756%
11/23 08:20:27AM evaluateStage_trainer.py:180 [INFO] Valid: [489/599] Final Prec@1 65.2200%
11/23 08:20:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:20:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [490][50/175]	Step 86290	lr 0.00294	Loss 0.0067 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:20:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [490][100/175]	Step 86340	lr 0.00294	Loss 0.0060 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [490][150/175]	Step 86390	lr 0.00294	Loss 0.0053 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [490][175/175]	Step 86415	lr 0.00294	Loss 0.0056 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:10AM evaluateStage_trainer.py:145 [INFO] Train: [490/599] Final Prec@1 99.9711%
11/23 08:21:13AM evaluateStage_trainer.py:180 [INFO] Valid: [490/599] Final Prec@1 64.8400%
11/23 08:21:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:21:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [491][50/175]	Step 86466	lr 0.0029	Loss 0.0058 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [491][100/175]	Step 86516	lr 0.0029	Loss 0.0061 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [491][150/175]	Step 86566	lr 0.0029	Loss 0.0071 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [491][175/175]	Step 86591	lr 0.0029	Loss 0.0067 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:21:56AM evaluateStage_trainer.py:145 [INFO] Train: [491/599] Final Prec@1 99.9711%
11/23 08:21:59AM evaluateStage_trainer.py:180 [INFO] Valid: [491/599] Final Prec@1 64.6600%
11/23 08:22:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:22:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [492][50/175]	Step 86642	lr 0.00287	Loss 0.0054 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:22:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [492][100/175]	Step 86692	lr 0.00287	Loss 0.0071 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:22:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [492][150/175]	Step 86742	lr 0.00287	Loss 0.0077 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:22:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [492][175/175]	Step 86767	lr 0.00287	Loss 0.0043 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:22:42AM evaluateStage_trainer.py:145 [INFO] Train: [492/599] Final Prec@1 99.9756%
11/23 08:22:45AM evaluateStage_trainer.py:180 [INFO] Valid: [492/599] Final Prec@1 65.0400%
11/23 08:22:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:22:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [493][50/175]	Step 86818	lr 0.00283	Loss 0.0135 (0.0073)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:23:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [493][100/175]	Step 86868	lr 0.00283	Loss 0.0057 (0.0072)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:23:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [493][150/175]	Step 86918	lr 0.00283	Loss 0.0086 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:23:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [493][175/175]	Step 86943	lr 0.00283	Loss 0.0070 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:23:28AM evaluateStage_trainer.py:145 [INFO] Train: [493/599] Final Prec@1 99.9711%
11/23 08:23:31AM evaluateStage_trainer.py:180 [INFO] Valid: [493/599] Final Prec@1 64.7800%
11/23 08:23:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:23:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [494][50/175]	Step 86994	lr 0.0028	Loss 0.0074 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:23:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [494][100/175]	Step 87044	lr 0.0028	Loss 0.0067 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:24:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [494][150/175]	Step 87094	lr 0.0028	Loss 0.0104 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:24:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [494][175/175]	Step 87119	lr 0.0028	Loss 0.0080 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:24:14AM evaluateStage_trainer.py:145 [INFO] Train: [494/599] Final Prec@1 99.9778%
11/23 08:24:17AM evaluateStage_trainer.py:180 [INFO] Valid: [494/599] Final Prec@1 64.8400%
11/23 08:24:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:24:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [495][50/175]	Step 87170	lr 0.00277	Loss 0.0057 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:24:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [495][100/175]	Step 87220	lr 0.00277	Loss 0.0054 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:24:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [495][150/175]	Step 87270	lr 0.00277	Loss 0.0063 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [495][175/175]	Step 87295	lr 0.00277	Loss 0.0068 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:00AM evaluateStage_trainer.py:145 [INFO] Train: [495/599] Final Prec@1 99.9689%
11/23 08:25:03AM evaluateStage_trainer.py:180 [INFO] Valid: [495/599] Final Prec@1 65.1200%
11/23 08:25:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:25:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [496][50/175]	Step 87346	lr 0.00274	Loss 0.0072 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [496][100/175]	Step 87396	lr 0.00274	Loss 0.0061 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [496][150/175]	Step 87446	lr 0.00274	Loss 0.0131 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [496][175/175]	Step 87471	lr 0.00274	Loss 0.0056 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:25:46AM evaluateStage_trainer.py:145 [INFO] Train: [496/599] Final Prec@1 99.9711%
11/23 08:25:49AM evaluateStage_trainer.py:180 [INFO] Valid: [496/599] Final Prec@1 64.4800%
11/23 08:25:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:26:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [497][50/175]	Step 87522	lr 0.0027	Loss 0.0058 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:26:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [497][100/175]	Step 87572	lr 0.0027	Loss 0.0055 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:26:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [497][150/175]	Step 87622	lr 0.0027	Loss 0.0075 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:26:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [497][175/175]	Step 87647	lr 0.0027	Loss 0.0056 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:26:32AM evaluateStage_trainer.py:145 [INFO] Train: [497/599] Final Prec@1 99.9756%
11/23 08:26:35AM evaluateStage_trainer.py:180 [INFO] Valid: [497/599] Final Prec@1 65.0800%
11/23 08:26:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:26:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [498][50/175]	Step 87698	lr 0.00267	Loss 0.0058 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [498][100/175]	Step 87748	lr 0.00267	Loss 0.0072 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [498][150/175]	Step 87798	lr 0.00267	Loss 0.0081 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [498][175/175]	Step 87823	lr 0.00267	Loss 0.0054 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:18AM evaluateStage_trainer.py:145 [INFO] Train: [498/599] Final Prec@1 99.9800%
11/23 08:27:21AM evaluateStage_trainer.py:180 [INFO] Valid: [498/599] Final Prec@1 64.5600%
11/23 08:27:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:27:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [499][50/175]	Step 87874	lr 0.00264	Loss 0.0073 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [499][100/175]	Step 87924	lr 0.00264	Loss 0.0041 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:27:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [499][150/175]	Step 87974	lr 0.00264	Loss 0.0086 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [499][175/175]	Step 87999	lr 0.00264	Loss 0.0048 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:04AM evaluateStage_trainer.py:145 [INFO] Train: [499/599] Final Prec@1 99.9778%
11/23 08:28:07AM evaluateStage_trainer.py:180 [INFO] Valid: [499/599] Final Prec@1 64.8200%
11/23 08:28:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:28:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [500][50/175]	Step 88050	lr 0.00261	Loss 0.0094 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [500][100/175]	Step 88100	lr 0.00261	Loss 0.0072 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [500][150/175]	Step 88150	lr 0.00261	Loss 0.0085 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [500][175/175]	Step 88175	lr 0.00261	Loss 0.0053 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:28:50AM evaluateStage_trainer.py:145 [INFO] Train: [500/599] Final Prec@1 99.9756%
11/23 08:28:53AM evaluateStage_trainer.py:180 [INFO] Valid: [500/599] Final Prec@1 64.5800%
11/23 08:28:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:29:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [501][50/175]	Step 88226	lr 0.00258	Loss 0.0059 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:29:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [501][100/175]	Step 88276	lr 0.00258	Loss 0.0054 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:29:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [501][150/175]	Step 88326	lr 0.00258	Loss 0.0077 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:29:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [501][175/175]	Step 88351	lr 0.00258	Loss 0.0058 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:29:36AM evaluateStage_trainer.py:145 [INFO] Train: [501/599] Final Prec@1 99.9778%
11/23 08:29:39AM evaluateStage_trainer.py:180 [INFO] Valid: [501/599] Final Prec@1 64.6600%
11/23 08:29:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:29:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [502][50/175]	Step 88402	lr 0.00255	Loss 0.0041 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:30:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [502][100/175]	Step 88452	lr 0.00255	Loss 0.0076 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:30:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [502][150/175]	Step 88502	lr 0.00255	Loss 0.0170 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:30:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [502][175/175]	Step 88527	lr 0.00255	Loss 0.0053 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:30:22AM evaluateStage_trainer.py:145 [INFO] Train: [502/599] Final Prec@1 99.9778%
11/23 08:30:25AM evaluateStage_trainer.py:180 [INFO] Valid: [502/599] Final Prec@1 64.7000%
11/23 08:30:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:30:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [503][50/175]	Step 88578	lr 0.00251	Loss 0.0074 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:30:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [503][100/175]	Step 88628	lr 0.00251	Loss 0.0044 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [503][150/175]	Step 88678	lr 0.00251	Loss 0.0050 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [503][175/175]	Step 88703	lr 0.00251	Loss 0.0048 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:08AM evaluateStage_trainer.py:145 [INFO] Train: [503/599] Final Prec@1 99.9800%
11/23 08:31:11AM evaluateStage_trainer.py:180 [INFO] Valid: [503/599] Final Prec@1 64.7000%
11/23 08:31:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:31:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [504][50/175]	Step 88754	lr 0.00248	Loss 0.0053 (0.0071)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [504][100/175]	Step 88804	lr 0.00248	Loss 0.0077 (0.0070)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [504][150/175]	Step 88854	lr 0.00248	Loss 0.0095 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [504][175/175]	Step 88879	lr 0.00248	Loss 0.0065 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:31:54AM evaluateStage_trainer.py:145 [INFO] Train: [504/599] Final Prec@1 99.9756%
11/23 08:31:57AM evaluateStage_trainer.py:180 [INFO] Valid: [504/599] Final Prec@1 64.8800%
11/23 08:31:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:32:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [505][50/175]	Step 88930	lr 0.00245	Loss 0.0059 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:32:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [505][100/175]	Step 88980	lr 0.00245	Loss 0.0063 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:32:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [505][150/175]	Step 89030	lr 0.00245	Loss 0.0086 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:32:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [505][175/175]	Step 89055	lr 0.00245	Loss 0.0055 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:32:40AM evaluateStage_trainer.py:145 [INFO] Train: [505/599] Final Prec@1 99.9711%
11/23 08:32:43AM evaluateStage_trainer.py:180 [INFO] Valid: [505/599] Final Prec@1 64.9600%
11/23 08:32:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:32:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [506][50/175]	Step 89106	lr 0.00242	Loss 0.0073 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:33:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [506][100/175]	Step 89156	lr 0.00242	Loss 0.0051 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:33:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [506][150/175]	Step 89206	lr 0.00242	Loss 0.0085 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:33:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [506][175/175]	Step 89231	lr 0.00242	Loss 0.0101 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:33:25AM evaluateStage_trainer.py:145 [INFO] Train: [506/599] Final Prec@1 99.9800%
11/23 08:33:29AM evaluateStage_trainer.py:180 [INFO] Valid: [506/599] Final Prec@1 64.2400%
11/23 08:33:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:33:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [507][50/175]	Step 89282	lr 0.00239	Loss 0.0054 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:33:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [507][100/175]	Step 89332	lr 0.00239	Loss 0.0080 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [507][150/175]	Step 89382	lr 0.00239	Loss 0.0062 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [507][175/175]	Step 89407	lr 0.00239	Loss 0.0079 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:11AM evaluateStage_trainer.py:145 [INFO] Train: [507/599] Final Prec@1 99.9756%
11/23 08:34:15AM evaluateStage_trainer.py:180 [INFO] Valid: [507/599] Final Prec@1 64.6200%
11/23 08:34:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:34:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [508][50/175]	Step 89458	lr 0.00237	Loss 0.0063 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [508][100/175]	Step 89508	lr 0.00237	Loss 0.0084 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [508][150/175]	Step 89558	lr 0.00237	Loss 0.0080 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [508][175/175]	Step 89583	lr 0.00237	Loss 0.0059 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:34:58AM evaluateStage_trainer.py:145 [INFO] Train: [508/599] Final Prec@1 99.9778%
11/23 08:35:01AM evaluateStage_trainer.py:180 [INFO] Valid: [508/599] Final Prec@1 65.0800%
11/23 08:35:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:35:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [509][50/175]	Step 89634	lr 0.00234	Loss 0.0047 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:35:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [509][100/175]	Step 89684	lr 0.00234	Loss 0.0075 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:35:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [509][150/175]	Step 89734	lr 0.00234	Loss 0.0073 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:35:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [509][175/175]	Step 89759	lr 0.00234	Loss 0.0067 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:35:43AM evaluateStage_trainer.py:145 [INFO] Train: [509/599] Final Prec@1 99.9778%
11/23 08:35:47AM evaluateStage_trainer.py:180 [INFO] Valid: [509/599] Final Prec@1 64.8000%
11/23 08:35:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:36:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [510][50/175]	Step 89810	lr 0.00231	Loss 0.0049 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:36:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [510][100/175]	Step 89860	lr 0.00231	Loss 0.0069 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:36:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [510][150/175]	Step 89910	lr 0.00231	Loss 0.0062 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:36:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [510][175/175]	Step 89935	lr 0.00231	Loss 0.0071 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:36:29AM evaluateStage_trainer.py:145 [INFO] Train: [510/599] Final Prec@1 99.9756%
11/23 08:36:33AM evaluateStage_trainer.py:180 [INFO] Valid: [510/599] Final Prec@1 64.9000%
11/23 08:36:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:36:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [511][50/175]	Step 89986	lr 0.00228	Loss 0.0078 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:36:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [511][100/175]	Step 90036	lr 0.00228	Loss 0.0051 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:37:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [511][150/175]	Step 90086	lr 0.00228	Loss 0.0065 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:37:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [511][175/175]	Step 90111	lr 0.00228	Loss 0.0086 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:37:15AM evaluateStage_trainer.py:145 [INFO] Train: [511/599] Final Prec@1 99.9711%
11/23 08:37:19AM evaluateStage_trainer.py:180 [INFO] Valid: [511/599] Final Prec@1 64.7200%
11/23 08:37:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:37:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [512][50/175]	Step 90162	lr 0.00225	Loss 0.0053 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:37:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [512][100/175]	Step 90212	lr 0.00225	Loss 0.0067 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:37:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [512][150/175]	Step 90262	lr 0.00225	Loss 0.0052 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [512][175/175]	Step 90287	lr 0.00225	Loss 0.0051 (0.0068)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:01AM evaluateStage_trainer.py:145 [INFO] Train: [512/599] Final Prec@1 99.9711%
11/23 08:38:04AM evaluateStage_trainer.py:180 [INFO] Valid: [512/599] Final Prec@1 65.1200%
11/23 08:38:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:38:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [513][50/175]	Step 90338	lr 0.00222	Loss 0.0071 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [513][100/175]	Step 90388	lr 0.00222	Loss 0.0041 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [513][150/175]	Step 90438	lr 0.00222	Loss 0.0060 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [513][175/175]	Step 90463	lr 0.00222	Loss 0.0081 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:38:47AM evaluateStage_trainer.py:145 [INFO] Train: [513/599] Final Prec@1 99.9756%
11/23 08:38:50AM evaluateStage_trainer.py:180 [INFO] Valid: [513/599] Final Prec@1 64.8400%
11/23 08:38:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:39:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [514][50/175]	Step 90514	lr 0.0022	Loss 0.0059 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:39:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [514][100/175]	Step 90564	lr 0.0022	Loss 0.0062 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:39:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [514][150/175]	Step 90614	lr 0.0022	Loss 0.0064 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:39:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [514][175/175]	Step 90639	lr 0.0022	Loss 0.0084 (0.0069)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:39:33AM evaluateStage_trainer.py:145 [INFO] Train: [514/599] Final Prec@1 99.9756%
11/23 08:39:36AM evaluateStage_trainer.py:180 [INFO] Valid: [514/599] Final Prec@1 64.9200%
11/23 08:39:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:39:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [515][50/175]	Step 90690	lr 0.00217	Loss 0.0058 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [515][100/175]	Step 90740	lr 0.00217	Loss 0.0052 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [515][150/175]	Step 90790	lr 0.00217	Loss 0.0077 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [515][175/175]	Step 90815	lr 0.00217	Loss 0.0069 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:19AM evaluateStage_trainer.py:145 [INFO] Train: [515/599] Final Prec@1 99.9778%
11/23 08:40:22AM evaluateStage_trainer.py:180 [INFO] Valid: [515/599] Final Prec@1 64.8000%
11/23 08:40:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:40:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [516][50/175]	Step 90866	lr 0.00214	Loss 0.0070 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [516][100/175]	Step 90916	lr 0.00214	Loss 0.0060 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:40:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [516][150/175]	Step 90966	lr 0.00214	Loss 0.0100 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [516][175/175]	Step 90991	lr 0.00214	Loss 0.0064 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:05AM evaluateStage_trainer.py:145 [INFO] Train: [516/599] Final Prec@1 99.9778%
11/23 08:41:08AM evaluateStage_trainer.py:180 [INFO] Valid: [516/599] Final Prec@1 64.9400%
11/23 08:41:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:41:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [517][50/175]	Step 91042	lr 0.00212	Loss 0.0065 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [517][100/175]	Step 91092	lr 0.00212	Loss 0.0092 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [517][150/175]	Step 91142	lr 0.00212	Loss 0.0061 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [517][175/175]	Step 91167	lr 0.00212	Loss 0.0067 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:41:51AM evaluateStage_trainer.py:145 [INFO] Train: [517/599] Final Prec@1 99.9644%
11/23 08:41:55AM evaluateStage_trainer.py:180 [INFO] Valid: [517/599] Final Prec@1 65.1000%
11/23 08:41:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.3400%
11/23 08:42:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [518][50/175]	Step 91218	lr 0.00209	Loss 0.0069 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:42:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [518][100/175]	Step 91268	lr 0.00209	Loss 0.0088 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:42:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [518][150/175]	Step 91318	lr 0.00209	Loss 0.0049 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:42:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [518][175/175]	Step 91343	lr 0.00209	Loss 0.0059 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:42:37AM evaluateStage_trainer.py:145 [INFO] Train: [518/599] Final Prec@1 99.9800%
11/23 08:42:41AM evaluateStage_trainer.py:180 [INFO] Valid: [518/599] Final Prec@1 65.4000%
11/23 08:42:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:42:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [519][50/175]	Step 91394	lr 0.00206	Loss 0.0053 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:43:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [519][100/175]	Step 91444	lr 0.00206	Loss 0.0070 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:43:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [519][150/175]	Step 91494	lr 0.00206	Loss 0.0065 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:43:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [519][175/175]	Step 91519	lr 0.00206	Loss 0.0056 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:43:24AM evaluateStage_trainer.py:145 [INFO] Train: [519/599] Final Prec@1 99.9778%
11/23 08:43:27AM evaluateStage_trainer.py:180 [INFO] Valid: [519/599] Final Prec@1 64.6600%
11/23 08:43:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:43:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [520][50/175]	Step 91570	lr 0.00204	Loss 0.0049 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:43:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [520][100/175]	Step 91620	lr 0.00204	Loss 0.0076 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [520][150/175]	Step 91670	lr 0.00204	Loss 0.0060 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [520][175/175]	Step 91695	lr 0.00204	Loss 0.0061 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:10AM evaluateStage_trainer.py:145 [INFO] Train: [520/599] Final Prec@1 99.9778%
11/23 08:44:13AM evaluateStage_trainer.py:180 [INFO] Valid: [520/599] Final Prec@1 64.9600%
11/23 08:44:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:44:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [521][50/175]	Step 91746	lr 0.00201	Loss 0.0048 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [521][100/175]	Step 91796	lr 0.00201	Loss 0.0052 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [521][150/175]	Step 91846	lr 0.00201	Loss 0.0050 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [521][175/175]	Step 91871	lr 0.00201	Loss 0.0064 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:44:56AM evaluateStage_trainer.py:145 [INFO] Train: [521/599] Final Prec@1 99.9733%
11/23 08:44:59AM evaluateStage_trainer.py:180 [INFO] Valid: [521/599] Final Prec@1 64.9800%
11/23 08:44:59AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:45:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [522][50/175]	Step 91922	lr 0.00199	Loss 0.0057 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:45:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [522][100/175]	Step 91972	lr 0.00199	Loss 0.0061 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:45:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [522][150/175]	Step 92022	lr 0.00199	Loss 0.0060 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:45:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [522][175/175]	Step 92047	lr 0.00199	Loss 0.0068 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:45:42AM evaluateStage_trainer.py:145 [INFO] Train: [522/599] Final Prec@1 99.9733%
11/23 08:45:45AM evaluateStage_trainer.py:180 [INFO] Valid: [522/599] Final Prec@1 64.9800%
11/23 08:45:45AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:45:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [523][50/175]	Step 92098	lr 0.00196	Loss 0.0057 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:46:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [523][100/175]	Step 92148	lr 0.00196	Loss 0.0135 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:46:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [523][150/175]	Step 92198	lr 0.00196	Loss 0.0056 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:46:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [523][175/175]	Step 92223	lr 0.00196	Loss 0.0057 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:46:28AM evaluateStage_trainer.py:145 [INFO] Train: [523/599] Final Prec@1 99.9756%
11/23 08:46:31AM evaluateStage_trainer.py:180 [INFO] Valid: [523/599] Final Prec@1 64.8800%
11/23 08:46:31AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:46:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [524][50/175]	Step 92274	lr 0.00194	Loss 0.0050 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:46:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [524][100/175]	Step 92324	lr 0.00194	Loss 0.0056 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [524][150/175]	Step 92374	lr 0.00194	Loss 0.0071 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [524][175/175]	Step 92399	lr 0.00194	Loss 0.0050 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:14AM evaluateStage_trainer.py:145 [INFO] Train: [524/599] Final Prec@1 99.9756%
11/23 08:47:17AM evaluateStage_trainer.py:180 [INFO] Valid: [524/599] Final Prec@1 64.5600%
11/23 08:47:17AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:47:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [525][50/175]	Step 92450	lr 0.00191	Loss 0.0066 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [525][100/175]	Step 92500	lr 0.00191	Loss 0.0050 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [525][150/175]	Step 92550	lr 0.00191	Loss 0.0059 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:47:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [525][175/175]	Step 92575	lr 0.00191	Loss 0.0047 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:48:00AM evaluateStage_trainer.py:145 [INFO] Train: [525/599] Final Prec@1 99.9844%
11/23 08:48:03AM evaluateStage_trainer.py:180 [INFO] Valid: [525/599] Final Prec@1 64.9400%
11/23 08:48:03AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:48:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [526][50/175]	Step 92626	lr 0.00189	Loss 0.0061 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:48:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [526][100/175]	Step 92676	lr 0.00189	Loss 0.0040 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:48:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [526][150/175]	Step 92726	lr 0.00189	Loss 0.0051 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:48:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [526][175/175]	Step 92751	lr 0.00189	Loss 0.0056 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:48:46AM evaluateStage_trainer.py:145 [INFO] Train: [526/599] Final Prec@1 99.9667%
11/23 08:48:49AM evaluateStage_trainer.py:180 [INFO] Valid: [526/599] Final Prec@1 64.6600%
11/23 08:48:49AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:49:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [527][50/175]	Step 92802	lr 0.00187	Loss 0.0068 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:49:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [527][100/175]	Step 92852	lr 0.00187	Loss 0.0070 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:49:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [527][150/175]	Step 92902	lr 0.00187	Loss 0.0066 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:49:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [527][175/175]	Step 92927	lr 0.00187	Loss 0.0044 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:49:32AM evaluateStage_trainer.py:145 [INFO] Train: [527/599] Final Prec@1 99.9778%
11/23 08:49:35AM evaluateStage_trainer.py:180 [INFO] Valid: [527/599] Final Prec@1 64.6600%
11/23 08:49:35AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:49:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [528][50/175]	Step 92978	lr 0.00184	Loss 0.0061 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [528][100/175]	Step 93028	lr 0.00184	Loss 0.0070 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [528][150/175]	Step 93078	lr 0.00184	Loss 0.0066 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [528][175/175]	Step 93103	lr 0.00184	Loss 0.0158 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:17AM evaluateStage_trainer.py:145 [INFO] Train: [528/599] Final Prec@1 99.9822%
11/23 08:50:21AM evaluateStage_trainer.py:180 [INFO] Valid: [528/599] Final Prec@1 64.8200%
11/23 08:50:21AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:50:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [529][50/175]	Step 93154	lr 0.00182	Loss 0.0064 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [529][100/175]	Step 93204	lr 0.00182	Loss 0.0052 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:50:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [529][150/175]	Step 93254	lr 0.00182	Loss 0.0143 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [529][175/175]	Step 93279	lr 0.00182	Loss 0.0055 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:03AM evaluateStage_trainer.py:145 [INFO] Train: [529/599] Final Prec@1 99.9844%
11/23 08:51:07AM evaluateStage_trainer.py:180 [INFO] Valid: [529/599] Final Prec@1 64.2800%
11/23 08:51:07AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:51:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [530][50/175]	Step 93330	lr 0.0018	Loss 0.0058 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [530][100/175]	Step 93380	lr 0.0018	Loss 0.0073 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [530][150/175]	Step 93430	lr 0.0018	Loss 0.0051 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [530][175/175]	Step 93455	lr 0.0018	Loss 0.0060 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:51:49AM evaluateStage_trainer.py:145 [INFO] Train: [530/599] Final Prec@1 99.9644%
11/23 08:51:53AM evaluateStage_trainer.py:180 [INFO] Valid: [530/599] Final Prec@1 64.5600%
11/23 08:51:53AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:52:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [531][50/175]	Step 93506	lr 0.00177	Loss 0.0062 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:52:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [531][100/175]	Step 93556	lr 0.00177	Loss 0.0067 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:52:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [531][150/175]	Step 93606	lr 0.00177	Loss 0.0062 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:52:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [531][175/175]	Step 93631	lr 0.00177	Loss 0.0107 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:52:35AM evaluateStage_trainer.py:145 [INFO] Train: [531/599] Final Prec@1 99.9733%
11/23 08:52:39AM evaluateStage_trainer.py:180 [INFO] Valid: [531/599] Final Prec@1 65.2600%
11/23 08:52:39AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:52:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [532][50/175]	Step 93682	lr 0.00175	Loss 0.0067 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:53:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [532][100/175]	Step 93732	lr 0.00175	Loss 0.0073 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:53:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [532][150/175]	Step 93782	lr 0.00175	Loss 0.0080 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:53:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [532][175/175]	Step 93807	lr 0.00175	Loss 0.0066 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:53:22AM evaluateStage_trainer.py:145 [INFO] Train: [532/599] Final Prec@1 99.9733%
11/23 08:53:25AM evaluateStage_trainer.py:180 [INFO] Valid: [532/599] Final Prec@1 64.3600%
11/23 08:53:25AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:53:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [533][50/175]	Step 93858	lr 0.00173	Loss 0.0052 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:53:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [533][100/175]	Step 93908	lr 0.00173	Loss 0.0065 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [533][150/175]	Step 93958	lr 0.00173	Loss 0.0045 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [533][175/175]	Step 93983	lr 0.00173	Loss 0.0041 (0.0066)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:08AM evaluateStage_trainer.py:145 [INFO] Train: [533/599] Final Prec@1 99.9733%
11/23 08:54:11AM evaluateStage_trainer.py:180 [INFO] Valid: [533/599] Final Prec@1 64.7800%
11/23 08:54:11AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:54:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [534][50/175]	Step 94034	lr 0.00171	Loss 0.0064 (0.0067)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [534][100/175]	Step 94084	lr 0.00171	Loss 0.0093 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [534][150/175]	Step 94134	lr 0.00171	Loss 0.0078 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [534][175/175]	Step 94159	lr 0.00171	Loss 0.0055 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:54:54AM evaluateStage_trainer.py:145 [INFO] Train: [534/599] Final Prec@1 99.9733%
11/23 08:54:57AM evaluateStage_trainer.py:180 [INFO] Valid: [534/599] Final Prec@1 64.8800%
11/23 08:54:57AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:55:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [535][50/175]	Step 94210	lr 0.00169	Loss 0.0113 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:55:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [535][100/175]	Step 94260	lr 0.00169	Loss 0.0048 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:55:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [535][150/175]	Step 94310	lr 0.00169	Loss 0.0067 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:55:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [535][175/175]	Step 94335	lr 0.00169	Loss 0.0051 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:55:40AM evaluateStage_trainer.py:145 [INFO] Train: [535/599] Final Prec@1 99.9800%
11/23 08:55:43AM evaluateStage_trainer.py:180 [INFO] Valid: [535/599] Final Prec@1 64.5400%
11/23 08:55:43AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:55:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [536][50/175]	Step 94386	lr 0.00167	Loss 0.0045 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:56:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [536][100/175]	Step 94436	lr 0.00167	Loss 0.0043 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:56:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [536][150/175]	Step 94486	lr 0.00167	Loss 0.0049 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:56:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [536][175/175]	Step 94511	lr 0.00167	Loss 0.0059 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:56:25AM evaluateStage_trainer.py:145 [INFO] Train: [536/599] Final Prec@1 99.9844%
11/23 08:56:29AM evaluateStage_trainer.py:180 [INFO] Valid: [536/599] Final Prec@1 65.1800%
11/23 08:56:29AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:56:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [537][50/175]	Step 94562	lr 0.00165	Loss 0.0105 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:56:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [537][100/175]	Step 94612	lr 0.00165	Loss 0.0053 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [537][150/175]	Step 94662	lr 0.00165	Loss 0.0051 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [537][175/175]	Step 94687	lr 0.00165	Loss 0.0058 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:11AM evaluateStage_trainer.py:145 [INFO] Train: [537/599] Final Prec@1 99.9822%
11/23 08:57:15AM evaluateStage_trainer.py:180 [INFO] Valid: [537/599] Final Prec@1 64.6800%
11/23 08:57:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:57:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [538][50/175]	Step 94738	lr 0.00163	Loss 0.0064 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [538][100/175]	Step 94788	lr 0.00163	Loss 0.0066 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [538][150/175]	Step 94838	lr 0.00163	Loss 0.0155 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [538][175/175]	Step 94863	lr 0.00163	Loss 0.0058 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:57:57AM evaluateStage_trainer.py:145 [INFO] Train: [538/599] Final Prec@1 99.9800%
11/23 08:58:01AM evaluateStage_trainer.py:180 [INFO] Valid: [538/599] Final Prec@1 64.4800%
11/23 08:58:01AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:58:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [539][50/175]	Step 94914	lr 0.00161	Loss 0.0053 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:58:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [539][100/175]	Step 94964	lr 0.00161	Loss 0.0057 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:58:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [539][150/175]	Step 95014	lr 0.00161	Loss 0.0065 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:58:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [539][175/175]	Step 95039	lr 0.00161	Loss 0.0066 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:58:43AM evaluateStage_trainer.py:145 [INFO] Train: [539/599] Final Prec@1 99.9800%
11/23 08:58:47AM evaluateStage_trainer.py:180 [INFO] Valid: [539/599] Final Prec@1 64.9000%
11/23 08:58:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:59:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [540][50/175]	Step 95090	lr 0.00159	Loss 0.0061 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:59:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [540][100/175]	Step 95140	lr 0.00159	Loss 0.0047 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:59:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [540][150/175]	Step 95190	lr 0.00159	Loss 0.0047 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:59:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [540][175/175]	Step 95215	lr 0.00159	Loss 0.0053 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:59:29AM evaluateStage_trainer.py:145 [INFO] Train: [540/599] Final Prec@1 99.9844%
11/23 08:59:33AM evaluateStage_trainer.py:180 [INFO] Valid: [540/599] Final Prec@1 64.7800%
11/23 08:59:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 08:59:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [541][50/175]	Step 95266	lr 0.00157	Loss 0.0051 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 08:59:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [541][100/175]	Step 95316	lr 0.00157	Loss 0.0067 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:00:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [541][150/175]	Step 95366	lr 0.00157	Loss 0.0065 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:00:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [541][175/175]	Step 95391	lr 0.00157	Loss 0.0062 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:00:15AM evaluateStage_trainer.py:145 [INFO] Train: [541/599] Final Prec@1 99.9889%
11/23 09:00:19AM evaluateStage_trainer.py:180 [INFO] Valid: [541/599] Final Prec@1 65.2600%
11/23 09:00:19AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:00:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [542][50/175]	Step 95442	lr 0.00155	Loss 0.0065 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:00:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [542][100/175]	Step 95492	lr 0.00155	Loss 0.0050 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:00:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [542][150/175]	Step 95542	lr 0.00155	Loss 0.0059 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [542][175/175]	Step 95567	lr 0.00155	Loss 0.0081 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:01AM evaluateStage_trainer.py:145 [INFO] Train: [542/599] Final Prec@1 99.9800%
11/23 09:01:04AM evaluateStage_trainer.py:180 [INFO] Valid: [542/599] Final Prec@1 64.6800%
11/23 09:01:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:01:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [543][50/175]	Step 95618	lr 0.00153	Loss 0.0061 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [543][100/175]	Step 95668	lr 0.00153	Loss 0.0114 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [543][150/175]	Step 95718	lr 0.00153	Loss 0.0048 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [543][175/175]	Step 95743	lr 0.00153	Loss 0.0045 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:01:47AM evaluateStage_trainer.py:145 [INFO] Train: [543/599] Final Prec@1 99.9778%
11/23 09:01:51AM evaluateStage_trainer.py:180 [INFO] Valid: [543/599] Final Prec@1 64.4400%
11/23 09:01:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:02:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [544][50/175]	Step 95794	lr 0.00151	Loss 0.0077 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:02:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [544][100/175]	Step 95844	lr 0.00151	Loss 0.0055 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:02:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [544][150/175]	Step 95894	lr 0.00151	Loss 0.0045 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:02:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [544][175/175]	Step 95919	lr 0.00151	Loss 0.0056 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:02:33AM evaluateStage_trainer.py:145 [INFO] Train: [544/599] Final Prec@1 99.9800%
11/23 09:02:37AM evaluateStage_trainer.py:180 [INFO] Valid: [544/599] Final Prec@1 65.1000%
11/23 09:02:37AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:02:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [545][50/175]	Step 95970	lr 0.00149	Loss 0.0088 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [545][100/175]	Step 96020	lr 0.00149	Loss 0.0084 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [545][150/175]	Step 96070	lr 0.00149	Loss 0.0121 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [545][175/175]	Step 96095	lr 0.00149	Loss 0.0065 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:19AM evaluateStage_trainer.py:145 [INFO] Train: [545/599] Final Prec@1 99.9822%
11/23 09:03:23AM evaluateStage_trainer.py:180 [INFO] Valid: [545/599] Final Prec@1 64.5200%
11/23 09:03:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:03:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [546][50/175]	Step 96146	lr 0.00148	Loss 0.0061 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [546][100/175]	Step 96196	lr 0.00148	Loss 0.0072 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:03:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [546][150/175]	Step 96246	lr 0.00148	Loss 0.0044 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [546][175/175]	Step 96271	lr 0.00148	Loss 0.0054 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:05AM evaluateStage_trainer.py:145 [INFO] Train: [546/599] Final Prec@1 99.9756%
11/23 09:04:08AM evaluateStage_trainer.py:180 [INFO] Valid: [546/599] Final Prec@1 64.2800%
11/23 09:04:09AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:04:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [547][50/175]	Step 96322	lr 0.00146	Loss 0.0063 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [547][100/175]	Step 96372	lr 0.00146	Loss 0.0062 (0.0063)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [547][150/175]	Step 96422	lr 0.00146	Loss 0.0100 (0.0065)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [547][175/175]	Step 96447	lr 0.00146	Loss 0.0044 (0.0064)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:04:51AM evaluateStage_trainer.py:145 [INFO] Train: [547/599] Final Prec@1 99.9667%
11/23 09:04:55AM evaluateStage_trainer.py:180 [INFO] Valid: [547/599] Final Prec@1 64.9000%
11/23 09:04:55AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:05:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [548][50/175]	Step 96498	lr 0.00144	Loss 0.0061 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:05:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [548][100/175]	Step 96548	lr 0.00144	Loss 0.0063 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:05:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [548][150/175]	Step 96598	lr 0.00144	Loss 0.0050 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:05:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [548][175/175]	Step 96623	lr 0.00144	Loss 0.0067 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:05:37AM evaluateStage_trainer.py:145 [INFO] Train: [548/599] Final Prec@1 99.9733%
11/23 09:05:41AM evaluateStage_trainer.py:180 [INFO] Valid: [548/599] Final Prec@1 64.7400%
11/23 09:05:41AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:05:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [549][50/175]	Step 96674	lr 0.00143	Loss 0.0063 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:06:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [549][100/175]	Step 96724	lr 0.00143	Loss 0.0090 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:06:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [549][150/175]	Step 96774	lr 0.00143	Loss 0.0050 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:06:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [549][175/175]	Step 96799	lr 0.00143	Loss 0.0060 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:06:23AM evaluateStage_trainer.py:145 [INFO] Train: [549/599] Final Prec@1 99.9822%
11/23 09:06:26AM evaluateStage_trainer.py:180 [INFO] Valid: [549/599] Final Prec@1 64.8800%
11/23 09:06:27AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:06:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [550][50/175]	Step 96850	lr 0.00141	Loss 0.0060 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:06:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [550][100/175]	Step 96900	lr 0.00141	Loss 0.0080 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [550][150/175]	Step 96950	lr 0.00141	Loss 0.0051 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [550][175/175]	Step 96975	lr 0.00141	Loss 0.0046 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:09AM evaluateStage_trainer.py:145 [INFO] Train: [550/599] Final Prec@1 99.9778%
11/23 09:07:12AM evaluateStage_trainer.py:180 [INFO] Valid: [550/599] Final Prec@1 64.9800%
11/23 09:07:13AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:07:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [551][50/175]	Step 97026	lr 0.00139	Loss 0.0047 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [551][100/175]	Step 97076	lr 0.00139	Loss 0.0074 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [551][150/175]	Step 97126	lr 0.00139	Loss 0.0058 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [551][175/175]	Step 97151	lr 0.00139	Loss 0.0044 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:07:55AM evaluateStage_trainer.py:145 [INFO] Train: [551/599] Final Prec@1 99.9778%
11/23 09:07:58AM evaluateStage_trainer.py:180 [INFO] Valid: [551/599] Final Prec@1 65.2200%
11/23 09:07:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:08:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [552][50/175]	Step 97202	lr 0.00138	Loss 0.0060 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:08:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [552][100/175]	Step 97252	lr 0.00138	Loss 0.0059 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:08:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [552][150/175]	Step 97302	lr 0.00138	Loss 0.0052 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:08:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [552][175/175]	Step 97327	lr 0.00138	Loss 0.0060 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:08:41AM evaluateStage_trainer.py:145 [INFO] Train: [552/599] Final Prec@1 99.9733%
11/23 09:08:44AM evaluateStage_trainer.py:180 [INFO] Valid: [552/599] Final Prec@1 64.3200%
11/23 09:08:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:08:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [553][50/175]	Step 97378	lr 0.00136	Loss 0.0053 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:09:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [553][100/175]	Step 97428	lr 0.00136	Loss 0.0057 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:09:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [553][150/175]	Step 97478	lr 0.00136	Loss 0.0068 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:09:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [553][175/175]	Step 97503	lr 0.00136	Loss 0.0080 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:09:27AM evaluateStage_trainer.py:145 [INFO] Train: [553/599] Final Prec@1 99.9778%
11/23 09:09:30AM evaluateStage_trainer.py:180 [INFO] Valid: [553/599] Final Prec@1 65.0800%
11/23 09:09:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:09:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [554][50/175]	Step 97554	lr 0.00135	Loss 0.0053 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:09:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [554][100/175]	Step 97604	lr 0.00135	Loss 0.0052 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [554][150/175]	Step 97654	lr 0.00135	Loss 0.0063 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [554][175/175]	Step 97679	lr 0.00135	Loss 0.0055 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:13AM evaluateStage_trainer.py:145 [INFO] Train: [554/599] Final Prec@1 99.9867%
11/23 09:10:16AM evaluateStage_trainer.py:180 [INFO] Valid: [554/599] Final Prec@1 64.4200%
11/23 09:10:16AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:10:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [555][50/175]	Step 97730	lr 0.00133	Loss 0.0052 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [555][100/175]	Step 97780	lr 0.00133	Loss 0.0048 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [555][150/175]	Step 97830	lr 0.00133	Loss 0.0166 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [555][175/175]	Step 97855	lr 0.00133	Loss 0.0045 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:10:59AM evaluateStage_trainer.py:145 [INFO] Train: [555/599] Final Prec@1 99.9778%
11/23 09:11:02AM evaluateStage_trainer.py:180 [INFO] Valid: [555/599] Final Prec@1 65.1200%
11/23 09:11:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:11:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [556][50/175]	Step 97906	lr 0.00132	Loss 0.0075 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:11:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [556][100/175]	Step 97956	lr 0.00132	Loss 0.0051 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:11:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [556][150/175]	Step 98006	lr 0.00132	Loss 0.0062 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:11:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [556][175/175]	Step 98031	lr 0.00132	Loss 0.0049 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:11:45AM evaluateStage_trainer.py:145 [INFO] Train: [556/599] Final Prec@1 99.9733%
11/23 09:11:48AM evaluateStage_trainer.py:180 [INFO] Valid: [556/599] Final Prec@1 65.0600%
11/23 09:11:48AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:12:01AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [557][50/175]	Step 98082	lr 0.0013	Loss 0.0042 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:12:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [557][100/175]	Step 98132	lr 0.0013	Loss 0.0057 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:12:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [557][150/175]	Step 98182	lr 0.0013	Loss 0.0057 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:12:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [557][175/175]	Step 98207	lr 0.0013	Loss 0.0054 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:12:31AM evaluateStage_trainer.py:145 [INFO] Train: [557/599] Final Prec@1 99.9822%
11/23 09:12:34AM evaluateStage_trainer.py:180 [INFO] Valid: [557/599] Final Prec@1 64.7600%
11/23 09:12:34AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:12:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [558][50/175]	Step 98258	lr 0.00129	Loss 0.0038 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:12:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [558][100/175]	Step 98308	lr 0.00129	Loss 0.0067 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:13:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [558][150/175]	Step 98358	lr 0.00129	Loss 0.0060 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:13:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [558][175/175]	Step 98383	lr 0.00129	Loss 0.0063 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:13:17AM evaluateStage_trainer.py:145 [INFO] Train: [558/599] Final Prec@1 99.9778%
11/23 09:13:20AM evaluateStage_trainer.py:180 [INFO] Valid: [558/599] Final Prec@1 64.8000%
11/23 09:13:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:13:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [559][50/175]	Step 98434	lr 0.00128	Loss 0.0048 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:13:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [559][100/175]	Step 98484	lr 0.00128	Loss 0.0049 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:13:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [559][150/175]	Step 98534	lr 0.00128	Loss 0.0070 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [559][175/175]	Step 98559	lr 0.00128	Loss 0.0054 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:03AM evaluateStage_trainer.py:145 [INFO] Train: [559/599] Final Prec@1 99.9778%
11/23 09:14:06AM evaluateStage_trainer.py:180 [INFO] Valid: [559/599] Final Prec@1 64.7000%
11/23 09:14:06AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:14:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [560][50/175]	Step 98610	lr 0.00126	Loss 0.0051 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [560][100/175]	Step 98660	lr 0.00126	Loss 0.0066 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [560][150/175]	Step 98710	lr 0.00126	Loss 0.0047 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [560][175/175]	Step 98735	lr 0.00126	Loss 0.0069 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:14:49AM evaluateStage_trainer.py:145 [INFO] Train: [560/599] Final Prec@1 99.9822%
11/23 09:14:52AM evaluateStage_trainer.py:180 [INFO] Valid: [560/599] Final Prec@1 64.6800%
11/23 09:14:52AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:15:05AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [561][50/175]	Step 98786	lr 0.00125	Loss 0.0054 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:15:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [561][100/175]	Step 98836	lr 0.00125	Loss 0.0056 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:15:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [561][150/175]	Step 98886	lr 0.00125	Loss 0.0061 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:15:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [561][175/175]	Step 98911	lr 0.00125	Loss 0.0041 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:15:35AM evaluateStage_trainer.py:145 [INFO] Train: [561/599] Final Prec@1 99.9889%
11/23 09:15:38AM evaluateStage_trainer.py:180 [INFO] Valid: [561/599] Final Prec@1 64.7200%
11/23 09:15:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:15:51AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [562][50/175]	Step 98962	lr 0.00124	Loss 0.0040 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:16:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [562][100/175]	Step 99012	lr 0.00124	Loss 0.0053 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:16:15AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [562][150/175]	Step 99062	lr 0.00124	Loss 0.0061 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:16:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [562][175/175]	Step 99087	lr 0.00124	Loss 0.0063 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:16:21AM evaluateStage_trainer.py:145 [INFO] Train: [562/599] Final Prec@1 99.9822%
11/23 09:16:24AM evaluateStage_trainer.py:180 [INFO] Valid: [562/599] Final Prec@1 65.1400%
11/23 09:16:24AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:16:37AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [563][50/175]	Step 99138	lr 0.00122	Loss 0.0056 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:16:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [563][100/175]	Step 99188	lr 0.00122	Loss 0.0063 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [563][150/175]	Step 99238	lr 0.00122	Loss 0.0046 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [563][175/175]	Step 99263	lr 0.00122	Loss 0.0065 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:07AM evaluateStage_trainer.py:145 [INFO] Train: [563/599] Final Prec@1 99.9733%
11/23 09:17:10AM evaluateStage_trainer.py:180 [INFO] Valid: [563/599] Final Prec@1 64.9800%
11/23 09:17:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:17:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [564][50/175]	Step 99314	lr 0.00121	Loss 0.0064 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [564][100/175]	Step 99364	lr 0.00121	Loss 0.0043 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [564][150/175]	Step 99414	lr 0.00121	Loss 0.0047 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [564][175/175]	Step 99439	lr 0.00121	Loss 0.0044 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:17:53AM evaluateStage_trainer.py:145 [INFO] Train: [564/599] Final Prec@1 99.9800%
11/23 09:17:56AM evaluateStage_trainer.py:180 [INFO] Valid: [564/599] Final Prec@1 64.9200%
11/23 09:17:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:18:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [565][50/175]	Step 99490	lr 0.0012	Loss 0.0060 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:18:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [565][100/175]	Step 99540	lr 0.0012	Loss 0.0048 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:18:33AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [565][150/175]	Step 99590	lr 0.0012	Loss 0.0068 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:18:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [565][175/175]	Step 99615	lr 0.0012	Loss 0.0056 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:18:39AM evaluateStage_trainer.py:145 [INFO] Train: [565/599] Final Prec@1 99.9756%
11/23 09:18:42AM evaluateStage_trainer.py:180 [INFO] Valid: [565/599] Final Prec@1 65.1200%
11/23 09:18:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:18:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [566][50/175]	Step 99666	lr 0.00119	Loss 0.0095 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:19:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [566][100/175]	Step 99716	lr 0.00119	Loss 0.0045 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:19:19AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [566][150/175]	Step 99766	lr 0.00119	Loss 0.0051 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:19:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [566][175/175]	Step 99791	lr 0.00119	Loss 0.0053 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:19:25AM evaluateStage_trainer.py:145 [INFO] Train: [566/599] Final Prec@1 99.9800%
11/23 09:19:28AM evaluateStage_trainer.py:180 [INFO] Valid: [566/599] Final Prec@1 64.7400%
11/23 09:19:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:19:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [567][50/175]	Step 99842	lr 0.00118	Loss 0.0118 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:19:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [567][100/175]	Step 99892	lr 0.00118	Loss 0.0049 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [567][150/175]	Step 99942	lr 0.00118	Loss 0.0050 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [567][175/175]	Step 99967	lr 0.00118	Loss 0.0044 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:11AM evaluateStage_trainer.py:145 [INFO] Train: [567/599] Final Prec@1 99.9778%
11/23 09:20:14AM evaluateStage_trainer.py:180 [INFO] Valid: [567/599] Final Prec@1 64.7400%
11/23 09:20:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:20:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [568][50/175]	Step 100018	lr 0.00117	Loss 0.0056 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [568][100/175]	Step 100068	lr 0.00117	Loss 0.0041 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [568][150/175]	Step 100118	lr 0.00117	Loss 0.0060 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [568][175/175]	Step 100143	lr 0.00117	Loss 0.0044 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:20:57AM evaluateStage_trainer.py:145 [INFO] Train: [568/599] Final Prec@1 99.9778%
11/23 09:21:00AM evaluateStage_trainer.py:180 [INFO] Valid: [568/599] Final Prec@1 64.7600%
11/23 09:21:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:21:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [569][50/175]	Step 100194	lr 0.00116	Loss 0.0054 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:21:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [569][100/175]	Step 100244	lr 0.00116	Loss 0.0048 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:21:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [569][150/175]	Step 100294	lr 0.00116	Loss 0.0052 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:21:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [569][175/175]	Step 100319	lr 0.00116	Loss 0.0046 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:21:43AM evaluateStage_trainer.py:145 [INFO] Train: [569/599] Final Prec@1 99.9800%
11/23 09:21:46AM evaluateStage_trainer.py:180 [INFO] Valid: [569/599] Final Prec@1 64.3800%
11/23 09:21:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:21:59AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [570][50/175]	Step 100370	lr 0.00115	Loss 0.0131 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:22:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [570][100/175]	Step 100420	lr 0.00115	Loss 0.0046 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:22:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [570][150/175]	Step 100470	lr 0.00115	Loss 0.0058 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:22:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [570][175/175]	Step 100495	lr 0.00115	Loss 0.0044 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:22:29AM evaluateStage_trainer.py:145 [INFO] Train: [570/599] Final Prec@1 99.9778%
11/23 09:22:32AM evaluateStage_trainer.py:180 [INFO] Valid: [570/599] Final Prec@1 64.5400%
11/23 09:22:32AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:22:45AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [571][50/175]	Step 100546	lr 0.00114	Loss 0.0094 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:22:57AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [571][100/175]	Step 100596	lr 0.00114	Loss 0.0045 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:23:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [571][150/175]	Step 100646	lr 0.00114	Loss 0.0055 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:23:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [571][175/175]	Step 100671	lr 0.00114	Loss 0.0045 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:23:15AM evaluateStage_trainer.py:145 [INFO] Train: [571/599] Final Prec@1 99.9800%
11/23 09:23:18AM evaluateStage_trainer.py:180 [INFO] Valid: [571/599] Final Prec@1 65.3000%
11/23 09:23:18AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:23:31AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [572][50/175]	Step 100722	lr 0.00113	Loss 0.0043 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:23:43AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [572][100/175]	Step 100772	lr 0.00113	Loss 0.0040 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:23:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [572][150/175]	Step 100822	lr 0.00113	Loss 0.0077 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [572][175/175]	Step 100847	lr 0.00113	Loss 0.0052 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:00AM evaluateStage_trainer.py:145 [INFO] Train: [572/599] Final Prec@1 99.9778%
11/23 09:24:04AM evaluateStage_trainer.py:180 [INFO] Valid: [572/599] Final Prec@1 64.7600%
11/23 09:24:04AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:24:17AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [573][50/175]	Step 100898	lr 0.00112	Loss 0.0038 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [573][100/175]	Step 100948	lr 0.00112	Loss 0.0060 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [573][150/175]	Step 100998	lr 0.00112	Loss 0.0057 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [573][175/175]	Step 101023	lr 0.00112	Loss 0.0065 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:24:46AM evaluateStage_trainer.py:145 [INFO] Train: [573/599] Final Prec@1 99.9822%
11/23 09:24:50AM evaluateStage_trainer.py:180 [INFO] Valid: [573/599] Final Prec@1 64.6800%
11/23 09:24:50AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:25:03AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [574][50/175]	Step 101074	lr 0.00111	Loss 0.0044 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:25:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [574][100/175]	Step 101124	lr 0.00111	Loss 0.0046 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:25:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [574][150/175]	Step 101174	lr 0.00111	Loss 0.0045 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:25:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [574][175/175]	Step 101199	lr 0.00111	Loss 0.0060 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:25:32AM evaluateStage_trainer.py:145 [INFO] Train: [574/599] Final Prec@1 99.9711%
11/23 09:25:36AM evaluateStage_trainer.py:180 [INFO] Valid: [574/599] Final Prec@1 64.9200%
11/23 09:25:36AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:25:49AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [575][50/175]	Step 101250	lr 0.0011	Loss 0.0058 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [575][100/175]	Step 101300	lr 0.0011	Loss 0.0042 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [575][150/175]	Step 101350	lr 0.0011	Loss 0.0049 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [575][175/175]	Step 101375	lr 0.0011	Loss 0.0047 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:18AM evaluateStage_trainer.py:145 [INFO] Train: [575/599] Final Prec@1 99.9778%
11/23 09:26:22AM evaluateStage_trainer.py:180 [INFO] Valid: [575/599] Final Prec@1 65.1200%
11/23 09:26:22AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:26:35AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [576][50/175]	Step 101426	lr 0.00109	Loss 0.0060 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:47AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [576][100/175]	Step 101476	lr 0.00109	Loss 0.0049 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:26:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [576][150/175]	Step 101526	lr 0.00109	Loss 0.0058 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [576][175/175]	Step 101551	lr 0.00109	Loss 0.0053 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:04AM evaluateStage_trainer.py:145 [INFO] Train: [576/599] Final Prec@1 99.9756%
11/23 09:27:08AM evaluateStage_trainer.py:180 [INFO] Valid: [576/599] Final Prec@1 64.6200%
11/23 09:27:08AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:27:21AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [577][50/175]	Step 101602	lr 0.00109	Loss 0.0052 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [577][100/175]	Step 101652	lr 0.00109	Loss 0.0057 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [577][150/175]	Step 101702	lr 0.00109	Loss 0.0043 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [577][175/175]	Step 101727	lr 0.00109	Loss 0.0058 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:27:50AM evaluateStage_trainer.py:145 [INFO] Train: [577/599] Final Prec@1 99.9800%
11/23 09:27:54AM evaluateStage_trainer.py:180 [INFO] Valid: [577/599] Final Prec@1 64.5800%
11/23 09:27:54AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:28:07AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [578][50/175]	Step 101778	lr 0.00108	Loss 0.0058 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:28:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [578][100/175]	Step 101828	lr 0.00108	Loss 0.0058 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:28:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [578][150/175]	Step 101878	lr 0.00108	Loss 0.0050 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:28:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [578][175/175]	Step 101903	lr 0.00108	Loss 0.0066 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:28:36AM evaluateStage_trainer.py:145 [INFO] Train: [578/599] Final Prec@1 99.9822%
11/23 09:28:40AM evaluateStage_trainer.py:180 [INFO] Valid: [578/599] Final Prec@1 64.8000%
11/23 09:28:40AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:28:53AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [579][50/175]	Step 101954	lr 0.00107	Loss 0.0054 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:29:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [579][100/175]	Step 102004	lr 0.00107	Loss 0.0059 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:29:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [579][150/175]	Step 102054	lr 0.00107	Loss 0.0073 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:29:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [579][175/175]	Step 102079	lr 0.00107	Loss 0.0043 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:29:22AM evaluateStage_trainer.py:145 [INFO] Train: [579/599] Final Prec@1 99.9800%
11/23 09:29:26AM evaluateStage_trainer.py:180 [INFO] Valid: [579/599] Final Prec@1 64.6000%
11/23 09:29:26AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:29:39AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [580][50/175]	Step 102130	lr 0.00107	Loss 0.0040 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:29:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [580][100/175]	Step 102180	lr 0.00107	Loss 0.0047 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [580][150/175]	Step 102230	lr 0.00107	Loss 0.0059 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [580][175/175]	Step 102255	lr 0.00107	Loss 0.0046 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:08AM evaluateStage_trainer.py:145 [INFO] Train: [580/599] Final Prec@1 99.9711%
11/23 09:30:12AM evaluateStage_trainer.py:180 [INFO] Valid: [580/599] Final Prec@1 64.5200%
11/23 09:30:12AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:30:25AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [581][50/175]	Step 102306	lr 0.00106	Loss 0.0047 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [581][100/175]	Step 102356	lr 0.00106	Loss 0.0048 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [581][150/175]	Step 102406	lr 0.00106	Loss 0.0046 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [581][175/175]	Step 102431	lr 0.00106	Loss 0.0036 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:30:54AM evaluateStage_trainer.py:145 [INFO] Train: [581/599] Final Prec@1 99.9778%
11/23 09:30:57AM evaluateStage_trainer.py:180 [INFO] Valid: [581/599] Final Prec@1 65.0200%
11/23 09:30:58AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:31:11AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [582][50/175]	Step 102482	lr 0.00105	Loss 0.0054 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:31:22AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [582][100/175]	Step 102532	lr 0.00105	Loss 0.0048 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:31:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [582][150/175]	Step 102582	lr 0.00105	Loss 0.0044 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:31:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [582][175/175]	Step 102607	lr 0.00105	Loss 0.0063 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:31:40AM evaluateStage_trainer.py:145 [INFO] Train: [582/599] Final Prec@1 99.9800%
11/23 09:31:44AM evaluateStage_trainer.py:180 [INFO] Valid: [582/599] Final Prec@1 65.2000%
11/23 09:31:44AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:31:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [583][50/175]	Step 102658	lr 0.00105	Loss 0.0041 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:32:08AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [583][100/175]	Step 102708	lr 0.00105	Loss 0.0051 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:32:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [583][150/175]	Step 102758	lr 0.00105	Loss 0.0062 (0.0061)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:32:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [583][175/175]	Step 102783	lr 0.00105	Loss 0.0064 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:32:26AM evaluateStage_trainer.py:145 [INFO] Train: [583/599] Final Prec@1 99.9733%
11/23 09:32:29AM evaluateStage_trainer.py:180 [INFO] Valid: [583/599] Final Prec@1 65.1800%
11/23 09:32:30AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:32:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [584][50/175]	Step 102834	lr 0.00104	Loss 0.0089 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:32:54AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [584][100/175]	Step 102884	lr 0.00104	Loss 0.0049 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [584][150/175]	Step 102934	lr 0.00104	Loss 0.0051 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [584][175/175]	Step 102959	lr 0.00104	Loss 0.0048 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:12AM evaluateStage_trainer.py:145 [INFO] Train: [584/599] Final Prec@1 99.9778%
11/23 09:33:15AM evaluateStage_trainer.py:180 [INFO] Valid: [584/599] Final Prec@1 65.1400%
11/23 09:33:15AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:33:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [585][50/175]	Step 103010	lr 0.00104	Loss 0.0054 (0.0054)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:40AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [585][100/175]	Step 103060	lr 0.00104	Loss 0.0105 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [585][150/175]	Step 103110	lr 0.00104	Loss 0.0053 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [585][175/175]	Step 103135	lr 0.00104	Loss 0.0052 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:33:58AM evaluateStage_trainer.py:145 [INFO] Train: [585/599] Final Prec@1 99.9756%
11/23 09:34:01AM evaluateStage_trainer.py:180 [INFO] Valid: [585/599] Final Prec@1 65.1000%
11/23 09:34:02AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:34:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [586][50/175]	Step 103186	lr 0.00103	Loss 0.0044 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:34:26AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [586][100/175]	Step 103236	lr 0.00103	Loss 0.0077 (0.0062)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:34:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [586][150/175]	Step 103286	lr 0.00103	Loss 0.0057 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:34:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [586][175/175]	Step 103311	lr 0.00103	Loss 0.0056 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:34:44AM evaluateStage_trainer.py:145 [INFO] Train: [586/599] Final Prec@1 99.9756%
11/23 09:34:47AM evaluateStage_trainer.py:180 [INFO] Valid: [586/599] Final Prec@1 64.6200%
11/23 09:34:47AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:35:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [587][50/175]	Step 103362	lr 0.00103	Loss 0.0054 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:35:12AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [587][100/175]	Step 103412	lr 0.00103	Loss 0.0055 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:35:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [587][150/175]	Step 103462	lr 0.00103	Loss 0.0079 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:35:29AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [587][175/175]	Step 103487	lr 0.00103	Loss 0.0044 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:35:30AM evaluateStage_trainer.py:145 [INFO] Train: [587/599] Final Prec@1 99.9822%
11/23 09:35:33AM evaluateStage_trainer.py:180 [INFO] Valid: [587/599] Final Prec@1 64.4800%
11/23 09:35:33AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:35:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [588][50/175]	Step 103538	lr 0.00102	Loss 0.0060 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:35:58AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [588][100/175]	Step 103588	lr 0.00102	Loss 0.0080 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:36:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [588][150/175]	Step 103638	lr 0.00102	Loss 0.0059 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:36:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [588][175/175]	Step 103663	lr 0.00102	Loss 0.0051 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:36:16AM evaluateStage_trainer.py:145 [INFO] Train: [588/599] Final Prec@1 99.9756%
11/23 09:36:19AM evaluateStage_trainer.py:180 [INFO] Valid: [588/599] Final Prec@1 64.8200%
11/23 09:36:20AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:36:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [589][50/175]	Step 103714	lr 0.00102	Loss 0.0052 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:36:44AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [589][100/175]	Step 103764	lr 0.00102	Loss 0.0062 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:36:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [589][150/175]	Step 103814	lr 0.00102	Loss 0.0041 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [589][175/175]	Step 103839	lr 0.00102	Loss 0.0051 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:02AM evaluateStage_trainer.py:145 [INFO] Train: [589/599] Final Prec@1 99.9822%
11/23 09:37:05AM evaluateStage_trainer.py:180 [INFO] Valid: [589/599] Final Prec@1 64.6000%
11/23 09:37:05AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:37:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [590][50/175]	Step 103890	lr 0.00102	Loss 0.0054 (0.0053)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:30AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [590][100/175]	Step 103940	lr 0.00102	Loss 0.0046 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [590][150/175]	Step 103990	lr 0.00102	Loss 0.0084 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [590][175/175]	Step 104015	lr 0.00102	Loss 0.0052 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:37:48AM evaluateStage_trainer.py:145 [INFO] Train: [590/599] Final Prec@1 99.9822%
11/23 09:37:51AM evaluateStage_trainer.py:180 [INFO] Valid: [590/599] Final Prec@1 64.6200%
11/23 09:37:51AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:38:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [591][50/175]	Step 104066	lr 0.00101	Loss 0.0060 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:38:16AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [591][100/175]	Step 104116	lr 0.00101	Loss 0.0097 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:38:28AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [591][150/175]	Step 104166	lr 0.00101	Loss 0.0053 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:38:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [591][175/175]	Step 104191	lr 0.00101	Loss 0.0047 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:38:34AM evaluateStage_trainer.py:145 [INFO] Train: [591/599] Final Prec@1 99.9822%
11/23 09:38:37AM evaluateStage_trainer.py:180 [INFO] Valid: [591/599] Final Prec@1 64.8800%
11/23 09:38:38AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:38:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [592][50/175]	Step 104242	lr 0.00101	Loss 0.0081 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:39:02AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [592][100/175]	Step 104292	lr 0.00101	Loss 0.0058 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:39:14AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [592][150/175]	Step 104342	lr 0.00101	Loss 0.0040 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:39:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [592][175/175]	Step 104367	lr 0.00101	Loss 0.0057 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:39:20AM evaluateStage_trainer.py:145 [INFO] Train: [592/599] Final Prec@1 99.9778%
11/23 09:39:23AM evaluateStage_trainer.py:180 [INFO] Valid: [592/599] Final Prec@1 64.6000%
11/23 09:39:23AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:39:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [593][50/175]	Step 104418	lr 0.00101	Loss 0.0051 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:39:48AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [593][100/175]	Step 104468	lr 0.00101	Loss 0.0065 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:00AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [593][150/175]	Step 104518	lr 0.00101	Loss 0.0056 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [593][175/175]	Step 104543	lr 0.00101	Loss 0.0061 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:06AM evaluateStage_trainer.py:145 [INFO] Train: [593/599] Final Prec@1 99.9822%
11/23 09:40:09AM evaluateStage_trainer.py:180 [INFO] Valid: [593/599] Final Prec@1 64.9000%
11/23 09:40:10AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:40:23AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [594][50/175]	Step 104594	lr 0.00101	Loss 0.0070 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:34AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [594][100/175]	Step 104644	lr 0.00101	Loss 0.0047 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:46AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [594][150/175]	Step 104694	lr 0.00101	Loss 0.0052 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [594][175/175]	Step 104719	lr 0.00101	Loss 0.0077 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:40:52AM evaluateStage_trainer.py:145 [INFO] Train: [594/599] Final Prec@1 99.9756%
11/23 09:40:56AM evaluateStage_trainer.py:180 [INFO] Valid: [594/599] Final Prec@1 64.7200%
11/23 09:40:56AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:41:09AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [595][50/175]	Step 104770	lr 0.001	Loss 0.0042 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:41:20AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [595][100/175]	Step 104820	lr 0.001	Loss 0.0061 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:41:32AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [595][150/175]	Step 104870	lr 0.001	Loss 0.0114 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:41:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [595][175/175]	Step 104895	lr 0.001	Loss 0.0059 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:41:38AM evaluateStage_trainer.py:145 [INFO] Train: [595/599] Final Prec@1 99.9844%
11/23 09:41:42AM evaluateStage_trainer.py:180 [INFO] Valid: [595/599] Final Prec@1 65.0600%
11/23 09:41:42AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:41:55AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [596][50/175]	Step 104946	lr 0.001	Loss 0.0055 (0.0054)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:42:06AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [596][100/175]	Step 104996	lr 0.001	Loss 0.0041 (0.0056)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:42:18AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [596][150/175]	Step 105046	lr 0.001	Loss 0.0042 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:42:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [596][175/175]	Step 105071	lr 0.001	Loss 0.0064 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:42:24AM evaluateStage_trainer.py:145 [INFO] Train: [596/599] Final Prec@1 99.9800%
11/23 09:42:28AM evaluateStage_trainer.py:180 [INFO] Valid: [596/599] Final Prec@1 64.2800%
11/23 09:42:28AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:42:41AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [597][50/175]	Step 105122	lr 0.001	Loss 0.0081 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:42:52AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [597][100/175]	Step 105172	lr 0.001	Loss 0.0062 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:04AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [597][150/175]	Step 105222	lr 0.001	Loss 0.0097 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:10AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [597][175/175]	Step 105247	lr 0.001	Loss 0.0059 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:10AM evaluateStage_trainer.py:145 [INFO] Train: [597/599] Final Prec@1 99.9733%
11/23 09:43:14AM evaluateStage_trainer.py:180 [INFO] Valid: [597/599] Final Prec@1 64.5800%
11/23 09:43:14AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:43:27AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [598][50/175]	Step 105298	lr 0.001	Loss 0.0041 (0.0060)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:38AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [598][100/175]	Step 105348	lr 0.001	Loss 0.0066 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:50AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [598][150/175]	Step 105398	lr 0.001	Loss 0.0057 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:56AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [598][175/175]	Step 105423	lr 0.001	Loss 0.0058 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:43:56AM evaluateStage_trainer.py:145 [INFO] Train: [598/599] Final Prec@1 99.9822%
11/23 09:44:00AM evaluateStage_trainer.py:180 [INFO] Valid: [598/599] Final Prec@1 65.1400%
11/23 09:44:00AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:44:13AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [599][50/175]	Step 105474	lr 0.001	Loss 0.0044 (0.0055)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:44:24AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [599][100/175]	Step 105524	lr 0.001	Loss 0.0116 (0.0057)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:44:36AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [599][150/175]	Step 105574	lr 0.001	Loss 0.0056 (0.0058)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:44:42AM evaluateStage_trainer.py:138 [INFO] Train: Epoch: [599][175/175]	Step 105599	lr 0.001	Loss 0.0145 (0.0059)	Prec@(1,5) (100.0%, 100.0%)	
11/23 09:44:42AM evaluateStage_trainer.py:145 [INFO] Train: [599/599] Final Prec@1 99.9756%
11/23 09:44:46AM evaluateStage_trainer.py:180 [INFO] Valid: [599/599] Final Prec@1 64.3200%
11/23 09:44:46AM evaluateStage_main.py:75 [INFO] Until now, best Prec@1 = 65.4000%
11/23 09:44:46AM evaluateStage_main.py:79 [INFO] Final best Prec@1 = 65.4000%
