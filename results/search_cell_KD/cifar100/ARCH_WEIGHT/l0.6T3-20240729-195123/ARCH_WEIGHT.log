07/29 07:51:23PM parser.py:28 [INFO] 
07/29 07:51:23PM parser.py:29 [INFO] Parameters:
07/29 07:51:23PM parser.py:31 [INFO] DAG_PATH=results/search_cell_KD/cifar100/ARCH_WEIGHT/l0.6T3-20240729-195123/DAG
07/29 07:51:23PM parser.py:31 [INFO] T=3.0
07/29 07:51:23PM parser.py:31 [INFO] ADVANCED=True
07/29 07:51:23PM parser.py:31 [INFO] ALPHA_LR=0.0003
07/29 07:51:23PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
07/29 07:51:23PM parser.py:31 [INFO] BATCH_SIZE=64
07/29 07:51:23PM parser.py:31 [INFO] CUTOUT_LENGTH=0
07/29 07:51:23PM parser.py:31 [INFO] DATA_PATH=../data/
07/29 07:51:23PM parser.py:31 [INFO] DATASET=cifar100
07/29 07:51:23PM parser.py:31 [INFO] DESCRIPTION=teacher-efficientnetv2S_lambda-0.6_T-3_
07/29 07:51:23PM parser.py:31 [INFO] EPOCHS=50
07/29 07:51:23PM parser.py:31 [INFO] EXP_NAME=l0.6T3-20240729-195123
07/29 07:51:23PM parser.py:31 [INFO] GPUS=[0]
07/29 07:51:23PM parser.py:31 [INFO] INIT_CHANNELS=16
07/29 07:51:23PM parser.py:31 [INFO] L=0.6
07/29 07:51:23PM parser.py:31 [INFO] LAYERS=8
07/29 07:51:23PM parser.py:31 [INFO] LOCAL_RANK=0
07/29 07:51:23PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/29 07:51:23PM parser.py:31 [INFO] NAME=ARCH_WEIGHT
07/29 07:51:23PM parser.py:31 [INFO] NONKD=False
07/29 07:51:23PM parser.py:31 [INFO] PATH=results/search_cell_KD/cifar100/ARCH_WEIGHT/l0.6T3-20240729-195123
07/29 07:51:23PM parser.py:31 [INFO] PCDARTS=False
07/29 07:51:23PM parser.py:31 [INFO] PLOT_PATH=results/search_cell_KD/cifar100/ARCH_WEIGHT/l0.6T3-20240729-195123/plots
07/29 07:51:23PM parser.py:31 [INFO] PRINT_FREQ=50
07/29 07:51:23PM parser.py:31 [INFO] RESUME_PATH=None
07/29 07:51:23PM parser.py:31 [INFO] SAVE=l0.6T3
07/29 07:51:23PM parser.py:31 [INFO] SEED=0
07/29 07:51:23PM parser.py:31 [INFO] TEACHER_NAME=efficientnet_v2_s
07/29 07:51:23PM parser.py:31 [INFO] TEACHER_PATH=/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar
07/29 07:51:23PM parser.py:31 [INFO] TRAIN_PORTION=0.5
07/29 07:51:23PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
07/29 07:51:23PM parser.py:31 [INFO] W_LR=0.025
07/29 07:51:23PM parser.py:31 [INFO] W_LR_MIN=0.001
07/29 07:51:23PM parser.py:31 [INFO] W_MOMENTUM=0.9
07/29 07:51:23PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
07/29 07:51:23PM parser.py:31 [INFO] WORKERS=4
07/29 07:51:23PM parser.py:32 [INFO] 
07/29 07:51:27PM searchCell_trainer.py:122 [INFO] --> Loaded teacher model 'efficientnet_v2_s' from '/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar' and Freezed parameters)
07/29 07:51:30PM eval_util.py:125 [INFO] Test: Step 000/390 Prec@(1,5) (98.4%, 100.0%)
07/29 07:51:53PM eval_util.py:125 [INFO] Test: Step 390/390 Prec@(1,5) (97.2%, 99.6%)
07/29 07:51:53PM eval_util.py:130 [INFO] Teacher=(efficientnet_v2_s <- (/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar)) achives Test Prec(@1, @5) = (97.1720%, 99.6480%)
07/29 07:52:30PM searchCell_trainer.py:129 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1249, 0.1250, 0.1250, 0.1252, 0.1251, 0.1250, 0.1248],
        [0.1250, 0.1248, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1248, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1250, 0.1249, 0.1251, 0.1251],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1249, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1250, 0.1248, 0.1250, 0.1251, 0.1252],
        [0.1251, 0.1252, 0.1251, 0.1252, 0.1248, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1249, 0.1248, 0.1250, 0.1249, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1249, 0.1251, 0.1250, 0.1252, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1251],
        [0.1250, 0.1252, 0.1251, 0.1251, 0.1249, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1249, 0.1249, 0.1251, 0.1251, 0.1251, 0.1252, 0.1249],
        [0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1251, 0.1249, 0.1248, 0.1251, 0.1249, 0.1251, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1250],
        [0.1252, 0.1248, 0.1251, 0.1250, 0.1250, 0.1249, 0.1251, 0.1248],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1249, 0.1249, 0.1250, 0.1251, 0.1250, 0.1252, 0.1250, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1247, 0.1248, 0.1252],
        [0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1249, 0.1248, 0.1252, 0.1250, 0.1248, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1251, 0.1250, 0.1249],
        [0.1250, 0.1252, 0.1251, 0.1250, 0.1249, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1247, 0.1249, 0.1251, 0.1250],
        [0.1248, 0.1250, 0.1250, 0.1250, 0.1253, 0.1252, 0.1250, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1251, 0.1248, 0.1250, 0.1249, 0.1252, 0.1249, 0.1252, 0.1249],
        [0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1249, 0.1252, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1247, 0.1251, 0.1253, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1248],
        [0.1248, 0.1251, 0.1248, 0.1248, 0.1253, 0.1249, 0.1250, 0.1253],
        [0.1250, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1251],
        [0.1250, 0.1251, 0.1248, 0.1252, 0.1249, 0.1249, 0.1251, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1252, 0.1251, 0.1249, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1252, 0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1249, 0.1252, 0.1248, 0.1254, 0.1249, 0.1249, 0.1252],
        [0.1251, 0.1249, 0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250],
        [0.1252, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1250, 0.1249, 0.1250],
        [0.1251, 0.1252, 0.1250, 0.1250, 0.1251, 0.1250, 0.1248, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1252, 0.1251, 0.1249, 0.1248, 0.1251, 0.1249, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1250, 0.1251, 0.1250, 0.1250, 0.1250, 0.1252, 0.1248],
        [0.1249, 0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1248, 0.1249, 0.1251, 0.1251, 0.1251],
        [0.1250, 0.1250, 0.1251, 0.1250, 0.1249, 0.1251, 0.1248, 0.1251],
        [0.1250, 0.1249, 0.1250, 0.1248, 0.1251, 0.1253, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1250, 0.1252],
        [0.1251, 0.1251, 0.1249, 0.1247, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1253, 0.1249, 0.1251, 0.1247, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1252],
        [0.1252, 0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1250, 0.1250, 0.1248, 0.1250, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1249, 0.1252, 0.1250, 0.1249, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1249, 0.1250, 0.1253, 0.1250, 0.1248, 0.1251, 0.1250],
        [0.1251, 0.1248, 0.1249, 0.1249, 0.1251, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1249, 0.1250, 0.1251, 0.1252, 0.1248, 0.1250],
        [0.1251, 0.1251, 0.1252, 0.1248, 0.1249, 0.1248, 0.1251, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1248, 0.1249, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 07:53:29PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 23.4064 (24.4758)	Hard Loss 4.3332 (4.5312)	Soft Loss 0.0181 (0.0180)	Arch Loss 23.2055 (24.2397)	Arch Hard Loss 4.2961 (4.4876)	Arch Soft Loss 0.0171 (0.0170)	Prec@(1,5) (2.7%, 11.9%)	
07/29 07:54:24PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 21.2925 (23.8412)	Hard Loss 3.9417 (4.4137)	Soft Loss 0.0184 (0.0175)	Arch Loss 22.7631 (23.7214)	Arch Hard Loss 4.2143 (4.3916)	Arch Soft Loss 0.0141 (0.0165)	Prec@(1,5) (3.5%, 15.0%)	
07/29 07:55:20PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 20.8479 (23.3000)	Hard Loss 3.8595 (4.3135)	Soft Loss 0.0162 (0.0171)	Arch Loss 20.1710 (23.2387)	Arch Hard Loss 3.7342 (4.3022)	Arch Soft Loss 0.0153 (0.0163)	Prec@(1,5) (4.4%, 17.4%)	
07/29 07:56:15PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 20.9575 (22.8443)	Hard Loss 3.8799 (4.2292)	Soft Loss 0.0156 (0.0169)	Arch Loss 21.3515 (22.7983)	Arch Hard Loss 3.9530 (4.2207)	Arch Soft Loss 0.0128 (0.0161)	Prec@(1,5) (5.4%, 20.1%)	
07/29 07:57:11PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 19.9563 (22.4478)	Hard Loss 3.6945 (4.1558)	Soft Loss 0.0151 (0.0167)	Arch Loss 21.2109 (22.3696)	Arch Hard Loss 3.9269 (4.1413)	Arch Soft Loss 0.0138 (0.0160)	Prec@(1,5) (6.3%, 22.4%)	
07/29 07:58:06PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 19.8366 (22.1024)	Hard Loss 3.6723 (4.0918)	Soft Loss 0.0158 (0.0165)	Arch Loss 21.2989 (21.9960)	Arch Hard Loss 3.9433 (4.0722)	Arch Soft Loss 0.0130 (0.0158)	Prec@(1,5) (7.2%, 24.4%)	
07/29 07:59:01PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 19.7400 (21.7900)	Hard Loss 3.6545 (4.0340)	Soft Loss 0.0139 (0.0164)	Arch Loss 19.8592 (21.7012)	Arch Hard Loss 3.6765 (4.0176)	Arch Soft Loss 0.0155 (0.0156)	Prec@(1,5) (8.0%, 26.2%)	
07/29 07:59:46PM searchCell_trainer.py:219 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 20.1431 (21.5637)	Hard Loss 3.7291 (3.9921)	Soft Loss 0.0144 (0.0162)	Arch Loss 18.0451 (21.4680)	Arch Hard Loss 3.3407 (3.9744)	Arch Soft Loss 0.0139 (0.0155)	Prec@(1,5) (8.6%, 27.6%)	
07/29 07:59:50PM searchCell_trainer.py:234 [INFO] Train: [  0/49] Final Prec@1 8.6400%
07/29 07:59:57PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 3.6609	Prec@(1,5) (12.7%, 37.6%)
07/29 08:00:04PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.7075	Prec@(1,5) (12.5%, 36.2%)
07/29 08:00:10PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 3.7165	Prec@(1,5) (12.7%, 35.9%)
07/29 08:00:17PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.7069	Prec@(1,5) (12.7%, 36.2%)
07/29 08:00:23PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 3.7058	Prec@(1,5) (12.6%, 36.4%)
07/29 08:00:30PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.7001	Prec@(1,5) (12.7%, 36.6%)
07/29 08:00:36PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 3.6991	Prec@(1,5) (12.8%, 36.6%)
07/29 08:00:41PM searchCell_trainer.py:262 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.6951	Prec@(1,5) (12.8%, 36.8%)
07/29 08:00:42PM searchCell_trainer.py:269 [INFO] Valid: [  0/49] Final Prec@1 12.8080%
07/29 08:00:42PM searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('sep_conv_3x3', 3)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 4)]], normal3_concat=range(2, 6))
07/29 08:00:42PM searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 12.8080%
####### ALPHA #######
# Alpha - normal
tensor([[0.1282, 0.1250, 0.1269, 0.1255, 0.1224, 0.1241, 0.1246, 0.1233],
        [0.1252, 0.1229, 0.1246, 0.1248, 0.1250, 0.1252, 0.1256, 0.1267]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1293, 0.1252, 0.1276, 0.1242, 0.1234, 0.1227, 0.1242, 0.1234],
        [0.1261, 0.1229, 0.1251, 0.1256, 0.1240, 0.1252, 0.1246, 0.1264],
        [0.1265, 0.1243, 0.1259, 0.1243, 0.1242, 0.1239, 0.1252, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1283, 0.1255, 0.1268, 0.1243, 0.1237, 0.1250, 0.1238, 0.1225],
        [0.1254, 0.1241, 0.1256, 0.1246, 0.1251, 0.1245, 0.1243, 0.1263],
        [0.1252, 0.1243, 0.1255, 0.1258, 0.1246, 0.1246, 0.1257, 0.1244],
        [0.1244, 0.1232, 0.1250, 0.1253, 0.1246, 0.1253, 0.1266, 0.1256]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1288, 0.1254, 0.1271, 0.1240, 0.1257, 0.1237, 0.1231, 0.1222],
        [0.1251, 0.1229, 0.1250, 0.1253, 0.1242, 0.1256, 0.1249, 0.1271],
        [0.1254, 0.1234, 0.1247, 0.1243, 0.1261, 0.1242, 0.1269, 0.1250],
        [0.1252, 0.1236, 0.1249, 0.1254, 0.1253, 0.1239, 0.1257, 0.1260],
        [0.1246, 0.1231, 0.1252, 0.1250, 0.1257, 0.1244, 0.1264, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1258, 0.1252, 0.1276, 0.1243, 0.1240, 0.1245, 0.1249, 0.1238],
        [0.1230, 0.1226, 0.1244, 0.1260, 0.1267, 0.1245, 0.1271, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1247, 0.1238, 0.1263, 0.1261, 0.1259, 0.1248, 0.1256, 0.1229],
        [0.1226, 0.1225, 0.1247, 0.1259, 0.1270, 0.1252, 0.1253, 0.1268],
        [0.1217, 0.1221, 0.1238, 0.1260, 0.1272, 0.1258, 0.1278, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1258, 0.1252, 0.1278, 0.1260, 0.1236, 0.1232, 0.1254, 0.1231],
        [0.1227, 0.1228, 0.1258, 0.1246, 0.1257, 0.1263, 0.1259, 0.1261],
        [0.1228, 0.1238, 0.1257, 0.1256, 0.1244, 0.1241, 0.1276, 0.1261],
        [0.1225, 0.1232, 0.1256, 0.1253, 0.1265, 0.1240, 0.1273, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1255, 0.1259, 0.1285, 0.1258, 0.1234, 0.1236, 0.1251, 0.1222],
        [0.1218, 0.1219, 0.1248, 0.1254, 0.1267, 0.1258, 0.1269, 0.1267],
        [0.1228, 0.1235, 0.1252, 0.1245, 0.1263, 0.1264, 0.1253, 0.1260],
        [0.1224, 0.1235, 0.1254, 0.1271, 0.1251, 0.1253, 0.1254, 0.1258],
        [0.1218, 0.1233, 0.1247, 0.1242, 0.1264, 0.1264, 0.1269, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1245, 0.1230, 0.1239, 0.1266, 0.1239, 0.1247, 0.1262, 0.1272],
        [0.1224, 0.1224, 0.1228, 0.1246, 0.1279, 0.1259, 0.1256, 0.1284]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1238, 0.1227, 0.1235, 0.1251, 0.1260, 0.1249, 0.1261, 0.1280],
        [0.1233, 0.1231, 0.1238, 0.1251, 0.1229, 0.1258, 0.1264, 0.1296],
        [0.1203, 0.1191, 0.1220, 0.1257, 0.1273, 0.1260, 0.1288, 0.1308]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1238, 0.1232, 0.1240, 0.1239, 0.1269, 0.1234, 0.1257, 0.1290],
        [0.1220, 0.1216, 0.1224, 0.1257, 0.1250, 0.1266, 0.1260, 0.1307],
        [0.1190, 0.1179, 0.1206, 0.1276, 0.1289, 0.1274, 0.1273, 0.1313],
        [0.1179, 0.1162, 0.1177, 0.1283, 0.1288, 0.1283, 0.1304, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1231, 0.1230, 0.1233, 0.1259, 0.1252, 0.1244, 0.1266, 0.1285],
        [0.1224, 0.1227, 0.1231, 0.1255, 0.1252, 0.1256, 0.1253, 0.1301],
        [0.1194, 0.1183, 0.1209, 0.1270, 0.1286, 0.1260, 0.1285, 0.1312],
        [0.1186, 0.1171, 0.1183, 0.1280, 0.1277, 0.1282, 0.1304, 0.1318],
        [0.1182, 0.1165, 0.1176, 0.1278, 0.1291, 0.1294, 0.1297, 0.1318]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1276, 0.1255, 0.1254, 0.1239, 0.1247, 0.1246, 0.1253, 0.1231],
        [0.1269, 0.1249, 0.1241, 0.1236, 0.1255, 0.1236, 0.1237, 0.1277]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1275, 0.1262, 0.1260, 0.1243, 0.1249, 0.1246, 0.1241, 0.1223],
        [0.1262, 0.1250, 0.1237, 0.1240, 0.1236, 0.1258, 0.1254, 0.1263],
        [0.1246, 0.1218, 0.1246, 0.1251, 0.1275, 0.1252, 0.1248, 0.1263]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1271, 0.1255, 0.1250, 0.1233, 0.1253, 0.1259, 0.1243, 0.1236],
        [0.1260, 0.1248, 0.1251, 0.1239, 0.1243, 0.1260, 0.1253, 0.1246],
        [0.1244, 0.1216, 0.1238, 0.1251, 0.1268, 0.1251, 0.1277, 0.1256],
        [0.1247, 0.1226, 0.1243, 0.1250, 0.1261, 0.1263, 0.1253, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1263, 0.1261, 0.1255, 0.1247, 0.1244, 0.1247, 0.1247, 0.1235],
        [0.1251, 0.1247, 0.1258, 0.1262, 0.1235, 0.1263, 0.1246, 0.1238],
        [0.1239, 0.1224, 0.1250, 0.1255, 0.1241, 0.1256, 0.1268, 0.1268],
        [0.1233, 0.1221, 0.1243, 0.1266, 0.1267, 0.1250, 0.1262, 0.1258],
        [0.1240, 0.1235, 0.1255, 0.1247, 0.1258, 0.1230, 0.1260, 0.1274]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1282, 0.1251, 0.1262, 0.1239, 0.1244, 0.1263, 0.1232, 0.1227],
        [0.1267, 0.1248, 0.1234, 0.1241, 0.1255, 0.1250, 0.1234, 0.1272]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1279, 0.1249, 0.1260, 0.1241, 0.1257, 0.1239, 0.1259, 0.1215],
        [0.1265, 0.1247, 0.1249, 0.1236, 0.1257, 0.1229, 0.1255, 0.1263],
        [0.1254, 0.1202, 0.1256, 0.1237, 0.1249, 0.1248, 0.1290, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1274, 0.1248, 0.1245, 0.1230, 0.1257, 0.1256, 0.1253, 0.1237],
        [0.1262, 0.1245, 0.1242, 0.1264, 0.1252, 0.1242, 0.1244, 0.1250],
        [0.1248, 0.1205, 0.1261, 0.1244, 0.1272, 0.1256, 0.1246, 0.1269],
        [0.1242, 0.1201, 0.1249, 0.1260, 0.1270, 0.1261, 0.1253, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1285, 0.1261, 0.1261, 0.1245, 0.1229, 0.1244, 0.1251, 0.1224],
        [0.1262, 0.1247, 0.1244, 0.1250, 0.1259, 0.1243, 0.1243, 0.1252],
        [0.1253, 0.1213, 0.1271, 0.1259, 0.1260, 0.1259, 0.1224, 0.1259],
        [0.1239, 0.1203, 0.1256, 0.1245, 0.1267, 0.1261, 0.1268, 0.1262],
        [0.1238, 0.1204, 0.1247, 0.1263, 0.1276, 0.1252, 0.1250, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:01:40午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 16.2302 (19.0149)	Hard Loss 3.0045 (3.5202)	Soft Loss 0.0153 (0.0148)	Arch Loss 17.7837 (18.8775)	Arch Hard Loss 3.2923 (3.4948)	Arch Soft Loss 0.0128 (0.0141)	Prec@(1,5) (15.5%, 41.7%)	
07/29 08:02:34午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 17.8446 (18.6060)	Hard Loss 3.3035 (3.4445)	Soft Loss 0.0142 (0.0148)	Arch Loss 17.8604 (18.6581)	Arch Hard Loss 3.3066 (3.4542)	Arch Soft Loss 0.0124 (0.0140)	Prec@(1,5) (16.9%, 43.6%)	
07/29 08:03:30午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 19.0610 (18.4725)	Hard Loss 3.5286 (3.4197)	Soft Loss 0.0169 (0.0147)	Arch Loss 18.7419 (18.5036)	Arch Hard Loss 3.4697 (3.4256)	Arch Soft Loss 0.0133 (0.0140)	Prec@(1,5) (17.1%, 44.2%)	
07/29 08:04:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 18.9754 (18.3719)	Hard Loss 3.5130 (3.4011)	Soft Loss 0.0127 (0.0145)	Arch Loss 17.7514 (18.3546)	Arch Hard Loss 3.2862 (3.3980)	Arch Soft Loss 0.0145 (0.0139)	Prec@(1,5) (17.4%, 45.0%)	
07/29 08:05:20午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 17.5161 (18.2465)	Hard Loss 3.2427 (3.3779)	Soft Loss 0.0135 (0.0145)	Arch Loss 16.9169 (18.1967)	Arch Hard Loss 3.1317 (3.3687)	Arch Soft Loss 0.0142 (0.0138)	Prec@(1,5) (17.8%, 45.5%)	
07/29 08:06:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 16.7793 (18.1314)	Hard Loss 3.1062 (3.3566)	Soft Loss 0.0144 (0.0144)	Arch Loss 16.3363 (18.0904)	Arch Hard Loss 3.0242 (3.3491)	Arch Soft Loss 0.0147 (0.0138)	Prec@(1,5) (18.2%, 46.2%)	
07/29 08:07:10午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 19.1804 (17.9845)	Hard Loss 3.5510 (3.3294)	Soft Loss 0.0129 (0.0144)	Arch Loss 17.7723 (17.9493)	Arch Hard Loss 3.2902 (3.3229)	Arch Soft Loss 0.0130 (0.0138)	Prec@(1,5) (18.7%, 47.0%)	
07/29 08:07:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 16.0291 (17.8602)	Hard Loss 2.9674 (3.3064)	Soft Loss 0.0127 (0.0143)	Arch Loss 16.8349 (17.8610)	Arch Hard Loss 3.1165 (3.3066)	Arch Soft Loss 0.0140 (0.0137)	Prec@(1,5) (19.1%, 47.6%)	
07/29 08:07:56午後 searchCell_trainer.py:234 [INFO] Train: [  1/49] Final Prec@1 19.0800%
07/29 08:08:03午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 3.2014	Prec@(1,5) (21.3%, 51.2%)
07/29 08:08:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.1902	Prec@(1,5) (21.2%, 50.8%)
07/29 08:08:16午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 3.1702	Prec@(1,5) (21.3%, 51.4%)
07/29 08:08:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.1614	Prec@(1,5) (21.6%, 51.7%)
07/29 08:08:29午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 3.1533	Prec@(1,5) (21.7%, 52.0%)
07/29 08:08:35午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.1629	Prec@(1,5) (21.5%, 51.7%)
07/29 08:08:41午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 3.1638	Prec@(1,5) (21.5%, 51.7%)
07/29 08:08:46午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.1651	Prec@(1,5) (21.4%, 51.7%)
07/29 08:08:47午後 searchCell_trainer.py:269 [INFO] Valid: [  1/49] Final Prec@1 21.3880%
07/29 08:08:47午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_5x5', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 4)]], normal3_concat=range(2, 6))
07/29 08:08:48午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 21.3880%
####### ALPHA #######
# Alpha - normal
tensor([[0.1315, 0.1259, 0.1305, 0.1247, 0.1214, 0.1218, 0.1228, 0.1214],
        [0.1233, 0.1192, 0.1242, 0.1279, 0.1259, 0.1257, 0.1264, 0.1275]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1323, 0.1254, 0.1305, 0.1217, 0.1241, 0.1204, 0.1246, 0.1209],
        [0.1249, 0.1192, 0.1249, 0.1278, 0.1255, 0.1273, 0.1219, 0.1286],
        [0.1258, 0.1223, 0.1275, 0.1256, 0.1247, 0.1241, 0.1239, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1324, 0.1265, 0.1307, 0.1227, 0.1211, 0.1234, 0.1211, 0.1222],
        [0.1235, 0.1206, 0.1254, 0.1268, 0.1266, 0.1257, 0.1231, 0.1284],
        [0.1249, 0.1222, 0.1269, 0.1288, 0.1233, 0.1244, 0.1254, 0.1241],
        [0.1225, 0.1198, 0.1250, 0.1252, 0.1262, 0.1260, 0.1287, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1333, 0.1264, 0.1307, 0.1228, 0.1221, 0.1218, 0.1217, 0.1211],
        [0.1226, 0.1176, 0.1229, 0.1286, 0.1245, 0.1275, 0.1270, 0.1294],
        [0.1254, 0.1210, 0.1255, 0.1255, 0.1265, 0.1240, 0.1289, 0.1232],
        [0.1240, 0.1201, 0.1246, 0.1272, 0.1267, 0.1251, 0.1266, 0.1255],
        [0.1225, 0.1188, 0.1249, 0.1275, 0.1260, 0.1262, 0.1251, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1296, 0.1263, 0.1327, 0.1241, 0.1220, 0.1227, 0.1218, 0.1208],
        [0.1219, 0.1197, 0.1244, 0.1268, 0.1266, 0.1246, 0.1277, 0.1282]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1289, 0.1250, 0.1311, 0.1250, 0.1228, 0.1227, 0.1250, 0.1195],
        [0.1231, 0.1209, 0.1257, 0.1241, 0.1271, 0.1246, 0.1260, 0.1286],
        [0.1224, 0.1203, 0.1243, 0.1243, 0.1274, 0.1259, 0.1299, 0.1256]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1292, 0.1265, 0.1327, 0.1257, 0.1207, 0.1214, 0.1248, 0.1191],
        [0.1215, 0.1197, 0.1263, 0.1233, 0.1266, 0.1262, 0.1275, 0.1289],
        [0.1219, 0.1214, 0.1257, 0.1250, 0.1236, 0.1268, 0.1285, 0.1272],
        [0.1214, 0.1208, 0.1259, 0.1252, 0.1271, 0.1236, 0.1273, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1298, 0.1278, 0.1348, 0.1256, 0.1222, 0.1199, 0.1211, 0.1188],
        [0.1210, 0.1193, 0.1257, 0.1258, 0.1261, 0.1260, 0.1272, 0.1288],
        [0.1231, 0.1217, 0.1263, 0.1240, 0.1285, 0.1245, 0.1251, 0.1269],
        [0.1220, 0.1214, 0.1261, 0.1283, 0.1242, 0.1250, 0.1250, 0.1280],
        [0.1214, 0.1219, 0.1254, 0.1228, 0.1246, 0.1251, 0.1299, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1233, 0.1216, 0.1231, 0.1271, 0.1250, 0.1256, 0.1273, 0.1269],
        [0.1205, 0.1210, 0.1218, 0.1244, 0.1303, 0.1263, 0.1255, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1231, 0.1220, 0.1234, 0.1233, 0.1281, 0.1257, 0.1259, 0.1286],
        [0.1221, 0.1223, 0.1235, 0.1241, 0.1209, 0.1263, 0.1280, 0.1328],
        [0.1191, 0.1156, 0.1197, 0.1259, 0.1297, 0.1258, 0.1312, 0.1332]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1237, 0.1229, 0.1245, 0.1227, 0.1273, 0.1227, 0.1258, 0.1305],
        [0.1200, 0.1200, 0.1214, 0.1262, 0.1246, 0.1276, 0.1256, 0.1347],
        [0.1176, 0.1143, 0.1179, 0.1288, 0.1298, 0.1290, 0.1282, 0.1345],
        [0.1155, 0.1120, 0.1134, 0.1317, 0.1298, 0.1299, 0.1319, 0.1358]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1226, 0.1230, 0.1238, 0.1257, 0.1254, 0.1235, 0.1264, 0.1296],
        [0.1213, 0.1220, 0.1229, 0.1249, 0.1248, 0.1256, 0.1244, 0.1340],
        [0.1178, 0.1151, 0.1188, 0.1265, 0.1300, 0.1266, 0.1312, 0.1341],
        [0.1165, 0.1134, 0.1146, 0.1291, 0.1292, 0.1294, 0.1323, 0.1353],
        [0.1157, 0.1123, 0.1133, 0.1295, 0.1318, 0.1301, 0.1316, 0.1357]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1299, 0.1256, 0.1268, 0.1237, 0.1227, 0.1238, 0.1251, 0.1224],
        [0.1288, 0.1252, 0.1251, 0.1238, 0.1234, 0.1214, 0.1236, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1297, 0.1261, 0.1267, 0.1261, 0.1216, 0.1237, 0.1240, 0.1221],
        [0.1284, 0.1257, 0.1225, 0.1245, 0.1237, 0.1249, 0.1249, 0.1253],
        [0.1234, 0.1175, 0.1247, 0.1255, 0.1285, 0.1250, 0.1262, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1303, 0.1256, 0.1241, 0.1218, 0.1269, 0.1250, 0.1242, 0.1222],
        [0.1276, 0.1248, 0.1269, 0.1233, 0.1238, 0.1254, 0.1252, 0.1230],
        [0.1254, 0.1192, 0.1242, 0.1241, 0.1263, 0.1251, 0.1291, 0.1265],
        [0.1238, 0.1206, 0.1240, 0.1256, 0.1252, 0.1277, 0.1266, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1288, 0.1269, 0.1253, 0.1245, 0.1235, 0.1241, 0.1248, 0.1222],
        [0.1275, 0.1259, 0.1243, 0.1288, 0.1218, 0.1271, 0.1225, 0.1221],
        [0.1240, 0.1201, 0.1261, 0.1258, 0.1236, 0.1256, 0.1269, 0.1279],
        [0.1218, 0.1196, 0.1240, 0.1286, 0.1272, 0.1244, 0.1282, 0.1263],
        [0.1229, 0.1217, 0.1272, 0.1258, 0.1266, 0.1213, 0.1247, 0.1298]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1322, 0.1265, 0.1283, 0.1221, 0.1246, 0.1239, 0.1217, 0.1207],
        [0.1273, 0.1231, 0.1223, 0.1230, 0.1271, 0.1265, 0.1211, 0.1297]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1319, 0.1262, 0.1266, 0.1239, 0.1252, 0.1228, 0.1250, 0.1184],
        [0.1275, 0.1233, 0.1239, 0.1251, 0.1259, 0.1222, 0.1238, 0.1283],
        [0.1260, 0.1167, 0.1250, 0.1242, 0.1255, 0.1240, 0.1308, 0.1277]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1310, 0.1260, 0.1218, 0.1220, 0.1251, 0.1265, 0.1252, 0.1224],
        [0.1271, 0.1232, 0.1239, 0.1264, 0.1267, 0.1222, 0.1240, 0.1266],
        [0.1233, 0.1156, 0.1242, 0.1241, 0.1326, 0.1258, 0.1271, 0.1272],
        [0.1241, 0.1160, 0.1234, 0.1283, 0.1264, 0.1270, 0.1274, 0.1274]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1323, 0.1273, 0.1256, 0.1237, 0.1228, 0.1230, 0.1247, 0.1206],
        [0.1277, 0.1240, 0.1237, 0.1249, 0.1258, 0.1232, 0.1243, 0.1265],
        [0.1262, 0.1178, 0.1262, 0.1253, 0.1285, 0.1250, 0.1249, 0.1260],
        [0.1248, 0.1169, 0.1243, 0.1242, 0.1298, 0.1250, 0.1275, 0.1275],
        [0.1237, 0.1169, 0.1229, 0.1254, 0.1309, 0.1247, 0.1274, 0.1280]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:09:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 17.2230 (16.3651)	Hard Loss 3.1886 (3.0296)	Soft Loss 0.0119 (0.0135)	Arch Loss 15.0795 (16.7092)	Arch Hard Loss 2.7916 (3.0933)	Arch Soft Loss 0.0125 (0.0129)	Prec@(1,5) (23.1%, 54.7%)	
07/29 08:10:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 14.6125 (16.2655)	Hard Loss 2.7050 (3.0111)	Soft Loss 0.0130 (0.0134)	Arch Loss 15.5853 (16.5981)	Arch Hard Loss 2.8851 (3.0728)	Arch Soft Loss 0.0138 (0.0128)	Prec@(1,5) (24.2%, 55.3%)	
07/29 08:11:35午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 15.4497 (16.1228)	Hard Loss 2.8601 (2.9847)	Soft Loss 0.0135 (0.0133)	Arch Loss 13.6870 (16.4453)	Arch Hard Loss 2.5337 (3.0445)	Arch Soft Loss 0.0120 (0.0129)	Prec@(1,5) (24.7%, 56.2%)	
07/29 08:12:31午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 13.8603 (15.9615)	Hard Loss 2.5657 (2.9549)	Soft Loss 0.0135 (0.0132)	Arch Loss 14.3954 (16.3148)	Arch Hard Loss 2.6648 (3.0203)	Arch Soft Loss 0.0131 (0.0128)	Prec@(1,5) (25.4%, 56.7%)	
07/29 08:13:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 17.2822 (15.9161)	Hard Loss 3.1993 (2.9464)	Soft Loss 0.0148 (0.0132)	Arch Loss 14.0842 (16.2796)	Arch Hard Loss 2.6073 (3.0138)	Arch Soft Loss 0.0120 (0.0128)	Prec@(1,5) (25.5%, 56.9%)	
07/29 08:14:21午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 14.8448 (15.8100)	Hard Loss 2.7481 (2.9268)	Soft Loss 0.0123 (0.0132)	Arch Loss 16.1681 (16.2104)	Arch Hard Loss 2.9930 (3.0010)	Arch Soft Loss 0.0142 (0.0127)	Prec@(1,5) (26.1%, 57.5%)	
07/29 08:15:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 16.2355 (15.7584)	Hard Loss 3.0055 (2.9173)	Soft Loss 0.0139 (0.0131)	Arch Loss 15.1152 (16.1391)	Arch Hard Loss 2.7983 (2.9878)	Arch Soft Loss 0.0115 (0.0126)	Prec@(1,5) (26.3%, 57.7%)	
07/29 08:16:00午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 16.0936 (15.7174)	Hard Loss 2.9792 (2.9097)	Soft Loss 0.0142 (0.0131)	Arch Loss 15.3993 (16.0769)	Arch Hard Loss 2.8508 (2.9763)	Arch Soft Loss 0.0130 (0.0126)	Prec@(1,5) (26.5%, 57.9%)	
07/29 08:16:01午後 searchCell_trainer.py:234 [INFO] Train: [  2/49] Final Prec@1 26.4880%
07/29 08:16:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 2.8720	Prec@(1,5) (27.8%, 58.8%)
07/29 08:16:15午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 2.8832	Prec@(1,5) (27.8%, 59.0%)
07/29 08:16:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 2.8856	Prec@(1,5) (27.5%, 59.2%)
07/29 08:16:28午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 2.8688	Prec@(1,5) (27.8%, 59.5%)
07/29 08:16:35午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 2.8761	Prec@(1,5) (27.8%, 59.3%)
07/29 08:16:41午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 2.8758	Prec@(1,5) (28.0%, 59.3%)
07/29 08:16:48午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 2.8844	Prec@(1,5) (27.8%, 59.0%)
07/29 08:16:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 2.8847	Prec@(1,5) (27.7%, 59.1%)
07/29 08:16:53午後 searchCell_trainer.py:269 [INFO] Valid: [  2/49] Final Prec@1 27.7640%
07/29 08:16:53午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_5x5', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 08:16:54午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 27.7640%
####### ALPHA #######
# Alpha - normal
tensor([[0.1337, 0.1264, 0.1342, 0.1239, 0.1200, 0.1204, 0.1213, 0.1201],
        [0.1208, 0.1154, 0.1247, 0.1304, 0.1264, 0.1274, 0.1254, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1354, 0.1264, 0.1343, 0.1215, 0.1232, 0.1183, 0.1224, 0.1186],
        [0.1243, 0.1166, 0.1270, 0.1291, 0.1263, 0.1280, 0.1194, 0.1294],
        [0.1231, 0.1190, 0.1290, 0.1242, 0.1253, 0.1259, 0.1251, 0.1284]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1359, 0.1275, 0.1346, 0.1234, 0.1192, 0.1204, 0.1191, 0.1199],
        [0.1230, 0.1182, 0.1268, 0.1280, 0.1260, 0.1245, 0.1237, 0.1298],
        [0.1220, 0.1175, 0.1260, 0.1339, 0.1262, 0.1241, 0.1261, 0.1241],
        [0.1192, 0.1149, 0.1242, 0.1271, 0.1265, 0.1270, 0.1323, 0.1289]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1381, 0.1286, 0.1358, 0.1207, 0.1195, 0.1191, 0.1196, 0.1186],
        [0.1225, 0.1156, 0.1244, 0.1286, 0.1254, 0.1259, 0.1263, 0.1313],
        [0.1233, 0.1174, 0.1253, 0.1284, 0.1276, 0.1259, 0.1289, 0.1233],
        [0.1205, 0.1151, 0.1234, 0.1313, 0.1295, 0.1265, 0.1272, 0.1264],
        [0.1195, 0.1147, 0.1251, 0.1300, 0.1261, 0.1263, 0.1269, 0.1314]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1326, 0.1297, 0.1379, 0.1232, 0.1204, 0.1201, 0.1199, 0.1161],
        [0.1193, 0.1176, 0.1239, 0.1270, 0.1257, 0.1249, 0.1292, 0.1325]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1329, 0.1292, 0.1375, 0.1229, 0.1195, 0.1194, 0.1234, 0.1153],
        [0.1231, 0.1211, 0.1281, 0.1228, 0.1257, 0.1233, 0.1239, 0.1321],
        [0.1223, 0.1199, 0.1257, 0.1231, 0.1253, 0.1250, 0.1327, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1322, 0.1305, 0.1385, 0.1244, 0.1183, 0.1185, 0.1238, 0.1138],
        [0.1193, 0.1183, 0.1269, 0.1232, 0.1259, 0.1262, 0.1282, 0.1320],
        [0.1211, 0.1208, 0.1273, 0.1241, 0.1213, 0.1270, 0.1287, 0.1296],
        [0.1192, 0.1198, 0.1271, 0.1245, 0.1264, 0.1220, 0.1275, 0.1334]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1335, 0.1322, 0.1420, 0.1234, 0.1192, 0.1167, 0.1187, 0.1142],
        [0.1196, 0.1188, 0.1276, 0.1256, 0.1246, 0.1258, 0.1272, 0.1308],
        [0.1228, 0.1219, 0.1280, 0.1221, 0.1302, 0.1223, 0.1254, 0.1273],
        [0.1205, 0.1211, 0.1282, 0.1277, 0.1240, 0.1234, 0.1242, 0.1309],
        [0.1195, 0.1217, 0.1261, 0.1212, 0.1239, 0.1249, 0.1308, 0.1320]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1208, 0.1193, 0.1220, 0.1279, 0.1270, 0.1268, 0.1284, 0.1279],
        [0.1181, 0.1191, 0.1207, 0.1229, 0.1326, 0.1269, 0.1269, 0.1327]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1209, 0.1200, 0.1226, 0.1228, 0.1293, 0.1268, 0.1268, 0.1308],
        [0.1202, 0.1208, 0.1229, 0.1230, 0.1202, 0.1261, 0.1302, 0.1365],
        [0.1155, 0.1103, 0.1167, 0.1264, 0.1308, 0.1268, 0.1356, 0.1381]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1220, 0.1211, 0.1238, 0.1222, 0.1289, 0.1213, 0.1273, 0.1335],
        [0.1182, 0.1184, 0.1205, 0.1256, 0.1243, 0.1282, 0.1260, 0.1387],
        [0.1140, 0.1091, 0.1144, 0.1301, 0.1321, 0.1308, 0.1297, 0.1398],
        [0.1112, 0.1062, 0.1081, 0.1343, 0.1313, 0.1314, 0.1357, 0.1417]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1207, 0.1213, 0.1229, 0.1264, 0.1263, 0.1229, 0.1269, 0.1327],
        [0.1203, 0.1215, 0.1232, 0.1245, 0.1221, 0.1256, 0.1242, 0.1386],
        [0.1142, 0.1101, 0.1154, 0.1284, 0.1321, 0.1265, 0.1339, 0.1394],
        [0.1123, 0.1079, 0.1095, 0.1311, 0.1301, 0.1320, 0.1355, 0.1417],
        [0.1111, 0.1064, 0.1071, 0.1317, 0.1348, 0.1314, 0.1359, 0.1415]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1318, 0.1246, 0.1288, 0.1233, 0.1221, 0.1250, 0.1223, 0.1220],
        [0.1302, 0.1247, 0.1265, 0.1238, 0.1232, 0.1203, 0.1232, 0.1281]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1325, 0.1260, 0.1276, 0.1271, 0.1196, 0.1222, 0.1237, 0.1214],
        [0.1293, 0.1245, 0.1243, 0.1263, 0.1234, 0.1248, 0.1234, 0.1241],
        [0.1239, 0.1155, 0.1255, 0.1275, 0.1276, 0.1244, 0.1252, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1335, 0.1250, 0.1232, 0.1236, 0.1274, 0.1247, 0.1224, 0.1202],
        [0.1300, 0.1256, 0.1272, 0.1230, 0.1229, 0.1243, 0.1238, 0.1233],
        [0.1266, 0.1180, 0.1260, 0.1249, 0.1253, 0.1250, 0.1273, 0.1268],
        [0.1231, 0.1200, 0.1254, 0.1260, 0.1239, 0.1274, 0.1258, 0.1284]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1314, 0.1269, 0.1255, 0.1235, 0.1237, 0.1244, 0.1239, 0.1206],
        [0.1295, 0.1261, 0.1242, 0.1306, 0.1208, 0.1278, 0.1210, 0.1198],
        [0.1246, 0.1190, 0.1288, 0.1258, 0.1226, 0.1248, 0.1243, 0.1300],
        [0.1213, 0.1192, 0.1257, 0.1293, 0.1260, 0.1219, 0.1283, 0.1283],
        [0.1216, 0.1194, 0.1278, 0.1277, 0.1274, 0.1205, 0.1228, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1338, 0.1281, 0.1293, 0.1199, 0.1264, 0.1241, 0.1196, 0.1188],
        [0.1287, 0.1238, 0.1213, 0.1195, 0.1280, 0.1265, 0.1207, 0.1315]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1339, 0.1278, 0.1264, 0.1242, 0.1259, 0.1215, 0.1241, 0.1161],
        [0.1290, 0.1241, 0.1231, 0.1250, 0.1268, 0.1201, 0.1218, 0.1300],
        [0.1241, 0.1135, 0.1253, 0.1249, 0.1254, 0.1241, 0.1328, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1346, 0.1293, 0.1203, 0.1208, 0.1244, 0.1262, 0.1240, 0.1206],
        [0.1287, 0.1239, 0.1242, 0.1262, 0.1283, 0.1208, 0.1211, 0.1269],
        [0.1214, 0.1129, 0.1250, 0.1228, 0.1354, 0.1257, 0.1286, 0.1281],
        [0.1212, 0.1125, 0.1239, 0.1299, 0.1266, 0.1281, 0.1288, 0.1289]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1353, 0.1303, 0.1270, 0.1209, 0.1219, 0.1217, 0.1248, 0.1180],
        [0.1283, 0.1244, 0.1237, 0.1249, 0.1255, 0.1232, 0.1234, 0.1267],
        [0.1247, 0.1153, 0.1270, 0.1244, 0.1309, 0.1234, 0.1272, 0.1271],
        [0.1223, 0.1139, 0.1250, 0.1233, 0.1311, 0.1249, 0.1285, 0.1312],
        [0.1217, 0.1133, 0.1229, 0.1251, 0.1320, 0.1263, 0.1285, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:17:50午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 15.9689 (14.5112)	Hard Loss 2.9563 (2.6863)	Soft Loss 0.0121 (0.0124)	Arch Loss 13.3922 (15.3368)	Arch Hard Loss 2.4791 (2.8393)	Arch Soft Loss 0.0125 (0.0119)	Prec@(1,5) (31.2%, 63.5%)	
07/29 08:18:46午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 14.5282 (14.4511)	Hard Loss 2.6893 (2.6752)	Soft Loss 0.0146 (0.0124)	Arch Loss 13.9734 (15.1631)	Arch Hard Loss 2.5868 (2.8071)	Arch Soft Loss 0.0123 (0.0120)	Prec@(1,5) (30.9%, 63.8%)	
07/29 08:19:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 14.7198 (14.3886)	Hard Loss 2.7250 (2.6636)	Soft Loss 0.0118 (0.0124)	Arch Loss 15.2703 (15.1032)	Arch Hard Loss 2.8270 (2.7960)	Arch Soft Loss 0.0111 (0.0119)	Prec@(1,5) (31.2%, 63.9%)	
07/29 08:20:36午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 13.5031 (14.3171)	Hard Loss 2.4996 (2.6504)	Soft Loss 0.0128 (0.0123)	Arch Loss 17.0864 (14.9743)	Arch Hard Loss 3.1632 (2.7721)	Arch Soft Loss 0.0126 (0.0118)	Prec@(1,5) (31.7%, 64.2%)	
07/29 08:21:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 14.0017 (14.3320)	Hard Loss 2.5920 (2.6532)	Soft Loss 0.0122 (0.0123)	Arch Loss 15.2198 (14.8598)	Arch Hard Loss 2.8177 (2.7509)	Arch Soft Loss 0.0113 (0.0118)	Prec@(1,5) (31.5%, 64.0%)	
07/29 08:22:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 12.4236 (14.3073)	Hard Loss 2.2998 (2.6486)	Soft Loss 0.0119 (0.0123)	Arch Loss 14.0843 (14.7913)	Arch Hard Loss 2.6074 (2.7382)	Arch Soft Loss 0.0104 (0.0118)	Prec@(1,5) (31.7%, 64.2%)	
07/29 08:23:22午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 14.4381 (14.2461)	Hard Loss 2.6727 (2.6373)	Soft Loss 0.0132 (0.0123)	Arch Loss 16.3260 (14.7199)	Arch Hard Loss 3.0225 (2.7250)	Arch Soft Loss 0.0109 (0.0118)	Prec@(1,5) (31.9%, 64.5%)	
07/29 08:24:06午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 14.2493 (14.2188)	Hard Loss 2.6379 (2.6322)	Soft Loss 0.0116 (0.0123)	Arch Loss 12.9272 (14.6755)	Arch Hard Loss 2.3931 (2.7168)	Arch Soft Loss 0.0114 (0.0118)	Prec@(1,5) (31.9%, 64.6%)	
07/29 08:24:07午後 searchCell_trainer.py:234 [INFO] Train: [  3/49] Final Prec@1 31.9520%
07/29 08:24:14午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 2.6262	Prec@(1,5) (34.2%, 64.0%)
07/29 08:24:21午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 2.6127	Prec@(1,5) (34.0%, 64.4%)
07/29 08:24:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 2.6051	Prec@(1,5) (33.8%, 64.9%)
07/29 08:24:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 2.6063	Prec@(1,5) (33.7%, 65.1%)
07/29 08:24:40午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 2.6024	Prec@(1,5) (33.7%, 65.3%)
07/29 08:24:47午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 2.5954	Prec@(1,5) (33.7%, 65.4%)
07/29 08:24:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 2.5956	Prec@(1,5) (33.5%, 65.4%)
07/29 08:24:58午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 2.5967	Prec@(1,5) (33.5%, 65.3%)
07/29 08:24:58午後 searchCell_trainer.py:269 [INFO] Valid: [  3/49] Final Prec@1 33.4880%
07/29 08:24:58午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_3x3', 1), ('skip_connect', 2)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 08:24:59午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 33.4880%
####### ALPHA #######
# Alpha - normal
tensor([[0.1343, 0.1258, 0.1366, 0.1242, 0.1185, 0.1203, 0.1212, 0.1190],
        [0.1187, 0.1115, 0.1241, 0.1323, 0.1266, 0.1301, 0.1263, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1251, 0.1355, 0.1224, 0.1229, 0.1169, 0.1234, 0.1188],
        [0.1223, 0.1129, 0.1266, 0.1317, 0.1286, 0.1298, 0.1190, 0.1291],
        [0.1201, 0.1152, 0.1285, 0.1252, 0.1274, 0.1293, 0.1265, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1349, 0.1252, 0.1349, 0.1245, 0.1195, 0.1209, 0.1196, 0.1204],
        [0.1201, 0.1143, 0.1261, 0.1327, 0.1266, 0.1238, 0.1254, 0.1311],
        [0.1192, 0.1129, 0.1252, 0.1402, 0.1263, 0.1244, 0.1277, 0.1241],
        [0.1149, 0.1098, 0.1227, 0.1298, 0.1277, 0.1308, 0.1356, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1401, 0.1288, 0.1385, 0.1203, 0.1183, 0.1164, 0.1185, 0.1191],
        [0.1197, 0.1111, 0.1220, 0.1341, 0.1243, 0.1280, 0.1266, 0.1342],
        [0.1214, 0.1143, 0.1251, 0.1300, 0.1288, 0.1268, 0.1311, 0.1225],
        [0.1174, 0.1112, 0.1228, 0.1349, 0.1294, 0.1301, 0.1289, 0.1253],
        [0.1163, 0.1101, 0.1238, 0.1312, 0.1278, 0.1297, 0.1290, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1345, 0.1321, 0.1425, 0.1233, 0.1187, 0.1177, 0.1185, 0.1127],
        [0.1173, 0.1164, 0.1241, 0.1267, 0.1248, 0.1263, 0.1290, 0.1354]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1356, 0.1326, 0.1430, 0.1209, 0.1175, 0.1176, 0.1212, 0.1116],
        [0.1218, 0.1210, 0.1301, 0.1222, 0.1263, 0.1217, 0.1220, 0.1349],
        [0.1217, 0.1193, 0.1272, 0.1239, 0.1234, 0.1242, 0.1337, 0.1267]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1330, 0.1324, 0.1421, 0.1247, 0.1174, 0.1171, 0.1233, 0.1099],
        [0.1165, 0.1165, 0.1274, 0.1245, 0.1263, 0.1265, 0.1285, 0.1338],
        [0.1191, 0.1192, 0.1278, 0.1248, 0.1195, 0.1274, 0.1303, 0.1319],
        [0.1166, 0.1184, 0.1274, 0.1263, 0.1255, 0.1200, 0.1275, 0.1382]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1356, 0.1347, 0.1470, 0.1235, 0.1178, 0.1139, 0.1164, 0.1111],
        [0.1176, 0.1179, 0.1289, 0.1267, 0.1232, 0.1259, 0.1282, 0.1317],
        [0.1216, 0.1209, 0.1290, 0.1235, 0.1306, 0.1210, 0.1251, 0.1282],
        [0.1183, 0.1198, 0.1288, 0.1260, 0.1250, 0.1232, 0.1241, 0.1347],
        [0.1169, 0.1202, 0.1256, 0.1211, 0.1234, 0.1244, 0.1338, 0.1346]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1182, 0.1170, 0.1209, 0.1293, 0.1284, 0.1285, 0.1287, 0.1289],
        [0.1162, 0.1173, 0.1194, 0.1225, 0.1346, 0.1268, 0.1283, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1180, 0.1178, 0.1216, 0.1231, 0.1312, 0.1281, 0.1279, 0.1323],
        [0.1187, 0.1195, 0.1222, 0.1222, 0.1190, 0.1264, 0.1320, 0.1401],
        [0.1115, 0.1052, 0.1139, 0.1279, 0.1314, 0.1267, 0.1394, 0.1440]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1202, 0.1195, 0.1236, 0.1210, 0.1303, 0.1203, 0.1286, 0.1365],
        [0.1167, 0.1168, 0.1193, 0.1259, 0.1237, 0.1293, 0.1261, 0.1420],
        [0.1104, 0.1042, 0.1113, 0.1318, 0.1334, 0.1334, 0.1299, 0.1456],
        [0.1070, 0.1011, 0.1034, 0.1370, 0.1321, 0.1328, 0.1398, 0.1469]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1187, 0.1195, 0.1220, 0.1284, 0.1271, 0.1217, 0.1266, 0.1359],
        [0.1195, 0.1210, 0.1232, 0.1245, 0.1200, 0.1251, 0.1247, 0.1420],
        [0.1108, 0.1054, 0.1125, 0.1305, 0.1334, 0.1271, 0.1352, 0.1452],
        [0.1084, 0.1029, 0.1049, 0.1329, 0.1304, 0.1353, 0.1374, 0.1478],
        [0.1067, 0.1008, 0.1014, 0.1356, 0.1383, 0.1317, 0.1384, 0.1471]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1329, 0.1222, 0.1298, 0.1239, 0.1216, 0.1261, 0.1217, 0.1217],
        [0.1317, 0.1233, 0.1286, 0.1233, 0.1229, 0.1208, 0.1224, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1249, 0.1285, 0.1269, 0.1194, 0.1216, 0.1222, 0.1215],
        [0.1301, 0.1225, 0.1234, 0.1287, 0.1246, 0.1256, 0.1224, 0.1228],
        [0.1241, 0.1138, 0.1264, 0.1291, 0.1249, 0.1256, 0.1251, 0.1309]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1363, 0.1236, 0.1243, 0.1236, 0.1266, 0.1250, 0.1210, 0.1197],
        [0.1325, 0.1252, 0.1269, 0.1237, 0.1231, 0.1244, 0.1211, 0.1231],
        [0.1276, 0.1168, 0.1272, 0.1250, 0.1251, 0.1249, 0.1265, 0.1269],
        [0.1229, 0.1180, 0.1257, 0.1280, 0.1241, 0.1268, 0.1254, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1323, 0.1250, 0.1274, 0.1248, 0.1236, 0.1247, 0.1241, 0.1180],
        [0.1320, 0.1260, 0.1218, 0.1338, 0.1194, 0.1291, 0.1189, 0.1191],
        [0.1247, 0.1179, 0.1317, 0.1254, 0.1217, 0.1237, 0.1225, 0.1324],
        [0.1211, 0.1173, 0.1265, 0.1311, 0.1260, 0.1219, 0.1266, 0.1296],
        [0.1198, 0.1169, 0.1276, 0.1287, 0.1285, 0.1203, 0.1213, 0.1368]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1342, 0.1287, 0.1291, 0.1202, 0.1275, 0.1246, 0.1181, 0.1176],
        [0.1300, 0.1242, 0.1200, 0.1178, 0.1295, 0.1257, 0.1208, 0.1320]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1335, 0.1275, 0.1272, 0.1258, 0.1264, 0.1203, 0.1247, 0.1147],
        [0.1308, 0.1249, 0.1215, 0.1251, 0.1264, 0.1194, 0.1210, 0.1307],
        [0.1217, 0.1110, 0.1257, 0.1242, 0.1259, 0.1254, 0.1351, 0.1310]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1359, 0.1309, 0.1174, 0.1198, 0.1259, 0.1273, 0.1230, 0.1196],
        [0.1303, 0.1249, 0.1254, 0.1257, 0.1286, 0.1187, 0.1190, 0.1273],
        [0.1190, 0.1110, 0.1265, 0.1235, 0.1372, 0.1252, 0.1294, 0.1284],
        [0.1184, 0.1099, 0.1244, 0.1306, 0.1262, 0.1284, 0.1317, 0.1305]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1358, 0.1310, 0.1276, 0.1193, 0.1242, 0.1209, 0.1249, 0.1164],
        [0.1297, 0.1248, 0.1232, 0.1252, 0.1257, 0.1225, 0.1230, 0.1259],
        [0.1224, 0.1127, 0.1280, 0.1254, 0.1313, 0.1247, 0.1279, 0.1277],
        [0.1195, 0.1108, 0.1258, 0.1233, 0.1321, 0.1252, 0.1288, 0.1346],
        [0.1186, 0.1085, 0.1213, 0.1273, 0.1327, 0.1292, 0.1288, 0.1336]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:25:56午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 14.1570 (13.0598)	Hard Loss 2.6208 (2.4176)	Soft Loss 0.0114 (0.0117)	Arch Loss 13.9592 (14.0611)	Arch Hard Loss 2.5842 (2.6031)	Arch Soft Loss 0.0106 (0.0114)	Prec@(1,5) (37.0%, 68.5%)	
07/29 08:26:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 11.9443 (13.0317)	Hard Loss 2.2110 (2.4124)	Soft Loss 0.0121 (0.0117)	Arch Loss 13.3483 (14.0761)	Arch Hard Loss 2.4712 (2.6058)	Arch Soft Loss 0.0099 (0.0114)	Prec@(1,5) (37.2%, 68.7%)	
07/29 08:27:47午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 11.6623 (13.0693)	Hard Loss 2.1588 (2.4194)	Soft Loss 0.0125 (0.0116)	Arch Loss 13.0452 (14.0132)	Arch Hard Loss 2.4149 (2.5942)	Arch Soft Loss 0.0114 (0.0113)	Prec@(1,5) (36.7%, 69.0%)	
07/29 08:28:42午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 12.1834 (13.0592)	Hard Loss 2.2553 (2.4175)	Soft Loss 0.0114 (0.0115)	Arch Loss 12.5378 (13.9280)	Arch Hard Loss 2.3212 (2.5784)	Arch Soft Loss 0.0088 (0.0113)	Prec@(1,5) (36.5%, 69.1%)	
07/29 08:29:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 11.8728 (13.0664)	Hard Loss 2.1978 (2.4189)	Soft Loss 0.0122 (0.0115)	Arch Loss 12.4463 (13.8954)	Arch Hard Loss 2.3040 (2.5724)	Arch Soft Loss 0.0114 (0.0113)	Prec@(1,5) (36.5%, 69.0%)	
07/29 08:30:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 12.2205 (13.0386)	Hard Loss 2.2622 (2.4137)	Soft Loss 0.0114 (0.0115)	Arch Loss 15.0124 (13.8076)	Arch Hard Loss 2.7792 (2.5561)	Arch Soft Loss 0.0117 (0.0113)	Prec@(1,5) (36.5%, 69.2%)	
07/29 08:31:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 13.8764 (13.0278)	Hard Loss 2.5689 (2.4117)	Soft Loss 0.0113 (0.0115)	Arch Loss 14.8687 (13.7811)	Arch Hard Loss 2.7527 (2.5512)	Arch Soft Loss 0.0110 (0.0112)	Prec@(1,5) (36.6%, 69.1%)	
07/29 08:32:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 12.7956 (12.9629)	Hard Loss 2.3687 (2.3997)	Soft Loss 0.0110 (0.0115)	Arch Loss 14.6562 (13.7379)	Arch Hard Loss 2.7132 (2.5432)	Arch Soft Loss 0.0125 (0.0112)	Prec@(1,5) (36.8%, 69.3%)	
07/29 08:32:13午後 searchCell_trainer.py:234 [INFO] Train: [  4/49] Final Prec@1 36.8560%
07/29 08:32:20午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 2.5031	Prec@(1,5) (36.3%, 67.1%)
07/29 08:32:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 2.5166	Prec@(1,5) (36.1%, 67.1%)
07/29 08:32:33午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 2.5215	Prec@(1,5) (35.8%, 67.1%)
07/29 08:32:40午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 2.5185	Prec@(1,5) (35.5%, 67.4%)
07/29 08:32:46午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 2.5145	Prec@(1,5) (35.4%, 67.4%)
07/29 08:32:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 2.5171	Prec@(1,5) (35.5%, 67.3%)
07/29 08:32:59午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 2.5154	Prec@(1,5) (35.5%, 67.4%)
07/29 08:33:05午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 2.5111	Prec@(1,5) (35.5%, 67.5%)
07/29 08:33:05午後 searchCell_trainer.py:269 [INFO] Valid: [  4/49] Final Prec@1 35.5120%
07/29 08:33:05午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('max_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('sep_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 08:33:06午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 35.5120%
####### ALPHA #######
# Alpha - normal
tensor([[0.1330, 0.1237, 0.1376, 0.1266, 0.1177, 0.1208, 0.1222, 0.1184],
        [0.1175, 0.1083, 0.1244, 0.1345, 0.1260, 0.1315, 0.1261, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1336, 0.1232, 0.1363, 0.1249, 0.1215, 0.1174, 0.1238, 0.1193],
        [0.1200, 0.1092, 0.1263, 0.1366, 0.1304, 0.1316, 0.1182, 0.1276],
        [0.1165, 0.1110, 0.1281, 0.1265, 0.1296, 0.1315, 0.1289, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1334, 0.1229, 0.1350, 0.1266, 0.1205, 0.1212, 0.1201, 0.1202],
        [0.1174, 0.1107, 0.1248, 0.1392, 0.1277, 0.1241, 0.1251, 0.1310],
        [0.1148, 0.1074, 0.1232, 0.1469, 0.1277, 0.1276, 0.1282, 0.1242],
        [0.1107, 0.1059, 0.1219, 0.1330, 0.1293, 0.1357, 0.1351, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1403, 0.1278, 0.1397, 0.1198, 0.1182, 0.1152, 0.1182, 0.1208],
        [0.1184, 0.1085, 0.1213, 0.1390, 0.1233, 0.1270, 0.1267, 0.1357],
        [0.1179, 0.1095, 0.1234, 0.1325, 0.1325, 0.1298, 0.1317, 0.1228],
        [0.1139, 0.1071, 0.1214, 0.1379, 0.1318, 0.1324, 0.1315, 0.1240],
        [0.1134, 0.1062, 0.1228, 0.1349, 0.1284, 0.1308, 0.1310, 0.1325]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1348, 0.1335, 0.1453, 0.1228, 0.1185, 0.1168, 0.1189, 0.1093],
        [0.1143, 0.1143, 0.1232, 0.1277, 0.1239, 0.1264, 0.1310, 0.1392]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1357, 0.1344, 0.1465, 0.1199, 0.1172, 0.1172, 0.1207, 0.1084],
        [0.1190, 0.1195, 0.1306, 0.1221, 0.1256, 0.1219, 0.1227, 0.1386],
        [0.1199, 0.1182, 0.1276, 0.1234, 0.1237, 0.1235, 0.1372, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1328, 0.1338, 0.1455, 0.1253, 0.1174, 0.1168, 0.1216, 0.1068],
        [0.1122, 0.1134, 0.1259, 0.1250, 0.1282, 0.1297, 0.1295, 0.1361],
        [0.1163, 0.1174, 0.1282, 0.1259, 0.1199, 0.1283, 0.1305, 0.1335],
        [0.1132, 0.1158, 0.1267, 0.1259, 0.1282, 0.1200, 0.1293, 0.1409]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1356, 0.1363, 0.1510, 0.1223, 0.1176, 0.1123, 0.1157, 0.1091],
        [0.1143, 0.1159, 0.1288, 0.1262, 0.1253, 0.1275, 0.1288, 0.1331],
        [0.1195, 0.1199, 0.1303, 0.1246, 0.1323, 0.1192, 0.1247, 0.1293],
        [0.1157, 0.1181, 0.1292, 0.1267, 0.1265, 0.1232, 0.1231, 0.1377],
        [0.1146, 0.1198, 0.1267, 0.1211, 0.1235, 0.1223, 0.1352, 0.1369]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1159, 0.1144, 0.1207, 0.1297, 0.1299, 0.1309, 0.1290, 0.1294],
        [0.1125, 0.1139, 0.1169, 0.1226, 0.1376, 0.1280, 0.1303, 0.1382]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1156, 0.1150, 0.1207, 0.1237, 0.1331, 0.1300, 0.1283, 0.1336],
        [0.1155, 0.1166, 0.1200, 0.1225, 0.1190, 0.1267, 0.1351, 0.1446],
        [0.1069, 0.0993, 0.1114, 0.1287, 0.1299, 0.1282, 0.1431, 0.1526]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1181, 0.1172, 0.1230, 0.1202, 0.1323, 0.1202, 0.1300, 0.1390],
        [0.1134, 0.1134, 0.1167, 0.1266, 0.1240, 0.1314, 0.1271, 0.1474],
        [0.1055, 0.0984, 0.1083, 0.1343, 0.1333, 0.1358, 0.1299, 0.1545],
        [0.1021, 0.0952, 0.0986, 0.1391, 0.1319, 0.1352, 0.1431, 0.1550]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1161, 0.1170, 0.1207, 0.1296, 0.1285, 0.1211, 0.1281, 0.1391],
        [0.1171, 0.1194, 0.1222, 0.1243, 0.1183, 0.1260, 0.1255, 0.1472],
        [0.1060, 0.0998, 0.1094, 0.1305, 0.1345, 0.1279, 0.1383, 0.1535],
        [0.1034, 0.0970, 0.0998, 0.1360, 0.1279, 0.1386, 0.1404, 0.1569],
        [0.1013, 0.0945, 0.0954, 0.1386, 0.1420, 0.1332, 0.1390, 0.1559]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1339, 0.1199, 0.1308, 0.1242, 0.1215, 0.1291, 0.1199, 0.1207],
        [0.1322, 0.1219, 0.1290, 0.1255, 0.1234, 0.1197, 0.1207, 0.1276]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1237, 0.1291, 0.1268, 0.1203, 0.1203, 0.1217, 0.1212],
        [0.1298, 0.1207, 0.1230, 0.1295, 0.1272, 0.1259, 0.1221, 0.1218],
        [0.1234, 0.1109, 0.1267, 0.1292, 0.1252, 0.1260, 0.1262, 0.1325]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1373, 0.1211, 0.1243, 0.1236, 0.1287, 0.1248, 0.1203, 0.1200],
        [0.1329, 0.1243, 0.1261, 0.1240, 0.1231, 0.1241, 0.1218, 0.1237],
        [0.1264, 0.1139, 0.1269, 0.1264, 0.1267, 0.1251, 0.1273, 0.1274],
        [0.1215, 0.1147, 0.1248, 0.1309, 0.1233, 0.1282, 0.1258, 0.1308]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1335, 0.1238, 0.1282, 0.1250, 0.1233, 0.1251, 0.1247, 0.1163],
        [0.1326, 0.1257, 0.1213, 0.1352, 0.1193, 0.1314, 0.1173, 0.1172],
        [0.1248, 0.1165, 0.1331, 0.1265, 0.1227, 0.1223, 0.1210, 0.1331],
        [0.1202, 0.1148, 0.1268, 0.1332, 0.1252, 0.1235, 0.1268, 0.1295],
        [0.1184, 0.1151, 0.1285, 0.1300, 0.1290, 0.1191, 0.1204, 0.1396]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1348, 0.1280, 0.1296, 0.1201, 0.1281, 0.1255, 0.1171, 0.1167],
        [0.1303, 0.1242, 0.1199, 0.1165, 0.1289, 0.1261, 0.1220, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1330, 0.1262, 0.1283, 0.1263, 0.1284, 0.1199, 0.1242, 0.1136],
        [0.1319, 0.1258, 0.1201, 0.1262, 0.1264, 0.1185, 0.1206, 0.1305],
        [0.1190, 0.1072, 0.1256, 0.1258, 0.1268, 0.1256, 0.1390, 0.1311]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1368, 0.1310, 0.1151, 0.1194, 0.1279, 0.1281, 0.1227, 0.1190],
        [0.1315, 0.1260, 0.1266, 0.1257, 0.1288, 0.1184, 0.1163, 0.1268],
        [0.1166, 0.1079, 0.1284, 0.1236, 0.1400, 0.1247, 0.1297, 0.1291],
        [0.1157, 0.1066, 0.1256, 0.1309, 0.1273, 0.1287, 0.1339, 0.1312]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1357, 0.1306, 0.1291, 0.1181, 0.1253, 0.1208, 0.1242, 0.1162],
        [0.1295, 0.1249, 0.1232, 0.1262, 0.1260, 0.1225, 0.1234, 0.1242],
        [0.1202, 0.1094, 0.1293, 0.1253, 0.1311, 0.1253, 0.1292, 0.1302],
        [0.1167, 0.1069, 0.1260, 0.1224, 0.1337, 0.1265, 0.1305, 0.1373],
        [0.1151, 0.1036, 0.1204, 0.1304, 0.1339, 0.1303, 0.1298, 0.1365]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:34:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 9.5902 (11.7705)	Hard Loss 1.7752 (2.1789)	Soft Loss 0.0103 (0.0110)	Arch Loss 14.6145 (13.1607)	Arch Hard Loss 2.7055 (2.4364)	Arch Soft Loss 0.0119 (0.0108)	Prec@(1,5) (41.0%, 73.8%)	
07/29 08:34:58午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 13.3034 (11.9529)	Hard Loss 2.4627 (2.2127)	Soft Loss 0.0117 (0.0109)	Arch Loss 11.7251 (13.2157)	Arch Hard Loss 2.1705 (2.4466)	Arch Soft Loss 0.0104 (0.0108)	Prec@(1,5) (40.3%, 73.4%)	
07/29 08:35:54午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 12.3750 (11.9153)	Hard Loss 2.2909 (2.2057)	Soft Loss 0.0110 (0.0110)	Arch Loss 14.7862 (13.1870)	Arch Hard Loss 2.7373 (2.4412)	Arch Soft Loss 0.0118 (0.0108)	Prec@(1,5) (40.8%, 73.5%)	
07/29 08:36:50午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 11.4118 (11.8957)	Hard Loss 2.1124 (2.2021)	Soft Loss 0.0115 (0.0109)	Arch Loss 12.8798 (13.0513)	Arch Hard Loss 2.3842 (2.4161)	Arch Soft Loss 0.0121 (0.0108)	Prec@(1,5) (41.1%, 73.6%)	
07/29 08:37:44午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 15.2237 (11.9679)	Hard Loss 2.8183 (2.2155)	Soft Loss 0.0124 (0.0110)	Arch Loss 11.7130 (13.0356)	Arch Hard Loss 2.1682 (2.4132)	Arch Soft Loss 0.0112 (0.0108)	Prec@(1,5) (40.9%, 73.3%)	
07/29 08:38:40午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 11.5543 (11.9679)	Hard Loss 2.1389 (2.2155)	Soft Loss 0.0110 (0.0110)	Arch Loss 15.2245 (13.0028)	Arch Hard Loss 2.8187 (2.4071)	Arch Soft Loss 0.0083 (0.0108)	Prec@(1,5) (40.9%, 73.3%)	
07/29 08:39:35午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 10.9763 (11.9824)	Hard Loss 2.0318 (2.2181)	Soft Loss 0.0108 (0.0109)	Arch Loss 13.0730 (12.9926)	Arch Hard Loss 2.4201 (2.4052)	Arch Soft Loss 0.0110 (0.0108)	Prec@(1,5) (40.8%, 73.2%)	
07/29 08:40:19午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 10.6160 (11.9794)	Hard Loss 1.9651 (2.2176)	Soft Loss 0.0108 (0.0109)	Arch Loss 13.6554 (12.9640)	Arch Hard Loss 2.5279 (2.3999)	Arch Soft Loss 0.0114 (0.0108)	Prec@(1,5) (40.9%, 73.2%)	
07/29 08:40:20午後 searchCell_trainer.py:234 [INFO] Train: [  5/49] Final Prec@1 40.8600%
07/29 08:40:28午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 2.3947	Prec@(1,5) (37.6%, 69.9%)
07/29 08:40:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.4030	Prec@(1,5) (36.9%, 69.7%)
07/29 08:40:41午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 2.4124	Prec@(1,5) (36.6%, 69.7%)
07/29 08:40:47午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.4122	Prec@(1,5) (36.9%, 69.8%)
07/29 08:40:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 2.4204	Prec@(1,5) (36.9%, 69.8%)
07/29 08:41:00午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.4154	Prec@(1,5) (36.9%, 69.8%)
07/29 08:41:06午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 2.4143	Prec@(1,5) (36.9%, 69.9%)
07/29 08:41:11午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.4107	Prec@(1,5) (37.0%, 69.9%)
07/29 08:41:11午後 searchCell_trainer.py:269 [INFO] Valid: [  5/49] Final Prec@1 37.0120%
07/29 08:41:11午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('skip_connect', 0)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('dil_conv_5x5', 3)], [('sep_conv_5x5', 4), ('sep_conv_5x5', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('sep_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 08:41:12午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 37.0120%
####### ALPHA #######
# Alpha - normal
tensor([[0.1302, 0.1209, 0.1374, 0.1303, 0.1167, 0.1209, 0.1228, 0.1208],
        [0.1164, 0.1059, 0.1257, 0.1364, 0.1255, 0.1342, 0.1262, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1313, 0.1211, 0.1363, 0.1283, 0.1198, 0.1189, 0.1240, 0.1203],
        [0.1169, 0.1059, 0.1261, 0.1399, 0.1320, 0.1353, 0.1179, 0.1260],
        [0.1126, 0.1064, 0.1275, 0.1283, 0.1324, 0.1344, 0.1299, 0.1284]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1313, 0.1205, 0.1345, 0.1283, 0.1209, 0.1224, 0.1201, 0.1221],
        [0.1149, 0.1074, 0.1235, 0.1463, 0.1292, 0.1226, 0.1265, 0.1297],
        [0.1116, 0.1027, 0.1212, 0.1531, 0.1284, 0.1309, 0.1281, 0.1240],
        [0.1068, 0.1020, 0.1207, 0.1362, 0.1314, 0.1386, 0.1351, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1400, 0.1272, 0.1408, 0.1207, 0.1178, 0.1143, 0.1173, 0.1219],
        [0.1173, 0.1063, 0.1212, 0.1464, 0.1226, 0.1274, 0.1244, 0.1345],
        [0.1155, 0.1059, 0.1227, 0.1355, 0.1346, 0.1332, 0.1299, 0.1227],
        [0.1107, 0.1038, 0.1209, 0.1393, 0.1352, 0.1354, 0.1322, 0.1226],
        [0.1107, 0.1028, 0.1223, 0.1397, 0.1291, 0.1324, 0.1316, 0.1313]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1355, 0.1501, 0.1234, 0.1167, 0.1153, 0.1179, 0.1061],
        [0.1110, 0.1140, 0.1233, 0.1271, 0.1231, 0.1271, 0.1321, 0.1423]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1371, 0.1377, 0.1516, 0.1193, 0.1152, 0.1153, 0.1188, 0.1049],
        [0.1167, 0.1200, 0.1318, 0.1220, 0.1250, 0.1201, 0.1227, 0.1418],
        [0.1181, 0.1186, 0.1297, 0.1238, 0.1233, 0.1215, 0.1368, 0.1281]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1337, 0.1364, 0.1508, 0.1245, 0.1172, 0.1149, 0.1194, 0.1031],
        [0.1089, 0.1127, 0.1259, 0.1237, 0.1286, 0.1315, 0.1305, 0.1382],
        [0.1142, 0.1175, 0.1304, 0.1288, 0.1174, 0.1283, 0.1273, 0.1362],
        [0.1098, 0.1146, 0.1270, 0.1258, 0.1291, 0.1200, 0.1294, 0.1444]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1397, 0.1575, 0.1218, 0.1150, 0.1092, 0.1137, 0.1061],
        [0.1115, 0.1159, 0.1297, 0.1258, 0.1264, 0.1277, 0.1293, 0.1337],
        [0.1172, 0.1199, 0.1322, 0.1248, 0.1325, 0.1186, 0.1242, 0.1306],
        [0.1127, 0.1171, 0.1298, 0.1278, 0.1267, 0.1236, 0.1220, 0.1404],
        [0.1117, 0.1195, 0.1274, 0.1207, 0.1232, 0.1232, 0.1353, 0.1392]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1117, 0.1107, 0.1191, 0.1315, 0.1320, 0.1336, 0.1302, 0.1313],
        [0.1098, 0.1110, 0.1149, 0.1225, 0.1406, 0.1283, 0.1326, 0.1404]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1112, 0.1115, 0.1190, 0.1246, 0.1350, 0.1324, 0.1294, 0.1370],
        [0.1129, 0.1147, 0.1187, 0.1226, 0.1186, 0.1270, 0.1376, 0.1478],
        [0.1012, 0.0934, 0.1083, 0.1298, 0.1281, 0.1310, 0.1461, 0.1621]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1139, 0.1134, 0.1211, 0.1199, 0.1354, 0.1208, 0.1319, 0.1436],
        [0.1103, 0.1108, 0.1145, 0.1270, 0.1238, 0.1336, 0.1275, 0.1525],
        [0.1001, 0.0925, 0.1051, 0.1340, 0.1336, 0.1387, 0.1314, 0.1646],
        [0.0960, 0.0887, 0.0930, 0.1418, 0.1317, 0.1379, 0.1463, 0.1647]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1118, 0.1133, 0.1181, 0.1318, 0.1298, 0.1211, 0.1290, 0.1449],
        [0.1147, 0.1180, 0.1213, 0.1249, 0.1164, 0.1259, 0.1264, 0.1523],
        [0.1004, 0.0936, 0.1053, 0.1317, 0.1355, 0.1294, 0.1408, 0.1633],
        [0.0974, 0.0904, 0.0940, 0.1372, 0.1270, 0.1413, 0.1440, 0.1687],
        [0.0953, 0.0879, 0.0889, 0.1416, 0.1444, 0.1343, 0.1411, 0.1664]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1354, 0.1179, 0.1299, 0.1240, 0.1221, 0.1317, 0.1199, 0.1191],
        [0.1315, 0.1182, 0.1295, 0.1273, 0.1226, 0.1214, 0.1213, 0.1282]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1393, 0.1220, 0.1298, 0.1272, 0.1206, 0.1197, 0.1210, 0.1205],
        [0.1301, 0.1175, 0.1237, 0.1321, 0.1283, 0.1249, 0.1211, 0.1225],
        [0.1236, 0.1098, 0.1267, 0.1304, 0.1245, 0.1268, 0.1262, 0.1320]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1395, 0.1192, 0.1254, 0.1245, 0.1291, 0.1248, 0.1187, 0.1188],
        [0.1330, 0.1212, 0.1270, 0.1252, 0.1237, 0.1252, 0.1213, 0.1234],
        [0.1276, 0.1129, 0.1283, 0.1260, 0.1268, 0.1233, 0.1274, 0.1277],
        [0.1214, 0.1139, 0.1254, 0.1307, 0.1238, 0.1271, 0.1254, 0.1322]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1351, 0.1228, 0.1288, 0.1250, 0.1223, 0.1257, 0.1252, 0.1150],
        [0.1332, 0.1230, 0.1207, 0.1372, 0.1188, 0.1332, 0.1167, 0.1172],
        [0.1251, 0.1150, 0.1350, 0.1271, 0.1217, 0.1210, 0.1201, 0.1350],
        [0.1193, 0.1134, 0.1264, 0.1345, 0.1253, 0.1240, 0.1266, 0.1305],
        [0.1160, 0.1129, 0.1270, 0.1318, 0.1311, 0.1182, 0.1202, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1351, 0.1286, 0.1303, 0.1202, 0.1278, 0.1269, 0.1155, 0.1157],
        [0.1301, 0.1244, 0.1204, 0.1159, 0.1289, 0.1266, 0.1221, 0.1316]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1325, 0.1265, 0.1297, 0.1279, 0.1283, 0.1200, 0.1228, 0.1123],
        [0.1329, 0.1269, 0.1191, 0.1272, 0.1273, 0.1168, 0.1197, 0.1301],
        [0.1163, 0.1042, 0.1260, 0.1267, 0.1288, 0.1257, 0.1406, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1322, 0.1135, 0.1187, 0.1289, 0.1302, 0.1213, 0.1183],
        [0.1319, 0.1274, 0.1265, 0.1264, 0.1293, 0.1175, 0.1142, 0.1267],
        [0.1136, 0.1052, 0.1300, 0.1232, 0.1410, 0.1262, 0.1306, 0.1302],
        [0.1120, 0.1036, 0.1264, 0.1324, 0.1279, 0.1290, 0.1356, 0.1331]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1313, 0.1314, 0.1175, 0.1260, 0.1212, 0.1233, 0.1142],
        [0.1291, 0.1256, 0.1235, 0.1266, 0.1259, 0.1226, 0.1238, 0.1228],
        [0.1171, 0.1064, 0.1311, 0.1256, 0.1307, 0.1263, 0.1305, 0.1323],
        [0.1130, 0.1034, 0.1263, 0.1224, 0.1351, 0.1269, 0.1327, 0.1402],
        [0.1100, 0.0989, 0.1190, 0.1307, 0.1364, 0.1325, 0.1304, 0.1420]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:42:09午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 10.4319 (11.0600)	Hard Loss 1.9310 (2.0474)	Soft Loss 0.0114 (0.0103)	Arch Loss 14.3916 (12.5304)	Arch Hard Loss 2.6643 (2.3197)	Arch Soft Loss 0.0115 (0.0106)	Prec@(1,5) (45.1%, 76.2%)	
07/29 08:43:04午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 11.5691 (11.0942)	Hard Loss 2.1417 (2.0537)	Soft Loss 0.0102 (0.0104)	Arch Loss 11.9843 (12.5093)	Arch Hard Loss 2.2185 (2.3158)	Arch Soft Loss 0.0107 (0.0105)	Prec@(1,5) (44.6%, 76.2%)	
07/29 08:43:59午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 12.2830 (11.1000)	Hard Loss 2.2739 (2.0548)	Soft Loss 0.0104 (0.0104)	Arch Loss 12.1259 (12.5482)	Arch Hard Loss 2.2446 (2.3230)	Arch Soft Loss 0.0121 (0.0105)	Prec@(1,5) (44.5%, 76.6%)	
07/29 08:44:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 14.1799 (11.1750)	Hard Loss 2.6251 (2.0687)	Soft Loss 0.0113 (0.0104)	Arch Loss 13.1609 (12.5790)	Arch Hard Loss 2.4365 (2.3287)	Arch Soft Loss 0.0100 (0.0105)	Prec@(1,5) (44.2%, 76.4%)	
07/29 08:45:50午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 11.8874 (11.1672)	Hard Loss 2.2006 (2.0672)	Soft Loss 0.0103 (0.0104)	Arch Loss 10.1331 (12.4902)	Arch Hard Loss 1.8758 (2.3122)	Arch Soft Loss 0.0098 (0.0104)	Prec@(1,5) (44.3%, 76.2%)	
07/29 08:46:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 9.7204 (11.1661)	Hard Loss 1.7993 (2.0670)	Soft Loss 0.0099 (0.0104)	Arch Loss 13.2998 (12.4661)	Arch Hard Loss 2.4621 (2.3078)	Arch Soft Loss 0.0107 (0.0105)	Prec@(1,5) (44.2%, 76.2%)	
07/29 08:47:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 10.9654 (11.1356)	Hard Loss 2.0298 (2.0614)	Soft Loss 0.0111 (0.0104)	Arch Loss 11.7602 (12.4415)	Arch Hard Loss 2.1770 (2.3032)	Arch Soft Loss 0.0104 (0.0104)	Prec@(1,5) (44.2%, 76.3%)	
07/29 08:48:25午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 10.7781 (11.1179)	Hard Loss 1.9952 (2.0581)	Soft Loss 0.0097 (0.0104)	Arch Loss 11.5837 (12.4055)	Arch Hard Loss 2.1444 (2.2965)	Arch Soft Loss 0.0100 (0.0104)	Prec@(1,5) (44.3%, 76.6%)	
07/29 08:48:26午後 searchCell_trainer.py:234 [INFO] Train: [  6/49] Final Prec@1 44.3200%
07/29 08:48:33午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 2.2905	Prec@(1,5) (40.1%, 72.7%)
07/29 08:48:40午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.2793	Prec@(1,5) (39.9%, 72.7%)
07/29 08:48:46午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 2.2650	Prec@(1,5) (40.0%, 72.7%)
07/29 08:48:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.2569	Prec@(1,5) (40.6%, 72.6%)
07/29 08:48:59午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 2.2652	Prec@(1,5) (40.5%, 72.6%)
07/29 08:49:05午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.2641	Prec@(1,5) (40.5%, 72.8%)
07/29 08:49:12午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 2.2638	Prec@(1,5) (40.5%, 72.8%)
07/29 08:49:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.2647	Prec@(1,5) (40.4%, 72.8%)
07/29 08:49:17午後 searchCell_trainer.py:269 [INFO] Valid: [  6/49] Final Prec@1 40.4360%
07/29 08:49:17午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('skip_connect', 0)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 4), ('sep_conv_5x5', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 4)]], normal3_concat=range(2, 6))
07/29 08:49:18午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 40.4360%
####### ALPHA #######
# Alpha - normal
tensor([[0.1276, 0.1183, 0.1370, 0.1351, 0.1164, 0.1213, 0.1232, 0.1211],
        [0.1148, 0.1029, 0.1253, 0.1396, 0.1253, 0.1354, 0.1272, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1296, 0.1193, 0.1366, 0.1324, 0.1175, 0.1195, 0.1236, 0.1216],
        [0.1143, 0.1024, 0.1251, 0.1440, 0.1337, 0.1382, 0.1175, 0.1247],
        [0.1103, 0.1031, 0.1276, 0.1301, 0.1349, 0.1354, 0.1305, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1292, 0.1185, 0.1344, 0.1308, 0.1207, 0.1224, 0.1196, 0.1243],
        [0.1121, 0.1043, 0.1220, 0.1539, 0.1295, 0.1212, 0.1281, 0.1289],
        [0.1088, 0.0987, 0.1196, 0.1561, 0.1302, 0.1345, 0.1284, 0.1236],
        [0.1036, 0.0984, 0.1193, 0.1390, 0.1330, 0.1422, 0.1353, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1393, 0.1256, 0.1409, 0.1214, 0.1177, 0.1147, 0.1168, 0.1236],
        [0.1161, 0.1038, 0.1203, 0.1539, 0.1224, 0.1266, 0.1237, 0.1331],
        [0.1142, 0.1028, 0.1223, 0.1370, 0.1343, 0.1355, 0.1303, 0.1235],
        [0.1080, 0.0999, 0.1181, 0.1418, 0.1399, 0.1380, 0.1333, 0.1210],
        [0.1081, 0.0993, 0.1209, 0.1430, 0.1309, 0.1344, 0.1332, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1370, 0.1375, 0.1548, 0.1220, 0.1148, 0.1152, 0.1159, 0.1028],
        [0.1099, 0.1139, 0.1248, 0.1256, 0.1210, 0.1269, 0.1316, 0.1464]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1396, 0.1401, 0.1562, 0.1180, 0.1142, 0.1142, 0.1160, 0.1018],
        [0.1163, 0.1196, 0.1332, 0.1210, 0.1234, 0.1201, 0.1228, 0.1436],
        [0.1165, 0.1177, 0.1309, 0.1235, 0.1235, 0.1207, 0.1376, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1340, 0.1377, 0.1549, 0.1259, 0.1167, 0.1132, 0.1173, 0.1003],
        [0.1064, 0.1105, 0.1256, 0.1222, 0.1303, 0.1326, 0.1320, 0.1404],
        [0.1111, 0.1158, 0.1312, 0.1279, 0.1171, 0.1299, 0.1259, 0.1411],
        [0.1055, 0.1113, 0.1249, 0.1283, 0.1293, 0.1219, 0.1315, 0.1473]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1398, 0.1430, 0.1638, 0.1213, 0.1122, 0.1070, 0.1098, 0.1032],
        [0.1100, 0.1150, 0.1306, 0.1240, 0.1281, 0.1285, 0.1297, 0.1341],
        [0.1154, 0.1195, 0.1343, 0.1254, 0.1310, 0.1176, 0.1249, 0.1319],
        [0.1102, 0.1158, 0.1302, 0.1287, 0.1274, 0.1244, 0.1218, 0.1414],
        [0.1109, 0.1203, 0.1305, 0.1213, 0.1208, 0.1226, 0.1320, 0.1416]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1078, 0.1075, 0.1184, 0.1335, 0.1324, 0.1358, 0.1317, 0.1328],
        [0.1063, 0.1075, 0.1125, 0.1217, 0.1435, 0.1309, 0.1339, 0.1437]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1076, 0.1085, 0.1185, 0.1255, 0.1356, 0.1342, 0.1303, 0.1397],
        [0.1099, 0.1119, 0.1168, 0.1226, 0.1180, 0.1277, 0.1410, 0.1520],
        [0.0953, 0.0877, 0.1060, 0.1305, 0.1250, 0.1325, 0.1491, 0.1738]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1104, 0.1104, 0.1201, 0.1202, 0.1358, 0.1212, 0.1339, 0.1480],
        [0.1066, 0.1073, 0.1119, 0.1272, 0.1249, 0.1349, 0.1280, 0.1592],
        [0.0946, 0.0872, 0.1027, 0.1350, 0.1333, 0.1402, 0.1300, 0.1770],
        [0.0899, 0.0827, 0.0880, 0.1425, 0.1313, 0.1406, 0.1481, 0.1768]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1081, 0.1107, 0.1172, 0.1337, 0.1280, 0.1212, 0.1292, 0.1520],
        [0.1119, 0.1161, 0.1203, 0.1254, 0.1135, 0.1260, 0.1270, 0.1597],
        [0.0945, 0.0881, 0.1020, 0.1322, 0.1367, 0.1297, 0.1420, 0.1748],
        [0.0912, 0.0843, 0.0886, 0.1384, 0.1246, 0.1438, 0.1475, 0.1816],
        [0.0892, 0.0815, 0.0829, 0.1440, 0.1451, 0.1359, 0.1421, 0.1793]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1379, 0.1160, 0.1298, 0.1238, 0.1227, 0.1323, 0.1196, 0.1179],
        [0.1316, 0.1161, 0.1282, 0.1279, 0.1231, 0.1229, 0.1209, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1427, 0.1198, 0.1301, 0.1272, 0.1212, 0.1214, 0.1182, 0.1194],
        [0.1310, 0.1150, 0.1249, 0.1340, 0.1294, 0.1241, 0.1189, 0.1227],
        [0.1240, 0.1082, 0.1264, 0.1318, 0.1237, 0.1273, 0.1257, 0.1329]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1436, 0.1175, 0.1248, 0.1249, 0.1286, 0.1261, 0.1161, 0.1184],
        [0.1331, 0.1181, 0.1269, 0.1264, 0.1236, 0.1270, 0.1215, 0.1235],
        [0.1282, 0.1106, 0.1284, 0.1273, 0.1269, 0.1233, 0.1271, 0.1283],
        [0.1210, 0.1119, 0.1248, 0.1318, 0.1251, 0.1281, 0.1239, 0.1335]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1384, 0.1216, 0.1300, 0.1234, 0.1209, 0.1279, 0.1242, 0.1135],
        [0.1340, 0.1201, 0.1217, 0.1388, 0.1177, 0.1352, 0.1155, 0.1170],
        [0.1244, 0.1123, 0.1344, 0.1299, 0.1213, 0.1220, 0.1204, 0.1352],
        [0.1183, 0.1107, 0.1253, 0.1358, 0.1267, 0.1260, 0.1264, 0.1307],
        [0.1146, 0.1108, 0.1257, 0.1331, 0.1316, 0.1182, 0.1200, 0.1460]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1358, 0.1288, 0.1306, 0.1194, 0.1299, 0.1279, 0.1137, 0.1139],
        [0.1300, 0.1237, 0.1195, 0.1160, 0.1296, 0.1269, 0.1226, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1330, 0.1265, 0.1317, 0.1288, 0.1289, 0.1190, 0.1222, 0.1100],
        [0.1331, 0.1269, 0.1195, 0.1262, 0.1292, 0.1149, 0.1200, 0.1301],
        [0.1144, 0.1014, 0.1279, 0.1273, 0.1307, 0.1243, 0.1418, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1382, 0.1336, 0.1121, 0.1182, 0.1291, 0.1299, 0.1211, 0.1178],
        [0.1328, 0.1279, 0.1288, 0.1263, 0.1299, 0.1165, 0.1118, 0.1260],
        [0.1119, 0.1026, 0.1337, 0.1230, 0.1410, 0.1274, 0.1289, 0.1315],
        [0.1098, 0.1009, 0.1291, 0.1343, 0.1261, 0.1299, 0.1346, 0.1353]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1357, 0.1323, 0.1343, 0.1168, 0.1256, 0.1193, 0.1235, 0.1126],
        [0.1279, 0.1249, 0.1257, 0.1276, 0.1247, 0.1230, 0.1242, 0.1219],
        [0.1150, 0.1037, 0.1335, 0.1255, 0.1296, 0.1270, 0.1317, 0.1341],
        [0.1103, 0.1003, 0.1277, 0.1232, 0.1355, 0.1272, 0.1329, 0.1429],
        [0.1057, 0.0946, 0.1178, 0.1311, 0.1374, 0.1351, 0.1304, 0.1479]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:50:14午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 10.6564 (10.2934)	Hard Loss 1.9727 (1.9054)	Soft Loss 0.0101 (0.0100)	Arch Loss 13.0603 (12.1107)	Arch Hard Loss 2.4178 (2.2420)	Arch Soft Loss 0.0103 (0.0101)	Prec@(1,5) (47.4%, 79.2%)	
07/29 08:51:10午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 10.2217 (10.3011)	Hard Loss 1.8923 (1.9069)	Soft Loss 0.0087 (0.0100)	Arch Loss 13.8695 (12.1494)	Arch Hard Loss 2.5677 (2.2491)	Arch Soft Loss 0.0094 (0.0101)	Prec@(1,5) (47.5%, 79.0%)	
07/29 08:52:05午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 8.4075 (10.2306)	Hard Loss 1.5562 (1.8938)	Soft Loss 0.0100 (0.0100)	Arch Loss 11.1035 (12.1103)	Arch Hard Loss 2.0555 (2.2419)	Arch Soft Loss 0.0102 (0.0101)	Prec@(1,5) (48.1%, 79.6%)	
07/29 08:53:00午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 8.8395 (10.2468)	Hard Loss 1.6363 (1.8968)	Soft Loss 0.0090 (0.0100)	Arch Loss 9.9895 (11.9963)	Arch Hard Loss 1.8491 (2.2208)	Arch Soft Loss 0.0107 (0.0101)	Prec@(1,5) (48.1%, 79.4%)	
07/29 08:53:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 9.2472 (10.2568)	Hard Loss 1.7118 (1.8987)	Soft Loss 0.0092 (0.0100)	Arch Loss 10.8495 (12.0076)	Arch Hard Loss 2.0084 (2.2229)	Arch Soft Loss 0.0097 (0.0100)	Prec@(1,5) (48.0%, 79.4%)	
07/29 08:54:50午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 13.1535 (10.2982)	Hard Loss 2.4350 (1.9063)	Soft Loss 0.0106 (0.0101)	Arch Loss 12.9263 (12.0008)	Arch Hard Loss 2.3930 (2.2216)	Arch Soft Loss 0.0100 (0.0100)	Prec@(1,5) (47.8%, 79.2%)	
07/29 08:55:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 8.7447 (10.3132)	Hard Loss 1.6187 (1.9091)	Soft Loss 0.0092 (0.0101)	Arch Loss 11.8846 (12.0256)	Arch Hard Loss 2.2001 (2.2262)	Arch Soft Loss 0.0106 (0.0101)	Prec@(1,5) (47.8%, 79.1%)	
07/29 08:56:30午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 13.2206 (10.3428)	Hard Loss 2.4475 (1.9146)	Soft Loss 0.0105 (0.0101)	Arch Loss 13.3871 (11.9629)	Arch Hard Loss 2.4784 (2.2146)	Arch Soft Loss 0.0089 (0.0100)	Prec@(1,5) (47.8%, 79.0%)	
07/29 08:56:31午後 searchCell_trainer.py:234 [INFO] Train: [  7/49] Final Prec@1 47.7440%
07/29 08:56:38午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 2.1076	Prec@(1,5) (44.2%, 76.4%)
07/29 08:56:45午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.1496	Prec@(1,5) (43.2%, 75.3%)
07/29 08:56:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 2.1499	Prec@(1,5) (43.6%, 75.1%)
07/29 08:56:58午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.1625	Prec@(1,5) (43.3%, 74.7%)
07/29 08:57:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 2.1587	Prec@(1,5) (43.5%, 74.6%)
07/29 08:57:11午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.1631	Prec@(1,5) (43.6%, 74.5%)
07/29 08:57:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 2.1680	Prec@(1,5) (43.5%, 74.4%)
07/29 08:57:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.1718	Prec@(1,5) (43.4%, 74.4%)
07/29 08:57:23午後 searchCell_trainer.py:269 [INFO] Valid: [  7/49] Final Prec@1 43.4200%
07/29 08:57:23午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('dil_conv_5x5', 3)], [('dil_conv_3x3', 4), ('skip_connect', 2)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 4)]], normal3_concat=range(2, 6))
07/29 08:57:23午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 43.4200%
####### ALPHA #######
# Alpha - normal
tensor([[0.1243, 0.1154, 0.1358, 0.1391, 0.1151, 0.1236, 0.1234, 0.1233],
        [0.1142, 0.1012, 0.1264, 0.1415, 0.1250, 0.1369, 0.1266, 0.1281]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1268, 0.1165, 0.1352, 0.1365, 0.1155, 0.1212, 0.1249, 0.1233],
        [0.1117, 0.0990, 0.1231, 0.1491, 0.1343, 0.1408, 0.1177, 0.1241],
        [0.1067, 0.0989, 0.1259, 0.1320, 0.1377, 0.1383, 0.1327, 0.1278]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1269, 0.1160, 0.1333, 0.1325, 0.1202, 0.1242, 0.1204, 0.1267],
        [0.1098, 0.1009, 0.1197, 0.1618, 0.1306, 0.1200, 0.1298, 0.1276],
        [0.1059, 0.0949, 0.1179, 0.1595, 0.1315, 0.1364, 0.1297, 0.1242],
        [0.1006, 0.0951, 0.1179, 0.1422, 0.1358, 0.1440, 0.1358, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1231, 0.1392, 0.1245, 0.1194, 0.1146, 0.1169, 0.1253],
        [0.1150, 0.1014, 0.1190, 0.1635, 0.1203, 0.1281, 0.1220, 0.1306],
        [0.1119, 0.0994, 0.1207, 0.1401, 0.1360, 0.1385, 0.1302, 0.1232],
        [0.1056, 0.0969, 0.1166, 0.1452, 0.1427, 0.1392, 0.1339, 0.1198],
        [0.1057, 0.0959, 0.1193, 0.1462, 0.1330, 0.1363, 0.1350, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1377, 0.1395, 0.1598, 0.1218, 0.1135, 0.1131, 0.1147, 0.0999],
        [0.1085, 0.1145, 0.1268, 0.1252, 0.1195, 0.1254, 0.1315, 0.1486]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1410, 0.1429, 0.1615, 0.1172, 0.1129, 0.1117, 0.1144, 0.0984],
        [0.1152, 0.1202, 0.1358, 0.1216, 0.1212, 0.1189, 0.1222, 0.1448],
        [0.1153, 0.1185, 0.1339, 0.1224, 0.1218, 0.1186, 0.1376, 0.1318]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1346, 0.1404, 0.1609, 0.1255, 0.1146, 0.1113, 0.1155, 0.0972],
        [0.1047, 0.1107, 0.1272, 0.1215, 0.1301, 0.1327, 0.1317, 0.1414],
        [0.1094, 0.1160, 0.1339, 0.1283, 0.1149, 0.1306, 0.1231, 0.1438],
        [0.1032, 0.1107, 0.1260, 0.1291, 0.1288, 0.1211, 0.1302, 0.1510]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1419, 0.1472, 0.1709, 0.1186, 0.1099, 0.1040, 0.1071, 0.1003],
        [0.1089, 0.1160, 0.1332, 0.1231, 0.1273, 0.1282, 0.1291, 0.1343],
        [0.1140, 0.1207, 0.1382, 0.1257, 0.1290, 0.1151, 0.1238, 0.1334],
        [0.1082, 0.1160, 0.1321, 0.1296, 0.1271, 0.1235, 0.1194, 0.1441],
        [0.1091, 0.1212, 0.1330, 0.1213, 0.1179, 0.1211, 0.1303, 0.1461]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1038, 0.1039, 0.1174, 0.1355, 0.1347, 0.1378, 0.1319, 0.1350],
        [0.1017, 0.1029, 0.1090, 0.1222, 0.1465, 0.1330, 0.1371, 0.1475]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1036, 0.1050, 0.1172, 0.1269, 0.1369, 0.1365, 0.1313, 0.1427],
        [0.1054, 0.1080, 0.1140, 0.1232, 0.1173, 0.1290, 0.1455, 0.1577],
        [0.0891, 0.0820, 0.1032, 0.1316, 0.1225, 0.1338, 0.1508, 0.1870]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1066, 0.1072, 0.1193, 0.1194, 0.1367, 0.1220, 0.1350, 0.1537],
        [0.1016, 0.1027, 0.1081, 0.1286, 0.1262, 0.1365, 0.1296, 0.1668],
        [0.0883, 0.0813, 0.0992, 0.1350, 0.1337, 0.1422, 0.1301, 0.1902],
        [0.0833, 0.0765, 0.0832, 0.1439, 0.1287, 0.1439, 0.1492, 0.1914]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1038, 0.1073, 0.1156, 0.1363, 0.1260, 0.1225, 0.1283, 0.1600],
        [0.1073, 0.1122, 0.1174, 0.1270, 0.1122, 0.1268, 0.1286, 0.1686],
        [0.0879, 0.0818, 0.0981, 0.1317, 0.1375, 0.1297, 0.1442, 0.1892],
        [0.0840, 0.0774, 0.0827, 0.1391, 0.1214, 0.1477, 0.1502, 0.1975],
        [0.0824, 0.0748, 0.0767, 0.1446, 0.1460, 0.1383, 0.1422, 0.1950]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1392, 0.1139, 0.1296, 0.1246, 0.1226, 0.1345, 0.1184, 0.1171],
        [0.1316, 0.1142, 0.1290, 0.1273, 0.1229, 0.1247, 0.1210, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1449, 0.1190, 0.1303, 0.1280, 0.1228, 0.1210, 0.1151, 0.1188],
        [0.1313, 0.1143, 0.1254, 0.1356, 0.1313, 0.1220, 0.1175, 0.1225],
        [0.1236, 0.1079, 0.1267, 0.1331, 0.1233, 0.1255, 0.1256, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1449, 0.1163, 0.1247, 0.1259, 0.1296, 0.1261, 0.1156, 0.1169],
        [0.1335, 0.1176, 0.1272, 0.1276, 0.1227, 0.1272, 0.1209, 0.1235],
        [0.1279, 0.1096, 0.1295, 0.1263, 0.1268, 0.1225, 0.1282, 0.1293],
        [0.1195, 0.1098, 0.1235, 0.1338, 0.1265, 0.1295, 0.1239, 0.1334]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1399, 0.1212, 0.1289, 0.1240, 0.1213, 0.1282, 0.1232, 0.1133],
        [0.1332, 0.1179, 0.1217, 0.1407, 0.1184, 0.1357, 0.1147, 0.1177],
        [0.1225, 0.1105, 0.1351, 0.1324, 0.1209, 0.1218, 0.1193, 0.1375],
        [0.1164, 0.1083, 0.1236, 0.1370, 0.1279, 0.1287, 0.1276, 0.1305],
        [0.1132, 0.1095, 0.1250, 0.1348, 0.1342, 0.1162, 0.1194, 0.1478]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1359, 0.1292, 0.1316, 0.1183, 0.1286, 0.1317, 0.1127, 0.1121],
        [0.1302, 0.1238, 0.1190, 0.1150, 0.1307, 0.1288, 0.1211, 0.1314]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1333, 0.1270, 0.1324, 0.1303, 0.1289, 0.1197, 0.1206, 0.1078],
        [0.1342, 0.1281, 0.1192, 0.1273, 0.1292, 0.1139, 0.1192, 0.1289],
        [0.1109, 0.0991, 0.1305, 0.1278, 0.1318, 0.1231, 0.1416, 0.1350]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1400, 0.1360, 0.1097, 0.1184, 0.1295, 0.1303, 0.1199, 0.1163],
        [0.1336, 0.1291, 0.1313, 0.1253, 0.1303, 0.1160, 0.1093, 0.1251],
        [0.1086, 0.1005, 0.1370, 0.1227, 0.1413, 0.1286, 0.1296, 0.1317],
        [0.1060, 0.0981, 0.1317, 0.1331, 0.1252, 0.1302, 0.1372, 0.1387]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1367, 0.1342, 0.1360, 0.1170, 0.1249, 0.1186, 0.1227, 0.1099],
        [0.1275, 0.1254, 0.1274, 0.1289, 0.1242, 0.1231, 0.1228, 0.1207],
        [0.1115, 0.1014, 0.1371, 0.1236, 0.1302, 0.1271, 0.1321, 0.1370],
        [0.1058, 0.0971, 0.1294, 0.1245, 0.1345, 0.1275, 0.1332, 0.1479],
        [0.0996, 0.0897, 0.1160, 0.1317, 0.1378, 0.1399, 0.1312, 0.1542]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 08:58:21午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 8.4281 (9.5078)	Hard Loss 1.5601 (1.7600)	Soft Loss 0.0095 (0.0096)	Arch Loss 9.3049 (11.9224)	Arch Hard Loss 1.7224 (2.2071)	Arch Soft Loss 0.0097 (0.0097)	Prec@(1,5) (51.6%, 81.7%)	
07/29 08:59:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 9.2706 (9.4621)	Hard Loss 1.7161 (1.7515)	Soft Loss 0.0098 (0.0096)	Arch Loss 12.1809 (11.6972)	Arch Hard Loss 2.2549 (2.1654)	Arch Soft Loss 0.0112 (0.0099)	Prec@(1,5) (51.6%, 82.0%)	
07/29 09:00:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 7.8917 (9.4903)	Hard Loss 1.4607 (1.7568)	Soft Loss 0.0100 (0.0096)	Arch Loss 10.1283 (11.6394)	Arch Hard Loss 1.8749 (2.1547)	Arch Soft Loss 0.0092 (0.0099)	Prec@(1,5) (51.3%, 82.1%)	
07/29 09:01:06午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 8.0297 (9.5045)	Hard Loss 1.4863 (1.7594)	Soft Loss 0.0098 (0.0096)	Arch Loss 10.8940 (11.6084)	Arch Hard Loss 2.0168 (2.1490)	Arch Soft Loss 0.0087 (0.0099)	Prec@(1,5) (51.3%, 81.9%)	
07/29 09:02:02午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 11.4940 (9.5346)	Hard Loss 2.1278 (1.7650)	Soft Loss 0.0093 (0.0096)	Arch Loss 12.1370 (11.6722)	Arch Hard Loss 2.2468 (2.1608)	Arch Soft Loss 0.0104 (0.0099)	Prec@(1,5) (51.2%, 81.7%)	
07/29 09:02:57午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 11.6049 (9.6339)	Hard Loss 2.1483 (1.7834)	Soft Loss 0.0098 (0.0096)	Arch Loss 11.4339 (11.6321)	Arch Hard Loss 2.1167 (2.1534)	Arch Soft Loss 0.0091 (0.0099)	Prec@(1,5) (50.9%, 81.4%)	
07/29 09:03:52午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 11.2865 (9.6600)	Hard Loss 2.0892 (1.7882)	Soft Loss 0.0114 (0.0096)	Arch Loss 13.1322 (11.6339)	Arch Hard Loss 2.4312 (2.1537)	Arch Soft Loss 0.0092 (0.0099)	Prec@(1,5) (50.7%, 81.4%)	
07/29 09:04:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 9.8848 (9.6903)	Hard Loss 1.8298 (1.7938)	Soft Loss 0.0092 (0.0096)	Arch Loss 12.0875 (11.6021)	Arch Hard Loss 2.2377 (2.1478)	Arch Soft Loss 0.0093 (0.0098)	Prec@(1,5) (50.7%, 81.2%)	
07/29 09:04:38午後 searchCell_trainer.py:234 [INFO] Train: [  8/49] Final Prec@1 50.7000%
07/29 09:04:45午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 2.1430	Prec@(1,5) (44.5%, 75.6%)
07/29 09:04:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.1026	Prec@(1,5) (45.2%, 75.9%)
07/29 09:04:58午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 2.0839	Prec@(1,5) (45.4%, 76.1%)
07/29 09:05:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.0871	Prec@(1,5) (45.5%, 75.9%)
07/29 09:05:10午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 2.0944	Prec@(1,5) (45.4%, 75.6%)
07/29 09:05:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.1006	Prec@(1,5) (45.3%, 75.5%)
07/29 09:05:23午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 2.0998	Prec@(1,5) (45.1%, 75.5%)
07/29 09:05:28午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.0940	Prec@(1,5) (45.2%, 75.6%)
07/29 09:05:28午後 searchCell_trainer.py:269 [INFO] Valid: [  8/49] Final Prec@1 45.2240%
07/29 09:05:28午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('dil_conv_5x5', 3)], [('dil_conv_3x3', 4), ('skip_connect', 2)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 4)]], normal3_concat=range(2, 6))
07/29 09:05:29午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 45.2240%
####### ALPHA #######
# Alpha - normal
tensor([[0.1202, 0.1122, 0.1341, 0.1439, 0.1157, 0.1256, 0.1242, 0.1240],
        [0.1120, 0.0989, 0.1260, 0.1451, 0.1241, 0.1396, 0.1260, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1232, 0.1134, 0.1331, 0.1406, 0.1152, 0.1221, 0.1271, 0.1252],
        [0.1083, 0.0956, 0.1210, 0.1527, 0.1354, 0.1441, 0.1174, 0.1255],
        [0.1034, 0.0953, 0.1240, 0.1336, 0.1417, 0.1405, 0.1346, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1237, 0.1131, 0.1316, 0.1341, 0.1204, 0.1259, 0.1222, 0.1290],
        [0.1071, 0.0979, 0.1176, 0.1719, 0.1295, 0.1170, 0.1311, 0.1281],
        [0.1029, 0.0913, 0.1161, 0.1634, 0.1326, 0.1387, 0.1308, 0.1242],
        [0.0969, 0.0917, 0.1157, 0.1464, 0.1378, 0.1470, 0.1375, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1356, 0.1219, 0.1392, 0.1260, 0.1190, 0.1145, 0.1172, 0.1266],
        [0.1139, 0.0996, 0.1180, 0.1747, 0.1175, 0.1274, 0.1216, 0.1273],
        [0.1103, 0.0968, 0.1201, 0.1417, 0.1365, 0.1397, 0.1318, 0.1233],
        [0.1032, 0.0945, 0.1145, 0.1497, 0.1448, 0.1404, 0.1347, 0.1183],
        [0.1040, 0.0939, 0.1190, 0.1491, 0.1344, 0.1361, 0.1366, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1394, 0.1426, 0.1669, 0.1201, 0.1116, 0.1110, 0.1124, 0.0961],
        [0.1066, 0.1139, 0.1275, 0.1244, 0.1182, 0.1250, 0.1321, 0.1524]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1432, 0.1465, 0.1684, 0.1152, 0.1115, 0.1087, 0.1118, 0.0946],
        [0.1140, 0.1203, 0.1376, 0.1213, 0.1190, 0.1181, 0.1217, 0.1481],
        [0.1142, 0.1185, 0.1365, 0.1222, 0.1198, 0.1178, 0.1370, 0.1340]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1355, 0.1434, 0.1676, 0.1242, 0.1145, 0.1091, 0.1127, 0.0929],
        [0.1029, 0.1098, 0.1281, 0.1212, 0.1307, 0.1327, 0.1311, 0.1437],
        [0.1081, 0.1159, 0.1372, 0.1266, 0.1145, 0.1297, 0.1208, 0.1473],
        [0.1013, 0.1094, 0.1267, 0.1289, 0.1273, 0.1196, 0.1306, 0.1561]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1438, 0.1515, 0.1789, 0.1180, 0.1076, 0.1004, 0.1036, 0.0963],
        [0.1074, 0.1158, 0.1346, 0.1222, 0.1270, 0.1279, 0.1296, 0.1355],
        [0.1123, 0.1206, 0.1409, 0.1275, 0.1269, 0.1136, 0.1236, 0.1347],
        [0.1064, 0.1154, 0.1337, 0.1302, 0.1266, 0.1214, 0.1192, 0.1471],
        [0.1082, 0.1214, 0.1358, 0.1200, 0.1169, 0.1188, 0.1277, 0.1512]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1003, 0.1003, 0.1167, 0.1373, 0.1349, 0.1410, 0.1330, 0.1365],
        [0.0975, 0.0985, 0.1059, 0.1214, 0.1488, 0.1361, 0.1388, 0.1529]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1000, 0.1014, 0.1165, 0.1294, 0.1365, 0.1385, 0.1324, 0.1453],
        [0.1010, 0.1036, 0.1107, 0.1236, 0.1154, 0.1305, 0.1507, 0.1645],
        [0.0831, 0.0763, 0.1004, 0.1319, 0.1189, 0.1351, 0.1521, 0.2021]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1032, 0.1039, 0.1190, 0.1191, 0.1352, 0.1232, 0.1361, 0.1602],
        [0.0968, 0.0979, 0.1041, 0.1291, 0.1273, 0.1379, 0.1306, 0.1763],
        [0.0822, 0.0756, 0.0958, 0.1349, 0.1330, 0.1455, 0.1279, 0.2052],
        [0.0769, 0.0704, 0.0783, 0.1451, 0.1249, 0.1472, 0.1487, 0.2085]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1004, 0.1040, 0.1144, 0.1381, 0.1226, 0.1235, 0.1280, 0.1691],
        [0.1033, 0.1086, 0.1151, 0.1276, 0.1096, 0.1267, 0.1302, 0.1789],
        [0.0819, 0.0762, 0.0947, 0.1303, 0.1358, 0.1308, 0.1451, 0.2052],
        [0.0775, 0.0711, 0.0773, 0.1374, 0.1179, 0.1504, 0.1530, 0.2155],
        [0.0760, 0.0686, 0.0709, 0.1428, 0.1465, 0.1403, 0.1422, 0.2127]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1408, 0.1127, 0.1306, 0.1253, 0.1209, 0.1351, 0.1180, 0.1168],
        [0.1324, 0.1132, 0.1301, 0.1263, 0.1235, 0.1257, 0.1197, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1476, 0.1178, 0.1300, 0.1289, 0.1222, 0.1204, 0.1138, 0.1192],
        [0.1322, 0.1130, 0.1258, 0.1369, 0.1341, 0.1220, 0.1150, 0.1208],
        [0.1228, 0.1062, 0.1266, 0.1353, 0.1235, 0.1245, 0.1255, 0.1356]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1474, 0.1159, 0.1250, 0.1257, 0.1292, 0.1271, 0.1136, 0.1160],
        [0.1347, 0.1171, 0.1263, 0.1303, 0.1218, 0.1281, 0.1196, 0.1222],
        [0.1263, 0.1084, 0.1302, 0.1274, 0.1272, 0.1236, 0.1273, 0.1296],
        [0.1185, 0.1088, 0.1242, 0.1355, 0.1252, 0.1300, 0.1232, 0.1346]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1413, 0.1206, 0.1289, 0.1232, 0.1221, 0.1285, 0.1226, 0.1128],
        [0.1341, 0.1177, 0.1217, 0.1404, 0.1166, 0.1381, 0.1140, 0.1173],
        [0.1209, 0.1087, 0.1361, 0.1337, 0.1200, 0.1213, 0.1198, 0.1395],
        [0.1145, 0.1065, 0.1236, 0.1380, 0.1265, 0.1302, 0.1291, 0.1315],
        [0.1110, 0.1068, 0.1235, 0.1364, 0.1371, 0.1159, 0.1192, 0.1502]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1385, 0.1310, 0.1325, 0.1159, 0.1289, 0.1328, 0.1110, 0.1095],
        [0.1316, 0.1244, 0.1172, 0.1144, 0.1307, 0.1280, 0.1212, 0.1324]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1354, 0.1280, 0.1330, 0.1306, 0.1294, 0.1191, 0.1197, 0.1047],
        [0.1372, 0.1301, 0.1176, 0.1266, 0.1289, 0.1119, 0.1180, 0.1297],
        [0.1086, 0.0968, 0.1333, 0.1272, 0.1324, 0.1232, 0.1407, 0.1378]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1419, 0.1377, 0.1092, 0.1184, 0.1283, 0.1315, 0.1189, 0.1139],
        [0.1357, 0.1302, 0.1333, 0.1250, 0.1307, 0.1151, 0.1068, 0.1232],
        [0.1061, 0.0979, 0.1404, 0.1221, 0.1423, 0.1280, 0.1305, 0.1327],
        [0.1020, 0.0947, 0.1339, 0.1342, 0.1236, 0.1296, 0.1387, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1389, 0.1365, 0.1391, 0.1159, 0.1235, 0.1178, 0.1213, 0.1070],
        [0.1287, 0.1262, 0.1291, 0.1301, 0.1210, 0.1219, 0.1225, 0.1204],
        [0.1087, 0.0991, 0.1408, 0.1234, 0.1292, 0.1270, 0.1318, 0.1401],
        [0.1019, 0.0937, 0.1314, 0.1247, 0.1329, 0.1290, 0.1340, 0.1524],
        [0.0957, 0.0862, 0.1162, 0.1310, 0.1355, 0.1422, 0.1327, 0.1606]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:06:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 8.1752 (8.5797)	Hard Loss 1.5132 (1.5882)	Soft Loss 0.0095 (0.0091)	Arch Loss 11.1215 (11.0800)	Arch Hard Loss 2.0588 (2.0511)	Arch Soft Loss 0.0094 (0.0096)	Prec@(1,5) (55.2%, 84.9%)	
07/29 09:07:22午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 7.1311 (8.7002)	Hard Loss 1.3198 (1.6105)	Soft Loss 0.0097 (0.0092)	Arch Loss 10.9785 (11.1953)	Arch Hard Loss 2.0323 (2.0725)	Arch Soft Loss 0.0100 (0.0096)	Prec@(1,5) (54.6%, 84.6%)	
07/29 09:08:17午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 10.4915 (8.9083)	Hard Loss 1.9421 (1.6490)	Soft Loss 0.0099 (0.0092)	Arch Loss 9.9733 (11.2833)	Arch Hard Loss 1.8462 (2.0888)	Arch Soft Loss 0.0092 (0.0097)	Prec@(1,5) (53.8%, 83.9%)	
07/29 09:09:13午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 8.8313 (8.9097)	Hard Loss 1.6347 (1.6493)	Soft Loss 0.0100 (0.0092)	Arch Loss 13.6824 (11.2101)	Arch Hard Loss 2.5331 (2.0752)	Arch Soft Loss 0.0095 (0.0096)	Prec@(1,5) (53.9%, 83.9%)	
07/29 09:10:08午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 10.8183 (9.0023)	Hard Loss 2.0026 (1.6664)	Soft Loss 0.0107 (0.0093)	Arch Loss 13.8349 (11.2593)	Arch Hard Loss 2.5613 (2.0843)	Arch Soft Loss 0.0092 (0.0096)	Prec@(1,5) (53.5%, 83.5%)	
07/29 09:11:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 7.1557 (9.0420)	Hard Loss 1.3245 (1.6737)	Soft Loss 0.0084 (0.0093)	Arch Loss 11.5563 (11.2960)	Arch Hard Loss 2.1393 (2.0911)	Arch Soft Loss 0.0108 (0.0097)	Prec@(1,5) (53.5%, 83.3%)	
07/29 09:11:58午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 9.2370 (9.0415)	Hard Loss 1.7099 (1.6737)	Soft Loss 0.0087 (0.0093)	Arch Loss 12.8410 (11.2892)	Arch Hard Loss 2.3772 (2.0899)	Arch Soft Loss 0.0098 (0.0096)	Prec@(1,5) (53.3%, 83.3%)	
07/29 09:12:42午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 7.3691 (9.0456)	Hard Loss 1.3641 (1.6744)	Soft Loss 0.0080 (0.0093)	Arch Loss 9.9446 (11.2446)	Arch Hard Loss 1.8409 (2.0816)	Arch Soft Loss 0.0097 (0.0096)	Prec@(1,5) (53.3%, 83.3%)	
07/29 09:12:43午後 searchCell_trainer.py:234 [INFO] Train: [  9/49] Final Prec@1 53.2280%
07/29 09:12:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 2.1593	Prec@(1,5) (44.2%, 74.7%)
07/29 09:12:57午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.1336	Prec@(1,5) (44.4%, 75.2%)
07/29 09:13:03午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 2.1335	Prec@(1,5) (44.4%, 75.6%)
07/29 09:13:10午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.1359	Prec@(1,5) (44.5%, 75.7%)
07/29 09:13:16午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 2.1280	Prec@(1,5) (44.7%, 75.7%)
07/29 09:13:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.1234	Prec@(1,5) (44.9%, 75.8%)
07/29 09:13:29午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 2.1304	Prec@(1,5) (44.7%, 75.6%)
07/29 09:13:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.1219	Prec@(1,5) (44.8%, 75.7%)
07/29 09:13:34午後 searchCell_trainer.py:269 [INFO] Valid: [  9/49] Final Prec@1 44.7560%
07/29 09:13:34午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('dil_conv_5x5', 3)], [('skip_connect', 2), ('dil_conv_3x3', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/29 09:13:34午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 45.2240%
####### ALPHA #######
# Alpha - normal
tensor([[0.1181, 0.1101, 0.1335, 0.1473, 0.1155, 0.1266, 0.1230, 0.1259],
        [0.1116, 0.0974, 0.1266, 0.1464, 0.1245, 0.1409, 0.1248, 0.1277]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1213, 0.1120, 0.1330, 0.1449, 0.1137, 0.1212, 0.1273, 0.1265],
        [0.1065, 0.0933, 0.1200, 0.1578, 0.1379, 0.1453, 0.1153, 0.1239],
        [0.1008, 0.0924, 0.1236, 0.1349, 0.1430, 0.1412, 0.1370, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1210, 0.1111, 0.1307, 0.1374, 0.1192, 0.1271, 0.1232, 0.1303],
        [0.1054, 0.0958, 0.1164, 0.1805, 0.1284, 0.1148, 0.1316, 0.1271],
        [0.0995, 0.0877, 0.1139, 0.1680, 0.1349, 0.1402, 0.1314, 0.1243],
        [0.0933, 0.0886, 0.1136, 0.1521, 0.1391, 0.1489, 0.1374, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1345, 0.1211, 0.1393, 0.1279, 0.1185, 0.1141, 0.1179, 0.1268],
        [0.1133, 0.0981, 0.1176, 0.1844, 0.1156, 0.1260, 0.1207, 0.1243],
        [0.1078, 0.0939, 0.1192, 0.1442, 0.1378, 0.1423, 0.1309, 0.1240],
        [0.1007, 0.0925, 0.1136, 0.1538, 0.1459, 0.1409, 0.1338, 0.1189],
        [0.1017, 0.0920, 0.1186, 0.1531, 0.1356, 0.1367, 0.1355, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1388, 0.1459, 0.1712, 0.1194, 0.1107, 0.1097, 0.1110, 0.0932],
        [0.1038, 0.1122, 0.1270, 0.1267, 0.1168, 0.1245, 0.1329, 0.1560]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1428, 0.1503, 0.1729, 0.1147, 0.1114, 0.1070, 0.1090, 0.0918],
        [0.1120, 0.1191, 0.1379, 0.1219, 0.1177, 0.1182, 0.1225, 0.1508],
        [0.1129, 0.1188, 0.1388, 0.1210, 0.1200, 0.1157, 0.1370, 0.1359]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1468, 0.1720, 0.1227, 0.1148, 0.1080, 0.1107, 0.0902],
        [0.0998, 0.1080, 0.1274, 0.1221, 0.1308, 0.1342, 0.1315, 0.1462],
        [0.1064, 0.1158, 0.1390, 0.1268, 0.1142, 0.1277, 0.1200, 0.1500],
        [0.0984, 0.1082, 0.1265, 0.1297, 0.1278, 0.1195, 0.1308, 0.1592]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1438, 0.1560, 0.1845, 0.1169, 0.1062, 0.0980, 0.1014, 0.0932],
        [0.1046, 0.1145, 0.1351, 0.1214, 0.1280, 0.1296, 0.1297, 0.1371],
        [0.1106, 0.1206, 0.1430, 0.1292, 0.1262, 0.1130, 0.1220, 0.1354],
        [0.1042, 0.1153, 0.1351, 0.1310, 0.1264, 0.1203, 0.1178, 0.1500],
        [0.1062, 0.1213, 0.1371, 0.1193, 0.1149, 0.1195, 0.1282, 0.1535]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0964, 0.0971, 0.1168, 0.1398, 0.1344, 0.1432, 0.1336, 0.1387],
        [0.0927, 0.0940, 0.1027, 0.1216, 0.1512, 0.1381, 0.1412, 0.1584]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0960, 0.0978, 0.1155, 0.1310, 0.1364, 0.1406, 0.1346, 0.1482],
        [0.0961, 0.0991, 0.1073, 0.1238, 0.1134, 0.1316, 0.1562, 0.1724],
        [0.0767, 0.0705, 0.0970, 0.1319, 0.1159, 0.1374, 0.1517, 0.2189]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0991, 0.1000, 0.1178, 0.1179, 0.1334, 0.1245, 0.1395, 0.1678],
        [0.0921, 0.0935, 0.1009, 0.1289, 0.1283, 0.1373, 0.1320, 0.1871],
        [0.0759, 0.0698, 0.0923, 0.1340, 0.1303, 0.1479, 0.1261, 0.2237],
        [0.0704, 0.0644, 0.0732, 0.1450, 0.1210, 0.1514, 0.1487, 0.2258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0960, 0.1004, 0.1130, 0.1389, 0.1187, 0.1258, 0.1267, 0.1806],
        [0.0985, 0.1045, 0.1121, 0.1281, 0.1070, 0.1273, 0.1325, 0.1900],
        [0.0756, 0.0703, 0.0912, 0.1273, 0.1331, 0.1305, 0.1461, 0.2259],
        [0.0707, 0.0649, 0.0718, 0.1353, 0.1123, 0.1533, 0.1558, 0.2359],
        [0.0694, 0.0624, 0.0651, 0.1418, 0.1440, 0.1403, 0.1425, 0.2344]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1432, 0.1112, 0.1282, 0.1260, 0.1207, 0.1364, 0.1181, 0.1162],
        [0.1320, 0.1116, 0.1300, 0.1271, 0.1234, 0.1262, 0.1197, 0.1301]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1491, 0.1157, 0.1290, 0.1316, 0.1238, 0.1204, 0.1115, 0.1189],
        [0.1312, 0.1116, 0.1255, 0.1392, 0.1385, 0.1201, 0.1134, 0.1205],
        [0.1216, 0.1030, 0.1252, 0.1389, 0.1231, 0.1262, 0.1250, 0.1371]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1488, 0.1138, 0.1243, 0.1260, 0.1308, 0.1274, 0.1132, 0.1156],
        [0.1335, 0.1161, 0.1261, 0.1321, 0.1224, 0.1294, 0.1202, 0.1202],
        [0.1253, 0.1065, 0.1303, 0.1288, 0.1269, 0.1228, 0.1280, 0.1314],
        [0.1171, 0.1067, 0.1223, 0.1386, 0.1270, 0.1309, 0.1231, 0.1345]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1426, 0.1193, 0.1285, 0.1244, 0.1224, 0.1282, 0.1221, 0.1126],
        [0.1330, 0.1159, 0.1223, 0.1422, 0.1161, 0.1396, 0.1145, 0.1164],
        [0.1189, 0.1060, 0.1354, 0.1363, 0.1198, 0.1222, 0.1192, 0.1422],
        [0.1133, 0.1041, 0.1220, 0.1401, 0.1274, 0.1325, 0.1297, 0.1308],
        [0.1103, 0.1053, 0.1230, 0.1387, 0.1355, 0.1165, 0.1182, 0.1525]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1384, 0.1310, 0.1314, 0.1154, 0.1295, 0.1361, 0.1096, 0.1085],
        [0.1330, 0.1258, 0.1158, 0.1130, 0.1331, 0.1276, 0.1209, 0.1309]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1356, 0.1286, 0.1330, 0.1308, 0.1296, 0.1193, 0.1196, 0.1036],
        [0.1402, 0.1331, 0.1174, 0.1250, 0.1302, 0.1107, 0.1160, 0.1275],
        [0.1053, 0.0943, 0.1360, 0.1278, 0.1325, 0.1225, 0.1420, 0.1396]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1422, 0.1388, 0.1086, 0.1186, 0.1289, 0.1311, 0.1184, 0.1134],
        [0.1387, 0.1331, 0.1354, 0.1235, 0.1309, 0.1134, 0.1038, 0.1211],
        [0.1030, 0.0958, 0.1447, 0.1211, 0.1424, 0.1276, 0.1308, 0.1347],
        [0.0989, 0.0921, 0.1369, 0.1342, 0.1230, 0.1283, 0.1405, 0.1460]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1400, 0.1390, 0.1413, 0.1143, 0.1217, 0.1170, 0.1205, 0.1063],
        [0.1299, 0.1288, 0.1295, 0.1310, 0.1181, 0.1210, 0.1235, 0.1182],
        [0.1051, 0.0970, 0.1444, 0.1226, 0.1294, 0.1272, 0.1327, 0.1417],
        [0.0986, 0.0909, 0.1339, 0.1246, 0.1313, 0.1297, 0.1352, 0.1558],
        [0.0912, 0.0823, 0.1155, 0.1314, 0.1349, 0.1442, 0.1337, 0.1670]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:14:31午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 9.5240 (8.1597)	Hard Loss 1.7630 (1.5104)	Soft Loss 0.0097 (0.0090)	Arch Loss 11.8468 (11.1997)	Arch Hard Loss 2.1932 (2.0733)	Arch Soft Loss 0.0091 (0.0095)	Prec@(1,5) (58.0%, 86.6%)	
07/29 09:15:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 8.1423 (8.3075)	Hard Loss 1.5072 (1.5378)	Soft Loss 0.0085 (0.0090)	Arch Loss 10.5527 (11.2784)	Arch Hard Loss 1.9535 (2.0879)	Arch Soft Loss 0.0101 (0.0094)	Prec@(1,5) (57.1%, 85.7%)	
07/29 09:16:21午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 8.1752 (8.3627)	Hard Loss 1.5132 (1.5480)	Soft Loss 0.0093 (0.0090)	Arch Loss 11.6487 (11.1588)	Arch Hard Loss 2.1565 (2.0658)	Arch Soft Loss 0.0095 (0.0094)	Prec@(1,5) (56.7%, 85.5%)	
07/29 09:17:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 7.6060 (8.3832)	Hard Loss 1.4078 (1.5518)	Soft Loss 0.0090 (0.0090)	Arch Loss 9.8467 (11.1015)	Arch Hard Loss 1.8229 (2.0551)	Arch Soft Loss 0.0075 (0.0094)	Prec@(1,5) (56.4%, 85.6%)	
07/29 09:18:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 8.1973 (8.4775)	Hard Loss 1.5174 (1.5692)	Soft Loss 0.0090 (0.0090)	Arch Loss 12.0009 (11.1179)	Arch Hard Loss 2.2216 (2.0582)	Arch Soft Loss 0.0101 (0.0095)	Prec@(1,5) (55.9%, 85.2%)	
07/29 09:19:07午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 9.6141 (8.5363)	Hard Loss 1.7797 (1.5801)	Soft Loss 0.0099 (0.0090)	Arch Loss 12.1638 (11.0822)	Arch Hard Loss 2.2519 (2.0516)	Arch Soft Loss 0.0086 (0.0094)	Prec@(1,5) (55.7%, 85.1%)	
07/29 09:20:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 7.8084 (8.5698)	Hard Loss 1.4453 (1.5863)	Soft Loss 0.0091 (0.0090)	Arch Loss 9.7941 (11.0488)	Arch Hard Loss 1.8130 (2.0454)	Arch Soft Loss 0.0095 (0.0094)	Prec@(1,5) (55.7%, 84.9%)	
07/29 09:20:46午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 7.7407 (8.5983)	Hard Loss 1.4328 (1.5916)	Soft Loss 0.0091 (0.0090)	Arch Loss 10.6586 (11.0589)	Arch Hard Loss 1.9731 (2.0472)	Arch Soft Loss 0.0099 (0.0094)	Prec@(1,5) (55.6%, 84.8%)	
07/29 09:20:47午後 searchCell_trainer.py:234 [INFO] Train: [ 10/49] Final Prec@1 55.5600%
07/29 09:20:55午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 2.0400	Prec@(1,5) (46.2%, 77.7%)
07/29 09:21:01午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.0703	Prec@(1,5) (45.4%, 76.7%)
07/29 09:21:08午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 2.0608	Prec@(1,5) (46.0%, 76.7%)
07/29 09:21:14午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.0534	Prec@(1,5) (45.9%, 76.8%)
07/29 09:21:20午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 2.0477	Prec@(1,5) (46.2%, 76.8%)
07/29 09:21:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.0628	Prec@(1,5) (45.9%, 76.7%)
07/29 09:21:33午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 2.0679	Prec@(1,5) (45.9%, 76.5%)
07/29 09:21:38午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.0647	Prec@(1,5) (45.9%, 76.6%)
07/29 09:21:38午後 searchCell_trainer.py:269 [INFO] Valid: [ 10/49] Final Prec@1 45.8680%
07/29 09:21:38午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('dil_conv_5x5', 3)], [('skip_connect', 2), ('dil_conv_3x3', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/29 09:21:39午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 45.8680%
####### ALPHA #######
# Alpha - normal
tensor([[0.1144, 0.1072, 0.1317, 0.1514, 0.1161, 0.1278, 0.1236, 0.1279],
        [0.1112, 0.0965, 0.1288, 0.1484, 0.1241, 0.1416, 0.1226, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1180, 0.1093, 0.1313, 0.1498, 0.1126, 0.1224, 0.1284, 0.1283],
        [0.1046, 0.0911, 0.1196, 0.1632, 0.1409, 0.1452, 0.1143, 0.1211],
        [0.0978, 0.0896, 0.1240, 0.1383, 0.1444, 0.1410, 0.1361, 0.1289]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1178, 0.1083, 0.1287, 0.1387, 0.1199, 0.1281, 0.1247, 0.1337],
        [0.1039, 0.0940, 0.1156, 0.1918, 0.1265, 0.1132, 0.1315, 0.1235],
        [0.0967, 0.0846, 0.1131, 0.1712, 0.1346, 0.1424, 0.1324, 0.1250],
        [0.0905, 0.0855, 0.1120, 0.1563, 0.1389, 0.1534, 0.1369, 0.1265]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1322, 0.1188, 0.1375, 0.1304, 0.1187, 0.1145, 0.1186, 0.1293],
        [0.1128, 0.0962, 0.1164, 0.1950, 0.1148, 0.1251, 0.1188, 0.1209],
        [0.1057, 0.0905, 0.1176, 0.1462, 0.1402, 0.1441, 0.1318, 0.1240],
        [0.0988, 0.0897, 0.1118, 0.1563, 0.1483, 0.1435, 0.1333, 0.1183],
        [0.0995, 0.0892, 0.1170, 0.1584, 0.1366, 0.1375, 0.1354, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1403, 0.1478, 0.1761, 0.1185, 0.1085, 0.1087, 0.1095, 0.0905],
        [0.1028, 0.1115, 0.1276, 0.1282, 0.1152, 0.1241, 0.1313, 0.1593]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1454, 0.1538, 0.1791, 0.1127, 0.1087, 0.1048, 0.1066, 0.0890],
        [0.1117, 0.1191, 0.1396, 0.1221, 0.1148, 0.1185, 0.1213, 0.1530],
        [0.1132, 0.1189, 0.1412, 0.1204, 0.1187, 0.1134, 0.1365, 0.1377]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1362, 0.1490, 0.1762, 0.1225, 0.1135, 0.1068, 0.1085, 0.0874],
        [0.0983, 0.1070, 0.1275, 0.1220, 0.1332, 0.1325, 0.1323, 0.1472],
        [0.1054, 0.1151, 0.1403, 0.1266, 0.1135, 0.1274, 0.1195, 0.1522],
        [0.0969, 0.1074, 0.1270, 0.1298, 0.1261, 0.1191, 0.1300, 0.1637]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1463, 0.1599, 0.1906, 0.1150, 0.1048, 0.0955, 0.0975, 0.0904],
        [0.1036, 0.1147, 0.1366, 0.1207, 0.1266, 0.1304, 0.1288, 0.1386],
        [0.1099, 0.1205, 0.1453, 0.1295, 0.1241, 0.1114, 0.1214, 0.1378],
        [0.1022, 0.1142, 0.1349, 0.1319, 0.1258, 0.1211, 0.1173, 0.1526],
        [0.1054, 0.1214, 0.1388, 0.1195, 0.1129, 0.1192, 0.1265, 0.1562]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0927, 0.0942, 0.1182, 0.1417, 0.1329, 0.1468, 0.1339, 0.1397],
        [0.0884, 0.0897, 0.0995, 0.1219, 0.1516, 0.1410, 0.1422, 0.1657]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0927, 0.0952, 0.1168, 0.1321, 0.1356, 0.1425, 0.1350, 0.1501],
        [0.0915, 0.0945, 0.1036, 0.1233, 0.1119, 0.1333, 0.1618, 0.1802],
        [0.0707, 0.0651, 0.0942, 0.1304, 0.1112, 0.1389, 0.1507, 0.2388]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0965, 0.0975, 0.1188, 0.1164, 0.1312, 0.1251, 0.1401, 0.1744],
        [0.0874, 0.0890, 0.0972, 0.1303, 0.1282, 0.1387, 0.1325, 0.1968],
        [0.0703, 0.0646, 0.0897, 0.1320, 0.1276, 0.1484, 0.1228, 0.2447],
        [0.0649, 0.0594, 0.0691, 0.1431, 0.1169, 0.1545, 0.1447, 0.2475]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0924, 0.0973, 0.1125, 0.1388, 0.1158, 0.1260, 0.1257, 0.1914],
        [0.0937, 0.0999, 0.1085, 0.1286, 0.1042, 0.1287, 0.1342, 0.2023],
        [0.0698, 0.0648, 0.0882, 0.1238, 0.1289, 0.1291, 0.1456, 0.2498],
        [0.0648, 0.0593, 0.0670, 0.1304, 0.1064, 0.1552, 0.1570, 0.2599],
        [0.0637, 0.0571, 0.0602, 0.1385, 0.1388, 0.1407, 0.1420, 0.2591]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1463, 0.1097, 0.1276, 0.1268, 0.1207, 0.1370, 0.1164, 0.1155],
        [0.1323, 0.1095, 0.1303, 0.1274, 0.1240, 0.1272, 0.1185, 0.1307]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1522, 0.1145, 0.1283, 0.1325, 0.1241, 0.1213, 0.1091, 0.1180],
        [0.1310, 0.1091, 0.1259, 0.1423, 0.1409, 0.1188, 0.1117, 0.1203],
        [0.1205, 0.1012, 0.1239, 0.1419, 0.1236, 0.1259, 0.1245, 0.1386]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1519, 0.1129, 0.1253, 0.1250, 0.1322, 0.1269, 0.1118, 0.1140],
        [0.1341, 0.1149, 0.1259, 0.1337, 0.1218, 0.1308, 0.1193, 0.1195],
        [0.1252, 0.1059, 0.1314, 0.1288, 0.1256, 0.1226, 0.1273, 0.1332],
        [0.1166, 0.1056, 0.1218, 0.1400, 0.1292, 0.1301, 0.1220, 0.1345]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1456, 0.1194, 0.1276, 0.1242, 0.1230, 0.1284, 0.1207, 0.1112],
        [0.1331, 0.1144, 0.1215, 0.1423, 0.1162, 0.1408, 0.1143, 0.1172],
        [0.1171, 0.1049, 0.1360, 0.1368, 0.1197, 0.1216, 0.1186, 0.1452],
        [0.1123, 0.1029, 0.1212, 0.1417, 0.1271, 0.1358, 0.1293, 0.1298],
        [0.1092, 0.1043, 0.1223, 0.1403, 0.1348, 0.1181, 0.1161, 0.1549]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1393, 0.1321, 0.1320, 0.1139, 0.1289, 0.1376, 0.1093, 0.1069],
        [0.1346, 0.1277, 0.1164, 0.1124, 0.1318, 0.1267, 0.1204, 0.1301]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1372, 0.1302, 0.1334, 0.1292, 0.1299, 0.1191, 0.1194, 0.1016],
        [0.1438, 0.1368, 0.1173, 0.1241, 0.1300, 0.1085, 0.1144, 0.1252],
        [0.1032, 0.0928, 0.1405, 0.1261, 0.1323, 0.1215, 0.1401, 0.1434]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1430, 0.1407, 0.1084, 0.1199, 0.1293, 0.1308, 0.1172, 0.1106],
        [0.1413, 0.1366, 0.1365, 0.1239, 0.1309, 0.1117, 0.1008, 0.1183],
        [0.1002, 0.0940, 0.1505, 0.1188, 0.1434, 0.1260, 0.1291, 0.1380],
        [0.0951, 0.0889, 0.1396, 0.1344, 0.1224, 0.1282, 0.1408, 0.1506]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1419, 0.1415, 0.1455, 0.1130, 0.1198, 0.1157, 0.1187, 0.1039],
        [0.1319, 0.1316, 0.1305, 0.1322, 0.1156, 0.1186, 0.1232, 0.1164],
        [0.1019, 0.0953, 0.1501, 0.1215, 0.1278, 0.1263, 0.1316, 0.1455],
        [0.0950, 0.0879, 0.1362, 0.1259, 0.1288, 0.1302, 0.1353, 0.1607],
        [0.0874, 0.0793, 0.1169, 0.1306, 0.1323, 0.1452, 0.1324, 0.1760]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:22:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 6.2824 (7.4072)	Hard Loss 1.1628 (1.3711)	Soft Loss 0.0082 (0.0087)	Arch Loss 11.5559 (11.1522)	Arch Hard Loss 2.1393 (2.0645)	Arch Soft Loss 0.0093 (0.0094)	Prec@(1,5) (60.5%, 89.0%)	
07/29 09:23:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 8.3256 (7.5073)	Hard Loss 1.5411 (1.3896)	Soft Loss 0.0091 (0.0086)	Arch Loss 13.8365 (10.9597)	Arch Hard Loss 2.5617 (2.0289)	Arch Soft Loss 0.0087 (0.0093)	Prec@(1,5) (60.2%, 88.4%)	
07/29 09:24:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 8.7195 (7.6518)	Hard Loss 1.6141 (1.4164)	Soft Loss 0.0086 (0.0086)	Arch Loss 10.3080 (10.8858)	Arch Hard Loss 1.9082 (2.0152)	Arch Soft Loss 0.0092 (0.0093)	Prec@(1,5) (59.7%, 87.5%)	
07/29 09:25:22午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 8.0204 (7.7869)	Hard Loss 1.4846 (1.4414)	Soft Loss 0.0082 (0.0087)	Arch Loss 11.3364 (10.8766)	Arch Hard Loss 2.0986 (2.0135)	Arch Soft Loss 0.0094 (0.0093)	Prec@(1,5) (59.1%, 87.1%)	
07/29 09:26:18午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 8.2332 (7.9192)	Hard Loss 1.5240 (1.4659)	Soft Loss 0.0093 (0.0087)	Arch Loss 10.7260 (10.8875)	Arch Hard Loss 1.9856 (2.0155)	Arch Soft Loss 0.0098 (0.0093)	Prec@(1,5) (58.3%, 86.7%)	
07/29 09:27:13午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 8.2235 (7.9376)	Hard Loss 1.5223 (1.4693)	Soft Loss 0.0079 (0.0087)	Arch Loss 11.7073 (10.8778)	Arch Hard Loss 2.1674 (2.0137)	Arch Soft Loss 0.0089 (0.0093)	Prec@(1,5) (58.4%, 86.7%)	
07/29 09:28:08午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 8.3607 (7.9456)	Hard Loss 1.5476 (1.4708)	Soft Loss 0.0091 (0.0087)	Arch Loss 11.8748 (10.8840)	Arch Hard Loss 2.1983 (2.0149)	Arch Soft Loss 0.0100 (0.0093)	Prec@(1,5) (58.3%, 86.8%)	
07/29 09:28:52午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 8.7259 (7.9767)	Hard Loss 1.6153 (1.4765)	Soft Loss 0.0084 (0.0087)	Arch Loss 10.6237 (10.8595)	Arch Hard Loss 1.9667 (2.0103)	Arch Soft Loss 0.0090 (0.0093)	Prec@(1,5) (58.0%, 86.8%)	
07/29 09:28:54午後 searchCell_trainer.py:234 [INFO] Train: [ 11/49] Final Prec@1 58.0000%
07/29 09:29:01午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 2.0097	Prec@(1,5) (47.3%, 78.8%)
07/29 09:29:07午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 1.9745	Prec@(1,5) (48.2%, 79.1%)
07/29 09:29:13午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 1.9875	Prec@(1,5) (47.6%, 78.7%)
07/29 09:29:19午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 1.9924	Prec@(1,5) (47.4%, 78.7%)
07/29 09:29:26午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 1.9944	Prec@(1,5) (47.3%, 78.6%)
07/29 09:29:32午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 1.9891	Prec@(1,5) (47.5%, 78.4%)
07/29 09:29:38午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 1.9850	Prec@(1,5) (47.5%, 78.4%)
07/29 09:29:43午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 1.9881	Prec@(1,5) (47.4%, 78.3%)
07/29 09:29:44午後 searchCell_trainer.py:269 [INFO] Valid: [ 11/49] Final Prec@1 47.4400%
07/29 09:29:44午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('dil_conv_5x5', 3)], [('skip_connect', 2), ('dil_conv_3x3', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/29 09:29:44午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 47.4400%
####### ALPHA #######
# Alpha - normal
tensor([[0.1109, 0.1045, 0.1298, 0.1552, 0.1162, 0.1295, 0.1230, 0.1309],
        [0.1102, 0.0955, 0.1306, 0.1506, 0.1234, 0.1431, 0.1221, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1153, 0.1076, 0.1304, 0.1530, 0.1118, 0.1227, 0.1291, 0.1302],
        [0.1021, 0.0892, 0.1187, 0.1687, 0.1437, 0.1461, 0.1128, 0.1188],
        [0.0953, 0.0876, 0.1236, 0.1403, 0.1484, 0.1395, 0.1362, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1146, 0.1059, 0.1265, 0.1405, 0.1198, 0.1293, 0.1269, 0.1366],
        [0.1011, 0.0922, 0.1142, 0.2021, 0.1265, 0.1118, 0.1307, 0.1214],
        [0.0942, 0.0824, 0.1118, 0.1759, 0.1355, 0.1435, 0.1318, 0.1250],
        [0.0873, 0.0829, 0.1099, 0.1592, 0.1411, 0.1559, 0.1377, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1296, 0.1168, 0.1358, 0.1329, 0.1197, 0.1143, 0.1194, 0.1315],
        [0.1110, 0.0943, 0.1151, 0.2034, 0.1148, 0.1248, 0.1181, 0.1185],
        [0.1038, 0.0885, 0.1174, 0.1475, 0.1410, 0.1449, 0.1319, 0.1250],
        [0.0965, 0.0879, 0.1107, 0.1584, 0.1499, 0.1445, 0.1334, 0.1187],
        [0.0970, 0.0870, 0.1152, 0.1629, 0.1372, 0.1399, 0.1358, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1399, 0.1497, 0.1799, 0.1174, 0.1088, 0.1072, 0.1095, 0.0876],
        [0.1014, 0.1105, 0.1283, 0.1281, 0.1138, 0.1247, 0.1295, 0.1637]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1451, 0.1565, 0.1839, 0.1115, 0.1081, 0.1028, 0.1053, 0.0869],
        [0.1103, 0.1183, 0.1405, 0.1223, 0.1130, 0.1191, 0.1210, 0.1555],
        [0.1119, 0.1181, 0.1426, 0.1206, 0.1182, 0.1126, 0.1364, 0.1397]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1360, 0.1515, 0.1812, 0.1202, 0.1132, 0.1064, 0.1063, 0.0853],
        [0.0963, 0.1059, 0.1279, 0.1218, 0.1345, 0.1317, 0.1330, 0.1489],
        [0.1037, 0.1140, 0.1405, 0.1280, 0.1139, 0.1287, 0.1196, 0.1516],
        [0.0952, 0.1062, 0.1269, 0.1300, 0.1257, 0.1190, 0.1311, 0.1659]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1462, 0.1627, 0.1957, 0.1147, 0.1032, 0.0931, 0.0964, 0.0879],
        [0.1016, 0.1137, 0.1373, 0.1207, 0.1266, 0.1307, 0.1287, 0.1407],
        [0.1084, 0.1197, 0.1471, 0.1312, 0.1212, 0.1105, 0.1218, 0.1402],
        [0.1004, 0.1130, 0.1347, 0.1329, 0.1260, 0.1217, 0.1164, 0.1549],
        [0.1044, 0.1212, 0.1407, 0.1204, 0.1103, 0.1191, 0.1251, 0.1589]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0886, 0.0901, 0.1180, 0.1440, 0.1315, 0.1500, 0.1352, 0.1426],
        [0.0843, 0.0852, 0.0965, 0.1205, 0.1535, 0.1428, 0.1447, 0.1725]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0888, 0.0915, 0.1171, 0.1329, 0.1343, 0.1463, 0.1354, 0.1536],
        [0.0868, 0.0894, 0.0996, 0.1234, 0.1088, 0.1345, 0.1684, 0.1892],
        [0.0646, 0.0595, 0.0909, 0.1277, 0.1066, 0.1389, 0.1492, 0.2626]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0931, 0.0942, 0.1194, 0.1141, 0.1289, 0.1262, 0.1405, 0.1836],
        [0.0822, 0.0836, 0.0926, 0.1324, 0.1275, 0.1393, 0.1335, 0.2089],
        [0.0644, 0.0592, 0.0864, 0.1295, 0.1227, 0.1495, 0.1192, 0.2692],
        [0.0593, 0.0542, 0.0648, 0.1386, 0.1125, 0.1570, 0.1400, 0.2737]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0882, 0.0932, 0.1112, 0.1380, 0.1116, 0.1279, 0.1240, 0.2058],
        [0.0886, 0.0946, 0.1043, 0.1284, 0.1010, 0.1290, 0.1373, 0.2168],
        [0.0638, 0.0592, 0.0847, 0.1206, 0.1232, 0.1269, 0.1436, 0.2780],
        [0.0588, 0.0536, 0.0618, 0.1261, 0.1004, 0.1558, 0.1570, 0.2864],
        [0.0578, 0.0516, 0.0550, 0.1343, 0.1335, 0.1410, 0.1398, 0.2871]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1477, 0.1083, 0.1276, 0.1269, 0.1213, 0.1372, 0.1157, 0.1153],
        [0.1323, 0.1088, 0.1312, 0.1279, 0.1233, 0.1285, 0.1172, 0.1308]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1544, 0.1137, 0.1270, 0.1320, 0.1237, 0.1221, 0.1082, 0.1189],
        [0.1307, 0.1080, 0.1243, 0.1455, 0.1435, 0.1182, 0.1110, 0.1188],
        [0.1202, 0.1003, 0.1232, 0.1423, 0.1231, 0.1269, 0.1250, 0.1390]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1540, 0.1124, 0.1256, 0.1254, 0.1317, 0.1277, 0.1106, 0.1127],
        [0.1335, 0.1142, 0.1260, 0.1363, 0.1215, 0.1320, 0.1190, 0.1174],
        [0.1251, 0.1056, 0.1318, 0.1288, 0.1256, 0.1230, 0.1267, 0.1334],
        [0.1159, 0.1049, 0.1221, 0.1404, 0.1310, 0.1291, 0.1204, 0.1361]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1480, 0.1198, 0.1273, 0.1249, 0.1222, 0.1289, 0.1195, 0.1094],
        [0.1329, 0.1138, 0.1217, 0.1436, 0.1168, 0.1395, 0.1134, 0.1184],
        [0.1167, 0.1041, 0.1377, 0.1359, 0.1177, 0.1215, 0.1178, 0.1485],
        [0.1115, 0.1019, 0.1209, 0.1416, 0.1280, 0.1375, 0.1290, 0.1297],
        [0.1085, 0.1033, 0.1223, 0.1417, 0.1335, 0.1191, 0.1153, 0.1563]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1407, 0.1337, 0.1312, 0.1134, 0.1272, 0.1402, 0.1080, 0.1056],
        [0.1358, 0.1291, 0.1176, 0.1111, 0.1315, 0.1272, 0.1192, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1389, 0.1324, 0.1349, 0.1286, 0.1286, 0.1183, 0.1188, 0.0995],
        [0.1475, 0.1407, 0.1166, 0.1222, 0.1291, 0.1063, 0.1128, 0.1248],
        [0.1002, 0.0910, 0.1443, 0.1261, 0.1312, 0.1202, 0.1419, 0.1450]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1448, 0.1435, 0.1079, 0.1202, 0.1290, 0.1291, 0.1166, 0.1088],
        [0.1438, 0.1395, 0.1390, 0.1235, 0.1300, 0.1097, 0.0985, 0.1159],
        [0.0970, 0.0919, 0.1562, 0.1167, 0.1446, 0.1253, 0.1293, 0.1391],
        [0.0915, 0.0857, 0.1425, 0.1338, 0.1216, 0.1280, 0.1425, 0.1545]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1439, 0.1456, 0.1499, 0.1114, 0.1172, 0.1137, 0.1166, 0.1016],
        [0.1327, 0.1340, 0.1318, 0.1328, 0.1139, 0.1166, 0.1235, 0.1148],
        [0.0978, 0.0936, 0.1569, 0.1197, 0.1243, 0.1257, 0.1321, 0.1498],
        [0.0902, 0.0846, 0.1388, 0.1262, 0.1272, 0.1305, 0.1366, 0.1659],
        [0.0827, 0.0760, 0.1179, 0.1299, 0.1293, 0.1470, 0.1324, 0.1849]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:30:42午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 6.6914 (7.2840)	Hard Loss 1.2385 (1.3483)	Soft Loss 0.0084 (0.0083)	Arch Loss 10.9455 (10.7636)	Arch Hard Loss 2.0263 (1.9926)	Arch Soft Loss 0.0094 (0.0092)	Prec@(1,5) (61.6%, 88.7%)	
07/29 09:31:36午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 7.5280 (7.3396)	Hard Loss 1.3935 (1.3586)	Soft Loss 0.0078 (0.0084)	Arch Loss 9.3728 (10.8027)	Arch Hard Loss 1.7350 (1.9998)	Arch Soft Loss 0.0094 (0.0092)	Prec@(1,5) (61.1%, 88.5%)	
07/29 09:32:31午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 6.8341 (7.3550)	Hard Loss 1.2649 (1.3614)	Soft Loss 0.0086 (0.0084)	Arch Loss 9.4552 (10.8509)	Arch Hard Loss 1.7503 (2.0087)	Arch Soft Loss 0.0090 (0.0092)	Prec@(1,5) (60.8%, 88.4%)	
07/29 09:33:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 8.4033 (7.4794)	Hard Loss 1.5556 (1.3844)	Soft Loss 0.0071 (0.0084)	Arch Loss 10.8895 (10.8105)	Arch Hard Loss 2.0159 (2.0013)	Arch Soft Loss 0.0095 (0.0092)	Prec@(1,5) (60.5%, 88.3%)	
07/29 09:34:21午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 7.3276 (7.5375)	Hard Loss 1.3563 (1.3952)	Soft Loss 0.0087 (0.0084)	Arch Loss 11.2376 (10.7967)	Arch Hard Loss 2.0803 (1.9987)	Arch Soft Loss 0.0100 (0.0092)	Prec@(1,5) (60.3%, 88.0%)	
07/29 09:35:17午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 8.1420 (7.5478)	Hard Loss 1.5071 (1.3971)	Soft Loss 0.0091 (0.0084)	Arch Loss 11.5484 (10.7498)	Arch Hard Loss 2.1378 (1.9900)	Arch Soft Loss 0.0110 (0.0092)	Prec@(1,5) (60.3%, 88.0%)	
07/29 09:36:11午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 8.0747 (7.5665)	Hard Loss 1.4947 (1.4006)	Soft Loss 0.0076 (0.0084)	Arch Loss 10.7428 (10.7347)	Arch Hard Loss 1.9887 (1.9872)	Arch Soft Loss 0.0090 (0.0092)	Prec@(1,5) (60.1%, 87.9%)	
07/29 09:36:56午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 8.4143 (7.6061)	Hard Loss 1.5576 (1.4079)	Soft Loss 0.0087 (0.0085)	Arch Loss 11.4368 (10.7357)	Arch Hard Loss 2.1171 (1.9874)	Arch Soft Loss 0.0110 (0.0092)	Prec@(1,5) (59.9%, 87.8%)	
07/29 09:36:57午後 searchCell_trainer.py:234 [INFO] Train: [ 12/49] Final Prec@1 59.8720%
07/29 09:37:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 2.0656	Prec@(1,5) (46.2%, 77.0%)
07/29 09:37:11午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.0414	Prec@(1,5) (46.7%, 77.5%)
07/29 09:37:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 2.0283	Prec@(1,5) (47.3%, 77.9%)
07/29 09:37:23午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.0421	Prec@(1,5) (47.0%, 77.6%)
07/29 09:37:30午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 2.0423	Prec@(1,5) (47.0%, 77.4%)
07/29 09:37:36午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.0460	Prec@(1,5) (46.9%, 77.4%)
07/29 09:37:42午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 2.0526	Prec@(1,5) (46.8%, 77.4%)
07/29 09:37:47午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.0512	Prec@(1,5) (46.8%, 77.4%)
07/29 09:37:48午後 searchCell_trainer.py:269 [INFO] Valid: [ 12/49] Final Prec@1 46.8240%
07/29 09:37:48午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('dil_conv_3x3', 4)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/29 09:37:48午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 47.4400%
####### ALPHA #######
# Alpha - normal
tensor([[0.1079, 0.1018, 0.1279, 0.1568, 0.1172, 0.1319, 0.1232, 0.1333],
        [0.1092, 0.0943, 0.1315, 0.1541, 0.1227, 0.1436, 0.1215, 0.1231]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1121, 0.1049, 0.1282, 0.1579, 0.1094, 0.1232, 0.1306, 0.1337],
        [0.0998, 0.0867, 0.1167, 0.1763, 0.1454, 0.1464, 0.1125, 0.1162],
        [0.0927, 0.0853, 0.1229, 0.1429, 0.1508, 0.1396, 0.1363, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1120, 0.1034, 0.1245, 0.1415, 0.1195, 0.1302, 0.1295, 0.1395],
        [0.0984, 0.0895, 0.1120, 0.2151, 0.1261, 0.1097, 0.1306, 0.1185],
        [0.0916, 0.0797, 0.1102, 0.1773, 0.1361, 0.1468, 0.1330, 0.1253],
        [0.0840, 0.0796, 0.1072, 0.1653, 0.1436, 0.1573, 0.1378, 0.1253]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1278, 0.1147, 0.1340, 0.1354, 0.1219, 0.1137, 0.1187, 0.1337],
        [0.1102, 0.0923, 0.1134, 0.2131, 0.1139, 0.1253, 0.1170, 0.1149],
        [0.1029, 0.0864, 0.1170, 0.1489, 0.1413, 0.1451, 0.1328, 0.1256],
        [0.0944, 0.0851, 0.1086, 0.1610, 0.1526, 0.1446, 0.1347, 0.1191],
        [0.0964, 0.0855, 0.1153, 0.1657, 0.1369, 0.1394, 0.1357, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1395, 0.1523, 0.1835, 0.1179, 0.1076, 0.1061, 0.1079, 0.0853],
        [0.1008, 0.1105, 0.1289, 0.1282, 0.1110, 0.1236, 0.1296, 0.1675]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1442, 0.1594, 0.1892, 0.1111, 0.1064, 0.1009, 0.1040, 0.0849],
        [0.1096, 0.1187, 0.1417, 0.1218, 0.1117, 0.1195, 0.1198, 0.1572],
        [0.1107, 0.1177, 0.1442, 0.1205, 0.1175, 0.1108, 0.1368, 0.1418]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1352, 0.1544, 0.1862, 0.1192, 0.1122, 0.1052, 0.1048, 0.0827],
        [0.0956, 0.1059, 0.1284, 0.1228, 0.1339, 0.1303, 0.1331, 0.1501],
        [0.1028, 0.1140, 0.1422, 0.1277, 0.1134, 0.1289, 0.1187, 0.1523],
        [0.0945, 0.1060, 0.1281, 0.1296, 0.1241, 0.1173, 0.1303, 0.1701]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1455, 0.1664, 0.2018, 0.1135, 0.1016, 0.0911, 0.0944, 0.0857],
        [0.1009, 0.1145, 0.1386, 0.1193, 0.1268, 0.1298, 0.1283, 0.1418],
        [0.1074, 0.1196, 0.1487, 0.1326, 0.1201, 0.1098, 0.1216, 0.1402],
        [0.0992, 0.1127, 0.1356, 0.1329, 0.1265, 0.1218, 0.1153, 0.1560],
        [0.1030, 0.1210, 0.1419, 0.1198, 0.1100, 0.1176, 0.1235, 0.1630]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0849, 0.0871, 0.1194, 0.1464, 0.1292, 0.1537, 0.1366, 0.1426],
        [0.0792, 0.0798, 0.0920, 0.1189, 0.1542, 0.1454, 0.1479, 0.1826]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0852, 0.0886, 0.1185, 0.1340, 0.1325, 0.1490, 0.1361, 0.1561],
        [0.0815, 0.0839, 0.0951, 0.1221, 0.1062, 0.1354, 0.1752, 0.2006],
        [0.0593, 0.0546, 0.0878, 0.1256, 0.1017, 0.1382, 0.1471, 0.2858]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0899, 0.0917, 0.1214, 0.1124, 0.1246, 0.1259, 0.1411, 0.1931],
        [0.0767, 0.0781, 0.0878, 0.1336, 0.1252, 0.1399, 0.1343, 0.2244],
        [0.0592, 0.0544, 0.0838, 0.1244, 0.1174, 0.1498, 0.1159, 0.2951],
        [0.0542, 0.0494, 0.0607, 0.1338, 0.1069, 0.1580, 0.1371, 0.2999]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0842, 0.0899, 0.1113, 0.1365, 0.1064, 0.1296, 0.1207, 0.2213],
        [0.0828, 0.0887, 0.0994, 0.1279, 0.0974, 0.1286, 0.1407, 0.2345],
        [0.0584, 0.0541, 0.0815, 0.1162, 0.1167, 0.1245, 0.1409, 0.3078],
        [0.0534, 0.0486, 0.0573, 0.1208, 0.0942, 0.1533, 0.1561, 0.3163],
        [0.0524, 0.0467, 0.0504, 0.1298, 0.1278, 0.1384, 0.1359, 0.3185]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1492, 0.1067, 0.1266, 0.1279, 0.1217, 0.1378, 0.1144, 0.1156],
        [0.1325, 0.1075, 0.1313, 0.1299, 0.1227, 0.1308, 0.1148, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1559, 0.1121, 0.1256, 0.1329, 0.1262, 0.1223, 0.1061, 0.1189],
        [0.1302, 0.1065, 0.1247, 0.1488, 0.1460, 0.1174, 0.1095, 0.1168],
        [0.1197, 0.0984, 0.1219, 0.1439, 0.1224, 0.1278, 0.1254, 0.1407]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1556, 0.1108, 0.1246, 0.1263, 0.1321, 0.1285, 0.1105, 0.1116],
        [0.1345, 0.1142, 0.1263, 0.1383, 0.1202, 0.1331, 0.1179, 0.1155],
        [0.1252, 0.1048, 0.1322, 0.1301, 0.1247, 0.1223, 0.1254, 0.1353],
        [0.1161, 0.1039, 0.1216, 0.1417, 0.1323, 0.1276, 0.1199, 0.1368]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1492, 0.1180, 0.1262, 0.1253, 0.1230, 0.1294, 0.1208, 0.1082],
        [0.1332, 0.1134, 0.1217, 0.1439, 0.1164, 0.1403, 0.1128, 0.1183],
        [0.1156, 0.1020, 0.1381, 0.1349, 0.1178, 0.1210, 0.1177, 0.1529],
        [0.1103, 0.1001, 0.1196, 0.1424, 0.1286, 0.1397, 0.1296, 0.1296],
        [0.1077, 0.1015, 0.1210, 0.1423, 0.1329, 0.1208, 0.1144, 0.1593]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1417, 0.1350, 0.1303, 0.1125, 0.1249, 0.1430, 0.1068, 0.1057],
        [0.1388, 0.1312, 0.1168, 0.1095, 0.1317, 0.1274, 0.1193, 0.1253]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1406, 0.1348, 0.1360, 0.1293, 0.1272, 0.1176, 0.1161, 0.0984],
        [0.1516, 0.1438, 0.1163, 0.1216, 0.1283, 0.1051, 0.1112, 0.1222],
        [0.0971, 0.0892, 0.1489, 0.1258, 0.1302, 0.1197, 0.1418, 0.1474]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1476, 0.1472, 0.1070, 0.1203, 0.1286, 0.1285, 0.1133, 0.1075],
        [0.1476, 0.1422, 0.1412, 0.1232, 0.1296, 0.1087, 0.0956, 0.1118],
        [0.0946, 0.0905, 0.1624, 0.1156, 0.1440, 0.1240, 0.1283, 0.1406],
        [0.0880, 0.0832, 0.1462, 0.1325, 0.1196, 0.1267, 0.1427, 0.1610]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1455, 0.1496, 0.1539, 0.1104, 0.1142, 0.1127, 0.1148, 0.0989],
        [0.1349, 0.1368, 0.1326, 0.1328, 0.1118, 0.1160, 0.1224, 0.1127],
        [0.0945, 0.0927, 0.1647, 0.1175, 0.1198, 0.1259, 0.1311, 0.1538],
        [0.0860, 0.0821, 0.1418, 0.1273, 0.1242, 0.1308, 0.1361, 0.1717],
        [0.0783, 0.0731, 0.1192, 0.1295, 0.1276, 0.1463, 0.1310, 0.1951]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:38:44午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 5.6908 (6.6970)	Hard Loss 1.0533 (1.2396)	Soft Loss 0.0080 (0.0082)	Arch Loss 9.6912 (10.8875)	Arch Hard Loss 1.7940 (2.0155)	Arch Soft Loss 0.0095 (0.0091)	Prec@(1,5) (64.9%, 89.9%)	
07/29 09:39:40午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 5.6172 (6.7208)	Hard Loss 1.0396 (1.2440)	Soft Loss 0.0086 (0.0082)	Arch Loss 9.3301 (10.8459)	Arch Hard Loss 1.7272 (2.0078)	Arch Soft Loss 0.0085 (0.0091)	Prec@(1,5) (64.2%, 90.1%)	
07/29 09:40:35午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 7.6927 (6.7893)	Hard Loss 1.4239 (1.2567)	Soft Loss 0.0085 (0.0082)	Arch Loss 12.8921 (10.7283)	Arch Hard Loss 2.3867 (1.9861)	Arch Soft Loss 0.0095 (0.0090)	Prec@(1,5) (63.5%, 89.9%)	
07/29 09:41:30午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 8.0142 (6.9014)	Hard Loss 1.4835 (1.2774)	Soft Loss 0.0088 (0.0082)	Arch Loss 10.8961 (10.6757)	Arch Hard Loss 2.0171 (1.9763)	Arch Soft Loss 0.0098 (0.0090)	Prec@(1,5) (63.1%, 89.7%)	
07/29 09:42:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 7.7432 (6.9518)	Hard Loss 1.4333 (1.2868)	Soft Loss 0.0083 (0.0083)	Arch Loss 12.0875 (10.6217)	Arch Hard Loss 2.2378 (1.9663)	Arch Soft Loss 0.0083 (0.0090)	Prec@(1,5) (62.8%, 89.6%)	
07/29 09:43:21午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 6.9215 (6.9998)	Hard Loss 1.2811 (1.2956)	Soft Loss 0.0082 (0.0083)	Arch Loss 10.8837 (10.6292)	Arch Hard Loss 2.0148 (1.9677)	Arch Soft Loss 0.0092 (0.0090)	Prec@(1,5) (62.6%, 89.3%)	
07/29 09:44:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 7.9968 (7.0867)	Hard Loss 1.4803 (1.3117)	Soft Loss 0.0079 (0.0083)	Arch Loss 11.6754 (10.6360)	Arch Hard Loss 2.1615 (1.9690)	Arch Soft Loss 0.0082 (0.0090)	Prec@(1,5) (62.2%, 89.1%)	
07/29 09:45:00午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 8.7500 (7.1305)	Hard Loss 1.6198 (1.3199)	Soft Loss 0.0083 (0.0083)	Arch Loss 12.6986 (10.5901)	Arch Hard Loss 2.3509 (1.9605)	Arch Soft Loss 0.0090 (0.0090)	Prec@(1,5) (62.0%, 89.1%)	
07/29 09:45:01午後 searchCell_trainer.py:234 [INFO] Train: [ 13/49] Final Prec@1 61.9560%
07/29 09:45:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 1.9483	Prec@(1,5) (49.3%, 79.4%)
07/29 09:45:16午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 1.9895	Prec@(1,5) (47.9%, 78.2%)
07/29 09:45:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 1.9844	Prec@(1,5) (48.4%, 78.6%)
07/29 09:45:29午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 1.9818	Prec@(1,5) (48.2%, 78.7%)
07/29 09:45:36午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 1.9782	Prec@(1,5) (48.2%, 78.8%)
07/29 09:45:42午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 1.9704	Prec@(1,5) (48.4%, 78.8%)
07/29 09:45:49午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 1.9766	Prec@(1,5) (48.2%, 78.7%)
07/29 09:45:54午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 1.9754	Prec@(1,5) (48.3%, 78.8%)
07/29 09:45:54午後 searchCell_trainer.py:269 [INFO] Valid: [ 13/49] Final Prec@1 48.2400%
07/29 09:45:54午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 0)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/29 09:45:55午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 48.2400%
####### ALPHA #######
# Alpha - normal
tensor([[0.1039, 0.0987, 0.1254, 0.1616, 0.1165, 0.1341, 0.1237, 0.1362],
        [0.1084, 0.0935, 0.1329, 0.1572, 0.1228, 0.1432, 0.1204, 0.1215]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1083, 0.1020, 0.1255, 0.1638, 0.1089, 0.1226, 0.1315, 0.1374],
        [0.0984, 0.0854, 0.1165, 0.1829, 0.1456, 0.1466, 0.1101, 0.1146],
        [0.0897, 0.0825, 0.1212, 0.1459, 0.1533, 0.1417, 0.1368, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1078, 0.0999, 0.1210, 0.1430, 0.1201, 0.1327, 0.1324, 0.1431],
        [0.0958, 0.0870, 0.1095, 0.2284, 0.1255, 0.1078, 0.1303, 0.1157],
        [0.0883, 0.0765, 0.1078, 0.1810, 0.1371, 0.1503, 0.1342, 0.1248],
        [0.0809, 0.0767, 0.1052, 0.1692, 0.1459, 0.1611, 0.1371, 0.1239]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1237, 0.1110, 0.1303, 0.1389, 0.1229, 0.1150, 0.1200, 0.1381],
        [0.1093, 0.0906, 0.1124, 0.2226, 0.1126, 0.1251, 0.1164, 0.1111],
        [0.1011, 0.0842, 0.1168, 0.1486, 0.1422, 0.1468, 0.1329, 0.1274],
        [0.0917, 0.0824, 0.1068, 0.1642, 0.1555, 0.1466, 0.1342, 0.1187],
        [0.0950, 0.0836, 0.1150, 0.1702, 0.1381, 0.1409, 0.1350, 0.1224]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1385, 0.1552, 0.1887, 0.1176, 0.1048, 0.1045, 0.1074, 0.0833],
        [0.0988, 0.1100, 0.1292, 0.1292, 0.1102, 0.1224, 0.1297, 0.1707]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1441, 0.1630, 0.1950, 0.1099, 0.1047, 0.0987, 0.1020, 0.0826],
        [0.1075, 0.1184, 0.1425, 0.1231, 0.1104, 0.1196, 0.1197, 0.1588],
        [0.1093, 0.1181, 0.1464, 0.1208, 0.1162, 0.1090, 0.1359, 0.1443]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1348, 0.1574, 0.1917, 0.1168, 0.1116, 0.1038, 0.1032, 0.0806],
        [0.0938, 0.1054, 0.1291, 0.1234, 0.1338, 0.1302, 0.1332, 0.1512],
        [0.1012, 0.1141, 0.1435, 0.1288, 0.1132, 0.1288, 0.1177, 0.1527],
        [0.0930, 0.1057, 0.1290, 0.1285, 0.1223, 0.1173, 0.1294, 0.1748]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1449, 0.1704, 0.2081, 0.1126, 0.1001, 0.0880, 0.0927, 0.0831],
        [0.0996, 0.1148, 0.1402, 0.1185, 0.1268, 0.1286, 0.1284, 0.1431],
        [0.1055, 0.1199, 0.1505, 0.1322, 0.1208, 0.1093, 0.1215, 0.1403],
        [0.0977, 0.1127, 0.1370, 0.1325, 0.1267, 0.1203, 0.1152, 0.1579],
        [0.1012, 0.1217, 0.1438, 0.1196, 0.1094, 0.1171, 0.1205, 0.1667]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0813, 0.0841, 0.1203, 0.1486, 0.1269, 0.1566, 0.1379, 0.1443],
        [0.0745, 0.0750, 0.0879, 0.1166, 0.1573, 0.1477, 0.1496, 0.1913]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0820, 0.0859, 0.1202, 0.1340, 0.1302, 0.1514, 0.1374, 0.1589],
        [0.0770, 0.0790, 0.0914, 0.1210, 0.1024, 0.1355, 0.1821, 0.2115],
        [0.0542, 0.0500, 0.0852, 0.1219, 0.0956, 0.1365, 0.1435, 0.3132]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0864, 0.0888, 0.1229, 0.1102, 0.1206, 0.1249, 0.1420, 0.2042],
        [0.0724, 0.0734, 0.0840, 0.1331, 0.1235, 0.1398, 0.1338, 0.2400],
        [0.0540, 0.0497, 0.0810, 0.1188, 0.1119, 0.1494, 0.1115, 0.3236],
        [0.0493, 0.0449, 0.0569, 0.1280, 0.1013, 0.1566, 0.1324, 0.3307]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0806, 0.0867, 0.1116, 0.1342, 0.1018, 0.1315, 0.1153, 0.2383],
        [0.0775, 0.0830, 0.0946, 0.1270, 0.0937, 0.1271, 0.1431, 0.2541],
        [0.0533, 0.0493, 0.0784, 0.1111, 0.1081, 0.1201, 0.1372, 0.3425],
        [0.0483, 0.0438, 0.0529, 0.1143, 0.0877, 0.1503, 0.1528, 0.3500],
        [0.0473, 0.0420, 0.0460, 0.1242, 0.1209, 0.1356, 0.1306, 0.3534]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1502, 0.1046, 0.1249, 0.1289, 0.1216, 0.1403, 0.1145, 0.1150],
        [0.1322, 0.1066, 0.1307, 0.1293, 0.1237, 0.1330, 0.1134, 0.1311]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1555, 0.1107, 0.1247, 0.1327, 0.1278, 0.1228, 0.1051, 0.1206],
        [0.1299, 0.1062, 0.1250, 0.1503, 0.1494, 0.1163, 0.1082, 0.1147],
        [0.1183, 0.0968, 0.1208, 0.1462, 0.1226, 0.1286, 0.1257, 0.1408]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1563, 0.1093, 0.1246, 0.1273, 0.1324, 0.1283, 0.1102, 0.1115],
        [0.1350, 0.1149, 0.1262, 0.1383, 0.1188, 0.1349, 0.1170, 0.1150],
        [0.1244, 0.1044, 0.1325, 0.1316, 0.1231, 0.1225, 0.1254, 0.1361],
        [0.1162, 0.1038, 0.1214, 0.1419, 0.1333, 0.1273, 0.1193, 0.1368]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1494, 0.1164, 0.1265, 0.1255, 0.1235, 0.1304, 0.1204, 0.1077],
        [0.1329, 0.1131, 0.1217, 0.1444, 0.1168, 0.1428, 0.1104, 0.1179],
        [0.1149, 0.1006, 0.1377, 0.1360, 0.1172, 0.1219, 0.1169, 0.1548],
        [0.1100, 0.0991, 0.1188, 0.1425, 0.1302, 0.1410, 0.1297, 0.1286],
        [0.1077, 0.1010, 0.1204, 0.1426, 0.1313, 0.1209, 0.1137, 0.1624]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1422, 0.1355, 0.1311, 0.1116, 0.1241, 0.1461, 0.1045, 0.1049],
        [0.1404, 0.1330, 0.1166, 0.1093, 0.1313, 0.1275, 0.1187, 0.1231]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1418, 0.1369, 0.1366, 0.1294, 0.1272, 0.1162, 0.1145, 0.0974],
        [0.1558, 0.1491, 0.1143, 0.1201, 0.1280, 0.1031, 0.1087, 0.1208],
        [0.0936, 0.0872, 0.1532, 0.1256, 0.1298, 0.1194, 0.1430, 0.1483]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1488, 0.1498, 0.1071, 0.1213, 0.1279, 0.1275, 0.1113, 0.1061],
        [0.1514, 0.1459, 0.1413, 0.1217, 0.1305, 0.1070, 0.0930, 0.1092],
        [0.0913, 0.0885, 0.1687, 0.1146, 0.1425, 0.1233, 0.1287, 0.1424],
        [0.0843, 0.0797, 0.1498, 0.1314, 0.1183, 0.1264, 0.1429, 0.1672]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1476, 0.1532, 0.1567, 0.1090, 0.1124, 0.1117, 0.1130, 0.0964],
        [0.1370, 0.1405, 0.1323, 0.1329, 0.1089, 0.1139, 0.1233, 0.1112],
        [0.0912, 0.0913, 0.1716, 0.1150, 0.1180, 0.1243, 0.1331, 0.1556],
        [0.0825, 0.0790, 0.1453, 0.1264, 0.1228, 0.1297, 0.1375, 0.1767],
        [0.0743, 0.0700, 0.1204, 0.1297, 0.1248, 0.1456, 0.1302, 0.2048]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:46:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 7.0139 (6.2205)	Hard Loss 1.2981 (1.1514)	Soft Loss 0.0099 (0.0078)	Arch Loss 10.4066 (10.4783)	Arch Hard Loss 1.9265 (1.9397)	Arch Soft Loss 0.0083 (0.0091)	Prec@(1,5) (66.5%, 91.8%)	
07/29 09:47:47午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 5.0755 (6.3316)	Hard Loss 0.9394 (1.1719)	Soft Loss 0.0068 (0.0078)	Arch Loss 9.5621 (10.5831)	Arch Hard Loss 1.7701 (1.9592)	Arch Soft Loss 0.0088 (0.0091)	Prec@(1,5) (65.9%, 91.6%)	
07/29 09:48:43午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 6.6467 (6.4580)	Hard Loss 1.2302 (1.1953)	Soft Loss 0.0088 (0.0079)	Arch Loss 11.7602 (10.4949)	Arch Hard Loss 2.1772 (1.9428)	Arch Soft Loss 0.0084 (0.0090)	Prec@(1,5) (65.6%, 91.0%)	
07/29 09:49:38午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 6.4399 (6.5120)	Hard Loss 1.1920 (1.2053)	Soft Loss 0.0078 (0.0079)	Arch Loss 10.4445 (10.4368)	Arch Hard Loss 1.9335 (1.9321)	Arch Soft Loss 0.0091 (0.0090)	Prec@(1,5) (65.1%, 90.9%)	
07/29 09:50:34午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 6.5083 (6.5615)	Hard Loss 1.2047 (1.2145)	Soft Loss 0.0077 (0.0080)	Arch Loss 10.4560 (10.4476)	Arch Hard Loss 1.9356 (1.9341)	Arch Soft Loss 0.0088 (0.0089)	Prec@(1,5) (64.8%, 90.8%)	
07/29 09:51:29午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 6.0821 (6.6425)	Hard Loss 1.1257 (1.2295)	Soft Loss 0.0082 (0.0080)	Arch Loss 10.8056 (10.4778)	Arch Hard Loss 2.0005 (1.9397)	Arch Soft Loss 0.0078 (0.0089)	Prec@(1,5) (64.5%, 90.4%)	
07/29 09:52:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 9.8349 (6.7006)	Hard Loss 1.8207 (1.2403)	Soft Loss 0.0081 (0.0080)	Arch Loss 10.8868 (10.4663)	Arch Hard Loss 2.0154 (1.9375)	Arch Soft Loss 0.0096 (0.0089)	Prec@(1,5) (64.1%, 90.3%)	
07/29 09:53:09午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 6.7554 (6.7440)	Hard Loss 1.2504 (1.2483)	Soft Loss 0.0083 (0.0080)	Arch Loss 12.4465 (10.4316)	Arch Hard Loss 2.3042 (1.9311)	Arch Soft Loss 0.0096 (0.0089)	Prec@(1,5) (63.9%, 90.2%)	
07/29 09:53:10午後 searchCell_trainer.py:234 [INFO] Train: [ 14/49] Final Prec@1 63.9080%
07/29 09:53:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 2.0055	Prec@(1,5) (49.5%, 78.1%)
07/29 09:53:24午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 1.9423	Prec@(1,5) (49.9%, 79.2%)
07/29 09:53:30午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 1.9666	Prec@(1,5) (49.3%, 79.1%)
07/29 09:53:37午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 1.9693	Prec@(1,5) (49.4%, 78.9%)
07/29 09:53:43午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 1.9614	Prec@(1,5) (49.8%, 79.2%)
07/29 09:53:50午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 1.9667	Prec@(1,5) (49.7%, 79.1%)
07/29 09:53:56午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 1.9712	Prec@(1,5) (49.6%, 79.0%)
07/29 09:54:02午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 1.9800	Prec@(1,5) (49.4%, 78.9%)
07/29 09:54:02午後 searchCell_trainer.py:269 [INFO] Valid: [ 14/49] Final Prec@1 49.3720%
07/29 09:54:02午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 0)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 09:54:03午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 49.3720%
####### ALPHA #######
# Alpha - normal
tensor([[0.1012, 0.0962, 0.1234, 0.1642, 0.1176, 0.1347, 0.1243, 0.1386],
        [0.1071, 0.0920, 0.1335, 0.1609, 0.1218, 0.1450, 0.1194, 0.1204]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1054, 0.0996, 0.1236, 0.1675, 0.1086, 0.1229, 0.1324, 0.1400],
        [0.0963, 0.0830, 0.1149, 0.1886, 0.1454, 0.1482, 0.1095, 0.1141],
        [0.0877, 0.0802, 0.1205, 0.1484, 0.1552, 0.1419, 0.1373, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1050, 0.0973, 0.1187, 0.1447, 0.1196, 0.1337, 0.1345, 0.1464],
        [0.0934, 0.0843, 0.1070, 0.2419, 0.1250, 0.1059, 0.1302, 0.1124],
        [0.0857, 0.0735, 0.1058, 0.1836, 0.1381, 0.1527, 0.1344, 0.1262],
        [0.0786, 0.0743, 0.1035, 0.1728, 0.1474, 0.1645, 0.1354, 0.1235]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1214, 0.1085, 0.1281, 0.1406, 0.1234, 0.1173, 0.1203, 0.1405],
        [0.1075, 0.0880, 0.1100, 0.2342, 0.1124, 0.1240, 0.1160, 0.1080],
        [0.0996, 0.0815, 0.1157, 0.1490, 0.1422, 0.1484, 0.1343, 0.1291],
        [0.0896, 0.0800, 0.1048, 0.1671, 0.1586, 0.1485, 0.1343, 0.1172],
        [0.0940, 0.0817, 0.1141, 0.1737, 0.1389, 0.1411, 0.1355, 0.1211]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1371, 0.1563, 0.1928, 0.1166, 0.1036, 0.1046, 0.1075, 0.0816],
        [0.0975, 0.1086, 0.1290, 0.1309, 0.1095, 0.1217, 0.1298, 0.1730]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1430, 0.1660, 0.2005, 0.1090, 0.1040, 0.0967, 0.1003, 0.0805],
        [0.1066, 0.1177, 0.1429, 0.1246, 0.1100, 0.1188, 0.1194, 0.1600],
        [0.1083, 0.1180, 0.1489, 0.1184, 0.1168, 0.1079, 0.1347, 0.1470]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1340, 0.1597, 0.1965, 0.1166, 0.1111, 0.1021, 0.1013, 0.0786],
        [0.0929, 0.1045, 0.1293, 0.1239, 0.1350, 0.1299, 0.1326, 0.1520],
        [0.1001, 0.1136, 0.1447, 0.1282, 0.1125, 0.1305, 0.1164, 0.1540],
        [0.0912, 0.1046, 0.1292, 0.1284, 0.1225, 0.1158, 0.1290, 0.1792]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1444, 0.1733, 0.2141, 0.1110, 0.0986, 0.0862, 0.0912, 0.0811],
        [0.0985, 0.1138, 0.1406, 0.1187, 0.1264, 0.1286, 0.1293, 0.1441],
        [0.1045, 0.1194, 0.1529, 0.1338, 0.1192, 0.1081, 0.1200, 0.1420],
        [0.0961, 0.1116, 0.1376, 0.1316, 0.1277, 0.1204, 0.1153, 0.1596],
        [0.1000, 0.1209, 0.1443, 0.1179, 0.1089, 0.1175, 0.1203, 0.1702]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0781, 0.0807, 0.1216, 0.1511, 0.1251, 0.1592, 0.1387, 0.1455],
        [0.0700, 0.0700, 0.0836, 0.1148, 0.1592, 0.1498, 0.1516, 0.2010]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0793, 0.0833, 0.1225, 0.1340, 0.1277, 0.1534, 0.1375, 0.1623],
        [0.0722, 0.0739, 0.0871, 0.1194, 0.1004, 0.1360, 0.1863, 0.2247],
        [0.0495, 0.0457, 0.0826, 0.1176, 0.0898, 0.1331, 0.1396, 0.3422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0843, 0.0864, 0.1258, 0.1078, 0.1156, 0.1251, 0.1404, 0.2147],
        [0.0674, 0.0680, 0.0792, 0.1336, 0.1205, 0.1397, 0.1335, 0.2580],
        [0.0494, 0.0454, 0.0782, 0.1136, 0.1061, 0.1464, 0.1059, 0.3550],
        [0.0448, 0.0407, 0.0529, 0.1216, 0.0955, 0.1565, 0.1256, 0.3624]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0774, 0.0835, 0.1122, 0.1324, 0.0964, 0.1316, 0.1104, 0.2561],
        [0.0723, 0.0772, 0.0898, 0.1253, 0.0892, 0.1259, 0.1446, 0.2757],
        [0.0486, 0.0449, 0.0751, 0.1052, 0.1001, 0.1162, 0.1323, 0.3775],
        [0.0438, 0.0395, 0.0488, 0.1090, 0.0808, 0.1442, 0.1466, 0.3874],
        [0.0428, 0.0380, 0.0422, 0.1185, 0.1140, 0.1290, 0.1241, 0.3914]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1520, 0.1028, 0.1249, 0.1301, 0.1220, 0.1401, 0.1142, 0.1139],
        [0.1317, 0.1053, 0.1312, 0.1281, 0.1250, 0.1342, 0.1123, 0.1322]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1567, 0.1090, 0.1236, 0.1344, 0.1291, 0.1225, 0.1041, 0.1206],
        [0.1301, 0.1053, 0.1254, 0.1522, 0.1507, 0.1154, 0.1067, 0.1142],
        [0.1171, 0.0958, 0.1201, 0.1471, 0.1228, 0.1290, 0.1255, 0.1426]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1583, 0.1077, 0.1236, 0.1271, 0.1343, 0.1286, 0.1096, 0.1108],
        [0.1356, 0.1141, 0.1261, 0.1397, 0.1178, 0.1371, 0.1160, 0.1136],
        [0.1239, 0.1041, 0.1320, 0.1325, 0.1236, 0.1227, 0.1263, 0.1350],
        [0.1151, 0.1020, 0.1199, 0.1441, 0.1352, 0.1274, 0.1188, 0.1375]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1521, 0.1167, 0.1252, 0.1240, 0.1218, 0.1319, 0.1200, 0.1083],
        [0.1331, 0.1124, 0.1205, 0.1445, 0.1168, 0.1435, 0.1100, 0.1191],
        [0.1140, 0.0999, 0.1381, 0.1352, 0.1171, 0.1224, 0.1175, 0.1560],
        [0.1089, 0.0973, 0.1177, 0.1426, 0.1302, 0.1432, 0.1315, 0.1285],
        [0.1074, 0.1012, 0.1211, 0.1419, 0.1322, 0.1208, 0.1129, 0.1625]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1427, 0.1355, 0.1315, 0.1115, 0.1229, 0.1482, 0.1033, 0.1043],
        [0.1413, 0.1342, 0.1158, 0.1089, 0.1308, 0.1288, 0.1188, 0.1213]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1430, 0.1381, 0.1372, 0.1297, 0.1264, 0.1159, 0.1140, 0.0956],
        [0.1600, 0.1533, 0.1120, 0.1198, 0.1284, 0.1008, 0.1074, 0.1185],
        [0.0912, 0.0851, 0.1575, 0.1260, 0.1288, 0.1189, 0.1425, 0.1500]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1516, 0.1529, 0.1063, 0.1201, 0.1281, 0.1265, 0.1095, 0.1050],
        [0.1532, 0.1491, 0.1437, 0.1202, 0.1308, 0.1041, 0.0918, 0.1071],
        [0.0882, 0.0865, 0.1749, 0.1124, 0.1409, 0.1233, 0.1288, 0.1450],
        [0.0813, 0.0770, 0.1537, 0.1291, 0.1177, 0.1257, 0.1436, 0.1719]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1504, 0.1578, 0.1598, 0.1070, 0.1104, 0.1091, 0.1112, 0.0943],
        [0.1390, 0.1438, 0.1316, 0.1318, 0.1075, 0.1133, 0.1239, 0.1092],
        [0.0883, 0.0895, 0.1793, 0.1116, 0.1161, 0.1237, 0.1324, 0.1592],
        [0.0799, 0.0764, 0.1492, 0.1252, 0.1215, 0.1295, 0.1379, 0.1803],
        [0.0705, 0.0672, 0.1223, 0.1299, 0.1224, 0.1450, 0.1282, 0.2145]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 09:55:00午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 6.0067 (5.7671)	Hard Loss 1.1117 (1.0674)	Soft Loss 0.0083 (0.0079)	Arch Loss 11.1453 (10.2667)	Arch Hard Loss 2.0633 (1.9006)	Arch Soft Loss 0.0093 (0.0088)	Prec@(1,5) (68.0%, 92.8%)	
07/29 09:55:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 6.7696 (5.8637)	Hard Loss 1.2531 (1.0853)	Soft Loss 0.0076 (0.0077)	Arch Loss 9.7897 (10.4052)	Arch Hard Loss 1.8123 (1.9262)	Arch Soft Loss 0.0087 (0.0088)	Prec@(1,5) (68.2%, 92.2%)	
07/29 09:56:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 5.4897 (6.0102)	Hard Loss 1.0160 (1.1124)	Soft Loss 0.0076 (0.0077)	Arch Loss 10.3666 (10.4139)	Arch Hard Loss 1.9191 (1.9279)	Arch Soft Loss 0.0092 (0.0088)	Prec@(1,5) (67.2%, 92.0%)	
07/29 09:57:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 5.4506 (6.0723)	Hard Loss 1.0088 (1.1239)	Soft Loss 0.0077 (0.0077)	Arch Loss 13.0917 (10.3793)	Arch Hard Loss 2.4238 (1.9214)	Arch Soft Loss 0.0086 (0.0088)	Prec@(1,5) (67.1%, 92.0%)	
07/29 09:58:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 7.5336 (6.1850)	Hard Loss 1.3945 (1.1448)	Soft Loss 0.0084 (0.0078)	Arch Loss 13.1504 (10.4205)	Arch Hard Loss 2.4345 (1.9291)	Arch Soft Loss 0.0098 (0.0088)	Prec@(1,5) (66.7%, 91.7%)	
07/29 09:59:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 5.3655 (6.2447)	Hard Loss 0.9930 (1.1558)	Soft Loss 0.0076 (0.0078)	Arch Loss 8.2077 (10.3984)	Arch Hard Loss 1.5194 (1.9250)	Arch Soft Loss 0.0076 (0.0089)	Prec@(1,5) (66.4%, 91.5%)	
07/29 10:00:31午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 7.0646 (6.2949)	Hard Loss 1.3076 (1.1651)	Soft Loss 0.0082 (0.0079)	Arch Loss 8.1634 (10.3736)	Arch Hard Loss 1.5112 (1.9204)	Arch Soft Loss 0.0078 (0.0088)	Prec@(1,5) (66.2%, 91.4%)	
07/29 10:01:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 6.8193 (6.3263)	Hard Loss 1.2623 (1.1710)	Soft Loss 0.0079 (0.0079)	Arch Loss 13.3639 (10.3685)	Arch Hard Loss 2.4742 (1.9194)	Arch Soft Loss 0.0079 (0.0088)	Prec@(1,5) (65.9%, 91.3%)	
07/29 10:01:17午後 searchCell_trainer.py:234 [INFO] Train: [ 15/49] Final Prec@1 65.9240%
07/29 10:01:24午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 1.9197	Prec@(1,5) (50.7%, 79.9%)
07/29 10:01:31午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 1.8993	Prec@(1,5) (50.7%, 80.5%)
07/29 10:01:37午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 1.8809	Prec@(1,5) (51.1%, 80.2%)
07/29 10:01:44午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 1.8882	Prec@(1,5) (50.8%, 80.1%)
07/29 10:01:50午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 1.8877	Prec@(1,5) (50.7%, 80.2%)
07/29 10:01:57午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 1.8924	Prec@(1,5) (50.6%, 80.2%)
07/29 10:02:03午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 1.8970	Prec@(1,5) (50.5%, 80.1%)
07/29 10:02:08午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 1.8997	Prec@(1,5) (50.4%, 80.1%)
07/29 10:02:09午後 searchCell_trainer.py:269 [INFO] Valid: [ 15/49] Final Prec@1 50.4320%
07/29 10:02:09午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 10:02:09午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 50.4320%
####### ALPHA #######
# Alpha - normal
tensor([[0.0981, 0.0933, 0.1209, 0.1686, 0.1180, 0.1357, 0.1237, 0.1417],
        [0.1058, 0.0909, 0.1339, 0.1651, 0.1208, 0.1454, 0.1189, 0.1192]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1029, 0.0978, 0.1223, 0.1736, 0.1076, 0.1225, 0.1307, 0.1426],
        [0.0944, 0.0816, 0.1138, 0.1964, 0.1475, 0.1460, 0.1074, 0.1130],
        [0.0849, 0.0776, 0.1188, 0.1530, 0.1585, 0.1411, 0.1364, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1033, 0.0959, 0.1176, 0.1465, 0.1190, 0.1339, 0.1346, 0.1491],
        [0.0910, 0.0824, 0.1049, 0.2546, 0.1242, 0.1032, 0.1297, 0.1100],
        [0.0834, 0.0712, 0.1038, 0.1882, 0.1396, 0.1544, 0.1331, 0.1263],
        [0.0762, 0.0720, 0.1016, 0.1783, 0.1469, 0.1663, 0.1354, 0.1232]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1197, 0.1068, 0.1266, 0.1428, 0.1237, 0.1172, 0.1206, 0.1426],
        [0.1064, 0.0865, 0.1088, 0.2441, 0.1110, 0.1231, 0.1149, 0.1053],
        [0.0975, 0.0793, 0.1141, 0.1514, 0.1426, 0.1504, 0.1350, 0.1298],
        [0.0877, 0.0784, 0.1032, 0.1690, 0.1609, 0.1492, 0.1347, 0.1170],
        [0.0928, 0.0803, 0.1135, 0.1770, 0.1412, 0.1399, 0.1363, 0.1192]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1362, 0.1578, 0.1965, 0.1163, 0.1024, 0.1044, 0.1069, 0.0796],
        [0.0950, 0.1066, 0.1280, 0.1330, 0.1087, 0.1224, 0.1302, 0.1761]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1419, 0.1684, 0.2048, 0.1081, 0.1034, 0.0952, 0.0998, 0.0784],
        [0.1049, 0.1171, 0.1433, 0.1246, 0.1095, 0.1201, 0.1188, 0.1617],
        [0.1072, 0.1180, 0.1503, 0.1180, 0.1162, 0.1078, 0.1348, 0.1478]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1333, 0.1616, 0.2017, 0.1160, 0.1107, 0.1003, 0.0997, 0.0766],
        [0.0909, 0.1034, 0.1295, 0.1239, 0.1354, 0.1294, 0.1331, 0.1543],
        [0.0992, 0.1135, 0.1462, 0.1279, 0.1124, 0.1307, 0.1150, 0.1551],
        [0.0900, 0.1046, 0.1302, 0.1279, 0.1218, 0.1139, 0.1286, 0.1830]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1425, 0.1754, 0.2196, 0.1111, 0.0976, 0.0848, 0.0899, 0.0792],
        [0.0966, 0.1131, 0.1412, 0.1189, 0.1257, 0.1295, 0.1289, 0.1460],
        [0.1027, 0.1190, 0.1548, 0.1339, 0.1187, 0.1084, 0.1188, 0.1437],
        [0.0942, 0.1114, 0.1382, 0.1302, 0.1273, 0.1214, 0.1149, 0.1624],
        [0.0972, 0.1195, 0.1431, 0.1176, 0.1092, 0.1180, 0.1218, 0.1736]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0751, 0.0783, 0.1241, 0.1531, 0.1227, 0.1613, 0.1390, 0.1464],
        [0.0649, 0.0649, 0.0788, 0.1136, 0.1616, 0.1508, 0.1545, 0.2109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0767, 0.0810, 0.1261, 0.1327, 0.1257, 0.1562, 0.1373, 0.1644],
        [0.0674, 0.0690, 0.0830, 0.1170, 0.0974, 0.1365, 0.1903, 0.2395],
        [0.0452, 0.0418, 0.0802, 0.1130, 0.0838, 0.1300, 0.1336, 0.3726]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0815, 0.0842, 0.1300, 0.1054, 0.1102, 0.1223, 0.1398, 0.2266],
        [0.0626, 0.0632, 0.0750, 0.1334, 0.1178, 0.1389, 0.1326, 0.2765],
        [0.0450, 0.0415, 0.0759, 0.1061, 0.0996, 0.1425, 0.1018, 0.3875],
        [0.0405, 0.0369, 0.0495, 0.1141, 0.0903, 0.1524, 0.1192, 0.3971]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0741, 0.0805, 0.1136, 0.1285, 0.0916, 0.1297, 0.1056, 0.2764],
        [0.0668, 0.0714, 0.0847, 0.1221, 0.0859, 0.1229, 0.1464, 0.2997],
        [0.0442, 0.0409, 0.0724, 0.0998, 0.0918, 0.1093, 0.1251, 0.4166],
        [0.0395, 0.0357, 0.0453, 0.1019, 0.0741, 0.1353, 0.1397, 0.4286],
        [0.0386, 0.0342, 0.0386, 0.1107, 0.1058, 0.1213, 0.1158, 0.4349]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1535, 0.1012, 0.1230, 0.1312, 0.1235, 0.1426, 0.1116, 0.1135],
        [0.1323, 0.1051, 0.1330, 0.1281, 0.1243, 0.1347, 0.1103, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1573, 0.1081, 0.1230, 0.1364, 0.1289, 0.1222, 0.1028, 0.1213],
        [0.1307, 0.1050, 0.1270, 0.1537, 0.1511, 0.1139, 0.1051, 0.1135],
        [0.1158, 0.0946, 0.1195, 0.1497, 0.1221, 0.1292, 0.1259, 0.1432]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1597, 0.1071, 0.1228, 0.1271, 0.1366, 0.1276, 0.1088, 0.1102],
        [0.1353, 0.1140, 0.1276, 0.1406, 0.1162, 0.1388, 0.1155, 0.1120],
        [0.1225, 0.1037, 0.1318, 0.1332, 0.1241, 0.1227, 0.1268, 0.1351],
        [0.1139, 0.1016, 0.1194, 0.1455, 0.1370, 0.1261, 0.1174, 0.1391]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1537, 0.1173, 0.1245, 0.1245, 0.1215, 0.1322, 0.1190, 0.1074],
        [0.1335, 0.1124, 0.1206, 0.1449, 0.1159, 0.1456, 0.1086, 0.1186],
        [0.1128, 0.0992, 0.1393, 0.1344, 0.1164, 0.1227, 0.1167, 0.1584],
        [0.1079, 0.0971, 0.1183, 0.1432, 0.1306, 0.1431, 0.1308, 0.1291],
        [0.1062, 0.1004, 0.1211, 0.1427, 0.1316, 0.1212, 0.1129, 0.1639]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1445, 0.1373, 0.1313, 0.1104, 0.1215, 0.1494, 0.1023, 0.1032],
        [0.1436, 0.1369, 0.1152, 0.1085, 0.1294, 0.1304, 0.1171, 0.1190]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1453, 0.1419, 0.1364, 0.1287, 0.1246, 0.1157, 0.1131, 0.0942],
        [0.1660, 0.1596, 0.1091, 0.1178, 0.1267, 0.0989, 0.1056, 0.1162],
        [0.0891, 0.0841, 0.1637, 0.1241, 0.1269, 0.1174, 0.1429, 0.1518]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1541, 0.1571, 0.1054, 0.1185, 0.1282, 0.1253, 0.1078, 0.1037],
        [0.1585, 0.1555, 0.1424, 0.1177, 0.1306, 0.1014, 0.0893, 0.1046],
        [0.0857, 0.0856, 0.1835, 0.1097, 0.1384, 0.1220, 0.1278, 0.1473],
        [0.0783, 0.0744, 0.1580, 0.1279, 0.1160, 0.1248, 0.1432, 0.1775]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1535, 0.1631, 0.1631, 0.1050, 0.1075, 0.1075, 0.1092, 0.0911],
        [0.1429, 0.1495, 0.1297, 0.1309, 0.1047, 0.1114, 0.1237, 0.1072],
        [0.0860, 0.0888, 0.1905, 0.1075, 0.1126, 0.1202, 0.1308, 0.1635],
        [0.0769, 0.0739, 0.1539, 0.1229, 0.1191, 0.1298, 0.1369, 0.1866],
        [0.0673, 0.0645, 0.1240, 0.1289, 0.1199, 0.1436, 0.1263, 0.2255]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:03:07午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 5.7620 (5.6237)	Hard Loss 1.0666 (1.0409)	Soft Loss 0.0064 (0.0075)	Arch Loss 11.1480 (10.1112)	Arch Hard Loss 2.0638 (1.8718)	Arch Soft Loss 0.0091 (0.0086)	Prec@(1,5) (69.6%, 93.2%)	
07/29 10:04:02午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 5.5842 (5.5836)	Hard Loss 1.0335 (1.0334)	Soft Loss 0.0077 (0.0075)	Arch Loss 8.8728 (10.3567)	Arch Hard Loss 1.6424 (1.9173)	Arch Soft Loss 0.0097 (0.0087)	Prec@(1,5) (69.7%, 93.3%)	
07/29 10:04:57午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 5.3101 (5.6465)	Hard Loss 0.9828 (1.0451)	Soft Loss 0.0076 (0.0075)	Arch Loss 9.6447 (10.3545)	Arch Hard Loss 1.7854 (1.9169)	Arch Soft Loss 0.0089 (0.0088)	Prec@(1,5) (69.5%, 93.2%)	
07/29 10:05:53午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 7.0407 (5.7216)	Hard Loss 1.3033 (1.0590)	Soft Loss 0.0078 (0.0076)	Arch Loss 11.3763 (10.3640)	Arch Hard Loss 2.1060 (1.9186)	Arch Soft Loss 0.0091 (0.0087)	Prec@(1,5) (68.8%, 93.0%)	
07/29 10:06:48午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 5.9496 (5.7874)	Hard Loss 1.1012 (1.0712)	Soft Loss 0.0074 (0.0076)	Arch Loss 10.3444 (10.3154)	Arch Hard Loss 1.9150 (1.9096)	Arch Soft Loss 0.0086 (0.0088)	Prec@(1,5) (68.4%, 92.8%)	
07/29 10:07:44午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 6.2274 (5.8664)	Hard Loss 1.1527 (1.0858)	Soft Loss 0.0072 (0.0076)	Arch Loss 10.2885 (10.2584)	Arch Hard Loss 1.9046 (1.8991)	Arch Soft Loss 0.0089 (0.0088)	Prec@(1,5) (68.0%, 92.7%)	
07/29 10:08:39午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 5.7562 (5.9104)	Hard Loss 1.0654 (1.0939)	Soft Loss 0.0080 (0.0076)	Arch Loss 7.9440 (10.2364)	Arch Hard Loss 1.4705 (1.8950)	Arch Soft Loss 0.0076 (0.0088)	Prec@(1,5) (67.8%, 92.5%)	
07/29 10:09:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 5.8788 (5.9514)	Hard Loss 1.0882 (1.1016)	Soft Loss 0.0069 (0.0077)	Arch Loss 9.4312 (10.2205)	Arch Hard Loss 1.7458 (1.8920)	Arch Soft Loss 0.0092 (0.0088)	Prec@(1,5) (67.5%, 92.4%)	
07/29 10:09:13午後 searchCell_trainer.py:234 [INFO] Train: [ 16/49] Final Prec@1 67.5560%
07/29 10:09:20午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 1.8204	Prec@(1,5) (53.0%, 81.2%)
07/29 10:09:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 1.8574	Prec@(1,5) (51.6%, 80.9%)
07/29 10:09:33午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 1.8753	Prec@(1,5) (51.3%, 80.4%)
07/29 10:09:39午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 1.8738	Prec@(1,5) (51.4%, 80.5%)
07/29 10:09:46午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 1.8827	Prec@(1,5) (51.1%, 80.6%)
07/29 10:09:52午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 1.8796	Prec@(1,5) (51.2%, 80.6%)
07/29 10:09:59午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 1.8923	Prec@(1,5) (50.9%, 80.4%)
07/29 10:10:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 1.8899	Prec@(1,5) (51.0%, 80.4%)
07/29 10:10:04午後 searchCell_trainer.py:269 [INFO] Valid: [ 16/49] Final Prec@1 51.0200%
07/29 10:10:04午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 10:10:05午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 51.0200%
####### ALPHA #######
# Alpha - normal
tensor([[0.0943, 0.0900, 0.1174, 0.1717, 0.1178, 0.1388, 0.1249, 0.1451],
        [0.1046, 0.0897, 0.1341, 0.1696, 0.1202, 0.1468, 0.1174, 0.1176]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0997, 0.0954, 0.1201, 0.1782, 0.1071, 0.1232, 0.1311, 0.1453],
        [0.0922, 0.0801, 0.1130, 0.2030, 0.1499, 0.1456, 0.1049, 0.1114],
        [0.0823, 0.0755, 0.1179, 0.1554, 0.1609, 0.1415, 0.1370, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1008, 0.0938, 0.1158, 0.1480, 0.1192, 0.1336, 0.1355, 0.1534],
        [0.0887, 0.0808, 0.1035, 0.2650, 0.1230, 0.1020, 0.1292, 0.1078],
        [0.0806, 0.0688, 0.1020, 0.1935, 0.1407, 0.1549, 0.1334, 0.1261],
        [0.0737, 0.0701, 0.1000, 0.1848, 0.1475, 0.1680, 0.1347, 0.1212]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1173, 0.1044, 0.1245, 0.1445, 0.1239, 0.1181, 0.1205, 0.1469],
        [0.1055, 0.0846, 0.1070, 0.2559, 0.1089, 0.1227, 0.1131, 0.1023],
        [0.0960, 0.0772, 0.1130, 0.1521, 0.1425, 0.1527, 0.1366, 0.1299],
        [0.0860, 0.0767, 0.1021, 0.1717, 0.1616, 0.1512, 0.1342, 0.1166],
        [0.0910, 0.0782, 0.1123, 0.1812, 0.1423, 0.1404, 0.1367, 0.1178]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1340, 0.1582, 0.1994, 0.1166, 0.1025, 0.1046, 0.1070, 0.0777],
        [0.0935, 0.1060, 0.1289, 0.1338, 0.1073, 0.1219, 0.1288, 0.1798]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1411, 0.1713, 0.2103, 0.1067, 0.1024, 0.0936, 0.0982, 0.0766],
        [0.1035, 0.1162, 0.1440, 0.1248, 0.1080, 0.1213, 0.1183, 0.1638],
        [0.1063, 0.1183, 0.1525, 0.1166, 0.1161, 0.1069, 0.1342, 0.1492]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1320, 0.1639, 0.2066, 0.1149, 0.1107, 0.0984, 0.0984, 0.0750],
        [0.0893, 0.1019, 0.1290, 0.1250, 0.1368, 0.1296, 0.1327, 0.1558],
        [0.0979, 0.1137, 0.1478, 0.1282, 0.1123, 0.1297, 0.1143, 0.1561],
        [0.0883, 0.1036, 0.1300, 0.1289, 0.1214, 0.1128, 0.1280, 0.1870]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1412, 0.1789, 0.2261, 0.1099, 0.0964, 0.0831, 0.0875, 0.0769],
        [0.0954, 0.1129, 0.1428, 0.1186, 0.1247, 0.1289, 0.1292, 0.1475],
        [0.1019, 0.1193, 0.1567, 0.1345, 0.1175, 0.1083, 0.1182, 0.1435],
        [0.0932, 0.1112, 0.1393, 0.1296, 0.1273, 0.1212, 0.1142, 0.1641],
        [0.0964, 0.1198, 0.1445, 0.1175, 0.1075, 0.1168, 0.1223, 0.1753]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0722, 0.0762, 0.1266, 0.1551, 0.1196, 0.1643, 0.1386, 0.1473],
        [0.0609, 0.0607, 0.0748, 0.1122, 0.1610, 0.1528, 0.1560, 0.2216]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0735, 0.0789, 0.1295, 0.1315, 0.1222, 0.1587, 0.1375, 0.1681],
        [0.0631, 0.0646, 0.0791, 0.1145, 0.0945, 0.1365, 0.1939, 0.2539],
        [0.0412, 0.0381, 0.0771, 0.1085, 0.0784, 0.1259, 0.1278, 0.4030]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0784, 0.0822, 0.1341, 0.1025, 0.1045, 0.1200, 0.1380, 0.2402],
        [0.0585, 0.0592, 0.0714, 0.1319, 0.1158, 0.1360, 0.1307, 0.2966],
        [0.0411, 0.0380, 0.0735, 0.1002, 0.0930, 0.1367, 0.0958, 0.4218],
        [0.0367, 0.0334, 0.0462, 0.1082, 0.0845, 0.1465, 0.1118, 0.4327]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0704, 0.0776, 0.1147, 0.1236, 0.0867, 0.1281, 0.1004, 0.2987],
        [0.0621, 0.0661, 0.0797, 0.1179, 0.0818, 0.1192, 0.1480, 0.3252],
        [0.0400, 0.0371, 0.0693, 0.0925, 0.0839, 0.1022, 0.1171, 0.4579],
        [0.0356, 0.0321, 0.0419, 0.0938, 0.0675, 0.1258, 0.1306, 0.4726],
        [0.0346, 0.0306, 0.0351, 0.1018, 0.0980, 0.1138, 0.1073, 0.4788]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1539, 0.0999, 0.1213, 0.1324, 0.1242, 0.1431, 0.1114, 0.1139],
        [0.1309, 0.1042, 0.1345, 0.1284, 0.1242, 0.1356, 0.1102, 0.1320]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1572, 0.1075, 0.1211, 0.1369, 0.1307, 0.1211, 0.1025, 0.1231],
        [0.1289, 0.1039, 0.1276, 0.1566, 0.1521, 0.1134, 0.1053, 0.1121],
        [0.1140, 0.0927, 0.1175, 0.1518, 0.1234, 0.1311, 0.1256, 0.1439]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1610, 0.1063, 0.1218, 0.1274, 0.1380, 0.1279, 0.1076, 0.1100],
        [0.1346, 0.1137, 0.1273, 0.1423, 0.1160, 0.1404, 0.1145, 0.1111],
        [0.1228, 0.1040, 0.1324, 0.1337, 0.1226, 0.1230, 0.1253, 0.1361],
        [0.1132, 0.1016, 0.1187, 0.1474, 0.1395, 0.1246, 0.1167, 0.1382]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1543, 0.1168, 0.1240, 0.1246, 0.1209, 0.1343, 0.1179, 0.1073],
        [0.1322, 0.1114, 0.1209, 0.1462, 0.1149, 0.1459, 0.1090, 0.1195],
        [0.1113, 0.0980, 0.1382, 0.1361, 0.1179, 0.1219, 0.1168, 0.1598],
        [0.1061, 0.0957, 0.1168, 0.1457, 0.1323, 0.1448, 0.1308, 0.1277],
        [0.1044, 0.0991, 0.1197, 0.1451, 0.1334, 0.1209, 0.1124, 0.1650]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1456, 0.1386, 0.1311, 0.1092, 0.1212, 0.1509, 0.1009, 0.1025],
        [0.1434, 0.1374, 0.1147, 0.1080, 0.1302, 0.1326, 0.1166, 0.1172]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1456, 0.1435, 0.1370, 0.1286, 0.1242, 0.1164, 0.1121, 0.0927],
        [0.1683, 0.1629, 0.1079, 0.1165, 0.1267, 0.0980, 0.1052, 0.1144],
        [0.0859, 0.0821, 0.1680, 0.1238, 0.1261, 0.1165, 0.1442, 0.1535]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1550, 0.1606, 0.1052, 0.1183, 0.1271, 0.1251, 0.1063, 0.1024],
        [0.1606, 0.1588, 0.1429, 0.1158, 0.1310, 0.0998, 0.0881, 0.1030],
        [0.0821, 0.0839, 0.1883, 0.1088, 0.1378, 0.1219, 0.1291, 0.1480],
        [0.0755, 0.0722, 0.1612, 0.1272, 0.1146, 0.1236, 0.1438, 0.1820]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1545, 0.1678, 0.1659, 0.1042, 0.1055, 0.1068, 0.1065, 0.0887],
        [0.1437, 0.1523, 0.1309, 0.1302, 0.1021, 0.1098, 0.1244, 0.1066],
        [0.0825, 0.0872, 0.1978, 0.1056, 0.1106, 0.1192, 0.1303, 0.1669],
        [0.0737, 0.0716, 0.1578, 0.1208, 0.1169, 0.1303, 0.1358, 0.1930],
        [0.0636, 0.0617, 0.1239, 0.1299, 0.1166, 0.1441, 0.1260, 0.2343]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:10:48午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 4.6197 (4.9845)	Hard Loss 0.8549 (0.9225)	Soft Loss 0.0078 (0.0073)	Arch Loss 9.4785 (10.0158)	Arch Hard Loss 1.7545 (1.8541)	Arch Soft Loss 0.0100 (0.0088)	Prec@(1,5) (73.3%, 94.6%)	
07/29 10:11:29午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 5.0563 (5.0306)	Hard Loss 0.9358 (0.9311)	Soft Loss 0.0072 (0.0074)	Arch Loss 9.9700 (10.2941)	Arch Hard Loss 1.8457 (1.9056)	Arch Soft Loss 0.0087 (0.0089)	Prec@(1,5) (73.0%, 94.2%)	
07/29 10:12:10午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 6.1023 (5.1700)	Hard Loss 1.1295 (0.9569)	Soft Loss 0.0075 (0.0074)	Arch Loss 8.7557 (10.3082)	Arch Hard Loss 1.6208 (1.9083)	Arch Soft Loss 0.0083 (0.0088)	Prec@(1,5) (72.1%, 93.9%)	
07/29 10:12:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 5.6059 (5.3140)	Hard Loss 1.0376 (0.9835)	Soft Loss 0.0072 (0.0074)	Arch Loss 11.7239 (10.2545)	Arch Hard Loss 2.1704 (1.8983)	Arch Soft Loss 0.0095 (0.0088)	Prec@(1,5) (71.1%, 93.6%)	
07/29 10:13:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 5.8196 (5.3893)	Hard Loss 1.0771 (0.9975)	Soft Loss 0.0077 (0.0074)	Arch Loss 8.7141 (10.2426)	Arch Hard Loss 1.6132 (1.8961)	Arch Soft Loss 0.0073 (0.0088)	Prec@(1,5) (70.6%, 93.5%)	
07/29 10:14:13午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 5.8351 (5.4296)	Hard Loss 1.0800 (1.0049)	Soft Loss 0.0076 (0.0074)	Arch Loss 9.2453 (10.2447)	Arch Hard Loss 1.7115 (1.8965)	Arch Soft Loss 0.0087 (0.0087)	Prec@(1,5) (70.4%, 93.4%)	
07/29 10:14:54午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 4.0151 (5.4926)	Hard Loss 0.7430 (1.0166)	Soft Loss 0.0072 (0.0074)	Arch Loss 9.7368 (10.2063)	Arch Hard Loss 1.8025 (1.8894)	Arch Soft Loss 0.0088 (0.0087)	Prec@(1,5) (70.2%, 93.3%)	
07/29 10:15:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 6.1118 (5.5423)	Hard Loss 1.1312 (1.0258)	Soft Loss 0.0081 (0.0075)	Arch Loss 10.1675 (10.2016)	Arch Hard Loss 1.8822 (1.8885)	Arch Soft Loss 0.0089 (0.0087)	Prec@(1,5) (69.9%, 93.2%)	
07/29 10:15:28午後 searchCell_trainer.py:234 [INFO] Train: [ 17/49] Final Prec@1 69.9240%
07/29 10:15:35午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 1.9403	Prec@(1,5) (49.6%, 79.4%)
07/29 10:15:42午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 1.9462	Prec@(1,5) (49.0%, 79.6%)
07/29 10:15:48午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 1.9239	Prec@(1,5) (49.7%, 79.6%)
07/29 10:15:54午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 1.9267	Prec@(1,5) (49.9%, 79.8%)
07/29 10:16:01午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 1.9305	Prec@(1,5) (50.0%, 79.6%)
07/29 10:16:07午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 1.9271	Prec@(1,5) (50.0%, 79.7%)
07/29 10:16:13午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 1.9210	Prec@(1,5) (50.0%, 79.8%)
07/29 10:16:18午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 1.9217	Prec@(1,5) (50.1%, 79.9%)
07/29 10:16:18午後 searchCell_trainer.py:269 [INFO] Valid: [ 17/49] Final Prec@1 50.0600%
07/29 10:16:18午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 10:16:19午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 51.0200%
####### ALPHA #######
# Alpha - normal
tensor([[0.0919, 0.0879, 0.1156, 0.1763, 0.1155, 0.1399, 0.1242, 0.1486],
        [0.1036, 0.0888, 0.1350, 0.1730, 0.1180, 0.1471, 0.1178, 0.1167]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0975, 0.0938, 0.1188, 0.1824, 0.1050, 0.1234, 0.1311, 0.1480],
        [0.0904, 0.0782, 0.1113, 0.2100, 0.1521, 0.1452, 0.1036, 0.1091],
        [0.0805, 0.0738, 0.1172, 0.1585, 0.1624, 0.1413, 0.1354, 0.1308]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0979, 0.0916, 0.1135, 0.1501, 0.1184, 0.1338, 0.1376, 0.1571],
        [0.0861, 0.0786, 0.1014, 0.2785, 0.1217, 0.0996, 0.1287, 0.1053],
        [0.0776, 0.0664, 0.0998, 0.1979, 0.1397, 0.1579, 0.1338, 0.1269],
        [0.0710, 0.0678, 0.0978, 0.1886, 0.1490, 0.1712, 0.1353, 0.1194]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1147, 0.1021, 0.1220, 0.1474, 0.1248, 0.1182, 0.1213, 0.1494],
        [0.1042, 0.0827, 0.1050, 0.2671, 0.1067, 0.1215, 0.1125, 0.1005],
        [0.0939, 0.0750, 0.1109, 0.1541, 0.1439, 0.1545, 0.1375, 0.1302],
        [0.0841, 0.0750, 0.1006, 0.1741, 0.1642, 0.1517, 0.1339, 0.1164],
        [0.0892, 0.0763, 0.1106, 0.1860, 0.1436, 0.1403, 0.1375, 0.1165]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1315, 0.1598, 0.2030, 0.1156, 0.1021, 0.1045, 0.1080, 0.0754],
        [0.0916, 0.1053, 0.1287, 0.1347, 0.1062, 0.1209, 0.1282, 0.1844]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1393, 0.1743, 0.2149, 0.1072, 0.1015, 0.0916, 0.0968, 0.0744],
        [0.1019, 0.1158, 0.1441, 0.1253, 0.1064, 0.1220, 0.1172, 0.1673],
        [0.1051, 0.1187, 0.1542, 0.1163, 0.1153, 0.1070, 0.1336, 0.1500]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1308, 0.1671, 0.2116, 0.1142, 0.1106, 0.0961, 0.0965, 0.0730],
        [0.0874, 0.1014, 0.1292, 0.1261, 0.1369, 0.1291, 0.1311, 0.1589],
        [0.0964, 0.1142, 0.1499, 0.1274, 0.1130, 0.1288, 0.1134, 0.1568],
        [0.0865, 0.1031, 0.1307, 0.1296, 0.1204, 0.1115, 0.1269, 0.1914]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1408, 0.1828, 0.2325, 0.1091, 0.0942, 0.0810, 0.0852, 0.0744],
        [0.0943, 0.1135, 0.1439, 0.1176, 0.1246, 0.1284, 0.1288, 0.1489],
        [0.1012, 0.1201, 0.1589, 0.1352, 0.1176, 0.1062, 0.1177, 0.1430],
        [0.0925, 0.1115, 0.1412, 0.1283, 0.1264, 0.1202, 0.1132, 0.1668],
        [0.0950, 0.1202, 0.1459, 0.1180, 0.1058, 0.1161, 0.1211, 0.1779]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0691, 0.0737, 0.1289, 0.1566, 0.1176, 0.1685, 0.1382, 0.1475],
        [0.0566, 0.0564, 0.0707, 0.1095, 0.1598, 0.1538, 0.1583, 0.2348]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0703, 0.0763, 0.1332, 0.1304, 0.1186, 0.1623, 0.1382, 0.1708],
        [0.0582, 0.0598, 0.0748, 0.1124, 0.0903, 0.1363, 0.1965, 0.2716],
        [0.0371, 0.0345, 0.0741, 0.1016, 0.0723, 0.1214, 0.1218, 0.4372]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0755, 0.0800, 0.1389, 0.0987, 0.0992, 0.1166, 0.1360, 0.2552],
        [0.0540, 0.0547, 0.0673, 0.1283, 0.1121, 0.1332, 0.1302, 0.3202],
        [0.0370, 0.0343, 0.0702, 0.0939, 0.0863, 0.1298, 0.0898, 0.4586],
        [0.0330, 0.0301, 0.0429, 0.1010, 0.0782, 0.1397, 0.1037, 0.4716]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0667, 0.0740, 0.1154, 0.1187, 0.0821, 0.1239, 0.0947, 0.3246],
        [0.0568, 0.0605, 0.0744, 0.1130, 0.0769, 0.1143, 0.1484, 0.3558],
        [0.0359, 0.0332, 0.0656, 0.0847, 0.0752, 0.0939, 0.1080, 0.5034],
        [0.0318, 0.0287, 0.0384, 0.0869, 0.0613, 0.1127, 0.1190, 0.5212],
        [0.0307, 0.0271, 0.0316, 0.0927, 0.0897, 0.1031, 0.0978, 0.5273]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1547, 0.0986, 0.1199, 0.1341, 0.1243, 0.1439, 0.1100, 0.1145],
        [0.1311, 0.1046, 0.1344, 0.1275, 0.1250, 0.1372, 0.1087, 0.1316]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1574, 0.1065, 0.1185, 0.1363, 0.1341, 0.1200, 0.1024, 0.1248],
        [0.1294, 0.1041, 0.1265, 0.1596, 0.1542, 0.1124, 0.1034, 0.1104],
        [0.1132, 0.0915, 0.1166, 0.1524, 0.1240, 0.1315, 0.1259, 0.1449]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1618, 0.1049, 0.1211, 0.1267, 0.1402, 0.1278, 0.1072, 0.1102],
        [0.1352, 0.1141, 0.1266, 0.1441, 0.1154, 0.1416, 0.1137, 0.1093],
        [0.1223, 0.1033, 0.1327, 0.1349, 0.1225, 0.1230, 0.1242, 0.1371],
        [0.1126, 0.1006, 0.1174, 0.1495, 0.1399, 0.1244, 0.1177, 0.1377]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1547, 0.1169, 0.1238, 0.1241, 0.1199, 0.1352, 0.1179, 0.1076],
        [0.1323, 0.1114, 0.1198, 0.1474, 0.1153, 0.1472, 0.1076, 0.1191],
        [0.1091, 0.0963, 0.1388, 0.1344, 0.1188, 0.1227, 0.1169, 0.1631],
        [0.1042, 0.0941, 0.1154, 0.1477, 0.1327, 0.1480, 0.1317, 0.1262],
        [0.1037, 0.0977, 0.1190, 0.1453, 0.1356, 0.1205, 0.1116, 0.1665]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1473, 0.1402, 0.1308, 0.1071, 0.1208, 0.1537, 0.0990, 0.1011],
        [0.1441, 0.1384, 0.1147, 0.1068, 0.1305, 0.1351, 0.1150, 0.1154]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1479, 0.1464, 0.1355, 0.1276, 0.1226, 0.1169, 0.1117, 0.0915],
        [0.1711, 0.1665, 0.1066, 0.1161, 0.1258, 0.0974, 0.1041, 0.1122],
        [0.0832, 0.0805, 0.1745, 0.1226, 0.1237, 0.1150, 0.1445, 0.1559]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1592, 0.1652, 0.1043, 0.1181, 0.1251, 0.1239, 0.1040, 0.1002],
        [0.1643, 0.1623, 0.1425, 0.1151, 0.1305, 0.0975, 0.0867, 0.1011],
        [0.0798, 0.0829, 0.1973, 0.1066, 0.1342, 0.1219, 0.1284, 0.1489],
        [0.0732, 0.0700, 0.1665, 0.1258, 0.1124, 0.1219, 0.1436, 0.1866]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1577, 0.1734, 0.1669, 0.1026, 0.1041, 0.1054, 0.1041, 0.0858],
        [0.1449, 0.1551, 0.1311, 0.1305, 0.0995, 0.1086, 0.1244, 0.1058],
        [0.0792, 0.0860, 0.2068, 0.1021, 0.1090, 0.1188, 0.1283, 0.1697],
        [0.0705, 0.0694, 0.1621, 0.1191, 0.1137, 0.1305, 0.1347, 0.1999],
        [0.0599, 0.0590, 0.1248, 0.1278, 0.1145, 0.1430, 0.1245, 0.2465]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:17:02午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 4.8641 (4.8399)	Hard Loss 0.9002 (0.8958)	Soft Loss 0.0074 (0.0072)	Arch Loss 8.9325 (10.3240)	Arch Hard Loss 1.6535 (1.9112)	Arch Soft Loss 0.0086 (0.0087)	Prec@(1,5) (73.2%, 94.9%)	
07/29 10:17:43午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 4.4001 (5.0321)	Hard Loss 0.8143 (0.9313)	Soft Loss 0.0074 (0.0073)	Arch Loss 8.2121 (10.2146)	Arch Hard Loss 1.5201 (1.8910)	Arch Soft Loss 0.0091 (0.0086)	Prec@(1,5) (72.2%, 94.5%)	
07/29 10:18:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 5.4259 (5.0481)	Hard Loss 1.0043 (0.9343)	Soft Loss 0.0070 (0.0073)	Arch Loss 12.0146 (10.2329)	Arch Hard Loss 2.2243 (1.8943)	Arch Soft Loss 0.0088 (0.0086)	Prec@(1,5) (71.9%, 94.6%)	
07/29 10:19:05午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 5.2335 (5.1114)	Hard Loss 0.9687 (0.9460)	Soft Loss 0.0069 (0.0073)	Arch Loss 12.1413 (10.1991)	Arch Hard Loss 2.2478 (1.8881)	Arch Soft Loss 0.0083 (0.0086)	Prec@(1,5) (71.7%, 94.4%)	
07/29 10:19:46午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 4.1787 (5.1559)	Hard Loss 0.7733 (0.9543)	Soft Loss 0.0074 (0.0073)	Arch Loss 8.7009 (10.1627)	Arch Hard Loss 1.6106 (1.8813)	Arch Soft Loss 0.0087 (0.0087)	Prec@(1,5) (71.5%, 94.2%)	
07/29 10:20:27午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 4.9811 (5.1901)	Hard Loss 0.9219 (0.9606)	Soft Loss 0.0070 (0.0073)	Arch Loss 9.9059 (10.1746)	Arch Hard Loss 1.8338 (1.8835)	Arch Soft Loss 0.0085 (0.0087)	Prec@(1,5) (71.4%, 94.1%)	
07/29 10:21:08午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 5.4971 (5.2144)	Hard Loss 1.0174 (0.9651)	Soft Loss 0.0078 (0.0073)	Arch Loss 12.2200 (10.1390)	Arch Hard Loss 2.2623 (1.8769)	Arch Soft Loss 0.0092 (0.0087)	Prec@(1,5) (71.4%, 94.0%)	
07/29 10:21:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 6.6922 (5.2382)	Hard Loss 1.2386 (0.9695)	Soft Loss 0.0093 (0.0074)	Arch Loss 9.9838 (10.1414)	Arch Hard Loss 1.8482 (1.8774)	Arch Soft Loss 0.0085 (0.0087)	Prec@(1,5) (71.4%, 93.9%)	
07/29 10:21:42午後 searchCell_trainer.py:234 [INFO] Train: [ 18/49] Final Prec@1 71.3760%
07/29 10:21:49午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 1.8746	Prec@(1,5) (52.2%, 81.7%)
07/29 10:21:56午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 1.8728	Prec@(1,5) (51.9%, 81.3%)
07/29 10:22:02午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 1.8734	Prec@(1,5) (51.5%, 81.2%)
07/29 10:22:08午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 1.8715	Prec@(1,5) (51.8%, 81.2%)
07/29 10:22:15午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 1.8814	Prec@(1,5) (51.8%, 80.9%)
07/29 10:22:21午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 1.8780	Prec@(1,5) (51.7%, 81.2%)
07/29 10:22:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 1.8821	Prec@(1,5) (51.8%, 80.9%)
07/29 10:22:32午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 1.8855	Prec@(1,5) (51.7%, 80.9%)
07/29 10:22:33午後 searchCell_trainer.py:269 [INFO] Valid: [ 18/49] Final Prec@1 51.6920%
07/29 10:22:33午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 3)]], normal3_concat=range(2, 6))
07/29 10:22:34午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 51.6920%
####### ALPHA #######
# Alpha - normal
tensor([[0.0888, 0.0851, 0.1127, 0.1779, 0.1154, 0.1423, 0.1252, 0.1526],
        [0.1027, 0.0872, 0.1352, 0.1779, 0.1167, 0.1477, 0.1175, 0.1150]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0947, 0.0916, 0.1167, 0.1864, 0.1030, 0.1239, 0.1314, 0.1522],
        [0.0878, 0.0758, 0.1089, 0.2193, 0.1554, 0.1450, 0.1021, 0.1057],
        [0.0780, 0.0714, 0.1159, 0.1611, 0.1638, 0.1427, 0.1357, 0.1313]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0962, 0.0902, 0.1122, 0.1514, 0.1173, 0.1336, 0.1378, 0.1613],
        [0.0840, 0.0767, 0.0997, 0.2919, 0.1194, 0.0979, 0.1274, 0.1031],
        [0.0754, 0.0641, 0.0978, 0.2022, 0.1413, 0.1591, 0.1327, 0.1275],
        [0.0689, 0.0657, 0.0964, 0.1954, 0.1483, 0.1737, 0.1331, 0.1186]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1124, 0.1000, 0.1199, 0.1511, 0.1242, 0.1193, 0.1212, 0.1519],
        [0.1016, 0.0807, 0.1033, 0.2793, 0.1054, 0.1212, 0.1107, 0.0979],
        [0.0919, 0.0728, 0.1096, 0.1536, 0.1460, 0.1564, 0.1383, 0.1314],
        [0.0821, 0.0731, 0.0991, 0.1790, 0.1653, 0.1515, 0.1337, 0.1161],
        [0.0871, 0.0743, 0.1089, 0.1926, 0.1428, 0.1407, 0.1378, 0.1158]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1296, 0.1610, 0.2053, 0.1158, 0.1018, 0.1039, 0.1088, 0.0739],
        [0.0908, 0.1047, 0.1293, 0.1349, 0.1047, 0.1197, 0.1279, 0.1880]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1767, 0.2185, 0.1068, 0.1021, 0.0903, 0.0960, 0.0728],
        [0.1012, 0.1156, 0.1452, 0.1243, 0.1053, 0.1225, 0.1171, 0.1689],
        [0.1035, 0.1187, 0.1556, 0.1151, 0.1144, 0.1062, 0.1348, 0.1518]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1295, 0.1698, 0.2165, 0.1133, 0.1109, 0.0939, 0.0946, 0.0716],
        [0.0867, 0.1012, 0.1303, 0.1270, 0.1364, 0.1280, 0.1302, 0.1601],
        [0.0949, 0.1145, 0.1516, 0.1271, 0.1128, 0.1292, 0.1122, 0.1577],
        [0.0853, 0.1030, 0.1316, 0.1291, 0.1195, 0.1100, 0.1267, 0.1949]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1394, 0.1869, 0.2372, 0.1081, 0.0932, 0.0792, 0.0839, 0.0721],
        [0.0933, 0.1134, 0.1454, 0.1176, 0.1245, 0.1282, 0.1283, 0.1493],
        [0.0998, 0.1209, 0.1616, 0.1363, 0.1162, 0.1046, 0.1158, 0.1447],
        [0.0915, 0.1119, 0.1426, 0.1260, 0.1252, 0.1206, 0.1131, 0.1691],
        [0.0935, 0.1207, 0.1472, 0.1169, 0.1058, 0.1154, 0.1203, 0.1801]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0664, 0.0711, 0.1318, 0.1590, 0.1142, 0.1719, 0.1375, 0.1482],
        [0.0524, 0.0522, 0.0664, 0.1072, 0.1594, 0.1547, 0.1604, 0.2473]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0679, 0.0742, 0.1384, 0.1290, 0.1144, 0.1655, 0.1370, 0.1736],
        [0.0540, 0.0554, 0.0706, 0.1094, 0.0869, 0.1367, 0.1979, 0.2892],
        [0.0336, 0.0313, 0.0714, 0.0955, 0.0663, 0.1165, 0.1143, 0.4712]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0730, 0.0778, 0.1444, 0.0951, 0.0946, 0.1137, 0.1306, 0.2707],
        [0.0498, 0.0504, 0.0632, 0.1253, 0.1077, 0.1288, 0.1281, 0.3467],
        [0.0335, 0.0310, 0.0674, 0.0861, 0.0797, 0.1215, 0.0830, 0.4977],
        [0.0296, 0.0270, 0.0397, 0.0931, 0.0720, 0.1302, 0.0955, 0.5128]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0639, 0.0713, 0.1179, 0.1128, 0.0767, 0.1189, 0.0881, 0.3504],
        [0.0521, 0.0554, 0.0695, 0.1070, 0.0721, 0.1089, 0.1457, 0.3894],
        [0.0322, 0.0298, 0.0622, 0.0777, 0.0680, 0.0848, 0.0973, 0.5481],
        [0.0284, 0.0255, 0.0353, 0.0788, 0.0552, 0.0999, 0.1054, 0.5715],
        [0.0271, 0.0241, 0.0284, 0.0835, 0.0812, 0.0917, 0.0871, 0.5770]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1552, 0.0968, 0.1186, 0.1343, 0.1246, 0.1460, 0.1089, 0.1155],
        [0.1316, 0.1040, 0.1358, 0.1284, 0.1260, 0.1377, 0.1064, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1589, 0.1061, 0.1166, 0.1369, 0.1356, 0.1201, 0.1007, 0.1252],
        [0.1307, 0.1040, 0.1256, 0.1625, 0.1545, 0.1122, 0.1014, 0.1091],
        [0.1130, 0.0910, 0.1160, 0.1550, 0.1241, 0.1317, 0.1243, 0.1450]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1626, 0.1042, 0.1215, 0.1257, 0.1413, 0.1279, 0.1066, 0.1102],
        [0.1360, 0.1146, 0.1286, 0.1455, 0.1134, 0.1420, 0.1126, 0.1072],
        [0.1218, 0.1032, 0.1333, 0.1362, 0.1215, 0.1225, 0.1230, 0.1384],
        [0.1120, 0.1000, 0.1168, 0.1519, 0.1423, 0.1232, 0.1167, 0.1371]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1560, 0.1180, 0.1226, 0.1223, 0.1186, 0.1366, 0.1179, 0.1081],
        [0.1325, 0.1116, 0.1190, 0.1462, 0.1154, 0.1489, 0.1068, 0.1197],
        [0.1075, 0.0953, 0.1392, 0.1342, 0.1190, 0.1231, 0.1152, 0.1664],
        [0.1030, 0.0932, 0.1150, 0.1501, 0.1330, 0.1494, 0.1307, 0.1256],
        [0.1026, 0.0968, 0.1192, 0.1466, 0.1355, 0.1209, 0.1102, 0.1681]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1495, 0.1428, 0.1292, 0.1059, 0.1199, 0.1557, 0.0973, 0.0996],
        [0.1464, 0.1405, 0.1148, 0.1059, 0.1286, 0.1359, 0.1140, 0.1139]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1506, 0.1509, 0.1347, 0.1260, 0.1212, 0.1164, 0.1104, 0.0897],
        [0.1755, 0.1719, 0.1035, 0.1148, 0.1245, 0.0963, 0.1030, 0.1104],
        [0.0805, 0.0791, 0.1825, 0.1201, 0.1202, 0.1142, 0.1442, 0.1591]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1621, 0.1707, 0.1028, 0.1179, 0.1235, 0.1225, 0.1023, 0.0982],
        [0.1686, 0.1670, 0.1408, 0.1132, 0.1293, 0.0963, 0.0853, 0.0994],
        [0.0772, 0.0816, 0.2067, 0.1051, 0.1315, 0.1198, 0.1274, 0.1508],
        [0.0708, 0.0676, 0.1712, 0.1227, 0.1099, 0.1221, 0.1429, 0.1929]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1617, 0.1815, 0.1674, 0.0996, 0.1015, 0.1039, 0.1016, 0.0829],
        [0.1468, 0.1588, 0.1293, 0.1305, 0.0982, 0.1073, 0.1250, 0.1041],
        [0.0762, 0.0846, 0.2169, 0.0997, 0.1079, 0.1171, 0.1264, 0.1712],
        [0.0678, 0.0670, 0.1664, 0.1183, 0.1109, 0.1292, 0.1345, 0.2060],
        [0.0565, 0.0560, 0.1251, 0.1262, 0.1122, 0.1406, 0.1233, 0.2601]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:23:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 3.8194 (4.3478)	Hard Loss 0.7067 (0.8046)	Soft Loss 0.0075 (0.0072)	Arch Loss 10.2937 (9.9349)	Arch Hard Loss 1.9056 (1.8391)	Arch Soft Loss 0.0088 (0.0087)	Prec@(1,5) (76.9%, 96.2%)	
07/29 10:23:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 5.2367 (4.5435)	Hard Loss 0.9692 (0.8409)	Soft Loss 0.0076 (0.0072)	Arch Loss 8.7170 (10.1001)	Arch Hard Loss 1.6137 (1.8698)	Arch Soft Loss 0.0079 (0.0087)	Prec@(1,5) (75.5%, 95.5%)	
07/29 10:24:35午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 4.5525 (4.5761)	Hard Loss 0.8426 (0.8469)	Soft Loss 0.0064 (0.0072)	Arch Loss 10.3193 (10.0217)	Arch Hard Loss 1.9103 (1.8552)	Arch Soft Loss 0.0088 (0.0086)	Prec@(1,5) (75.2%, 95.4%)	
07/29 10:25:15午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 5.1860 (4.6660)	Hard Loss 0.9598 (0.8636)	Soft Loss 0.0070 (0.0072)	Arch Loss 10.1046 (10.1811)	Arch Hard Loss 1.8706 (1.8847)	Arch Soft Loss 0.0085 (0.0087)	Prec@(1,5) (74.6%, 95.2%)	
07/29 10:25:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 5.1507 (4.7071)	Hard Loss 0.9533 (0.8712)	Soft Loss 0.0074 (0.0072)	Arch Loss 9.1270 (10.1246)	Arch Hard Loss 1.6895 (1.8743)	Arch Soft Loss 0.0088 (0.0087)	Prec@(1,5) (74.4%, 95.1%)	
07/29 10:26:34午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 6.2526 (4.8155)	Hard Loss 1.1573 (0.8912)	Soft Loss 0.0078 (0.0072)	Arch Loss 9.4580 (10.1062)	Arch Hard Loss 1.7509 (1.8709)	Arch Soft Loss 0.0079 (0.0087)	Prec@(1,5) (73.7%, 95.0%)	
07/29 10:27:14午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 6.0060 (4.8456)	Hard Loss 1.1116 (0.8968)	Soft Loss 0.0080 (0.0072)	Arch Loss 7.5584 (10.0867)	Arch Hard Loss 1.3991 (1.8673)	Arch Soft Loss 0.0087 (0.0087)	Prec@(1,5) (73.4%, 94.9%)	
07/29 10:27:47午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 4.8317 (4.8621)	Hard Loss 0.8942 (0.8999)	Soft Loss 0.0078 (0.0072)	Arch Loss 9.8481 (10.0874)	Arch Hard Loss 1.8231 (1.8674)	Arch Soft Loss 0.0088 (0.0086)	Prec@(1,5) (73.2%, 94.9%)	
07/29 10:27:48午後 searchCell_trainer.py:234 [INFO] Train: [ 19/49] Final Prec@1 73.2160%
07/29 10:27:56午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 1.8066	Prec@(1,5) (52.9%, 82.1%)
07/29 10:28:02午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 1.8716	Prec@(1,5) (51.9%, 81.1%)
07/29 10:28:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 1.8757	Prec@(1,5) (51.8%, 81.2%)
07/29 10:28:15午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 1.8732	Prec@(1,5) (51.9%, 81.3%)
07/29 10:28:21午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 1.8852	Prec@(1,5) (51.7%, 81.2%)
07/29 10:28:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 1.9060	Prec@(1,5) (51.5%, 81.0%)
07/29 10:28:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 1.9087	Prec@(1,5) (51.5%, 80.9%)
07/29 10:28:39午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 1.9003	Prec@(1,5) (51.6%, 81.0%)
07/29 10:28:39午後 searchCell_trainer.py:269 [INFO] Valid: [ 19/49] Final Prec@1 51.6040%
07/29 10:28:39午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('sep_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 3), ('skip_connect', 0)], [('dil_conv_5x5', 1), ('skip_connect', 0)]], normal3_concat=range(2, 6))
07/29 10:28:39午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 51.6920%
####### ALPHA #######
# Alpha - normal
tensor([[0.0864, 0.0827, 0.1102, 0.1817, 0.1153, 0.1428, 0.1256, 0.1554],
        [0.1023, 0.0861, 0.1353, 0.1819, 0.1153, 0.1477, 0.1169, 0.1144]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0919, 0.0892, 0.1144, 0.1928, 0.1013, 0.1241, 0.1308, 0.1554],
        [0.0862, 0.0743, 0.1072, 0.2267, 0.1566, 0.1445, 0.1004, 0.1041],
        [0.0762, 0.0694, 0.1150, 0.1617, 0.1665, 0.1437, 0.1351, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0941, 0.0884, 0.1104, 0.1535, 0.1170, 0.1350, 0.1374, 0.1642],
        [0.0826, 0.0755, 0.0985, 0.3029, 0.1179, 0.0959, 0.1258, 0.1010],
        [0.0734, 0.0619, 0.0957, 0.2060, 0.1418, 0.1616, 0.1317, 0.1278],
        [0.0666, 0.0635, 0.0941, 0.1999, 0.1498, 0.1765, 0.1319, 0.1177]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1103, 0.0977, 0.1173, 0.1528, 0.1242, 0.1193, 0.1232, 0.1553],
        [0.1005, 0.0785, 0.1011, 0.2901, 0.1042, 0.1196, 0.1103, 0.0958],
        [0.0908, 0.0706, 0.1077, 0.1552, 0.1466, 0.1597, 0.1382, 0.1311],
        [0.0809, 0.0713, 0.0974, 0.1809, 0.1668, 0.1522, 0.1346, 0.1160],
        [0.0862, 0.0726, 0.1070, 0.1969, 0.1441, 0.1421, 0.1365, 0.1145]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1279, 0.1623, 0.2087, 0.1174, 0.1010, 0.1026, 0.1081, 0.0720],
        [0.0889, 0.1031, 0.1281, 0.1356, 0.1039, 0.1201, 0.1285, 0.1917]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1350, 0.1788, 0.2226, 0.1077, 0.1014, 0.0890, 0.0948, 0.0707],
        [0.0992, 0.1146, 0.1452, 0.1246, 0.1050, 0.1224, 0.1167, 0.1723],
        [0.1017, 0.1183, 0.1565, 0.1141, 0.1159, 0.1066, 0.1355, 0.1514]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1278, 0.1720, 0.2212, 0.1128, 0.1108, 0.0925, 0.0931, 0.0697],
        [0.0847, 0.1004, 0.1302, 0.1281, 0.1374, 0.1271, 0.1295, 0.1626],
        [0.0934, 0.1147, 0.1537, 0.1266, 0.1125, 0.1296, 0.1115, 0.1580],
        [0.0835, 0.1030, 0.1325, 0.1285, 0.1190, 0.1086, 0.1265, 0.1984]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1380, 0.1893, 0.2430, 0.1070, 0.0924, 0.0775, 0.0825, 0.0704],
        [0.0921, 0.1138, 0.1470, 0.1175, 0.1242, 0.1270, 0.1271, 0.1514],
        [0.0986, 0.1214, 0.1647, 0.1358, 0.1156, 0.1029, 0.1152, 0.1459],
        [0.0903, 0.1125, 0.1442, 0.1244, 0.1243, 0.1196, 0.1132, 0.1715],
        [0.0923, 0.1215, 0.1485, 0.1162, 0.1051, 0.1137, 0.1194, 0.1831]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0635, 0.0685, 0.1334, 0.1599, 0.1127, 0.1764, 0.1369, 0.1487],
        [0.0488, 0.0484, 0.0625, 0.1048, 0.1585, 0.1552, 0.1612, 0.2605]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0650, 0.0719, 0.1433, 0.1275, 0.1103, 0.1674, 0.1366, 0.1779],
        [0.0499, 0.0510, 0.0664, 0.1062, 0.0824, 0.1354, 0.2009, 0.3079],
        [0.0303, 0.0282, 0.0680, 0.0889, 0.0604, 0.1115, 0.1066, 0.5061]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0702, 0.0759, 0.1511, 0.0913, 0.0897, 0.1086, 0.1260, 0.2872],
        [0.0460, 0.0466, 0.0597, 0.1206, 0.1022, 0.1238, 0.1249, 0.3762],
        [0.0302, 0.0281, 0.0648, 0.0785, 0.0730, 0.1125, 0.0759, 0.5370],
        [0.0264, 0.0242, 0.0366, 0.0862, 0.0662, 0.1184, 0.0861, 0.5558]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0606, 0.0681, 0.1198, 0.1056, 0.0715, 0.1126, 0.0822, 0.3795],
        [0.0479, 0.0506, 0.0649, 0.0996, 0.0666, 0.1028, 0.1414, 0.4263],
        [0.0287, 0.0266, 0.0583, 0.0704, 0.0606, 0.0756, 0.0864, 0.5933],
        [0.0251, 0.0226, 0.0321, 0.0706, 0.0490, 0.0871, 0.0919, 0.6217],
        [0.0238, 0.0211, 0.0253, 0.0747, 0.0735, 0.0810, 0.0770, 0.6235]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1552, 0.0960, 0.1172, 0.1348, 0.1258, 0.1473, 0.1073, 0.1164],
        [0.1310, 0.1032, 0.1365, 0.1297, 0.1261, 0.1387, 0.1056, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1584, 0.1061, 0.1142, 0.1366, 0.1376, 0.1197, 0.1003, 0.1271],
        [0.1293, 0.1032, 0.1246, 0.1660, 0.1594, 0.1114, 0.0993, 0.1068],
        [0.1116, 0.0898, 0.1147, 0.1589, 0.1247, 0.1316, 0.1234, 0.1453]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1616, 0.1031, 0.1215, 0.1258, 0.1443, 0.1271, 0.1064, 0.1101],
        [0.1361, 0.1151, 0.1280, 0.1472, 0.1125, 0.1430, 0.1126, 0.1055],
        [0.1210, 0.1030, 0.1331, 0.1385, 0.1203, 0.1230, 0.1213, 0.1397],
        [0.1116, 0.0995, 0.1158, 0.1545, 0.1440, 0.1231, 0.1144, 0.1372]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1554, 0.1187, 0.1224, 0.1213, 0.1187, 0.1372, 0.1178, 0.1085],
        [0.1321, 0.1119, 0.1180, 0.1484, 0.1156, 0.1497, 0.1050, 0.1194],
        [0.1059, 0.0938, 0.1396, 0.1333, 0.1185, 0.1238, 0.1146, 0.1705],
        [0.1020, 0.0923, 0.1142, 0.1510, 0.1342, 0.1510, 0.1301, 0.1253],
        [0.1020, 0.0958, 0.1185, 0.1471, 0.1369, 0.1211, 0.1101, 0.1685]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1511, 0.1445, 0.1277, 0.1048, 0.1207, 0.1575, 0.0953, 0.0983],
        [0.1476, 0.1428, 0.1130, 0.1060, 0.1288, 0.1377, 0.1122, 0.1120]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1529, 0.1547, 0.1338, 0.1247, 0.1193, 0.1165, 0.1092, 0.0889],
        [0.1799, 0.1777, 0.1012, 0.1133, 0.1236, 0.0948, 0.1016, 0.1079],
        [0.0783, 0.0778, 0.1897, 0.1195, 0.1177, 0.1117, 0.1439, 0.1615]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1648, 0.1760, 0.1020, 0.1172, 0.1217, 0.1216, 0.1000, 0.0966],
        [0.1737, 0.1735, 0.1397, 0.1109, 0.1275, 0.0940, 0.0832, 0.0975],
        [0.0747, 0.0807, 0.2167, 0.1024, 0.1290, 0.1179, 0.1261, 0.1525],
        [0.0681, 0.0654, 0.1758, 0.1194, 0.1074, 0.1227, 0.1424, 0.1988]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1642, 0.1870, 0.1695, 0.0977, 0.1004, 0.1020, 0.0992, 0.0801],
        [0.1497, 0.1641, 0.1280, 0.1287, 0.0961, 0.1062, 0.1252, 0.1020],
        [0.0735, 0.0836, 0.2278, 0.0976, 0.1053, 0.1147, 0.1240, 0.1735],
        [0.0649, 0.0648, 0.1705, 0.1166, 0.1082, 0.1295, 0.1339, 0.2117],
        [0.0531, 0.0534, 0.1254, 0.1244, 0.1103, 0.1378, 0.1214, 0.2742]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:29:22午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 3.7184 (4.2309)	Hard Loss 0.6881 (0.7830)	Soft Loss 0.0070 (0.0068)	Arch Loss 8.9726 (9.8544)	Arch Hard Loss 1.6610 (1.8242)	Arch Soft Loss 0.0084 (0.0087)	Prec@(1,5) (77.4%, 95.8%)	
07/29 10:30:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 2.6432 (4.2372)	Hard Loss 0.4890 (0.7842)	Soft Loss 0.0072 (0.0069)	Arch Loss 7.7468 (10.0069)	Arch Hard Loss 1.4340 (1.8525)	Arch Soft Loss 0.0083 (0.0087)	Prec@(1,5) (77.2%, 96.1%)	
07/29 10:30:44午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 3.9458 (4.3018)	Hard Loss 0.7302 (0.7961)	Soft Loss 0.0067 (0.0069)	Arch Loss 8.5289 (10.0288)	Arch Hard Loss 1.5788 (1.8565)	Arch Soft Loss 0.0079 (0.0087)	Prec@(1,5) (76.6%, 95.9%)	
07/29 10:31:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 5.6278 (4.3817)	Hard Loss 1.0417 (0.8109)	Soft Loss 0.0070 (0.0070)	Arch Loss 10.7764 (10.1309)	Arch Hard Loss 1.9950 (1.8754)	Arch Soft Loss 0.0090 (0.0087)	Prec@(1,5) (76.0%, 95.9%)	
07/29 10:32:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 4.8972 (4.4404)	Hard Loss 0.9064 (0.8218)	Soft Loss 0.0064 (0.0070)	Arch Loss 8.3966 (10.0601)	Arch Hard Loss 1.5543 (1.8623)	Arch Soft Loss 0.0085 (0.0086)	Prec@(1,5) (75.9%, 95.8%)	
07/29 10:32:43午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 4.8915 (4.5014)	Hard Loss 0.9053 (0.8331)	Soft Loss 0.0076 (0.0070)	Arch Loss 10.3297 (10.0917)	Arch Hard Loss 1.9123 (1.8682)	Arch Soft Loss 0.0088 (0.0086)	Prec@(1,5) (75.5%, 95.6%)	
07/29 10:33:23午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 6.1380 (4.5897)	Hard Loss 1.1361 (0.8494)	Soft Loss 0.0075 (0.0070)	Arch Loss 9.8188 (10.0256)	Arch Hard Loss 1.8177 (1.8560)	Arch Soft Loss 0.0085 (0.0086)	Prec@(1,5) (75.1%, 95.4%)	
07/29 10:33:55午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 4.1490 (4.6360)	Hard Loss 0.7678 (0.8580)	Soft Loss 0.0071 (0.0070)	Arch Loss 11.1443 (10.0862)	Arch Hard Loss 2.0631 (1.8672)	Arch Soft Loss 0.0095 (0.0086)	Prec@(1,5) (74.7%, 95.3%)	
07/29 10:33:56午後 searchCell_trainer.py:234 [INFO] Train: [ 20/49] Final Prec@1 74.6920%
07/29 10:34:03午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 1.8055	Prec@(1,5) (52.8%, 82.5%)
07/29 10:34:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 1.8225	Prec@(1,5) (52.6%, 82.4%)
07/29 10:34:16午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 1.8394	Prec@(1,5) (52.5%, 81.8%)
07/29 10:34:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 1.8384	Prec@(1,5) (52.8%, 81.9%)
07/29 10:34:28午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 1.8446	Prec@(1,5) (52.6%, 81.6%)
07/29 10:34:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 1.8355	Prec@(1,5) (52.7%, 81.7%)
07/29 10:34:41午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 1.8336	Prec@(1,5) (52.8%, 81.7%)
07/29 10:34:46午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 1.8373	Prec@(1,5) (52.7%, 81.7%)
07/29 10:34:46午後 searchCell_trainer.py:269 [INFO] Valid: [ 20/49] Final Prec@1 52.7160%
07/29 10:34:46午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('dil_conv_3x3', 3)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('skip_connect', 0), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 1), ('skip_connect', 0)]], normal3_concat=range(2, 6))
07/29 10:34:47午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 52.7160%
####### ALPHA #######
# Alpha - normal
tensor([[0.0844, 0.0808, 0.1084, 0.1845, 0.1150, 0.1428, 0.1253, 0.1587],
        [0.1008, 0.0845, 0.1341, 0.1860, 0.1160, 0.1484, 0.1167, 0.1135]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0895, 0.0872, 0.1124, 0.2000, 0.0999, 0.1243, 0.1295, 0.1571],
        [0.0848, 0.0725, 0.1057, 0.2344, 0.1582, 0.1435, 0.0993, 0.1016],
        [0.0742, 0.0673, 0.1146, 0.1647, 0.1680, 0.1440, 0.1341, 0.1331]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0924, 0.0868, 0.1090, 0.1553, 0.1155, 0.1354, 0.1384, 0.1672],
        [0.0803, 0.0734, 0.0959, 0.3146, 0.1171, 0.0948, 0.1251, 0.0988],
        [0.0711, 0.0597, 0.0936, 0.2101, 0.1426, 0.1642, 0.1308, 0.1279],
        [0.0647, 0.0614, 0.0924, 0.2052, 0.1489, 0.1812, 0.1300, 0.1163]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1080, 0.0957, 0.1152, 0.1559, 0.1236, 0.1193, 0.1245, 0.1577],
        [0.0991, 0.0770, 0.0993, 0.2990, 0.1032, 0.1193, 0.1091, 0.0940],
        [0.0891, 0.0688, 0.1066, 0.1566, 0.1473, 0.1595, 0.1395, 0.1325],
        [0.0794, 0.0697, 0.0965, 0.1821, 0.1681, 0.1527, 0.1348, 0.1167],
        [0.0849, 0.0714, 0.1064, 0.1999, 0.1432, 0.1433, 0.1365, 0.1145]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1262, 0.1639, 0.2107, 0.1186, 0.1008, 0.1016, 0.1079, 0.0704],
        [0.0870, 0.1022, 0.1277, 0.1365, 0.1017, 0.1196, 0.1292, 0.1961]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1338, 0.1817, 0.2247, 0.1077, 0.1014, 0.0880, 0.0940, 0.0687],
        [0.0975, 0.1139, 0.1450, 0.1250, 0.1038, 0.1226, 0.1172, 0.1751],
        [0.1011, 0.1184, 0.1575, 0.1129, 0.1158, 0.1063, 0.1360, 0.1521]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1256, 0.1744, 0.2243, 0.1137, 0.1112, 0.0917, 0.0910, 0.0682],
        [0.0833, 0.1001, 0.1307, 0.1273, 0.1378, 0.1254, 0.1293, 0.1660],
        [0.0921, 0.1149, 0.1547, 0.1256, 0.1129, 0.1300, 0.1101, 0.1596],
        [0.0820, 0.1026, 0.1324, 0.1283, 0.1181, 0.1064, 0.1282, 0.2020]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1366, 0.1925, 0.2465, 0.1071, 0.0916, 0.0762, 0.0809, 0.0686],
        [0.0907, 0.1141, 0.1477, 0.1173, 0.1261, 0.1266, 0.1257, 0.1518],
        [0.0981, 0.1221, 0.1666, 0.1351, 0.1140, 0.1017, 0.1154, 0.1470],
        [0.0896, 0.1124, 0.1444, 0.1237, 0.1244, 0.1192, 0.1140, 0.1725],
        [0.0913, 0.1217, 0.1493, 0.1151, 0.1050, 0.1131, 0.1182, 0.1863]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0608, 0.0660, 0.1344, 0.1591, 0.1099, 0.1824, 0.1375, 0.1500],
        [0.0454, 0.0448, 0.0587, 0.1020, 0.1582, 0.1549, 0.1617, 0.2743]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0627, 0.0700, 0.1484, 0.1258, 0.1061, 0.1702, 0.1360, 0.1809],
        [0.0459, 0.0468, 0.0620, 0.1024, 0.0788, 0.1337, 0.2016, 0.3288],
        [0.0273, 0.0254, 0.0646, 0.0815, 0.0551, 0.1050, 0.0989, 0.5423]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0673, 0.0735, 0.1564, 0.0885, 0.0850, 0.1046, 0.1207, 0.3039],
        [0.0423, 0.0427, 0.0558, 0.1161, 0.0969, 0.1179, 0.1213, 0.4070],
        [0.0270, 0.0252, 0.0615, 0.0712, 0.0668, 0.1024, 0.0684, 0.5775],
        [0.0234, 0.0215, 0.0336, 0.0787, 0.0599, 0.1069, 0.0772, 0.5988]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0572, 0.0648, 0.1209, 0.0991, 0.0664, 0.1053, 0.0766, 0.4097],
        [0.0437, 0.0458, 0.0600, 0.0918, 0.0612, 0.0957, 0.1343, 0.4675],
        [0.0253, 0.0234, 0.0538, 0.0627, 0.0537, 0.0670, 0.0761, 0.6380],
        [0.0220, 0.0198, 0.0288, 0.0628, 0.0430, 0.0753, 0.0800, 0.6683],
        [0.0208, 0.0185, 0.0224, 0.0660, 0.0649, 0.0708, 0.0673, 0.6694]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1558, 0.0951, 0.1161, 0.1351, 0.1265, 0.1482, 0.1063, 0.1168],
        [0.1307, 0.1032, 0.1366, 0.1304, 0.1259, 0.1396, 0.1048, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1583, 0.1056, 0.1122, 0.1377, 0.1398, 0.1198, 0.0991, 0.1274],
        [0.1288, 0.1031, 0.1245, 0.1702, 0.1599, 0.1108, 0.0970, 0.1058],
        [0.1102, 0.0891, 0.1132, 0.1613, 0.1261, 0.1316, 0.1228, 0.1458]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1613, 0.1020, 0.1218, 0.1262, 0.1464, 0.1278, 0.1056, 0.1089],
        [0.1359, 0.1155, 0.1272, 0.1473, 0.1116, 0.1454, 0.1125, 0.1047],
        [0.1205, 0.1027, 0.1326, 0.1403, 0.1196, 0.1240, 0.1192, 0.1412],
        [0.1115, 0.0993, 0.1151, 0.1556, 0.1461, 0.1224, 0.1119, 0.1381]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1551, 0.1183, 0.1227, 0.1220, 0.1186, 0.1383, 0.1174, 0.1075],
        [0.1310, 0.1118, 0.1178, 0.1500, 0.1152, 0.1495, 0.1051, 0.1196],
        [0.1042, 0.0924, 0.1390, 0.1321, 0.1191, 0.1247, 0.1141, 0.1746],
        [0.1013, 0.0914, 0.1137, 0.1519, 0.1354, 0.1521, 0.1286, 0.1256],
        [0.1011, 0.0944, 0.1172, 0.1473, 0.1401, 0.1212, 0.1096, 0.1692]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1530, 0.1473, 0.1280, 0.1032, 0.1202, 0.1572, 0.0937, 0.0973],
        [0.1493, 0.1442, 0.1123, 0.1051, 0.1283, 0.1394, 0.1113, 0.1102]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1539, 0.1576, 0.1323, 0.1237, 0.1193, 0.1161, 0.1089, 0.0882],
        [0.1844, 0.1823, 0.0987, 0.1123, 0.1224, 0.0933, 0.1012, 0.1054],
        [0.0759, 0.0761, 0.1960, 0.1182, 0.1146, 0.1113, 0.1435, 0.1643]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1671, 0.1802, 0.1012, 0.1161, 0.1200, 0.1213, 0.0989, 0.0953],
        [0.1783, 0.1776, 0.1385, 0.1089, 0.1274, 0.0919, 0.0816, 0.0957],
        [0.0723, 0.0788, 0.2230, 0.1010, 0.1273, 0.1178, 0.1262, 0.1537],
        [0.0655, 0.0630, 0.1799, 0.1167, 0.1042, 0.1227, 0.1423, 0.2057]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1667, 0.1932, 0.1697, 0.0964, 0.0985, 0.1001, 0.0979, 0.0775],
        [0.1517, 0.1669, 0.1278, 0.1267, 0.0950, 0.1054, 0.1255, 0.1010],
        [0.0712, 0.0821, 0.2375, 0.0954, 0.1033, 0.1127, 0.1232, 0.1746],
        [0.0624, 0.0627, 0.1741, 0.1147, 0.1053, 0.1287, 0.1343, 0.2179],
        [0.0501, 0.0509, 0.1253, 0.1234, 0.1077, 0.1368, 0.1197, 0.2861]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:35:30午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 2.8400 (3.8235)	Hard Loss 0.5254 (0.7076)	Soft Loss 0.0068 (0.0067)	Arch Loss 11.4722 (9.8508)	Arch Hard Loss 2.1239 (1.8236)	Arch Soft Loss 0.0084 (0.0085)	Prec@(1,5) (79.7%, 96.9%)	
07/29 10:36:10午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 4.2884 (4.0740)	Hard Loss 0.7936 (0.7539)	Soft Loss 0.0072 (0.0067)	Arch Loss 10.2066 (10.0094)	Arch Hard Loss 1.8896 (1.8530)	Arch Soft Loss 0.0075 (0.0085)	Prec@(1,5) (78.1%, 96.4%)	
07/29 10:36:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 3.6293 (4.1347)	Hard Loss 0.6716 (0.7652)	Soft Loss 0.0071 (0.0068)	Arch Loss 10.9663 (10.0589)	Arch Hard Loss 2.0301 (1.8621)	Arch Soft Loss 0.0100 (0.0085)	Prec@(1,5) (77.8%, 96.3%)	
07/29 10:37:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 3.9409 (4.1894)	Hard Loss 0.7293 (0.7753)	Soft Loss 0.0073 (0.0068)	Arch Loss 10.6103 (10.0688)	Arch Hard Loss 1.9642 (1.8640)	Arch Soft Loss 0.0088 (0.0085)	Prec@(1,5) (77.4%, 96.1%)	
07/29 10:38:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 3.4660 (4.1946)	Hard Loss 0.6413 (0.7763)	Soft Loss 0.0069 (0.0068)	Arch Loss 6.6710 (10.0285)	Arch Hard Loss 1.2348 (1.8565)	Arch Soft Loss 0.0082 (0.0085)	Prec@(1,5) (77.5%, 96.1%)	
07/29 10:38:53午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 3.2472 (4.2571)	Hard Loss 0.6008 (0.7878)	Soft Loss 0.0066 (0.0069)	Arch Loss 7.2309 (10.0086)	Arch Hard Loss 1.3385 (1.8528)	Arch Soft Loss 0.0080 (0.0085)	Prec@(1,5) (76.9%, 96.0%)	
07/29 10:39:34午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 5.5339 (4.3030)	Hard Loss 1.0242 (0.7963)	Soft Loss 0.0076 (0.0069)	Arch Loss 11.3927 (9.9863)	Arch Hard Loss 2.1091 (1.8487)	Arch Soft Loss 0.0091 (0.0085)	Prec@(1,5) (76.7%, 95.9%)	
07/29 10:40:06午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 3.5719 (4.3307)	Hard Loss 0.6610 (0.8015)	Soft Loss 0.0068 (0.0069)	Arch Loss 9.8836 (9.9652)	Arch Hard Loss 1.8296 (1.8448)	Arch Soft Loss 0.0092 (0.0085)	Prec@(1,5) (76.5%, 95.8%)	
07/29 10:40:08午後 searchCell_trainer.py:234 [INFO] Train: [ 21/49] Final Prec@1 76.4960%
07/29 10:40:15午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 1.8093	Prec@(1,5) (54.2%, 82.3%)
07/29 10:40:21午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 1.8337	Prec@(1,5) (53.7%, 81.5%)
07/29 10:40:28午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 1.8147	Prec@(1,5) (53.9%, 81.5%)
07/29 10:40:34午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 1.8215	Prec@(1,5) (53.7%, 81.6%)
07/29 10:40:40午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 1.8259	Prec@(1,5) (53.4%, 81.8%)
07/29 10:40:47午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 1.8247	Prec@(1,5) (53.5%, 81.8%)
07/29 10:40:53午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 1.8345	Prec@(1,5) (53.4%, 81.6%)
07/29 10:40:59午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 1.8421	Prec@(1,5) (53.3%, 81.5%)
07/29 10:40:59午後 searchCell_trainer.py:269 [INFO] Valid: [ 21/49] Final Prec@1 53.2800%
07/29 10:40:59午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_3x3', 3)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('skip_connect', 0), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 1), ('skip_connect', 0)]], normal3_concat=range(2, 6))
07/29 10:41:00午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 53.2800%
####### ALPHA #######
# Alpha - normal
tensor([[0.0814, 0.0781, 0.1056, 0.1894, 0.1151, 0.1438, 0.1249, 0.1617],
        [0.0992, 0.0831, 0.1340, 0.1913, 0.1153, 0.1485, 0.1162, 0.1123]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0868, 0.0849, 0.1102, 0.2062, 0.0990, 0.1238, 0.1294, 0.1597],
        [0.0829, 0.0709, 0.1041, 0.2442, 0.1591, 0.1420, 0.0974, 0.0994],
        [0.0724, 0.0656, 0.1143, 0.1673, 0.1699, 0.1444, 0.1330, 0.1331]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0905, 0.0851, 0.1074, 0.1575, 0.1143, 0.1352, 0.1396, 0.1705],
        [0.0783, 0.0720, 0.0944, 0.3245, 0.1160, 0.0933, 0.1240, 0.0976],
        [0.0689, 0.0574, 0.0918, 0.2133, 0.1426, 0.1668, 0.1307, 0.1286],
        [0.0626, 0.0592, 0.0906, 0.2125, 0.1485, 0.1826, 0.1288, 0.1152]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1061, 0.0940, 0.1136, 0.1590, 0.1233, 0.1181, 0.1246, 0.1612],
        [0.0976, 0.0759, 0.0980, 0.3096, 0.1017, 0.1175, 0.1079, 0.0918],
        [0.0873, 0.0671, 0.1059, 0.1562, 0.1479, 0.1598, 0.1417, 0.1340],
        [0.0777, 0.0682, 0.0954, 0.1829, 0.1696, 0.1538, 0.1360, 0.1164],
        [0.0835, 0.0698, 0.1050, 0.2056, 0.1443, 0.1427, 0.1363, 0.1128]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1243, 0.1650, 0.2134, 0.1189, 0.1003, 0.1010, 0.1084, 0.0687],
        [0.0862, 0.1016, 0.1285, 0.1360, 0.1002, 0.1186, 0.1282, 0.2007]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1322, 0.1843, 0.2284, 0.1078, 0.1005, 0.0870, 0.0930, 0.0667],
        [0.0968, 0.1136, 0.1462, 0.1255, 0.1022, 0.1235, 0.1158, 0.1765],
        [0.1003, 0.1183, 0.1587, 0.1116, 0.1154, 0.1067, 0.1362, 0.1527]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1240, 0.1766, 0.2301, 0.1127, 0.1109, 0.0900, 0.0889, 0.0668],
        [0.0821, 0.0994, 0.1318, 0.1289, 0.1376, 0.1250, 0.1274, 0.1678],
        [0.0906, 0.1147, 0.1565, 0.1241, 0.1131, 0.1300, 0.1094, 0.1617],
        [0.0806, 0.1025, 0.1331, 0.1279, 0.1177, 0.1052, 0.1271, 0.2058]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1352, 0.1951, 0.2540, 0.1056, 0.0901, 0.0746, 0.0786, 0.0668],
        [0.0900, 0.1143, 0.1498, 0.1166, 0.1266, 0.1251, 0.1245, 0.1530],
        [0.0972, 0.1224, 0.1693, 0.1352, 0.1123, 0.1006, 0.1145, 0.1485],
        [0.0889, 0.1131, 0.1465, 0.1223, 0.1232, 0.1190, 0.1135, 0.1734],
        [0.0904, 0.1223, 0.1509, 0.1139, 0.1038, 0.1119, 0.1176, 0.1892]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0583, 0.0638, 0.1363, 0.1599, 0.1070, 0.1855, 0.1385, 0.1506],
        [0.0423, 0.0416, 0.0551, 0.0993, 0.1555, 0.1542, 0.1618, 0.2903]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0609, 0.0687, 0.1559, 0.1240, 0.1013, 0.1716, 0.1331, 0.1846],
        [0.0426, 0.0431, 0.0581, 0.0977, 0.0746, 0.1295, 0.2039, 0.3506],
        [0.0246, 0.0229, 0.0612, 0.0747, 0.0505, 0.0985, 0.0912, 0.5764]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0651, 0.0719, 0.1644, 0.0846, 0.0800, 0.0991, 0.1149, 0.3200],
        [0.0390, 0.0393, 0.0525, 0.1111, 0.0911, 0.1101, 0.1158, 0.4411],
        [0.0243, 0.0227, 0.0583, 0.0643, 0.0612, 0.0928, 0.0619, 0.6144],
        [0.0208, 0.0191, 0.0306, 0.0712, 0.0548, 0.0945, 0.0690, 0.6400]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0544, 0.0622, 0.1238, 0.0920, 0.0618, 0.0974, 0.0708, 0.4377],
        [0.0400, 0.0417, 0.0560, 0.0836, 0.0557, 0.0888, 0.1246, 0.5095],
        [0.0223, 0.0207, 0.0494, 0.0562, 0.0472, 0.0588, 0.0666, 0.6787],
        [0.0193, 0.0173, 0.0258, 0.0553, 0.0376, 0.0645, 0.0685, 0.7117],
        [0.0182, 0.0161, 0.0198, 0.0581, 0.0573, 0.0614, 0.0586, 0.7106]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1566, 0.0936, 0.1153, 0.1348, 0.1278, 0.1501, 0.1039, 0.1179],
        [0.1314, 0.1031, 0.1375, 0.1302, 0.1261, 0.1414, 0.1024, 0.1280]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1582, 0.1046, 0.1110, 0.1360, 0.1415, 0.1207, 0.0989, 0.1292],
        [0.1294, 0.1037, 0.1241, 0.1714, 0.1634, 0.1098, 0.0943, 0.1040],
        [0.1102, 0.0884, 0.1122, 0.1630, 0.1259, 0.1314, 0.1214, 0.1475]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1609, 0.1003, 0.1216, 0.1274, 0.1480, 0.1273, 0.1064, 0.1081],
        [0.1370, 0.1163, 0.1276, 0.1473, 0.1105, 0.1472, 0.1115, 0.1026],
        [0.1199, 0.1022, 0.1327, 0.1421, 0.1185, 0.1235, 0.1179, 0.1432],
        [0.1107, 0.0979, 0.1139, 0.1582, 0.1476, 0.1224, 0.1104, 0.1389]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1557, 0.1171, 0.1220, 0.1231, 0.1182, 0.1395, 0.1171, 0.1073],
        [0.1312, 0.1117, 0.1158, 0.1514, 0.1154, 0.1517, 0.1038, 0.1191],
        [0.1037, 0.0914, 0.1392, 0.1308, 0.1194, 0.1249, 0.1122, 0.1784],
        [0.1001, 0.0897, 0.1123, 0.1505, 0.1391, 0.1535, 0.1296, 0.1252],
        [0.1007, 0.0935, 0.1170, 0.1475, 0.1413, 0.1212, 0.1099, 0.1689]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1555, 0.1509, 0.1285, 0.1006, 0.1196, 0.1575, 0.0913, 0.0961],
        [0.1511, 0.1459, 0.1107, 0.1041, 0.1291, 0.1408, 0.1103, 0.1080]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1573, 0.1628, 0.1309, 0.1212, 0.1182, 0.1160, 0.1065, 0.0872],
        [0.1893, 0.1877, 0.0960, 0.1105, 0.1211, 0.0918, 0.1001, 0.1035],
        [0.0743, 0.0753, 0.2046, 0.1165, 0.1114, 0.1091, 0.1418, 0.1670]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1707, 0.1863, 0.1000, 0.1148, 0.1185, 0.1188, 0.0971, 0.0937],
        [0.1838, 0.1832, 0.1362, 0.1072, 0.1267, 0.0895, 0.0800, 0.0936],
        [0.0706, 0.0783, 0.2328, 0.0982, 0.1248, 0.1161, 0.1238, 0.1553],
        [0.0634, 0.0615, 0.1852, 0.1140, 0.1016, 0.1224, 0.1412, 0.2108]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1715, 0.2014, 0.1682, 0.0944, 0.0956, 0.0977, 0.0960, 0.0752],
        [0.1545, 0.1703, 0.1264, 0.1261, 0.0924, 0.1041, 0.1257, 0.1006],
        [0.0694, 0.0816, 0.2485, 0.0925, 0.1004, 0.1107, 0.1215, 0.1755],
        [0.0600, 0.0608, 0.1775, 0.1132, 0.1026, 0.1285, 0.1334, 0.2239],
        [0.0477, 0.0490, 0.1260, 0.1211, 0.1048, 0.1360, 0.1173, 0.2981]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:41:43午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 3.1945 (3.7511)	Hard Loss 0.5911 (0.6942)	Soft Loss 0.0069 (0.0066)	Arch Loss 11.4807 (9.8539)	Arch Hard Loss 2.1254 (1.8242)	Arch Soft Loss 0.0082 (0.0084)	Prec@(1,5) (79.0%, 97.1%)	
07/29 10:42:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 4.8252 (3.8224)	Hard Loss 0.8931 (0.7074)	Soft Loss 0.0064 (0.0066)	Arch Loss 10.6499 (10.0716)	Arch Hard Loss 1.9715 (1.8645)	Arch Soft Loss 0.0095 (0.0086)	Prec@(1,5) (79.3%, 96.8%)	
07/29 10:43:05午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 5.6439 (3.8758)	Hard Loss 1.0446 (0.7173)	Soft Loss 0.0074 (0.0067)	Arch Loss 9.7887 (10.0580)	Arch Hard Loss 1.8121 (1.8620)	Arch Soft Loss 0.0087 (0.0086)	Prec@(1,5) (79.1%, 96.8%)	
07/29 10:43:46午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 4.7801 (3.9406)	Hard Loss 0.8847 (0.7292)	Soft Loss 0.0071 (0.0067)	Arch Loss 9.7368 (9.9241)	Arch Hard Loss 1.8025 (1.8372)	Arch Soft Loss 0.0081 (0.0085)	Prec@(1,5) (78.5%, 96.7%)	
07/29 10:44:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 3.5561 (3.9739)	Hard Loss 0.6580 (0.7354)	Soft Loss 0.0067 (0.0068)	Arch Loss 11.1572 (9.8791)	Arch Hard Loss 2.0655 (1.8288)	Arch Soft Loss 0.0091 (0.0085)	Prec@(1,5) (78.5%, 96.6%)	
07/29 10:45:06午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 3.8982 (4.0317)	Hard Loss 0.7214 (0.7461)	Soft Loss 0.0065 (0.0068)	Arch Loss 10.3268 (9.8760)	Arch Hard Loss 1.9118 (1.8283)	Arch Soft Loss 0.0077 (0.0085)	Prec@(1,5) (78.1%, 96.5%)	
07/29 10:45:46午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 3.3763 (4.0837)	Hard Loss 0.6247 (0.7557)	Soft Loss 0.0068 (0.0068)	Arch Loss 10.6857 (9.8820)	Arch Hard Loss 1.9782 (1.8294)	Arch Soft Loss 0.0091 (0.0085)	Prec@(1,5) (77.9%, 96.4%)	
07/29 10:46:18午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 3.8336 (4.1074)	Hard Loss 0.7094 (0.7601)	Soft Loss 0.0068 (0.0068)	Arch Loss 9.3401 (9.8904)	Arch Hard Loss 1.7291 (1.8309)	Arch Soft Loss 0.0076 (0.0085)	Prec@(1,5) (77.7%, 96.3%)	
07/29 10:46:19午後 searchCell_trainer.py:234 [INFO] Train: [ 22/49] Final Prec@1 77.7240%
07/29 10:46:26午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 1.8025	Prec@(1,5) (53.9%, 81.8%)
07/29 10:46:32午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 1.8157	Prec@(1,5) (53.7%, 81.8%)
07/29 10:46:39午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 1.8171	Prec@(1,5) (53.7%, 82.0%)
07/29 10:46:45午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 1.8221	Prec@(1,5) (53.4%, 81.9%)
07/29 10:46:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 1.8292	Prec@(1,5) (53.4%, 81.8%)
07/29 10:46:57午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 1.8317	Prec@(1,5) (53.2%, 81.9%)
07/29 10:47:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 1.8315	Prec@(1,5) (53.2%, 81.9%)
07/29 10:47:09午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 1.8295	Prec@(1,5) (53.3%, 81.8%)
07/29 10:47:09午後 searchCell_trainer.py:269 [INFO] Valid: [ 22/49] Final Prec@1 53.2840%
07/29 10:47:09午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('skip_connect', 0)]], normal3_concat=range(2, 6))
07/29 10:47:10午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 53.2840%
####### ALPHA #######
# Alpha - normal
tensor([[0.0787, 0.0756, 0.1029, 0.1936, 0.1149, 0.1445, 0.1242, 0.1656],
        [0.0990, 0.0823, 0.1354, 0.1941, 0.1134, 0.1488, 0.1159, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0846, 0.0830, 0.1083, 0.2119, 0.0972, 0.1234, 0.1285, 0.1630],
        [0.0817, 0.0695, 0.1031, 0.2501, 0.1605, 0.1415, 0.0955, 0.0982],
        [0.0707, 0.0640, 0.1138, 0.1690, 0.1730, 0.1435, 0.1322, 0.1337]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0886, 0.0834, 0.1056, 0.1591, 0.1131, 0.1360, 0.1401, 0.1742],
        [0.0767, 0.0702, 0.0927, 0.3370, 0.1141, 0.0920, 0.1223, 0.0950],
        [0.0674, 0.0559, 0.0906, 0.2187, 0.1412, 0.1673, 0.1285, 0.1303],
        [0.0606, 0.0572, 0.0885, 0.2171, 0.1496, 0.1859, 0.1274, 0.1137]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1043, 0.0924, 0.1120, 0.1611, 0.1231, 0.1186, 0.1242, 0.1643],
        [0.0962, 0.0742, 0.0964, 0.3210, 0.1000, 0.1163, 0.1068, 0.0890],
        [0.0858, 0.0657, 0.1049, 0.1584, 0.1493, 0.1589, 0.1425, 0.1345],
        [0.0763, 0.0670, 0.0946, 0.1843, 0.1688, 0.1560, 0.1364, 0.1166],
        [0.0827, 0.0688, 0.1046, 0.2097, 0.1453, 0.1416, 0.1359, 0.1114]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1227, 0.1665, 0.2169, 0.1181, 0.0996, 0.1006, 0.1087, 0.0669],
        [0.0847, 0.1006, 0.1284, 0.1359, 0.0984, 0.1178, 0.1284, 0.2059]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1307, 0.1877, 0.2338, 0.1070, 0.0994, 0.0856, 0.0912, 0.0646],
        [0.0958, 0.1136, 0.1475, 0.1259, 0.1004, 0.1233, 0.1140, 0.1794],
        [0.0994, 0.1185, 0.1598, 0.1107, 0.1160, 0.1062, 0.1363, 0.1532]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1221, 0.1791, 0.2362, 0.1115, 0.1107, 0.0879, 0.0872, 0.0653],
        [0.0810, 0.0993, 0.1325, 0.1286, 0.1377, 0.1247, 0.1261, 0.1701],
        [0.0891, 0.1149, 0.1578, 0.1229, 0.1130, 0.1304, 0.1080, 0.1639],
        [0.0793, 0.1023, 0.1335, 0.1263, 0.1164, 0.1040, 0.1269, 0.2112]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1339, 0.1983, 0.2597, 0.1048, 0.0896, 0.0725, 0.0765, 0.0648],
        [0.0894, 0.1149, 0.1517, 0.1161, 0.1264, 0.1242, 0.1234, 0.1538],
        [0.0965, 0.1231, 0.1719, 0.1352, 0.1107, 0.0991, 0.1139, 0.1497],
        [0.0882, 0.1133, 0.1476, 0.1211, 0.1236, 0.1183, 0.1135, 0.1744],
        [0.0892, 0.1225, 0.1512, 0.1136, 0.1027, 0.1111, 0.1172, 0.1925]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0562, 0.0620, 0.1386, 0.1603, 0.1039, 0.1886, 0.1384, 0.1520],
        [0.0393, 0.0385, 0.0518, 0.0960, 0.1532, 0.1538, 0.1614, 0.3060]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0591, 0.0675, 0.1629, 0.1208, 0.0964, 0.1721, 0.1320, 0.1891],
        [0.0394, 0.0397, 0.0546, 0.0935, 0.0702, 0.1245, 0.2020, 0.3762],
        [0.0222, 0.0206, 0.0576, 0.0683, 0.0463, 0.0910, 0.0837, 0.6104]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0627, 0.0703, 0.1720, 0.0811, 0.0751, 0.0940, 0.1087, 0.3362],
        [0.0358, 0.0361, 0.0494, 0.1030, 0.0842, 0.1027, 0.1093, 0.4795],
        [0.0216, 0.0202, 0.0545, 0.0577, 0.0557, 0.0824, 0.0551, 0.6528],
        [0.0184, 0.0169, 0.0275, 0.0640, 0.0496, 0.0830, 0.0613, 0.6793]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0517, 0.0597, 0.1257, 0.0853, 0.0573, 0.0911, 0.0652, 0.4641],
        [0.0366, 0.0381, 0.0525, 0.0759, 0.0502, 0.0811, 0.1138, 0.5518],
        [0.0198, 0.0184, 0.0452, 0.0501, 0.0418, 0.0518, 0.0586, 0.7143],
        [0.0169, 0.0152, 0.0229, 0.0487, 0.0330, 0.0554, 0.0588, 0.7490],
        [0.0159, 0.0141, 0.0175, 0.0507, 0.0506, 0.0531, 0.0507, 0.7474]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1562, 0.0921, 0.1141, 0.1346, 0.1291, 0.1526, 0.1018, 0.1195],
        [0.1316, 0.1024, 0.1383, 0.1309, 0.1261, 0.1416, 0.1029, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1575, 0.1037, 0.1094, 0.1375, 0.1424, 0.1202, 0.0981, 0.1312],
        [0.1289, 0.1027, 0.1247, 0.1756, 0.1646, 0.1089, 0.0924, 0.1022],
        [0.1095, 0.0872, 0.1109, 0.1643, 0.1276, 0.1316, 0.1203, 0.1485]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1598, 0.0991, 0.1208, 0.1278, 0.1500, 0.1279, 0.1060, 0.1085],
        [0.1366, 0.1159, 0.1274, 0.1493, 0.1091, 0.1497, 0.1107, 0.1013],
        [0.1201, 0.1014, 0.1314, 0.1437, 0.1183, 0.1244, 0.1167, 0.1440],
        [0.1100, 0.0966, 0.1126, 0.1618, 0.1500, 0.1217, 0.1091, 0.1384]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1551, 0.1168, 0.1218, 0.1241, 0.1177, 0.1414, 0.1155, 0.1076],
        [0.1309, 0.1117, 0.1153, 0.1517, 0.1156, 0.1533, 0.1033, 0.1182],
        [0.1032, 0.0907, 0.1392, 0.1288, 0.1200, 0.1252, 0.1113, 0.1816],
        [0.0991, 0.0892, 0.1119, 0.1516, 0.1401, 0.1542, 0.1293, 0.1246],
        [0.1001, 0.0930, 0.1168, 0.1467, 0.1425, 0.1220, 0.1092, 0.1696]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1591, 0.1536, 0.1275, 0.0992, 0.1186, 0.1571, 0.0895, 0.0953],
        [0.1530, 0.1482, 0.1102, 0.1033, 0.1290, 0.1413, 0.1101, 0.1050]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1611, 0.1669, 0.1295, 0.1194, 0.1160, 0.1158, 0.1051, 0.0862],
        [0.1940, 0.1944, 0.0937, 0.1092, 0.1186, 0.0900, 0.0985, 0.1017],
        [0.0730, 0.0748, 0.2127, 0.1157, 0.1080, 0.1075, 0.1410, 0.1674]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1751, 0.1925, 0.0983, 0.1133, 0.1172, 0.1161, 0.0948, 0.0928],
        [0.1898, 0.1904, 0.1321, 0.1049, 0.1260, 0.0870, 0.0780, 0.0918],
        [0.0691, 0.0775, 0.2431, 0.0966, 0.1212, 0.1146, 0.1223, 0.1557],
        [0.0616, 0.0600, 0.1916, 0.1102, 0.0984, 0.1218, 0.1400, 0.2165]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1769, 0.2091, 0.1665, 0.0925, 0.0929, 0.0953, 0.0939, 0.0730],
        [0.1579, 0.1758, 0.1245, 0.1243, 0.0903, 0.1028, 0.1244, 0.1000],
        [0.0676, 0.0808, 0.2588, 0.0912, 0.0977, 0.1084, 0.1204, 0.1750],
        [0.0580, 0.0594, 0.1831, 0.1100, 0.1004, 0.1264, 0.1336, 0.2292],
        [0.0453, 0.0469, 0.1264, 0.1190, 0.1004, 0.1347, 0.1151, 0.3124]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:47:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 3.7734 (3.4152)	Hard Loss 0.6983 (0.6320)	Soft Loss 0.0070 (0.0064)	Arch Loss 7.4816 (9.1770)	Arch Hard Loss 1.3849 (1.6988)	Arch Soft Loss 0.0075 (0.0083)	Prec@(1,5) (81.9%, 97.4%)	
07/29 10:48:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 2.7213 (3.5867)	Hard Loss 0.5035 (0.6637)	Soft Loss 0.0062 (0.0066)	Arch Loss 11.8665 (9.6107)	Arch Hard Loss 2.1968 (1.7791)	Arch Soft Loss 0.0091 (0.0084)	Prec@(1,5) (80.6%, 97.2%)	
07/29 10:49:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 4.5076 (3.6674)	Hard Loss 0.8341 (0.6787)	Soft Loss 0.0084 (0.0067)	Arch Loss 10.8608 (9.7579)	Arch Hard Loss 2.0106 (1.8064)	Arch Soft Loss 0.0087 (0.0085)	Prec@(1,5) (80.2%, 97.2%)	
07/29 10:49:52午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 3.6758 (3.7146)	Hard Loss 0.6802 (0.6874)	Soft Loss 0.0065 (0.0067)	Arch Loss 9.7582 (9.8247)	Arch Hard Loss 1.8065 (1.8188)	Arch Soft Loss 0.0079 (0.0084)	Prec@(1,5) (80.1%, 97.1%)	
07/29 10:50:32午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 3.6879 (3.7650)	Hard Loss 0.6825 (0.6967)	Soft Loss 0.0065 (0.0067)	Arch Loss 6.5642 (9.8372)	Arch Hard Loss 1.2150 (1.8211)	Arch Soft Loss 0.0076 (0.0085)	Prec@(1,5) (79.9%, 97.0%)	
07/29 10:51:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 3.7963 (3.7991)	Hard Loss 0.7025 (0.7030)	Soft Loss 0.0071 (0.0067)	Arch Loss 8.3365 (9.8953)	Arch Hard Loss 1.5432 (1.8318)	Arch Soft Loss 0.0076 (0.0085)	Prec@(1,5) (79.7%, 96.9%)	
07/29 10:51:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 3.5769 (3.8630)	Hard Loss 0.6619 (0.7149)	Soft Loss 0.0066 (0.0067)	Arch Loss 11.1091 (9.8988)	Arch Hard Loss 2.0566 (1.8325)	Arch Soft Loss 0.0087 (0.0085)	Prec@(1,5) (79.2%, 96.8%)	
07/29 10:52:23午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 4.1086 (3.8997)	Hard Loss 0.7604 (0.7217)	Soft Loss 0.0064 (0.0067)	Arch Loss 7.1864 (9.8688)	Arch Hard Loss 1.3302 (1.8269)	Arch Soft Loss 0.0081 (0.0085)	Prec@(1,5) (79.0%, 96.8%)	
07/29 10:52:24午後 searchCell_trainer.py:234 [INFO] Train: [ 23/49] Final Prec@1 78.9760%
07/29 10:52:32午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 1.7943	Prec@(1,5) (53.8%, 82.2%)
07/29 10:52:38午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.7995	Prec@(1,5) (54.3%, 82.1%)
07/29 10:52:45午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 1.8039	Prec@(1,5) (54.2%, 81.9%)
07/29 10:52:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.7997	Prec@(1,5) (54.2%, 82.0%)
07/29 10:52:57午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 1.7960	Prec@(1,5) (54.4%, 82.1%)
07/29 10:53:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.8040	Prec@(1,5) (54.4%, 81.9%)
07/29 10:53:10午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 1.7930	Prec@(1,5) (54.5%, 82.0%)
07/29 10:53:15午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.7938	Prec@(1,5) (54.3%, 82.0%)
07/29 10:53:16午後 searchCell_trainer.py:269 [INFO] Valid: [ 23/49] Final Prec@1 54.3360%
07/29 10:53:16午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 10:53:16午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 54.3360%
####### ALPHA #######
# Alpha - normal
tensor([[0.0762, 0.0731, 0.1004, 0.1963, 0.1143, 0.1457, 0.1237, 0.1704],
        [0.0980, 0.0811, 0.1356, 0.1957, 0.1132, 0.1503, 0.1172, 0.1089]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0823, 0.0810, 0.1064, 0.2165, 0.0968, 0.1225, 0.1276, 0.1670],
        [0.0810, 0.0681, 0.1022, 0.2564, 0.1611, 0.1405, 0.0941, 0.0966],
        [0.0699, 0.0626, 0.1139, 0.1697, 0.1745, 0.1427, 0.1325, 0.1341]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0865, 0.0813, 0.1034, 0.1612, 0.1130, 0.1360, 0.1410, 0.1775],
        [0.0746, 0.0680, 0.0903, 0.3497, 0.1127, 0.0905, 0.1215, 0.0927],
        [0.0656, 0.0538, 0.0885, 0.2236, 0.1424, 0.1686, 0.1273, 0.1301],
        [0.0596, 0.0561, 0.0879, 0.2195, 0.1495, 0.1889, 0.1258, 0.1128]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1017, 0.0903, 0.1098, 0.1631, 0.1235, 0.1189, 0.1247, 0.1680],
        [0.0942, 0.0727, 0.0950, 0.3317, 0.0983, 0.1151, 0.1059, 0.0869],
        [0.0843, 0.0641, 0.1040, 0.1591, 0.1501, 0.1596, 0.1435, 0.1354],
        [0.0744, 0.0655, 0.0936, 0.1852, 0.1702, 0.1566, 0.1381, 0.1164],
        [0.0811, 0.0672, 0.1034, 0.2135, 0.1461, 0.1425, 0.1366, 0.1095]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1210, 0.1666, 0.2183, 0.1193, 0.0991, 0.1005, 0.1092, 0.0659],
        [0.0837, 0.0997, 0.1285, 0.1352, 0.0975, 0.1172, 0.1285, 0.2098]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1287, 0.1892, 0.2364, 0.1074, 0.0996, 0.0846, 0.0907, 0.0634],
        [0.0946, 0.1131, 0.1474, 0.1262, 0.0997, 0.1237, 0.1143, 0.1810],
        [0.0978, 0.1179, 0.1599, 0.1107, 0.1179, 0.1061, 0.1357, 0.1539]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1207, 0.1799, 0.2389, 0.1110, 0.1125, 0.0864, 0.0861, 0.0646],
        [0.0798, 0.0986, 0.1323, 0.1298, 0.1385, 0.1243, 0.1259, 0.1710],
        [0.0879, 0.1146, 0.1585, 0.1223, 0.1133, 0.1304, 0.1080, 0.1650],
        [0.0783, 0.1022, 0.1337, 0.1268, 0.1156, 0.1029, 0.1275, 0.2130]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1324, 0.1994, 0.2628, 0.1044, 0.0895, 0.0718, 0.0758, 0.0640],
        [0.0891, 0.1154, 0.1533, 0.1166, 0.1262, 0.1232, 0.1216, 0.1547],
        [0.0949, 0.1225, 0.1726, 0.1358, 0.1100, 0.0993, 0.1145, 0.1504],
        [0.0873, 0.1136, 0.1486, 0.1208, 0.1219, 0.1181, 0.1141, 0.1756],
        [0.0880, 0.1219, 0.1508, 0.1130, 0.1034, 0.1106, 0.1183, 0.1940]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0541, 0.0601, 0.1401, 0.1602, 0.1009, 0.1915, 0.1396, 0.1534],
        [0.0366, 0.0358, 0.0487, 0.0928, 0.1490, 0.1526, 0.1594, 0.3251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0576, 0.0661, 0.1703, 0.1170, 0.0927, 0.1728, 0.1294, 0.1940],
        [0.0363, 0.0365, 0.0511, 0.0879, 0.0656, 0.1197, 0.2014, 0.4015],
        [0.0199, 0.0185, 0.0536, 0.0620, 0.0421, 0.0837, 0.0762, 0.6441]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0606, 0.0690, 0.1797, 0.0779, 0.0705, 0.0884, 0.1027, 0.3512],
        [0.0328, 0.0330, 0.0463, 0.0958, 0.0775, 0.0952, 0.1028, 0.5166],
        [0.0192, 0.0180, 0.0507, 0.0516, 0.0508, 0.0735, 0.0489, 0.6873],
        [0.0163, 0.0150, 0.0249, 0.0570, 0.0446, 0.0725, 0.0540, 0.7158]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0489, 0.0570, 0.1276, 0.0783, 0.0528, 0.0834, 0.0596, 0.4924],
        [0.0333, 0.0346, 0.0488, 0.0681, 0.0449, 0.0742, 0.1020, 0.5941],
        [0.0174, 0.0161, 0.0407, 0.0445, 0.0367, 0.0452, 0.0508, 0.7485],
        [0.0147, 0.0133, 0.0203, 0.0428, 0.0287, 0.0474, 0.0504, 0.7823],
        [0.0138, 0.0123, 0.0155, 0.0440, 0.0444, 0.0458, 0.0439, 0.7804]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1572, 0.0903, 0.1131, 0.1337, 0.1313, 0.1544, 0.1002, 0.1199],
        [0.1316, 0.1016, 0.1386, 0.1310, 0.1268, 0.1415, 0.1027, 0.1261]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1561, 0.1022, 0.1088, 0.1374, 0.1439, 0.1200, 0.0989, 0.1327],
        [0.1284, 0.1023, 0.1244, 0.1787, 0.1671, 0.1072, 0.0916, 0.1003],
        [0.1077, 0.0862, 0.1092, 0.1665, 0.1302, 0.1310, 0.1201, 0.1491]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1607, 0.0976, 0.1211, 0.1283, 0.1507, 0.1266, 0.1059, 0.1090],
        [0.1368, 0.1152, 0.1281, 0.1508, 0.1089, 0.1508, 0.1098, 0.0996],
        [0.1203, 0.1010, 0.1304, 0.1453, 0.1172, 0.1246, 0.1160, 0.1452],
        [0.1105, 0.0962, 0.1118, 0.1638, 0.1509, 0.1219, 0.1078, 0.1371]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1560, 0.1167, 0.1210, 0.1234, 0.1177, 0.1417, 0.1152, 0.1084],
        [0.1307, 0.1117, 0.1149, 0.1520, 0.1156, 0.1539, 0.1023, 0.1189],
        [0.1019, 0.0897, 0.1392, 0.1283, 0.1193, 0.1259, 0.1094, 0.1863],
        [0.0980, 0.0886, 0.1109, 0.1531, 0.1410, 0.1544, 0.1299, 0.1240],
        [0.0993, 0.0924, 0.1156, 0.1485, 0.1441, 0.1223, 0.1091, 0.1688]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1612, 0.1562, 0.1267, 0.0980, 0.1182, 0.1571, 0.0881, 0.0945],
        [0.1537, 0.1503, 0.1093, 0.1030, 0.1286, 0.1422, 0.1099, 0.1029]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1632, 0.1710, 0.1290, 0.1183, 0.1142, 0.1146, 0.1038, 0.0858],
        [0.1969, 0.2002, 0.0919, 0.1084, 0.1173, 0.0886, 0.0968, 0.0998],
        [0.0709, 0.0736, 0.2198, 0.1151, 0.1044, 0.1072, 0.1406, 0.1684]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1771, 0.1968, 0.0974, 0.1135, 0.1155, 0.1149, 0.0934, 0.0915],
        [0.1924, 0.1951, 0.1305, 0.1038, 0.1255, 0.0855, 0.0770, 0.0902],
        [0.0672, 0.0765, 0.2518, 0.0946, 0.1195, 0.1137, 0.1209, 0.1559],
        [0.0593, 0.0581, 0.1955, 0.1074, 0.0958, 0.1219, 0.1409, 0.2209]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1804, 0.2147, 0.1654, 0.0912, 0.0910, 0.0939, 0.0925, 0.0709],
        [0.1586, 0.1784, 0.1242, 0.1245, 0.0894, 0.1019, 0.1244, 0.0986],
        [0.0656, 0.0794, 0.2699, 0.0885, 0.0938, 0.1066, 0.1191, 0.1771],
        [0.0557, 0.0575, 0.1871, 0.1080, 0.0978, 0.1248, 0.1322, 0.2369],
        [0.0430, 0.0450, 0.1276, 0.1171, 0.0970, 0.1325, 0.1121, 0.3257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 10:53:58午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 3.4413 (3.2615)	Hard Loss 0.6368 (0.6035)	Soft Loss 0.0068 (0.0065)	Arch Loss 8.9734 (10.0221)	Arch Hard Loss 1.6611 (1.8553)	Arch Soft Loss 0.0080 (0.0086)	Prec@(1,5) (83.4%, 98.1%)	
07/29 10:54:38午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 2.8729 (3.4120)	Hard Loss 0.5315 (0.6314)	Soft Loss 0.0066 (0.0065)	Arch Loss 8.8091 (9.9649)	Arch Hard Loss 1.6307 (1.8447)	Arch Soft Loss 0.0090 (0.0086)	Prec@(1,5) (82.7%, 97.3%)	
07/29 10:55:19午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 3.5542 (3.4001)	Hard Loss 0.6577 (0.6292)	Soft Loss 0.0063 (0.0065)	Arch Loss 9.0003 (9.8619)	Arch Hard Loss 1.6661 (1.8256)	Arch Soft Loss 0.0086 (0.0086)	Prec@(1,5) (82.6%, 97.5%)	
07/29 10:56:00午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 4.2031 (3.5204)	Hard Loss 0.7779 (0.6514)	Soft Loss 0.0068 (0.0066)	Arch Loss 8.7318 (9.8414)	Arch Hard Loss 1.6164 (1.8219)	Arch Soft Loss 0.0075 (0.0085)	Prec@(1,5) (81.6%, 97.4%)	
07/29 10:56:41午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 3.7413 (3.5919)	Hard Loss 0.6923 (0.6647)	Soft Loss 0.0070 (0.0066)	Arch Loss 10.9210 (9.8127)	Arch Hard Loss 2.0218 (1.8165)	Arch Soft Loss 0.0084 (0.0085)	Prec@(1,5) (81.0%, 97.3%)	
07/29 10:57:22午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 3.8557 (3.6097)	Hard Loss 0.7136 (0.6680)	Soft Loss 0.0060 (0.0066)	Arch Loss 13.3672 (9.8230)	Arch Hard Loss 2.4748 (1.8185)	Arch Soft Loss 0.0083 (0.0085)	Prec@(1,5) (80.8%, 97.3%)	
07/29 10:58:03午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 3.6219 (3.6466)	Hard Loss 0.6702 (0.6748)	Soft Loss 0.0065 (0.0066)	Arch Loss 9.0322 (9.8077)	Arch Hard Loss 1.6720 (1.8156)	Arch Soft Loss 0.0080 (0.0085)	Prec@(1,5) (80.7%, 97.2%)	
07/29 10:58:36午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 3.6677 (3.6766)	Hard Loss 0.6787 (0.6804)	Soft Loss 0.0067 (0.0066)	Arch Loss 8.2694 (9.8011)	Arch Hard Loss 1.5308 (1.8144)	Arch Soft Loss 0.0075 (0.0085)	Prec@(1,5) (80.4%, 97.2%)	
07/29 10:58:37午後 searchCell_trainer.py:234 [INFO] Train: [ 24/49] Final Prec@1 80.4200%
07/29 10:58:44午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 1.8079	Prec@(1,5) (53.7%, 82.1%)
07/29 10:58:51午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 1.7969	Prec@(1,5) (54.3%, 82.1%)
07/29 10:58:57午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 1.7999	Prec@(1,5) (54.2%, 82.2%)
07/29 10:59:03午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 1.8121	Prec@(1,5) (54.0%, 82.2%)
07/29 10:59:10午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 1.8176	Prec@(1,5) (53.9%, 82.2%)
07/29 10:59:16午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 1.8254	Prec@(1,5) (53.8%, 82.1%)
07/29 10:59:22午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 1.8227	Prec@(1,5) (53.8%, 82.2%)
07/29 10:59:27午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 1.8243	Prec@(1,5) (53.7%, 82.2%)
07/29 10:59:28午後 searchCell_trainer.py:269 [INFO] Valid: [ 24/49] Final Prec@1 53.6800%
07/29 10:59:28午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 10:59:28午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 54.3360%
####### ALPHA #######
# Alpha - normal
tensor([[0.0736, 0.0708, 0.0978, 0.1985, 0.1136, 0.1476, 0.1238, 0.1742],
        [0.0970, 0.0797, 0.1353, 0.1981, 0.1134, 0.1522, 0.1161, 0.1081]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0800, 0.0789, 0.1041, 0.2212, 0.0960, 0.1216, 0.1268, 0.1714],
        [0.0799, 0.0668, 0.1013, 0.2653, 0.1617, 0.1383, 0.0922, 0.0946],
        [0.0683, 0.0610, 0.1133, 0.1730, 0.1765, 0.1428, 0.1318, 0.1333]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0851, 0.0801, 0.1023, 0.1624, 0.1118, 0.1365, 0.1401, 0.1818],
        [0.0731, 0.0665, 0.0887, 0.3608, 0.1109, 0.0891, 0.1198, 0.0912],
        [0.0638, 0.0521, 0.0870, 0.2283, 0.1423, 0.1700, 0.1258, 0.1306],
        [0.0581, 0.0547, 0.0865, 0.2233, 0.1513, 0.1907, 0.1244, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1001, 0.0888, 0.1081, 0.1644, 0.1227, 0.1198, 0.1251, 0.1710],
        [0.0934, 0.0714, 0.0937, 0.3389, 0.0963, 0.1153, 0.1053, 0.0856],
        [0.0831, 0.0628, 0.1029, 0.1606, 0.1508, 0.1623, 0.1434, 0.1343],
        [0.0735, 0.0645, 0.0929, 0.1859, 0.1702, 0.1574, 0.1396, 0.1160],
        [0.0799, 0.0659, 0.1024, 0.2159, 0.1473, 0.1445, 0.1360, 0.1081]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1197, 0.1691, 0.2231, 0.1182, 0.0976, 0.0992, 0.1091, 0.0641],
        [0.0819, 0.0986, 0.1277, 0.1358, 0.0969, 0.1156, 0.1279, 0.2155]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1274, 0.1922, 0.2406, 0.1073, 0.0984, 0.0829, 0.0895, 0.0616],
        [0.0931, 0.1121, 0.1472, 0.1273, 0.0983, 0.1235, 0.1144, 0.1841],
        [0.0965, 0.1178, 0.1605, 0.1099, 0.1193, 0.1056, 0.1356, 0.1547]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1204, 0.1837, 0.2426, 0.1090, 0.1127, 0.0842, 0.0846, 0.0628],
        [0.0784, 0.0982, 0.1324, 0.1300, 0.1386, 0.1248, 0.1241, 0.1736],
        [0.0872, 0.1158, 0.1607, 0.1188, 0.1133, 0.1298, 0.1073, 0.1671],
        [0.0779, 0.1033, 0.1353, 0.1257, 0.1150, 0.1010, 0.1262, 0.2156]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1319, 0.2032, 0.2682, 0.1022, 0.0882, 0.0700, 0.0742, 0.0621],
        [0.0876, 0.1151, 0.1547, 0.1158, 0.1261, 0.1233, 0.1205, 0.1569],
        [0.0939, 0.1230, 0.1749, 0.1351, 0.1090, 0.0985, 0.1133, 0.1523],
        [0.0869, 0.1142, 0.1499, 0.1194, 0.1219, 0.1175, 0.1140, 0.1763],
        [0.0870, 0.1223, 0.1515, 0.1116, 0.1032, 0.1102, 0.1181, 0.1961]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0525, 0.0589, 0.1419, 0.1592, 0.0986, 0.1929, 0.1420, 0.1541],
        [0.0342, 0.0334, 0.0460, 0.0899, 0.1440, 0.1489, 0.1582, 0.3455]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0564, 0.0655, 0.1791, 0.1127, 0.0882, 0.1720, 0.1257, 0.2004],
        [0.0338, 0.0339, 0.0483, 0.0823, 0.0612, 0.1149, 0.1963, 0.4293],
        [0.0180, 0.0167, 0.0502, 0.0565, 0.0381, 0.0758, 0.0691, 0.6756]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0582, 0.0677, 0.1878, 0.0737, 0.0664, 0.0823, 0.0970, 0.3668],
        [0.0302, 0.0304, 0.0435, 0.0881, 0.0707, 0.0871, 0.0954, 0.5547],
        [0.0171, 0.0161, 0.0470, 0.0459, 0.0460, 0.0647, 0.0434, 0.7197],
        [0.0144, 0.0133, 0.0223, 0.0508, 0.0402, 0.0632, 0.0475, 0.7484]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0462, 0.0548, 0.1308, 0.0716, 0.0485, 0.0760, 0.0542, 0.5180],
        [0.0303, 0.0313, 0.0454, 0.0610, 0.0403, 0.0669, 0.0902, 0.6346],
        [0.0153, 0.0143, 0.0368, 0.0391, 0.0320, 0.0392, 0.0440, 0.7793],
        [0.0128, 0.0116, 0.0178, 0.0372, 0.0250, 0.0406, 0.0433, 0.8117],
        [0.0120, 0.0107, 0.0136, 0.0382, 0.0389, 0.0395, 0.0380, 0.8090]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1564, 0.0884, 0.1124, 0.1334, 0.1339, 0.1559, 0.0984, 0.1212],
        [0.1314, 0.1015, 0.1401, 0.1302, 0.1278, 0.1418, 0.1022, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1538, 0.1005, 0.1075, 0.1392, 0.1446, 0.1195, 0.0995, 0.1354],
        [0.1270, 0.1023, 0.1233, 0.1838, 0.1689, 0.1059, 0.0909, 0.0979],
        [0.1060, 0.0853, 0.1076, 0.1698, 0.1319, 0.1315, 0.1182, 0.1497]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1604, 0.0962, 0.1205, 0.1283, 0.1529, 0.1260, 0.1063, 0.1094],
        [0.1370, 0.1157, 0.1285, 0.1515, 0.1075, 0.1527, 0.1091, 0.0980],
        [0.1203, 0.1010, 0.1306, 0.1456, 0.1164, 0.1240, 0.1161, 0.1460],
        [0.1102, 0.0953, 0.1107, 0.1659, 0.1529, 0.1219, 0.1070, 0.1361]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1549, 0.1161, 0.1210, 0.1236, 0.1188, 0.1432, 0.1134, 0.1091],
        [0.1300, 0.1115, 0.1145, 0.1528, 0.1159, 0.1557, 0.1016, 0.1179],
        [0.0998, 0.0890, 0.1391, 0.1269, 0.1199, 0.1273, 0.1087, 0.1893],
        [0.0969, 0.0878, 0.1099, 0.1548, 0.1416, 0.1554, 0.1300, 0.1237],
        [0.0982, 0.0921, 0.1152, 0.1478, 0.1455, 0.1215, 0.1087, 0.1710]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1649, 0.1609, 0.1262, 0.0957, 0.1174, 0.1568, 0.0857, 0.0925],
        [0.1553, 0.1529, 0.1073, 0.1016, 0.1283, 0.1442, 0.1087, 0.1016]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1665, 0.1772, 0.1272, 0.1159, 0.1131, 0.1138, 0.1016, 0.0847],
        [0.2020, 0.2063, 0.0893, 0.1074, 0.1154, 0.0869, 0.0947, 0.0981],
        [0.0694, 0.0731, 0.2290, 0.1126, 0.1006, 0.1061, 0.1384, 0.1707]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1808, 0.2034, 0.0958, 0.1129, 0.1133, 0.1129, 0.0914, 0.0895],
        [0.1972, 0.2011, 0.1272, 0.1018, 0.1244, 0.0832, 0.0760, 0.0892],
        [0.0658, 0.0762, 0.2611, 0.0925, 0.1182, 0.1122, 0.1186, 0.1554],
        [0.0576, 0.0567, 0.2001, 0.1045, 0.0932, 0.1215, 0.1407, 0.2258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1857, 0.2244, 0.1628, 0.0885, 0.0886, 0.0923, 0.0893, 0.0683],
        [0.1601, 0.1825, 0.1224, 0.1250, 0.0877, 0.1005, 0.1237, 0.0981],
        [0.0641, 0.0792, 0.2830, 0.0856, 0.0910, 0.1026, 0.1167, 0.1777],
        [0.0540, 0.0561, 0.1918, 0.1057, 0.0943, 0.1221, 0.1306, 0.2454],
        [0.0410, 0.0436, 0.1288, 0.1148, 0.0938, 0.1296, 0.1085, 0.3400]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 11:00:11午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 2.8417 (3.2687)	Hard Loss 0.5258 (0.6048)	Soft Loss 0.0058 (0.0065)	Arch Loss 10.7660 (9.8585)	Arch Hard Loss 1.9930 (1.8250)	Arch Soft Loss 0.0090 (0.0084)	Prec@(1,5) (83.0%, 97.6%)	
07/29 11:00:52午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 4.2056 (3.2677)	Hard Loss 0.7783 (0.6046)	Soft Loss 0.0068 (0.0065)	Arch Loss 11.4264 (9.7277)	Arch Hard Loss 2.1154 (1.8008)	Arch Soft Loss 0.0084 (0.0084)	Prec@(1,5) (82.6%, 97.8%)	
07/29 11:01:34午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 3.1621 (3.2684)	Hard Loss 0.5851 (0.6048)	Soft Loss 0.0064 (0.0065)	Arch Loss 8.7849 (9.7651)	Arch Hard Loss 1.6263 (1.8077)	Arch Soft Loss 0.0078 (0.0084)	Prec@(1,5) (82.7%, 97.9%)	
07/29 11:02:15午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 2.5612 (3.3242)	Hard Loss 0.4738 (0.6151)	Soft Loss 0.0063 (0.0065)	Arch Loss 7.3749 (9.7731)	Arch Hard Loss 1.3652 (1.8092)	Arch Soft Loss 0.0075 (0.0084)	Prec@(1,5) (82.4%, 97.9%)	
07/29 11:02:56午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 3.7943 (3.3990)	Hard Loss 0.7022 (0.6290)	Soft Loss 0.0065 (0.0065)	Arch Loss 9.0072 (9.7726)	Arch Hard Loss 1.6674 (1.8091)	Arch Soft Loss 0.0079 (0.0084)	Prec@(1,5) (82.0%, 97.6%)	
07/29 11:03:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 2.4480 (3.4556)	Hard Loss 0.4529 (0.6394)	Soft Loss 0.0063 (0.0065)	Arch Loss 8.6863 (9.7417)	Arch Hard Loss 1.6079 (1.8034)	Arch Soft Loss 0.0084 (0.0084)	Prec@(1,5) (81.8%, 97.5%)	
07/29 11:04:19午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 3.6959 (3.4857)	Hard Loss 0.6839 (0.6450)	Soft Loss 0.0066 (0.0065)	Arch Loss 8.8883 (9.7430)	Arch Hard Loss 1.6453 (1.8036)	Arch Soft Loss 0.0086 (0.0084)	Prec@(1,5) (81.5%, 97.5%)	
07/29 11:04:51午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 3.5103 (3.5035)	Hard Loss 0.6496 (0.6483)	Soft Loss 0.0060 (0.0066)	Arch Loss 10.2001 (9.7242)	Arch Hard Loss 1.8883 (1.8001)	Arch Soft Loss 0.0081 (0.0084)	Prec@(1,5) (81.3%, 97.5%)	
07/29 11:04:52午後 searchCell_trainer.py:234 [INFO] Train: [ 25/49] Final Prec@1 81.3440%
07/29 11:05:00午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 1.8814	Prec@(1,5) (53.3%, 80.5%)
07/29 11:05:06午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 1.8509	Prec@(1,5) (53.0%, 81.6%)
07/29 11:05:12午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 1.8299	Prec@(1,5) (53.6%, 81.7%)
07/29 11:05:19午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 1.8226	Prec@(1,5) (53.7%, 81.9%)
07/29 11:05:25午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 1.8178	Prec@(1,5) (53.9%, 81.9%)
07/29 11:05:31午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 1.8195	Prec@(1,5) (53.7%, 81.8%)
07/29 11:05:38午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 1.8176	Prec@(1,5) (53.7%, 81.8%)
07/29 11:05:43午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 1.8154	Prec@(1,5) (53.8%, 81.8%)
07/29 11:05:43午後 searchCell_trainer.py:269 [INFO] Valid: [ 25/49] Final Prec@1 53.7600%
07/29 11:05:43午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 11:05:43午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 54.3360%
####### ALPHA #######
# Alpha - normal
tensor([[0.0711, 0.0686, 0.0953, 0.2025, 0.1134, 0.1486, 0.1232, 0.1774],
        [0.0964, 0.0789, 0.1363, 0.1985, 0.1136, 0.1528, 0.1158, 0.1076]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0781, 0.0773, 0.1025, 0.2258, 0.0957, 0.1211, 0.1260, 0.1735],
        [0.0784, 0.0651, 0.0997, 0.2727, 0.1627, 0.1374, 0.0911, 0.0929],
        [0.0671, 0.0597, 0.1132, 0.1753, 0.1764, 0.1422, 0.1323, 0.1337]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0836, 0.0787, 0.1009, 0.1634, 0.1109, 0.1363, 0.1413, 0.1849],
        [0.0720, 0.0652, 0.0873, 0.3697, 0.1090, 0.0884, 0.1192, 0.0892],
        [0.0626, 0.0508, 0.0861, 0.2303, 0.1425, 0.1719, 0.1247, 0.1311],
        [0.0571, 0.0538, 0.0859, 0.2269, 0.1512, 0.1926, 0.1228, 0.1098]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0987, 0.0874, 0.1067, 0.1673, 0.1217, 0.1195, 0.1241, 0.1746],
        [0.0922, 0.0695, 0.0919, 0.3498, 0.0949, 0.1143, 0.1037, 0.0836],
        [0.0821, 0.0610, 0.1014, 0.1616, 0.1518, 0.1636, 0.1446, 0.1339],
        [0.0727, 0.0635, 0.0923, 0.1860, 0.1689, 0.1599, 0.1397, 0.1169],
        [0.0794, 0.0649, 0.1017, 0.2185, 0.1470, 0.1459, 0.1349, 0.1077]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1188, 0.1708, 0.2251, 0.1193, 0.0968, 0.0986, 0.1082, 0.0626],
        [0.0806, 0.0979, 0.1273, 0.1346, 0.0956, 0.1150, 0.1285, 0.2204]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1263, 0.1957, 0.2434, 0.1074, 0.0977, 0.0815, 0.0880, 0.0600],
        [0.0921, 0.1125, 0.1479, 0.1276, 0.0967, 0.1226, 0.1135, 0.1872],
        [0.0962, 0.1185, 0.1618, 0.1091, 0.1186, 0.1048, 0.1352, 0.1556]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1198, 0.1869, 0.2458, 0.1084, 0.1120, 0.0830, 0.0825, 0.0615],
        [0.0773, 0.0984, 0.1324, 0.1307, 0.1375, 0.1254, 0.1223, 0.1761],
        [0.0865, 0.1166, 0.1625, 0.1166, 0.1139, 0.1300, 0.1061, 0.1679],
        [0.0774, 0.1040, 0.1360, 0.1255, 0.1145, 0.0994, 0.1247, 0.2185]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1320, 0.2073, 0.2714, 0.1012, 0.0862, 0.0690, 0.0722, 0.0607],
        [0.0869, 0.1161, 0.1556, 0.1149, 0.1259, 0.1231, 0.1192, 0.1582],
        [0.0938, 0.1240, 0.1772, 0.1347, 0.1080, 0.0975, 0.1130, 0.1519],
        [0.0869, 0.1150, 0.1512, 0.1173, 0.1215, 0.1166, 0.1137, 0.1778],
        [0.0865, 0.1235, 0.1526, 0.1100, 0.1021, 0.1097, 0.1172, 0.1983]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0510, 0.0577, 0.1442, 0.1587, 0.0954, 0.1956, 0.1417, 0.1557],
        [0.0318, 0.0310, 0.0433, 0.0861, 0.1401, 0.1456, 0.1562, 0.3658]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0553, 0.0648, 0.1883, 0.1080, 0.0838, 0.1692, 0.1222, 0.2084],
        [0.0313, 0.0314, 0.0456, 0.0770, 0.0572, 0.1088, 0.1907, 0.4582],
        [0.0162, 0.0150, 0.0464, 0.0509, 0.0347, 0.0686, 0.0627, 0.7055]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0565, 0.0671, 0.1971, 0.0688, 0.0622, 0.0765, 0.0904, 0.3813],
        [0.0276, 0.0278, 0.0406, 0.0799, 0.0633, 0.0793, 0.0882, 0.5933],
        [0.0152, 0.0144, 0.0431, 0.0405, 0.0412, 0.0566, 0.0382, 0.7509],
        [0.0127, 0.0117, 0.0200, 0.0448, 0.0358, 0.0549, 0.0416, 0.7786]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0441, 0.0533, 0.1343, 0.0657, 0.0447, 0.0691, 0.0495, 0.5392],
        [0.0275, 0.0282, 0.0417, 0.0545, 0.0358, 0.0596, 0.0791, 0.6736],
        [0.0135, 0.0126, 0.0331, 0.0345, 0.0280, 0.0341, 0.0382, 0.8059],
        [0.0112, 0.0101, 0.0157, 0.0325, 0.0218, 0.0351, 0.0373, 0.8362],
        [0.0105, 0.0094, 0.0120, 0.0333, 0.0339, 0.0341, 0.0329, 0.8339]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1587, 0.0866, 0.1116, 0.1312, 0.1358, 0.1574, 0.0970, 0.1217],
        [0.1319, 0.1008, 0.1424, 0.1304, 0.1276, 0.1421, 0.1006, 0.1242]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1541, 0.0992, 0.1063, 0.1401, 0.1443, 0.1197, 0.0995, 0.1368],
        [0.1273, 0.1024, 0.1226, 0.1866, 0.1705, 0.1048, 0.0896, 0.0961],
        [0.1054, 0.0844, 0.1064, 0.1731, 0.1323, 0.1305, 0.1170, 0.1510]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1625, 0.0951, 0.1190, 0.1277, 0.1551, 0.1249, 0.1064, 0.1093],
        [0.1383, 0.1163, 0.1284, 0.1524, 0.1060, 0.1526, 0.1093, 0.0967],
        [0.1211, 0.1012, 0.1307, 0.1464, 0.1153, 0.1240, 0.1152, 0.1461],
        [0.1112, 0.0952, 0.1106, 0.1676, 0.1534, 0.1214, 0.1054, 0.1352]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1567, 0.1157, 0.1193, 0.1240, 0.1188, 0.1442, 0.1122, 0.1091],
        [0.1310, 0.1116, 0.1132, 0.1525, 0.1157, 0.1575, 0.1006, 0.1178],
        [0.0994, 0.0887, 0.1400, 0.1261, 0.1195, 0.1257, 0.1072, 0.1934],
        [0.0973, 0.0880, 0.1101, 0.1549, 0.1417, 0.1559, 0.1285, 0.1235],
        [0.0984, 0.0926, 0.1157, 0.1469, 0.1474, 0.1203, 0.1070, 0.1717]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1680, 0.1637, 0.1260, 0.0945, 0.1158, 0.1561, 0.0847, 0.0914],
        [0.1577, 0.1561, 0.1052, 0.1018, 0.1268, 0.1454, 0.1073, 0.0996]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1700, 0.1819, 0.1253, 0.1144, 0.1114, 0.1128, 0.1000, 0.0842],
        [0.2091, 0.2141, 0.0858, 0.1055, 0.1128, 0.0851, 0.0930, 0.0947],
        [0.0682, 0.0725, 0.2383, 0.1105, 0.0969, 0.1041, 0.1357, 0.1738]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1856, 0.2106, 0.0943, 0.1114, 0.1115, 0.1104, 0.0888, 0.0875],
        [0.2035, 0.2080, 0.1237, 0.0996, 0.1229, 0.0807, 0.0747, 0.0868],
        [0.0648, 0.0759, 0.2726, 0.0898, 0.1154, 0.1104, 0.1151, 0.1560],
        [0.0563, 0.0553, 0.2065, 0.1020, 0.0894, 0.1199, 0.1387, 0.2318]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1914, 0.2338, 0.1603, 0.0857, 0.0864, 0.0900, 0.0863, 0.0661],
        [0.1633, 0.1879, 0.1202, 0.1234, 0.0865, 0.0990, 0.1230, 0.0968],
        [0.0625, 0.0788, 0.2968, 0.0832, 0.0872, 0.0994, 0.1133, 0.1788],
        [0.0523, 0.0547, 0.1973, 0.1030, 0.0905, 0.1197, 0.1280, 0.2546],
        [0.0390, 0.0421, 0.1300, 0.1115, 0.0898, 0.1269, 0.1047, 0.3561]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 11:06:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 3.4119 (3.1905)	Hard Loss 0.6314 (0.5904)	Soft Loss 0.0061 (0.0064)	Arch Loss 7.9832 (9.6162)	Arch Hard Loss 1.4777 (1.7801)	Arch Soft Loss 0.0095 (0.0084)	Prec@(1,5) (83.4%, 98.1%)	
07/29 11:07:06午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 3.4087 (3.2122)	Hard Loss 0.6308 (0.5944)	Soft Loss 0.0057 (0.0064)	Arch Loss 10.0260 (9.5276)	Arch Hard Loss 1.8560 (1.7638)	Arch Soft Loss 0.0085 (0.0083)	Prec@(1,5) (83.6%, 98.0%)	
07/29 11:07:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 3.7425 (3.1990)	Hard Loss 0.6926 (0.5919)	Soft Loss 0.0068 (0.0064)	Arch Loss 10.8136 (9.5299)	Arch Hard Loss 2.0019 (1.7642)	Arch Soft Loss 0.0086 (0.0083)	Prec@(1,5) (83.5%, 98.0%)	
07/29 11:08:25午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 2.8565 (3.1877)	Hard Loss 0.5285 (0.5898)	Soft Loss 0.0061 (0.0064)	Arch Loss 9.7467 (9.5596)	Arch Hard Loss 1.8042 (1.7697)	Arch Soft Loss 0.0098 (0.0083)	Prec@(1,5) (83.5%, 98.0%)	
07/29 11:09:05午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 3.4439 (3.2325)	Hard Loss 0.6372 (0.5981)	Soft Loss 0.0071 (0.0064)	Arch Loss 8.8208 (9.5757)	Arch Hard Loss 1.6329 (1.7727)	Arch Soft Loss 0.0081 (0.0083)	Prec@(1,5) (83.2%, 98.0%)	
07/29 11:09:44午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 3.1185 (3.2794)	Hard Loss 0.5770 (0.6068)	Soft Loss 0.0063 (0.0064)	Arch Loss 9.5568 (9.5666)	Arch Hard Loss 1.7692 (1.7710)	Arch Soft Loss 0.0077 (0.0083)	Prec@(1,5) (82.9%, 97.9%)	
07/29 11:10:24午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 3.6478 (3.3058)	Hard Loss 0.6751 (0.6117)	Soft Loss 0.0060 (0.0065)	Arch Loss 10.1828 (9.6001)	Arch Hard Loss 1.8850 (1.7772)	Arch Soft Loss 0.0090 (0.0084)	Prec@(1,5) (82.7%, 97.9%)	
07/29 11:10:56午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 4.0052 (3.3388)	Hard Loss 0.7412 (0.6178)	Soft Loss 0.0065 (0.0065)	Arch Loss 10.6596 (9.6240)	Arch Hard Loss 1.9733 (1.7816)	Arch Soft Loss 0.0096 (0.0084)	Prec@(1,5) (82.5%, 97.9%)	
07/29 11:10:57午後 searchCell_trainer.py:234 [INFO] Train: [ 26/49] Final Prec@1 82.4440%
07/29 11:11:04午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 1.8040	Prec@(1,5) (53.6%, 81.9%)
07/29 11:11:11午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.7726	Prec@(1,5) (54.0%, 82.4%)
07/29 11:11:17午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 1.7612	Prec@(1,5) (54.5%, 82.6%)
07/29 11:11:23午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.7612	Prec@(1,5) (54.3%, 82.7%)
07/29 11:11:30午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 1.7589	Prec@(1,5) (54.4%, 82.3%)
07/29 11:11:36午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.7605	Prec@(1,5) (54.4%, 82.2%)
07/29 11:11:42午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 1.7665	Prec@(1,5) (54.3%, 82.2%)
07/29 11:11:47午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.7663	Prec@(1,5) (54.2%, 82.3%)
07/29 11:11:48午後 searchCell_trainer.py:269 [INFO] Valid: [ 26/49] Final Prec@1 54.2120%
07/29 11:11:48午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 11:11:48午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 54.3360%
####### ALPHA #######
# Alpha - normal
tensor([[0.0687, 0.0665, 0.0928, 0.2060, 0.1133, 0.1492, 0.1217, 0.1818],
        [0.0960, 0.0779, 0.1369, 0.2004, 0.1140, 0.1523, 0.1154, 0.1072]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0757, 0.0752, 0.1003, 0.2299, 0.0953, 0.1207, 0.1261, 0.1768],
        [0.0772, 0.0637, 0.0986, 0.2796, 0.1624, 0.1370, 0.0897, 0.0919],
        [0.0654, 0.0580, 0.1120, 0.1772, 0.1790, 0.1421, 0.1325, 0.1338]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0819, 0.0774, 0.0994, 0.1650, 0.1100, 0.1367, 0.1412, 0.1883],
        [0.0700, 0.0634, 0.0853, 0.3835, 0.1072, 0.0868, 0.1173, 0.0866],
        [0.0607, 0.0491, 0.0843, 0.2350, 0.1435, 0.1736, 0.1226, 0.1313],
        [0.0558, 0.0526, 0.0847, 0.2331, 0.1513, 0.1922, 0.1212, 0.1091]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0975, 0.0865, 0.1057, 0.1700, 0.1207, 0.1184, 0.1235, 0.1778],
        [0.0905, 0.0679, 0.0900, 0.3639, 0.0927, 0.1118, 0.1018, 0.0813],
        [0.0806, 0.0597, 0.1006, 0.1637, 0.1516, 0.1636, 0.1448, 0.1355],
        [0.0714, 0.0622, 0.0911, 0.1881, 0.1686, 0.1604, 0.1416, 0.1167],
        [0.0783, 0.0638, 0.1007, 0.2232, 0.1475, 0.1464, 0.1337, 0.1063]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1180, 0.1723, 0.2278, 0.1195, 0.0961, 0.0968, 0.1085, 0.0610],
        [0.0792, 0.0970, 0.1268, 0.1333, 0.0948, 0.1141, 0.1283, 0.2265]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1980, 0.2474, 0.1073, 0.0973, 0.0802, 0.0866, 0.0582],
        [0.0909, 0.1121, 0.1477, 0.1281, 0.0946, 0.1222, 0.1134, 0.1911],
        [0.0953, 0.1185, 0.1625, 0.1080, 0.1193, 0.1050, 0.1348, 0.1566]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1192, 0.1900, 0.2512, 0.1062, 0.1123, 0.0808, 0.0803, 0.0601],
        [0.0764, 0.0981, 0.1326, 0.1309, 0.1375, 0.1251, 0.1212, 0.1782],
        [0.0860, 0.1175, 0.1646, 0.1147, 0.1139, 0.1295, 0.1048, 0.1689],
        [0.0770, 0.1049, 0.1374, 0.1248, 0.1122, 0.0984, 0.1246, 0.2207]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1304, 0.2096, 0.2774, 0.1002, 0.0849, 0.0671, 0.0710, 0.0593],
        [0.0864, 0.1167, 0.1567, 0.1142, 0.1260, 0.1221, 0.1186, 0.1591],
        [0.0930, 0.1244, 0.1791, 0.1343, 0.1075, 0.0965, 0.1127, 0.1524],
        [0.0865, 0.1156, 0.1523, 0.1157, 0.1217, 0.1154, 0.1134, 0.1793],
        [0.0860, 0.1241, 0.1532, 0.1084, 0.1016, 0.1094, 0.1167, 0.2007]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0494, 0.0559, 0.1445, 0.1582, 0.0928, 0.1980, 0.1430, 0.1582],
        [0.0300, 0.0290, 0.0410, 0.0823, 0.1360, 0.1407, 0.1531, 0.3879]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0539, 0.0636, 0.1957, 0.1041, 0.0795, 0.1670, 0.1185, 0.2176],
        [0.0291, 0.0290, 0.0429, 0.0722, 0.0533, 0.1020, 0.1803, 0.4912],
        [0.0146, 0.0134, 0.0425, 0.0458, 0.0314, 0.0618, 0.0562, 0.7343]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0545, 0.0650, 0.2037, 0.0647, 0.0582, 0.0706, 0.0842, 0.3992],
        [0.0254, 0.0254, 0.0380, 0.0726, 0.0570, 0.0718, 0.0799, 0.6297],
        [0.0136, 0.0127, 0.0393, 0.0358, 0.0370, 0.0494, 0.0336, 0.7787],
        [0.0112, 0.0103, 0.0177, 0.0395, 0.0318, 0.0480, 0.0363, 0.8051]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0418, 0.0510, 0.1357, 0.0601, 0.0413, 0.0628, 0.0452, 0.5622],
        [0.0249, 0.0253, 0.0383, 0.0488, 0.0318, 0.0534, 0.0692, 0.7084],
        [0.0119, 0.0111, 0.0295, 0.0304, 0.0246, 0.0299, 0.0333, 0.8294],
        [0.0098, 0.0089, 0.0138, 0.0284, 0.0189, 0.0303, 0.0321, 0.8579],
        [0.0092, 0.0082, 0.0107, 0.0289, 0.0297, 0.0295, 0.0285, 0.8553]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1605, 0.0854, 0.1102, 0.1300, 0.1365, 0.1585, 0.0956, 0.1233],
        [0.1325, 0.1000, 0.1438, 0.1309, 0.1282, 0.1428, 0.0991, 0.1228]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1535, 0.0982, 0.1050, 0.1421, 0.1454, 0.1194, 0.0988, 0.1376],
        [0.1277, 0.1019, 0.1221, 0.1879, 0.1733, 0.1047, 0.0880, 0.0944],
        [0.1045, 0.0836, 0.1054, 0.1750, 0.1331, 0.1299, 0.1162, 0.1524]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1626, 0.0942, 0.1189, 0.1272, 0.1573, 0.1249, 0.1061, 0.1089],
        [0.1392, 0.1167, 0.1289, 0.1536, 0.1038, 0.1535, 0.1089, 0.0955],
        [0.1203, 0.1011, 0.1308, 0.1465, 0.1143, 0.1251, 0.1140, 0.1480],
        [0.1106, 0.0945, 0.1099, 0.1702, 0.1541, 0.1214, 0.1045, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1580, 0.1158, 0.1202, 0.1229, 0.1182, 0.1454, 0.1109, 0.1086],
        [0.1316, 0.1116, 0.1121, 0.1515, 0.1156, 0.1593, 0.1000, 0.1183],
        [0.0989, 0.0884, 0.1399, 0.1237, 0.1195, 0.1253, 0.1066, 0.1977],
        [0.0971, 0.0876, 0.1095, 0.1555, 0.1418, 0.1560, 0.1289, 0.1235],
        [0.0988, 0.0921, 0.1150, 0.1465, 0.1498, 0.1195, 0.1063, 0.1720]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1717, 0.1680, 0.1249, 0.0926, 0.1144, 0.1556, 0.0831, 0.0897],
        [0.1601, 0.1586, 0.1048, 0.1013, 0.1241, 0.1464, 0.1062, 0.0984]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1729, 0.1870, 0.1229, 0.1134, 0.1103, 0.1122, 0.0986, 0.0827],
        [0.2135, 0.2199, 0.0842, 0.1039, 0.1102, 0.0838, 0.0917, 0.0928],
        [0.0665, 0.0712, 0.2465, 0.1096, 0.0932, 0.1026, 0.1338, 0.1766]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1893, 0.2174, 0.0927, 0.1102, 0.1094, 0.1084, 0.0867, 0.0859],
        [0.2082, 0.2126, 0.1215, 0.0981, 0.1214, 0.0796, 0.0733, 0.0853],
        [0.0631, 0.0747, 0.2829, 0.0881, 0.1135, 0.1092, 0.1120, 0.1565],
        [0.0545, 0.0536, 0.2126, 0.0984, 0.0871, 0.1188, 0.1372, 0.2378]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1962, 0.2420, 0.1578, 0.0836, 0.0843, 0.0884, 0.0840, 0.0637],
        [0.1660, 0.1923, 0.1181, 0.1215, 0.0857, 0.0983, 0.1217, 0.0962],
        [0.0608, 0.0775, 0.3092, 0.0803, 0.0847, 0.0967, 0.1114, 0.1796],
        [0.0505, 0.0530, 0.2025, 0.0999, 0.0876, 0.1188, 0.1265, 0.2611],
        [0.0372, 0.0404, 0.1310, 0.1091, 0.0860, 0.1235, 0.1021, 0.3706]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 11:12:31午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 2.4243 (2.8727)	Hard Loss 0.4485 (0.5315)	Soft Loss 0.0064 (0.0062)	Arch Loss 11.7188 (9.4829)	Arch Hard Loss 2.1695 (1.7555)	Arch Soft Loss 0.0086 (0.0084)	Prec@(1,5) (85.7%, 98.5%)	
07/29 11:13:12午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 3.7798 (2.9652)	Hard Loss 0.6995 (0.5487)	Soft Loss 0.0065 (0.0062)	Arch Loss 9.6721 (9.6728)	Arch Hard Loss 1.7905 (1.7906)	Arch Soft Loss 0.0080 (0.0084)	Prec@(1,5) (85.1%, 98.2%)	
07/29 11:13:53午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 3.2375 (3.0033)	Hard Loss 0.5991 (0.5557)	Soft Loss 0.0062 (0.0063)	Arch Loss 10.2426 (9.6917)	Arch Hard Loss 1.8961 (1.7941)	Arch Soft Loss 0.0087 (0.0084)	Prec@(1,5) (84.9%, 98.1%)	
07/29 11:14:35午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 3.5127 (3.0255)	Hard Loss 0.6500 (0.5598)	Soft Loss 0.0065 (0.0063)	Arch Loss 10.2008 (9.6699)	Arch Hard Loss 1.8884 (1.7901)	Arch Soft Loss 0.0085 (0.0084)	Prec@(1,5) (84.7%, 98.1%)	
07/29 11:15:16午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 2.5685 (3.0722)	Hard Loss 0.4752 (0.5685)	Soft Loss 0.0057 (0.0063)	Arch Loss 9.3096 (9.6278)	Arch Hard Loss 1.7234 (1.7823)	Arch Soft Loss 0.0084 (0.0083)	Prec@(1,5) (84.4%, 98.1%)	
07/29 11:15:57午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 4.2615 (3.1162)	Hard Loss 0.7887 (0.5766)	Soft Loss 0.0069 (0.0063)	Arch Loss 9.3210 (9.6283)	Arch Hard Loss 1.7255 (1.7824)	Arch Soft Loss 0.0079 (0.0083)	Prec@(1,5) (84.2%, 98.0%)	
07/29 11:16:37午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 3.1348 (3.1580)	Hard Loss 0.5801 (0.5843)	Soft Loss 0.0062 (0.0063)	Arch Loss 11.5613 (9.5974)	Arch Hard Loss 2.1404 (1.7767)	Arch Soft Loss 0.0077 (0.0083)	Prec@(1,5) (83.9%, 98.1%)	
07/29 11:17:10午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 3.0346 (3.1755)	Hard Loss 0.5615 (0.5876)	Soft Loss 0.0065 (0.0063)	Arch Loss 9.7527 (9.5953)	Arch Hard Loss 1.8054 (1.7763)	Arch Soft Loss 0.0084 (0.0083)	Prec@(1,5) (83.8%, 98.0%)	
07/29 11:17:11午後 searchCell_trainer.py:234 [INFO] Train: [ 27/49] Final Prec@1 83.8120%
07/29 11:17:18午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 1.7693	Prec@(1,5) (52.1%, 82.7%)
07/29 11:17:24午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.7794	Prec@(1,5) (53.1%, 82.0%)
07/29 11:17:31午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 1.7880	Prec@(1,5) (53.5%, 82.0%)
07/29 11:17:37午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.7773	Prec@(1,5) (53.8%, 82.3%)
07/29 11:17:43午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 1.7515	Prec@(1,5) (54.3%, 82.7%)
07/29 11:17:50午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.7581	Prec@(1,5) (54.4%, 82.6%)
07/29 11:17:56午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 1.7596	Prec@(1,5) (54.2%, 82.6%)
07/29 11:18:01午後 searchCell_trainer.py:262 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.7589	Prec@(1,5) (54.2%, 82.6%)
07/29 11:18:02午後 searchCell_trainer.py:269 [INFO] Valid: [ 27/49] Final Prec@1 54.2080%
07/29 11:18:02午後 searchCell_KD_main.py:58 [INFO] genotype = Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 2)]], normal2_concat=range(2, 6), reduce2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)]], normal3_concat=range(2, 6))
07/29 11:18:02午後 searchCell_KD_main.py:92 [INFO] Until now, best Prec@1 = 54.3360%
####### ALPHA #######
# Alpha - normal
tensor([[0.0670, 0.0649, 0.0913, 0.2081, 0.1135, 0.1486, 0.1207, 0.1858],
        [0.0937, 0.0762, 0.1362, 0.2035, 0.1142, 0.1536, 0.1160, 0.1067]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0740, 0.0741, 0.0992, 0.2332, 0.0949, 0.1207, 0.1241, 0.1797],
        [0.0752, 0.0621, 0.0970, 0.2887, 0.1617, 0.1363, 0.0885, 0.0906],
        [0.0637, 0.0566, 0.1112, 0.1805, 0.1810, 0.1423, 0.1316, 0.1331]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0808, 0.0765, 0.0985, 0.1664, 0.1092, 0.1365, 0.1413, 0.1908],
        [0.0684, 0.0623, 0.0841, 0.3921, 0.1064, 0.0857, 0.1157, 0.0854],
        [0.0591, 0.0478, 0.0830, 0.2386, 0.1445, 0.1738, 0.1218, 0.1313],
        [0.0544, 0.0514, 0.0836, 0.2383, 0.1513, 0.1921, 0.1207, 0.1081]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0960, 0.0853, 0.1045, 0.1721, 0.1202, 0.1183, 0.1227, 0.1808],
        [0.0879, 0.0661, 0.0879, 0.3748, 0.0913, 0.1115, 0.1008, 0.0797],
        [0.0792, 0.0584, 0.0997, 0.1646, 0.1516, 0.1639, 0.1456, 0.1369],
        [0.0701, 0.0610, 0.0900, 0.1889, 0.1674, 0.1622, 0.1440, 0.1163],
        [0.0769, 0.0626, 0.0997, 0.2276, 0.1480, 0.1472, 0.1332, 0.1047]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1161, 0.1744, 0.2308, 0.1201, 0.0957, 0.0953, 0.1079, 0.0596],
        [0.0779, 0.0961, 0.1262, 0.1324, 0.0940, 0.1141, 0.1283, 0.2310]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1235, 0.2023, 0.2506, 0.1057, 0.0972, 0.0787, 0.0852, 0.0567],
        [0.0897, 0.1117, 0.1471, 0.1284, 0.0939, 0.1230, 0.1122, 0.1942],
        [0.0953, 0.1200, 0.1645, 0.1068, 0.1188, 0.1039, 0.1340, 0.1567]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1175, 0.1942, 0.2551, 0.1046, 0.1120, 0.0791, 0.0784, 0.0591],
        [0.0752, 0.0977, 0.1328, 0.1302, 0.1376, 0.1259, 0.1198, 0.1807],
        [0.0855, 0.1187, 0.1663, 0.1137, 0.1138, 0.1290, 0.1033, 0.1697],
        [0.0763, 0.1057, 0.1380, 0.1246, 0.1108, 0.0968, 0.1232, 0.2246]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1291, 0.2139, 0.2797, 0.0991, 0.0846, 0.0658, 0.0699, 0.0578],
        [0.0857, 0.1178, 0.1583, 0.1141, 0.1259, 0.1211, 0.1168, 0.1603],
        [0.0929, 0.1261, 0.1815, 0.1329, 0.1067, 0.0959, 0.1114, 0.1525],
        [0.0861, 0.1169, 0.1531, 0.1146, 0.1219, 0.1137, 0.1128, 0.1809],
        [0.0854, 0.1252, 0.1535, 0.1080, 0.1004, 0.1081, 0.1158, 0.2035]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0476, 0.0543, 0.1443, 0.1583, 0.0905, 0.2004, 0.1444, 0.1603],
        [0.0280, 0.0270, 0.0387, 0.0782, 0.1313, 0.1351, 0.1481, 0.4136]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0526, 0.0626, 0.2031, 0.1004, 0.0750, 0.1641, 0.1151, 0.2270],
        [0.0270, 0.0267, 0.0404, 0.0663, 0.0490, 0.0948, 0.1674, 0.5285],
        [0.0131, 0.0120, 0.0386, 0.0410, 0.0283, 0.0555, 0.0508, 0.7608]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0525, 0.0636, 0.2099, 0.0606, 0.0550, 0.0663, 0.0783, 0.4138],
        [0.0232, 0.0233, 0.0356, 0.0653, 0.0513, 0.0640, 0.0720, 0.6653],
        [0.0121, 0.0113, 0.0356, 0.0318, 0.0334, 0.0433, 0.0299, 0.8026],
        [0.0098, 0.0090, 0.0156, 0.0346, 0.0280, 0.0416, 0.0317, 0.8297]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0398, 0.0492, 0.1372, 0.0553, 0.0384, 0.0574, 0.0414, 0.5813],
        [0.0226, 0.0228, 0.0353, 0.0436, 0.0281, 0.0474, 0.0611, 0.7391],
        [0.0105, 0.0098, 0.0262, 0.0269, 0.0216, 0.0263, 0.0292, 0.8495],
        [0.0086, 0.0077, 0.0121, 0.0247, 0.0165, 0.0262, 0.0279, 0.8762],
        [0.0081, 0.0072, 0.0095, 0.0254, 0.0262, 0.0256, 0.0250, 0.8730]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1609, 0.0840, 0.1101, 0.1293, 0.1372, 0.1597, 0.0941, 0.1248],
        [0.1333, 0.0993, 0.1461, 0.1315, 0.1287, 0.1420, 0.0973, 0.1218]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1526, 0.0970, 0.1031, 0.1429, 0.1471, 0.1191, 0.0984, 0.1397],
        [0.1285, 0.1015, 0.1225, 0.1898, 0.1752, 0.1034, 0.0867, 0.0924],
        [0.1038, 0.0828, 0.1038, 0.1776, 0.1335, 0.1295, 0.1149, 0.1541]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1639, 0.0929, 0.1179, 0.1285, 0.1589, 0.1237, 0.1053, 0.1089],
        [0.1401, 0.1163, 0.1295, 0.1532, 0.1033, 0.1550, 0.1080, 0.0946],
        [0.1203, 0.1009, 0.1305, 0.1471, 0.1133, 0.1243, 0.1135, 0.1501],
        [0.1106, 0.0942, 0.1087, 0.1715, 0.1544, 0.1221, 0.1041, 0.1344]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1585, 0.1155, 0.1211, 0.1227, 0.1186, 0.1452, 0.1100, 0.1084],
        [0.1321, 0.1112, 0.1127, 0.1505, 0.1154, 0.1610, 0.0992, 0.1180],
        [0.0984, 0.0882, 0.1392, 0.1226, 0.1206, 0.1246, 0.1052, 0.2012],
        [0.0963, 0.0871, 0.1079, 0.1581, 0.1421, 0.1566, 0.1298, 0.1220],
        [0.0982, 0.0918, 0.1141, 0.1447, 0.1516, 0.1199, 0.1064, 0.1733]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1752, 0.1725, 0.1232, 0.0907, 0.1135, 0.1551, 0.0811, 0.0886],
        [0.1619, 0.1616, 0.1040, 0.0997, 0.1238, 0.1477, 0.1046, 0.0967]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1746, 0.1916, 0.1214, 0.1126, 0.1083, 0.1115, 0.0978, 0.0822],
        [0.2186, 0.2278, 0.0818, 0.1018, 0.1079, 0.0821, 0.0896, 0.0904],
        [0.0648, 0.0699, 0.2550, 0.1066, 0.0904, 0.1013, 0.1324, 0.1797]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1922, 0.2240, 0.0906, 0.1091, 0.1076, 0.1062, 0.0854, 0.0848],
        [0.2138, 0.2187, 0.1184, 0.0960, 0.1199, 0.0780, 0.0719, 0.0833],
        [0.0615, 0.0736, 0.2938, 0.0856, 0.1111, 0.1075, 0.1101, 0.1568],
        [0.0526, 0.0519, 0.2171, 0.0960, 0.0836, 0.1183, 0.1361, 0.2444]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1989, 0.2485, 0.1556, 0.0822, 0.0827, 0.0876, 0.0828, 0.0618],
        [0.1681, 0.1970, 0.1165, 0.1203, 0.0844, 0.0976, 0.1213, 0.0947],
        [0.0591, 0.0764, 0.3212, 0.0777, 0.0821, 0.0934, 0.1105, 0.1796],
        [0.0488, 0.0514, 0.2064, 0.0965, 0.0856, 0.1172, 0.1246, 0.2695],
        [0.0353, 0.0388, 0.1311, 0.1065, 0.0823, 0.1207, 0.1001, 0.3852]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
07/29 11:18:45午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 2.8740 (2.7386)	Hard Loss 0.5318 (0.5067)	Soft Loss 0.0061 (0.0061)	Arch Loss 9.8932 (9.5728)	Arch Hard Loss 1.8315 (1.7721)	Arch Soft Loss 0.0081 (0.0084)	Prec@(1,5) (86.9%, 98.8%)	
07/29 11:19:26午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 3.9891 (2.8377)	Hard Loss 0.7382 (0.5250)	Soft Loss 0.0070 (0.0062)	Arch Loss 10.5568 (9.6713)	Arch Hard Loss 1.9544 (1.7904)	Arch Soft Loss 0.0080 (0.0083)	Prec@(1,5) (86.3%, 98.6%)	
07/29 11:20:07午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 3.2476 (2.8625)	Hard Loss 0.6009 (0.5296)	Soft Loss 0.0064 (0.0062)	Arch Loss 9.9503 (9.6768)	Arch Hard Loss 1.8420 (1.7914)	Arch Soft Loss 0.0090 (0.0083)	Prec@(1,5) (86.2%, 98.5%)	
07/29 11:20:48午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 2.5751 (2.9295)	Hard Loss 0.4764 (0.5420)	Soft Loss 0.0060 (0.0062)	Arch Loss 9.6142 (9.6837)	Arch Hard Loss 1.7798 (1.7927)	Arch Soft Loss 0.0084 (0.0083)	Prec@(1,5) (85.7%, 98.5%)	
07/29 11:21:30午後 searchCell_trainer.py:219 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 3.5740 (2.9418)	Hard Loss 0.6614 (0.5443)	Soft Loss 0.0063 (0.0062)	Arch Loss 7.3675 (9.6836)	Arch Hard Loss 1.3637 (1.7926)	Arch Soft Loss 0.0086 (0.0083)	Prec@(1,5) (85.6%, 98.4%)	
