06/29 03:40:34PM parser.py:28 [INFO] 
06/29 03:40:34PM parser.py:29 [INFO] Parameters:
06/29 03:40:34PM parser.py:31 [INFO] DAG_PATH=results/search_cell_KD/cifar10/ONLY_Weight/l0.7T7-20240629-154034/DAG
06/29 03:40:34PM parser.py:31 [INFO] T=7.0
06/29 03:40:34PM parser.py:31 [INFO] ALPHA_LR=0.0003
06/29 03:40:34PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
06/29 03:40:34PM parser.py:31 [INFO] BATCH_SIZE=64
06/29 03:40:34PM parser.py:31 [INFO] CUTOUT_LENGTH=0
06/29 03:40:34PM parser.py:31 [INFO] DATA_PATH=../data/
06/29 03:40:34PM parser.py:31 [INFO] DATASET=cifar10
06/29 03:40:34PM parser.py:31 [INFO] EPOCHS=50
06/29 03:40:34PM parser.py:31 [INFO] EXP_NAME=l0.7T7-20240629-154034
06/29 03:40:34PM parser.py:31 [INFO] GPUS=[0]
06/29 03:40:34PM parser.py:31 [INFO] INIT_CHANNELS=16
06/29 03:40:34PM parser.py:31 [INFO] L=0.7
06/29 03:40:34PM parser.py:31 [INFO] LAYERS=8
06/29 03:40:34PM parser.py:31 [INFO] LOCAL_RANK=0
06/29 03:40:34PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
06/29 03:40:34PM parser.py:31 [INFO] NAME=ONLY_Weight
06/29 03:40:34PM parser.py:31 [INFO] PATH=results/search_cell_KD/cifar10/ONLY_Weight/l0.7T7-20240629-154034
06/29 03:40:34PM parser.py:31 [INFO] PLOT_PATH=results/search_cell_KD/cifar10/ONLY_Weight/l0.7T7-20240629-154034/plots
06/29 03:40:34PM parser.py:31 [INFO] PRINT_FREQ=50
06/29 03:40:34PM parser.py:31 [INFO] RESUME_PATH=None
06/29 03:40:34PM parser.py:31 [INFO] SAVE=l0.7T7
06/29 03:40:34PM parser.py:31 [INFO] SEED=0
06/29 03:40:34PM parser.py:31 [INFO] TEACHER_NAME=densenet121
06/29 03:40:34PM parser.py:31 [INFO] TEACHER_PATH=/home/miura/lab/KD-hdas/results/teacher/cifar10/densenet121/FINETUNE/E25-20240626-151440/best.pth.tar
06/29 03:40:34PM parser.py:31 [INFO] TRAIN_PORTION=0.5
06/29 03:40:34PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
06/29 03:40:34PM parser.py:31 [INFO] W_LR=0.025
06/29 03:40:34PM parser.py:31 [INFO] W_LR_MIN=0.001
06/29 03:40:34PM parser.py:31 [INFO] W_MOMENTUM=0.9
06/29 03:40:34PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
06/29 03:40:34PM parser.py:31 [INFO] WORKERS=4
06/29 03:40:34PM parser.py:32 [INFO] 
06/29 03:40:36PM searchCell_trainer.py:130 [INFO] --> Loaded teacher model 'densenet121' and Freezed parameters)
06/29 03:41:12PM searchCell_trainer.py:136 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1249, 0.1250, 0.1250, 0.1252, 0.1251, 0.1250, 0.1248],
        [0.1250, 0.1248, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1248, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1250, 0.1249, 0.1251, 0.1251],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1249, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1250, 0.1248, 0.1250, 0.1251, 0.1252],
        [0.1251, 0.1252, 0.1251, 0.1252, 0.1248, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1249, 0.1248, 0.1250, 0.1249, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1249, 0.1251, 0.1250, 0.1252, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1251],
        [0.1250, 0.1252, 0.1251, 0.1251, 0.1249, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1249, 0.1249, 0.1251, 0.1251, 0.1251, 0.1252, 0.1249],
        [0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1251, 0.1249, 0.1248, 0.1251, 0.1249, 0.1251, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1250],
        [0.1252, 0.1248, 0.1251, 0.1250, 0.1250, 0.1249, 0.1251, 0.1248],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1249, 0.1249, 0.1250, 0.1251, 0.1250, 0.1252, 0.1250, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1247, 0.1248, 0.1252],
        [0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1249, 0.1248, 0.1252, 0.1250, 0.1248, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1251, 0.1250, 0.1249],
        [0.1250, 0.1252, 0.1251, 0.1250, 0.1249, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1247, 0.1249, 0.1251, 0.1250],
        [0.1248, 0.1250, 0.1250, 0.1250, 0.1253, 0.1252, 0.1250, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1251, 0.1248, 0.1250, 0.1249, 0.1252, 0.1249, 0.1252, 0.1249],
        [0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1249, 0.1252, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1247, 0.1251, 0.1253, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1248],
        [0.1248, 0.1251, 0.1248, 0.1248, 0.1253, 0.1249, 0.1250, 0.1253],
        [0.1250, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1251],
        [0.1250, 0.1251, 0.1248, 0.1252, 0.1249, 0.1249, 0.1251, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1252, 0.1251, 0.1249, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1252, 0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1249, 0.1252, 0.1248, 0.1254, 0.1249, 0.1249, 0.1252],
        [0.1251, 0.1249, 0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250],
        [0.1252, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1250, 0.1249, 0.1250],
        [0.1251, 0.1252, 0.1250, 0.1250, 0.1251, 0.1250, 0.1248, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1252, 0.1251, 0.1249, 0.1248, 0.1251, 0.1249, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1248, 0.1250, 0.1251, 0.1250, 0.1250, 0.1250, 0.1252, 0.1248],
        [0.1249, 0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1248, 0.1249, 0.1251, 0.1251, 0.1251],
        [0.1250, 0.1250, 0.1251, 0.1250, 0.1249, 0.1251, 0.1248, 0.1251],
        [0.1250, 0.1249, 0.1250, 0.1248, 0.1251, 0.1253, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1250, 0.1252],
        [0.1251, 0.1251, 0.1249, 0.1247, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1253, 0.1249, 0.1251, 0.1247, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1252],
        [0.1252, 0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1250, 0.1250, 0.1248, 0.1250, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1249, 0.1252, 0.1250, 0.1249, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1249, 0.1250, 0.1253, 0.1250, 0.1248, 0.1251, 0.1250],
        [0.1251, 0.1248, 0.1249, 0.1249, 0.1251, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1249, 0.1250, 0.1251, 0.1252, 0.1248, 0.1250],
        [0.1251, 0.1251, 0.1252, 0.1248, 0.1249, 0.1248, 0.1251, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1248, 0.1249, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
06/29 03:41:52PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 56.5053 (70.5884)	Hard Loss 1.6471 (2.0577)	Soft Loss 0.0275 (0.0315)	Arch Loss 2.2023 (2.3222)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (25.7%, 78.9%)	
06/29 03:42:30PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 55.9061 (67.4641)	Hard Loss 1.6296 (1.9666)	Soft Loss 0.0305 (0.0305)	Arch Loss 2.2786 (2.3239)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (27.9%, 81.7%)	
06/29 03:43:08PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 59.1793 (65.0609)	Hard Loss 1.7251 (1.8966)	Soft Loss 0.0293 (0.0298)	Arch Loss 2.4392 (2.3277)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (30.5%, 83.3%)	
06/29 03:43:46PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 57.9813 (63.2462)	Hard Loss 1.6902 (1.8437)	Soft Loss 0.0263 (0.0292)	Arch Loss 2.2555 (2.3297)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (32.4%, 84.8%)	
06/29 03:44:25PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 47.1350 (61.7561)	Hard Loss 1.3740 (1.8002)	Soft Loss 0.0249 (0.0288)	Arch Loss 2.4049 (2.3284)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (33.9%, 85.7%)	
06/29 03:45:04PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 44.9815 (60.3360)	Hard Loss 1.3112 (1.7588)	Soft Loss 0.0247 (0.0283)	Arch Loss 2.2601 (2.3288)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (35.4%, 86.6%)	
06/29 03:45:43PM searchCell_trainer.py:216 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 49.5014 (58.8198)	Hard Loss 1.4430 (1.7146)	Soft Loss 0.0226 (0.0277)	Arch Loss 2.4093 (2.3285)	Arch Hard Loss 0.0000 (0.0000)	Arch Soft Loss 0.0000 (0.0000)	Prec@(1,5) (37.1%, 87.5%)	
