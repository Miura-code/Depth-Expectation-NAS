08/11 12:05:51AM parser.py:28 [INFO] 
08/11 12:05:51AM parser.py:29 [INFO] Parameters:
08/11 12:05:51AM parser.py:31 [INFO] DAG_PATH=results/search_stage/cifar10/BASELINE/nonDepthLoss-20240811-000551/DAG
08/11 12:05:51AM parser.py:31 [INFO] ADVANCED=False
08/11 12:05:51AM parser.py:31 [INFO] ALPHA_LR=0.0003
08/11 12:05:51AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
08/11 12:05:51AM parser.py:31 [INFO] BATCH_SIZE=64
08/11 12:05:51AM parser.py:31 [INFO] CUTOUT_LENGTH=0
08/11 12:05:51AM parser.py:31 [INFO] DATA_PATH=../data/
08/11 12:05:51AM parser.py:31 [INFO] DATASET=cifar10
08/11 12:05:51AM parser.py:31 [INFO] EPOCHS=50
08/11 12:05:51AM parser.py:31 [INFO] EXP_NAME=nonDepthLoss-20240811-000551
08/11 12:05:51AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
08/11 12:05:51AM parser.py:31 [INFO] GPUS=[0]
08/11 12:05:51AM parser.py:31 [INFO] INIT_CHANNELS=16
08/11 12:05:51AM parser.py:31 [INFO] LAYERS=20
08/11 12:05:51AM parser.py:31 [INFO] LOCAL_RANK=0
08/11 12:05:51AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
08/11 12:05:51AM parser.py:31 [INFO] NAME=BASELINE
08/11 12:05:51AM parser.py:31 [INFO] PATH=results/search_stage/cifar10/BASELINE/nonDepthLoss-20240811-000551
08/11 12:05:51AM parser.py:31 [INFO] PLOT_PATH=results/search_stage/cifar10/BASELINE/nonDepthLoss-20240811-000551/plots
08/11 12:05:51AM parser.py:31 [INFO] PRINT_FREQ=50
08/11 12:05:51AM parser.py:31 [INFO] RESUME_PATH=None
08/11 12:05:51AM parser.py:31 [INFO] SAVE=nonDepthLoss
08/11 12:05:51AM parser.py:31 [INFO] SEED=0
08/11 12:05:51AM parser.py:31 [INFO] SPEC_CELL=True
08/11 12:05:51AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
08/11 12:05:51AM parser.py:31 [INFO] W_LR=0.025
08/11 12:05:51AM parser.py:31 [INFO] W_LR_MIN=0.001
08/11 12:05:51AM parser.py:31 [INFO] W_MOMENTUM=0.9
08/11 12:05:51AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
08/11 12:05:51AM parser.py:31 [INFO] WORKERS=4
08/11 12:05:51AM parser.py:32 [INFO] 
08/11 12:05:59AM searchStage_trainer.py:99 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:06:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.4168 (2.2608)	Prec@(1,5) (18.6%, 68.8%)	
08/11 12:07:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.0623 (2.2432)	Prec@(1,5) (19.8%, 71.9%)	
08/11 12:08:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.7340 (2.1424)	Prec@(1,5) (22.5%, 75.6%)	
08/11 12:09:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.8258 (2.0641)	Prec@(1,5) (24.7%, 77.8%)	
08/11 12:10:21AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.9802 (2.0058)	Prec@(1,5) (26.4%, 79.5%)	
08/11 12:11:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.6907 (1.9636)	Prec@(1,5) (27.6%, 80.7%)	
08/11 12:12:05AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.6937 (1.9191)	Prec@(1,5) (29.3%, 81.9%)	
08/11 12:12:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.7608 (1.8897)	Prec@(1,5) (30.3%, 82.6%)	
08/11 12:12:51AM searchStage_trainer.py:210 [INFO] Train: [  0/49] Final Prec@1 30.2600%
08/11 12:13:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.6751	Prec@(1,5) (38.2%, 88.3%)
08/11 12:13:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.6775	Prec@(1,5) (38.0%, 88.4%)
08/11 12:13:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.6811	Prec@(1,5) (37.7%, 88.5%)
08/11 12:13:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.6852	Prec@(1,5) (37.9%, 88.4%)
08/11 12:13:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.6805	Prec@(1,5) (37.9%, 88.6%)
08/11 12:13:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.6792	Prec@(1,5) (38.1%, 88.5%)
08/11 12:13:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.6818	Prec@(1,5) (38.0%, 88.5%)
08/11 12:13:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.6819	Prec@(1,5) (37.9%, 88.5%)
08/11 12:13:58AM searchStage_trainer.py:249 [INFO] Valid: [  0/49] Final Prec@1 37.9200%
08/11 12:13:58AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
08/11 12:13:58AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 37.9200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2509, 0.2490, 0.2498, 0.2502],
        [0.2502, 0.2484, 0.2510, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2482, 0.2498, 0.2497],
        [0.2519, 0.2483, 0.2487, 0.2510],
        [0.2508, 0.2497, 0.2499, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2499, 0.2503, 0.2482],
        [0.2496, 0.2502, 0.2495, 0.2507],
        [0.2498, 0.2506, 0.2496, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2504, 0.2497],
        [0.2493, 0.2505, 0.2511, 0.2492],
        [0.2498, 0.2498, 0.2494, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2494, 0.2507, 0.2492],
        [0.2502, 0.2502, 0.2502, 0.2494],
        [0.2495, 0.2495, 0.2496, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2507, 0.2495],
        [0.2496, 0.2490, 0.2505, 0.2510],
        [0.2494, 0.2500, 0.2498, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2493, 0.2515, 0.2496],
        [0.2486, 0.2499, 0.2520, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2489, 0.2515, 0.2486],
        [0.2507, 0.2481, 0.2520, 0.2491],
        [0.2498, 0.2483, 0.2499, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2501, 0.2514, 0.2479],
        [0.2484, 0.2488, 0.2518, 0.2510],
        [0.2476, 0.2494, 0.2510, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2493, 0.2491, 0.2511, 0.2506],
        [0.2485, 0.2498, 0.2505, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2507, 0.2518, 0.2485],
        [0.2489, 0.2490, 0.2516, 0.2505],
        [0.2487, 0.2485, 0.2509, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2501, 0.2519, 0.2500],
        [0.2497, 0.2479, 0.2504, 0.2520],
        [0.2494, 0.2474, 0.2526, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2478, 0.2589, 0.2490],
        [0.2442, 0.2484, 0.2576, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2473, 0.2582, 0.2480],
        [0.2463, 0.2464, 0.2579, 0.2493],
        [0.2457, 0.2478, 0.2539, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2492, 0.2580, 0.2451],
        [0.2440, 0.2466, 0.2548, 0.2546],
        [0.2463, 0.2465, 0.2556, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2467, 0.2524, 0.2526],
        [0.2481, 0.2467, 0.2559, 0.2493],
        [0.2477, 0.2461, 0.2557, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2476, 0.2568, 0.2489],
        [0.2444, 0.2468, 0.2571, 0.2517],
        [0.2448, 0.2469, 0.2553, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2465, 0.2571, 0.2510],
        [0.2471, 0.2456, 0.2576, 0.2496],
        [0.2465, 0.2451, 0.2557, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:14:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.4397 (1.6210)	Prec@(1,5) (39.3%, 88.9%)	
08/11 12:15:41AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.6284 (1.6091)	Prec@(1,5) (39.5%, 89.7%)	
08/11 12:16:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.5987 (1.6042)	Prec@(1,5) (40.2%, 89.4%)	
08/11 12:17:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.6829 (1.5870)	Prec@(1,5) (40.9%, 89.9%)	
08/11 12:18:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.5215 (1.5712)	Prec@(1,5) (41.7%, 90.1%)	
08/11 12:19:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.5270 (1.5566)	Prec@(1,5) (42.3%, 90.5%)	
08/11 12:20:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.5840 (1.5475)	Prec@(1,5) (42.7%, 90.5%)	
08/11 12:20:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.4423 (1.5424)	Prec@(1,5) (43.0%, 90.7%)	
08/11 12:20:43AM searchStage_trainer.py:210 [INFO] Train: [  1/49] Final Prec@1 42.9960%
08/11 12:20:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.5319	Prec@(1,5) (43.4%, 91.7%)
08/11 12:21:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5392	Prec@(1,5) (43.4%, 91.6%)
08/11 12:21:09AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.5306	Prec@(1,5) (43.6%, 91.6%)
08/11 12:21:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.5259	Prec@(1,5) (43.5%, 91.8%)
08/11 12:21:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.5245	Prec@(1,5) (43.7%, 91.8%)
08/11 12:21:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.5257	Prec@(1,5) (43.9%, 91.7%)
08/11 12:21:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.5288	Prec@(1,5) (43.8%, 91.6%)
08/11 12:21:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.5322	Prec@(1,5) (43.6%, 91.6%)
08/11 12:21:50AM searchStage_trainer.py:249 [INFO] Valid: [  1/49] Final Prec@1 43.5880%
08/11 12:21:50AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 6), ('max_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:21:50AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 43.5880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2450, 0.2527, 0.2521],
        [0.2511, 0.2453, 0.2547, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2399, 0.2488, 0.2502],
        [0.2591, 0.2424, 0.2474, 0.2512],
        [0.2549, 0.2465, 0.2487, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2584, 0.2443, 0.2521, 0.2452],
        [0.2511, 0.2477, 0.2494, 0.2518],
        [0.2504, 0.2486, 0.2492, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2492, 0.2514, 0.2493],
        [0.2497, 0.2498, 0.2518, 0.2487],
        [0.2495, 0.2487, 0.2493, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2470, 0.2522, 0.2479],
        [0.2535, 0.2471, 0.2498, 0.2496],
        [0.2515, 0.2478, 0.2474, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2459, 0.2519, 0.2501],
        [0.2516, 0.2458, 0.2505, 0.2521],
        [0.2482, 0.2491, 0.2525, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2463, 0.2544, 0.2494],
        [0.2481, 0.2475, 0.2551, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2469, 0.2541, 0.2455],
        [0.2532, 0.2459, 0.2558, 0.2450],
        [0.2499, 0.2443, 0.2470, 0.2588]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2485, 0.2518, 0.2448],
        [0.2510, 0.2458, 0.2520, 0.2512],
        [0.2468, 0.2472, 0.2502, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2543, 0.2484],
        [0.2487, 0.2464, 0.2534, 0.2514],
        [0.2484, 0.2491, 0.2497, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2482, 0.2582, 0.2429],
        [0.2497, 0.2468, 0.2527, 0.2508],
        [0.2477, 0.2461, 0.2473, 0.2590]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2483, 0.2545, 0.2479],
        [0.2496, 0.2464, 0.2537, 0.2503],
        [0.2487, 0.2459, 0.2503, 0.2550]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2424, 0.2425, 0.2661, 0.2490],
        [0.2430, 0.2436, 0.2640, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2436, 0.2452, 0.2662, 0.2450],
        [0.2444, 0.2425, 0.2645, 0.2486],
        [0.2447, 0.2455, 0.2542, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2479, 0.2647, 0.2405],
        [0.2431, 0.2438, 0.2605, 0.2526],
        [0.2441, 0.2427, 0.2552, 0.2579]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2451, 0.2556, 0.2491],
        [0.2474, 0.2428, 0.2588, 0.2510],
        [0.2470, 0.2432, 0.2559, 0.2539]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2450, 0.2597, 0.2469],
        [0.2442, 0.2424, 0.2609, 0.2525],
        [0.2436, 0.2440, 0.2568, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2438, 0.2625, 0.2469],
        [0.2455, 0.2441, 0.2594, 0.2510],
        [0.2440, 0.2403, 0.2587, 0.2570]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:22:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.5354 (1.4093)	Prec@(1,5) (49.1%, 92.6%)	
08/11 12:23:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.2919 (1.4189)	Prec@(1,5) (48.2%, 92.5%)	
08/11 12:24:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.3609 (1.4101)	Prec@(1,5) (48.3%, 92.8%)	
08/11 12:25:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.3431 (1.4081)	Prec@(1,5) (48.3%, 92.9%)	
08/11 12:26:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.3957 (1.4014)	Prec@(1,5) (48.6%, 93.0%)	
08/11 12:27:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.2298 (1.3896)	Prec@(1,5) (49.0%, 93.2%)	
08/11 12:27:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.3090 (1.3824)	Prec@(1,5) (49.4%, 93.3%)	
08/11 12:28:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.3149 (1.3705)	Prec@(1,5) (49.8%, 93.3%)	
08/11 12:28:35AM searchStage_trainer.py:210 [INFO] Train: [  2/49] Final Prec@1 49.7960%
08/11 12:28:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.3681	Prec@(1,5) (50.4%, 93.2%)
08/11 12:28:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.3623	Prec@(1,5) (51.0%, 93.1%)
08/11 12:29:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.3533	Prec@(1,5) (50.9%, 93.2%)
08/11 12:29:09AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.3487	Prec@(1,5) (51.1%, 93.3%)
08/11 12:29:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.3504	Prec@(1,5) (51.0%, 93.4%)
08/11 12:29:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.3518	Prec@(1,5) (50.7%, 93.4%)
08/11 12:29:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.3558	Prec@(1,5) (50.5%, 93.4%)
08/11 12:29:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.3607	Prec@(1,5) (50.3%, 93.3%)
08/11 12:29:42AM searchStage_trainer.py:249 [INFO] Valid: [  2/49] Final Prec@1 50.2920%
08/11 12:29:42AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:29:42AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 50.2920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2476, 0.2384, 0.2616, 0.2524],
        [0.2479, 0.2379, 0.2647, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2677, 0.2324, 0.2489, 0.2510],
        [0.2647, 0.2379, 0.2495, 0.2480],
        [0.2556, 0.2436, 0.2477, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2624, 0.2357, 0.2623, 0.2396],
        [0.2509, 0.2445, 0.2507, 0.2539],
        [0.2503, 0.2455, 0.2485, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2457, 0.2539, 0.2490],
        [0.2488, 0.2499, 0.2526, 0.2488],
        [0.2492, 0.2465, 0.2506, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2447, 0.2555, 0.2474],
        [0.2548, 0.2434, 0.2530, 0.2487],
        [0.2507, 0.2465, 0.2468, 0.2560]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2426, 0.2541, 0.2488],
        [0.2527, 0.2429, 0.2512, 0.2532],
        [0.2466, 0.2480, 0.2538, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2463, 0.2576, 0.2482],
        [0.2438, 0.2472, 0.2593, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2475, 0.2584, 0.2430],
        [0.2518, 0.2471, 0.2593, 0.2418],
        [0.2460, 0.2452, 0.2461, 0.2627]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2492, 0.2537, 0.2428],
        [0.2475, 0.2463, 0.2532, 0.2530],
        [0.2438, 0.2494, 0.2513, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2476, 0.2568, 0.2485],
        [0.2448, 0.2475, 0.2560, 0.2517],
        [0.2453, 0.2512, 0.2503, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2485, 0.2621, 0.2404],
        [0.2477, 0.2469, 0.2535, 0.2520],
        [0.2458, 0.2458, 0.2469, 0.2615]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2476, 0.2570, 0.2474],
        [0.2473, 0.2462, 0.2564, 0.2501],
        [0.2466, 0.2448, 0.2515, 0.2571]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2387, 0.2391, 0.2732, 0.2490],
        [0.2405, 0.2402, 0.2706, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2426, 0.2433, 0.2722, 0.2419],
        [0.2439, 0.2392, 0.2699, 0.2470],
        [0.2456, 0.2431, 0.2519, 0.2594]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2469, 0.2715, 0.2357],
        [0.2432, 0.2411, 0.2626, 0.2530],
        [0.2431, 0.2396, 0.2552, 0.2621]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2449, 0.2559, 0.2478],
        [0.2471, 0.2407, 0.2581, 0.2541],
        [0.2461, 0.2447, 0.2562, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2440, 0.2615, 0.2456],
        [0.2431, 0.2404, 0.2624, 0.2541],
        [0.2426, 0.2436, 0.2576, 0.2562]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2431, 0.2626, 0.2461],
        [0.2454, 0.2449, 0.2600, 0.2498],
        [0.2433, 0.2392, 0.2583, 0.2592]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:30:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.2794 (1.2445)	Prec@(1,5) (55.7%, 94.3%)	
08/11 12:31:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.1498 (1.2567)	Prec@(1,5) (54.7%, 94.6%)	
08/11 12:32:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.2089 (1.2603)	Prec@(1,5) (54.5%, 94.6%)	
08/11 12:33:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.1058 (1.2645)	Prec@(1,5) (54.4%, 94.5%)	
08/11 12:34:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.1350 (1.2558)	Prec@(1,5) (54.4%, 94.7%)	
08/11 12:34:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.4054 (1.2522)	Prec@(1,5) (54.6%, 94.6%)	
08/11 12:35:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.1718 (1.2466)	Prec@(1,5) (54.9%, 94.6%)	
08/11 12:36:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.1659 (1.2395)	Prec@(1,5) (55.0%, 94.7%)	
08/11 12:36:27AM searchStage_trainer.py:210 [INFO] Train: [  3/49] Final Prec@1 54.9400%
08/11 12:36:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.2184	Prec@(1,5) (56.0%, 94.5%)
08/11 12:36:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.2327	Prec@(1,5) (55.7%, 94.3%)
08/11 12:36:53AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.2347	Prec@(1,5) (55.3%, 94.4%)
08/11 12:37:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.2420	Prec@(1,5) (55.2%, 94.4%)
08/11 12:37:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.2362	Prec@(1,5) (55.3%, 94.6%)
08/11 12:37:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.2407	Prec@(1,5) (55.1%, 94.5%)
08/11 12:37:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.2452	Prec@(1,5) (55.0%, 94.4%)
08/11 12:37:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.2453	Prec@(1,5) (55.0%, 94.5%)
08/11 12:37:34AM searchStage_trainer.py:249 [INFO] Valid: [  3/49] Final Prec@1 55.0280%
08/11 12:37:34AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:37:34AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 55.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2308, 0.2740, 0.2523],
        [0.2411, 0.2288, 0.2801, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2758, 0.2242, 0.2508, 0.2492],
        [0.2711, 0.2280, 0.2516, 0.2493],
        [0.2609, 0.2371, 0.2479, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.2264, 0.2735, 0.2342],
        [0.2521, 0.2376, 0.2549, 0.2554],
        [0.2475, 0.2416, 0.2493, 0.2616]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2433, 0.2583, 0.2458],
        [0.2463, 0.2498, 0.2531, 0.2508],
        [0.2462, 0.2460, 0.2524, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2412, 0.2592, 0.2464],
        [0.2565, 0.2398, 0.2569, 0.2468],
        [0.2510, 0.2425, 0.2451, 0.2614]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2414, 0.2583, 0.2467],
        [0.2504, 0.2434, 0.2527, 0.2535],
        [0.2418, 0.2485, 0.2556, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2460, 0.2622, 0.2463],
        [0.2385, 0.2471, 0.2638, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2484, 0.2633, 0.2384],
        [0.2494, 0.2474, 0.2647, 0.2385],
        [0.2430, 0.2441, 0.2441, 0.2689]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2516, 0.2586, 0.2357],
        [0.2437, 0.2462, 0.2532, 0.2570],
        [0.2404, 0.2484, 0.2521, 0.2591]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2480, 0.2628, 0.2425],
        [0.2429, 0.2462, 0.2585, 0.2524],
        [0.2438, 0.2486, 0.2480, 0.2596]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2442, 0.2700, 0.2357],
        [0.2454, 0.2444, 0.2526, 0.2575],
        [0.2444, 0.2443, 0.2469, 0.2643]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2481, 0.2607, 0.2442],
        [0.2455, 0.2454, 0.2574, 0.2518],
        [0.2438, 0.2451, 0.2513, 0.2598]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2344, 0.2343, 0.2817, 0.2496],
        [0.2357, 0.2369, 0.2801, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2397, 0.2405, 0.2801, 0.2397],
        [0.2407, 0.2371, 0.2780, 0.2442],
        [0.2474, 0.2398, 0.2495, 0.2632]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2436, 0.2804, 0.2319],
        [0.2421, 0.2354, 0.2664, 0.2561],
        [0.2405, 0.2378, 0.2584, 0.2633]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2420, 0.2577, 0.2487],
        [0.2443, 0.2385, 0.2615, 0.2557],
        [0.2423, 0.2451, 0.2604, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2421, 0.2649, 0.2460],
        [0.2403, 0.2362, 0.2682, 0.2553],
        [0.2393, 0.2409, 0.2629, 0.2570]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2396, 0.2670, 0.2459],
        [0.2431, 0.2436, 0.2652, 0.2482],
        [0.2406, 0.2353, 0.2616, 0.2625]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:38:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.1740 (1.1875)	Prec@(1,5) (56.1%, 94.9%)	
08/11 12:39:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.3751 (1.1765)	Prec@(1,5) (57.1%, 95.2%)	
08/11 12:40:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.3468 (1.1785)	Prec@(1,5) (57.2%, 95.2%)	
08/11 12:41:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.1361 (1.1635)	Prec@(1,5) (57.8%, 95.5%)	
08/11 12:41:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.1127 (1.1586)	Prec@(1,5) (57.8%, 95.5%)	
08/11 12:42:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 0.9768 (1.1530)	Prec@(1,5) (58.2%, 95.5%)	
08/11 12:43:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 1.1318 (1.1487)	Prec@(1,5) (58.5%, 95.5%)	
08/11 12:44:19AM searchStage_trainer.py:200 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 0.9487 (1.1428)	Prec@(1,5) (58.9%, 95.5%)	
08/11 12:44:19AM searchStage_trainer.py:210 [INFO] Train: [  4/49] Final Prec@1 58.8720%
08/11 12:44:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.3649	Prec@(1,5) (53.6%, 92.5%)
08/11 12:44:37AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.3585	Prec@(1,5) (54.0%, 93.1%)
08/11 12:44:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.3542	Prec@(1,5) (53.8%, 93.2%)
08/11 12:44:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.3603	Prec@(1,5) (53.6%, 93.0%)
08/11 12:45:02AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.3555	Prec@(1,5) (53.5%, 93.0%)
08/11 12:45:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.3583	Prec@(1,5) (53.7%, 93.0%)
08/11 12:45:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.3621	Prec@(1,5) (53.6%, 92.9%)
08/11 12:45:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.3587	Prec@(1,5) (53.6%, 92.9%)
08/11 12:45:26AM searchStage_trainer.py:249 [INFO] Valid: [  4/49] Final Prec@1 53.6520%
08/11 12:45:26AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:45:27AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 55.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2358, 0.2222, 0.2884, 0.2536],
        [0.2328, 0.2181, 0.3003, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.2174, 0.2545, 0.2485],
        [0.2742, 0.2209, 0.2556, 0.2494],
        [0.2630, 0.2322, 0.2499, 0.2548]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2640, 0.2192, 0.2875, 0.2292],
        [0.2516, 0.2302, 0.2592, 0.2591],
        [0.2438, 0.2388, 0.2522, 0.2652]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2409, 0.2649, 0.2418],
        [0.2424, 0.2494, 0.2550, 0.2532],
        [0.2445, 0.2430, 0.2543, 0.2582]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2410, 0.2648, 0.2404],
        [0.2559, 0.2342, 0.2607, 0.2492],
        [0.2504, 0.2389, 0.2425, 0.2683]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2383, 0.2654, 0.2419],
        [0.2494, 0.2386, 0.2531, 0.2589],
        [0.2395, 0.2456, 0.2593, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2413, 0.2448, 0.2674, 0.2465],
        [0.2339, 0.2472, 0.2698, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2471, 0.2648, 0.2359],
        [0.2493, 0.2447, 0.2675, 0.2385],
        [0.2449, 0.2412, 0.2423, 0.2716]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2518, 0.2626, 0.2298],
        [0.2450, 0.2427, 0.2537, 0.2586],
        [0.2409, 0.2455, 0.2493, 0.2643]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2446, 0.2665, 0.2388],
        [0.2447, 0.2449, 0.2613, 0.2492],
        [0.2474, 0.2435, 0.2426, 0.2665]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2420, 0.2757, 0.2313],
        [0.2458, 0.2420, 0.2507, 0.2614],
        [0.2462, 0.2408, 0.2455, 0.2674]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2462, 0.2626, 0.2457],
        [0.2445, 0.2447, 0.2609, 0.2500],
        [0.2443, 0.2433, 0.2519, 0.2606]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2318, 0.2303, 0.2882, 0.2498],
        [0.2336, 0.2330, 0.2871, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2366, 0.2389, 0.2866, 0.2380],
        [0.2382, 0.2352, 0.2854, 0.2413],
        [0.2478, 0.2377, 0.2477, 0.2668]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.2457, 0.2890, 0.2220],
        [0.2407, 0.2329, 0.2671, 0.2593],
        [0.2369, 0.2351, 0.2577, 0.2704]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2408, 0.2593, 0.2474],
        [0.2430, 0.2373, 0.2647, 0.2551],
        [0.2420, 0.2433, 0.2607, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2406, 0.2683, 0.2450],
        [0.2392, 0.2328, 0.2710, 0.2570],
        [0.2390, 0.2384, 0.2650, 0.2576]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2373, 0.2720, 0.2437],
        [0.2420, 0.2428, 0.2674, 0.2477],
        [0.2382, 0.2311, 0.2637, 0.2669]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:46:19AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.8423 (1.1090)	Prec@(1,5) (60.7%, 95.7%)	
08/11 12:47:10AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.0246 (1.0822)	Prec@(1,5) (61.5%, 95.9%)	
08/11 12:48:02AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 1.2673 (1.0733)	Prec@(1,5) (61.6%, 96.0%)	
08/11 12:48:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 1.1098 (1.0658)	Prec@(1,5) (61.7%, 96.2%)	
08/11 12:49:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 0.9060 (1.0540)	Prec@(1,5) (62.1%, 96.3%)	
08/11 12:50:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 1.2301 (1.0558)	Prec@(1,5) (62.1%, 96.3%)	
08/11 12:51:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 1.2139 (1.0503)	Prec@(1,5) (62.4%, 96.3%)	
08/11 12:52:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.9909 (1.0449)	Prec@(1,5) (62.7%, 96.3%)	
08/11 12:52:12AM searchStage_trainer.py:210 [INFO] Train: [  5/49] Final Prec@1 62.6760%
08/11 12:52:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.1145	Prec@(1,5) (62.1%, 95.5%)
08/11 12:52:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.1382	Prec@(1,5) (61.1%, 95.1%)
08/11 12:52:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.1369	Prec@(1,5) (61.1%, 95.2%)
08/11 12:52:46AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.1460	Prec@(1,5) (60.9%, 95.2%)
08/11 12:52:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.1537	Prec@(1,5) (60.5%, 95.1%)
08/11 12:53:03AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.1503	Prec@(1,5) (60.5%, 95.1%)
08/11 12:53:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.1498	Prec@(1,5) (60.6%, 95.1%)
08/11 12:53:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.1504	Prec@(1,5) (60.7%, 95.0%)
08/11 12:53:19AM searchStage_trainer.py:249 [INFO] Valid: [  5/49] Final Prec@1 60.6520%
08/11 12:53:19AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:53:19AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 60.6520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2273, 0.2110, 0.3076, 0.2541],
        [0.2243, 0.2059, 0.3218, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2810, 0.2085, 0.2602, 0.2502],
        [0.2777, 0.2125, 0.2609, 0.2489],
        [0.2659, 0.2281, 0.2527, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2136, 0.2999, 0.2256],
        [0.2467, 0.2251, 0.2669, 0.2613],
        [0.2403, 0.2347, 0.2562, 0.2688]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2365, 0.2673, 0.2406],
        [0.2399, 0.2469, 0.2570, 0.2562],
        [0.2464, 0.2402, 0.2559, 0.2575]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2351, 0.2699, 0.2395],
        [0.2598, 0.2266, 0.2664, 0.2472],
        [0.2506, 0.2352, 0.2400, 0.2742]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2581, 0.2329, 0.2719, 0.2371],
        [0.2490, 0.2347, 0.2515, 0.2649],
        [0.2381, 0.2442, 0.2612, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2383, 0.2431, 0.2721, 0.2465],
        [0.2327, 0.2442, 0.2745, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2461, 0.2689, 0.2330],
        [0.2510, 0.2432, 0.2702, 0.2356],
        [0.2439, 0.2392, 0.2401, 0.2767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2516, 0.2663, 0.2248],
        [0.2452, 0.2393, 0.2523, 0.2632],
        [0.2410, 0.2443, 0.2489, 0.2658]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2413, 0.2675, 0.2383],
        [0.2427, 0.2447, 0.2613, 0.2513],
        [0.2491, 0.2416, 0.2436, 0.2657]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2439, 0.2815, 0.2276],
        [0.2422, 0.2424, 0.2511, 0.2643],
        [0.2427, 0.2413, 0.2457, 0.2703]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2449, 0.2669, 0.2426],
        [0.2446, 0.2427, 0.2629, 0.2497],
        [0.2450, 0.2416, 0.2483, 0.2651]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2267, 0.2276, 0.2981, 0.2476],
        [0.2280, 0.2292, 0.2957, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2335, 0.2360, 0.2940, 0.2365],
        [0.2345, 0.2309, 0.2929, 0.2417],
        [0.2493, 0.2361, 0.2473, 0.2674]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2397, 0.2433, 0.2986, 0.2184],
        [0.2395, 0.2292, 0.2692, 0.2621],
        [0.2351, 0.2330, 0.2598, 0.2721]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2387, 0.2603, 0.2494],
        [0.2410, 0.2357, 0.2674, 0.2559],
        [0.2405, 0.2441, 0.2636, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2387, 0.2704, 0.2469],
        [0.2373, 0.2299, 0.2749, 0.2579],
        [0.2371, 0.2369, 0.2695, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2352, 0.2757, 0.2426],
        [0.2392, 0.2422, 0.2698, 0.2487],
        [0.2359, 0.2307, 0.2657, 0.2677]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:54:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.9805 (0.9625)	Prec@(1,5) (65.8%, 96.9%)	
08/11 12:55:02AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.9630 (0.9640)	Prec@(1,5) (65.7%, 96.9%)	
08/11 12:55:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 1.0341 (0.9804)	Prec@(1,5) (65.4%, 96.9%)	
08/11 12:56:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.8095 (0.9776)	Prec@(1,5) (65.3%, 97.1%)	
08/11 12:57:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.9503 (0.9719)	Prec@(1,5) (65.6%, 97.1%)	
08/11 12:58:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 1.0147 (0.9733)	Prec@(1,5) (65.6%, 97.0%)	
08/11 12:59:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.8878 (0.9698)	Prec@(1,5) (65.9%, 96.9%)	
08/11 01:00:03AM searchStage_trainer.py:200 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 1.2139 (0.9688)	Prec@(1,5) (65.9%, 97.0%)	
08/11 01:00:04AM searchStage_trainer.py:210 [INFO] Train: [  6/49] Final Prec@1 65.8760%
08/11 01:00:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 0.9201	Prec@(1,5) (66.6%, 97.2%)
08/11 01:00:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 0.9234	Prec@(1,5) (66.6%, 97.1%)
08/11 01:00:30AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 0.9298	Prec@(1,5) (66.2%, 97.1%)
08/11 01:00:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 0.9271	Prec@(1,5) (66.4%, 97.2%)
08/11 01:00:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 0.9265	Prec@(1,5) (66.7%, 97.2%)
08/11 01:00:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 0.9250	Prec@(1,5) (66.8%, 97.2%)
08/11 01:01:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 0.9237	Prec@(1,5) (66.8%, 97.2%)
08/11 01:01:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 0.9234	Prec@(1,5) (67.0%, 97.1%)
08/11 01:01:11AM searchStage_trainer.py:249 [INFO] Valid: [  6/49] Final Prec@1 66.9760%
08/11 01:01:11AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:01:11AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 66.9760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2183, 0.2009, 0.3279, 0.2529],
        [0.2145, 0.1942, 0.3433, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2853, 0.2008, 0.2637, 0.2501],
        [0.2804, 0.2058, 0.2659, 0.2479],
        [0.2672, 0.2252, 0.2539, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2056, 0.3158, 0.2193],
        [0.2459, 0.2173, 0.2734, 0.2634],
        [0.2392, 0.2279, 0.2558, 0.2771]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2341, 0.2718, 0.2390],
        [0.2393, 0.2439, 0.2594, 0.2574],
        [0.2427, 0.2391, 0.2595, 0.2587]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2327, 0.2752, 0.2358],
        [0.2639, 0.2207, 0.2696, 0.2458],
        [0.2492, 0.2302, 0.2374, 0.2832]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2649, 0.2275, 0.2764, 0.2311],
        [0.2499, 0.2316, 0.2506, 0.2679],
        [0.2389, 0.2411, 0.2577, 0.2623]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2351, 0.2418, 0.2771, 0.2461],
        [0.2308, 0.2437, 0.2773, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2454, 0.2729, 0.2300],
        [0.2501, 0.2419, 0.2723, 0.2357],
        [0.2448, 0.2387, 0.2374, 0.2791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2531, 0.2694, 0.2217],
        [0.2429, 0.2405, 0.2550, 0.2616],
        [0.2369, 0.2443, 0.2479, 0.2709]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2416, 0.2707, 0.2355],
        [0.2390, 0.2462, 0.2612, 0.2537],
        [0.2477, 0.2423, 0.2434, 0.2666]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2454, 0.2863, 0.2235],
        [0.2409, 0.2404, 0.2487, 0.2701],
        [0.2414, 0.2422, 0.2459, 0.2705]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2455, 0.2697, 0.2416],
        [0.2434, 0.2435, 0.2636, 0.2495],
        [0.2427, 0.2415, 0.2492, 0.2666]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2223, 0.2268, 0.3055, 0.2454],
        [0.2223, 0.2274, 0.3021, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2313, 0.2344, 0.2986, 0.2357],
        [0.2306, 0.2285, 0.2974, 0.2435],
        [0.2505, 0.2368, 0.2465, 0.2662]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2375, 0.2427, 0.3061, 0.2138],
        [0.2374, 0.2272, 0.2697, 0.2657],
        [0.2339, 0.2311, 0.2605, 0.2745]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2373, 0.2607, 0.2521],
        [0.2402, 0.2338, 0.2688, 0.2571],
        [0.2405, 0.2447, 0.2664, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2403, 0.2391, 0.2770, 0.2436],
        [0.2340, 0.2266, 0.2784, 0.2609],
        [0.2335, 0.2355, 0.2725, 0.2585]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2330, 0.2798, 0.2429],
        [0.2357, 0.2415, 0.2738, 0.2490],
        [0.2345, 0.2302, 0.2678, 0.2675]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:02:03AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.9516 (0.9030)	Prec@(1,5) (68.1%, 97.1%)	
08/11 01:02:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.7274 (0.9050)	Prec@(1,5) (68.4%, 97.2%)	
08/11 01:03:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 1.3922 (0.9103)	Prec@(1,5) (68.0%, 97.1%)	
08/11 01:04:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.9960 (0.9049)	Prec@(1,5) (67.9%, 97.3%)	
08/11 01:05:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.8764 (0.8987)	Prec@(1,5) (68.3%, 97.3%)	
08/11 01:06:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.9644 (0.8971)	Prec@(1,5) (68.4%, 97.4%)	
08/11 01:07:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.8151 (0.9026)	Prec@(1,5) (68.4%, 97.3%)	
08/11 01:07:56AM searchStage_trainer.py:200 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.8024 (0.8996)	Prec@(1,5) (68.4%, 97.3%)	
08/11 01:07:56AM searchStage_trainer.py:210 [INFO] Train: [  7/49] Final Prec@1 68.4320%
08/11 01:08:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.8962	Prec@(1,5) (68.4%, 97.4%)
08/11 01:08:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.8998	Prec@(1,5) (68.8%, 97.5%)
08/11 01:08:22AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.9085	Prec@(1,5) (68.5%, 97.4%)
08/11 01:08:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.9072	Prec@(1,5) (68.5%, 97.3%)
08/11 01:08:39AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.9065	Prec@(1,5) (68.6%, 97.3%)
08/11 01:08:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.9071	Prec@(1,5) (68.5%, 97.3%)
08/11 01:08:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.9056	Prec@(1,5) (68.7%, 97.2%)
08/11 01:09:03AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.9088	Prec@(1,5) (68.5%, 97.2%)
08/11 01:09:03AM searchStage_trainer.py:249 [INFO] Valid: [  7/49] Final Prec@1 68.5200%
08/11 01:09:03AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:09:04AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 68.5200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2114, 0.1890, 0.3496, 0.2501],
        [0.2068, 0.1814, 0.3626, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2844, 0.1949, 0.2694, 0.2513],
        [0.2811, 0.2006, 0.2718, 0.2464],
        [0.2669, 0.2218, 0.2583, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.1967, 0.3297, 0.2190],
        [0.2443, 0.2098, 0.2811, 0.2649],
        [0.2374, 0.2268, 0.2602, 0.2757]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2304, 0.2769, 0.2375],
        [0.2395, 0.2400, 0.2611, 0.2594],
        [0.2435, 0.2352, 0.2621, 0.2592]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2289, 0.2779, 0.2360],
        [0.2700, 0.2148, 0.2740, 0.2412],
        [0.2484, 0.2253, 0.2354, 0.2909]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2670, 0.2225, 0.2823, 0.2282],
        [0.2498, 0.2303, 0.2524, 0.2675],
        [0.2364, 0.2389, 0.2576, 0.2671]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2311, 0.2430, 0.2802, 0.2456],
        [0.2266, 0.2450, 0.2805, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2470, 0.2766, 0.2299],
        [0.2455, 0.2422, 0.2763, 0.2360],
        [0.2446, 0.2400, 0.2374, 0.2781]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2548, 0.2717, 0.2191],
        [0.2424, 0.2414, 0.2588, 0.2575],
        [0.2354, 0.2415, 0.2445, 0.2786]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2394, 0.2712, 0.2351],
        [0.2375, 0.2464, 0.2622, 0.2538],
        [0.2488, 0.2409, 0.2434, 0.2669]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2485, 0.2898, 0.2198],
        [0.2391, 0.2408, 0.2482, 0.2720],
        [0.2409, 0.2409, 0.2445, 0.2738]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2410, 0.2456, 0.2708, 0.2425],
        [0.2439, 0.2435, 0.2644, 0.2483],
        [0.2423, 0.2420, 0.2492, 0.2666]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2165, 0.2245, 0.3148, 0.2442],
        [0.2167, 0.2249, 0.3105, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2282, 0.2330, 0.3043, 0.2345],
        [0.2258, 0.2264, 0.3039, 0.2439],
        [0.2512, 0.2355, 0.2466, 0.2666]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2332, 0.2411, 0.3151, 0.2105],
        [0.2361, 0.2231, 0.2735, 0.2673],
        [0.2325, 0.2292, 0.2611, 0.2772]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2358, 0.2620, 0.2552],
        [0.2389, 0.2322, 0.2719, 0.2571],
        [0.2394, 0.2439, 0.2709, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2382, 0.2362, 0.2835, 0.2421],
        [0.2311, 0.2216, 0.2824, 0.2648],
        [0.2314, 0.2314, 0.2780, 0.2592]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2301, 0.2863, 0.2418],
        [0.2322, 0.2386, 0.2790, 0.2502],
        [0.2313, 0.2284, 0.2720, 0.2683]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:09:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.6389 (0.8909)	Prec@(1,5) (68.7%, 97.1%)	
08/11 01:10:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.6873 (0.8461)	Prec@(1,5) (70.5%, 97.2%)	
08/11 01:11:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.8641 (0.8514)	Prec@(1,5) (70.3%, 97.4%)	
08/11 01:12:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.7441 (0.8456)	Prec@(1,5) (70.5%, 97.5%)	
08/11 01:13:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.9658 (0.8541)	Prec@(1,5) (70.1%, 97.4%)	
08/11 01:14:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.6247 (0.8501)	Prec@(1,5) (70.1%, 97.5%)	
08/11 01:15:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.7885 (0.8515)	Prec@(1,5) (70.1%, 97.5%)	
08/11 01:15:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.7643 (0.8488)	Prec@(1,5) (70.2%, 97.5%)	
08/11 01:15:49AM searchStage_trainer.py:210 [INFO] Train: [  8/49] Final Prec@1 70.1960%
08/11 01:15:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.8795	Prec@(1,5) (69.7%, 97.6%)
08/11 01:16:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.8770	Prec@(1,5) (69.3%, 97.7%)
08/11 01:16:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.8753	Prec@(1,5) (69.2%, 97.4%)
08/11 01:16:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.8737	Prec@(1,5) (69.3%, 97.4%)
08/11 01:16:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.8740	Prec@(1,5) (69.2%, 97.5%)
08/11 01:16:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.8710	Prec@(1,5) (69.2%, 97.5%)
08/11 01:16:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.8728	Prec@(1,5) (69.3%, 97.6%)
08/11 01:16:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.8717	Prec@(1,5) (69.4%, 97.5%)
08/11 01:16:56AM searchStage_trainer.py:249 [INFO] Valid: [  8/49] Final Prec@1 69.3440%
08/11 01:16:56AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:16:56AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 69.3440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2002, 0.1795, 0.3719, 0.2484],
        [0.1955, 0.1716, 0.3850, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2848, 0.1903, 0.2759, 0.2490],
        [0.2811, 0.1948, 0.2771, 0.2470],
        [0.2669, 0.2167, 0.2630, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.1901, 0.3434, 0.2157],
        [0.2422, 0.2015, 0.2895, 0.2668],
        [0.2349, 0.2234, 0.2628, 0.2789]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2267, 0.2816, 0.2361],
        [0.2405, 0.2377, 0.2617, 0.2601],
        [0.2431, 0.2324, 0.2642, 0.2603]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2237, 0.2856, 0.2321],
        [0.2736, 0.2090, 0.2759, 0.2415],
        [0.2460, 0.2220, 0.2343, 0.2977]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2678, 0.2190, 0.2892, 0.2240],
        [0.2489, 0.2285, 0.2526, 0.2700],
        [0.2363, 0.2356, 0.2574, 0.2708]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2284, 0.2451, 0.2821, 0.2444],
        [0.2218, 0.2472, 0.2827, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2450, 0.2772, 0.2291],
        [0.2464, 0.2417, 0.2771, 0.2348],
        [0.2450, 0.2393, 0.2354, 0.2803]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2584, 0.2753, 0.2137],
        [0.2440, 0.2397, 0.2583, 0.2580],
        [0.2342, 0.2388, 0.2419, 0.2851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2373, 0.2737, 0.2339],
        [0.2369, 0.2458, 0.2635, 0.2538],
        [0.2494, 0.2387, 0.2430, 0.2690]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2481, 0.2956, 0.2123],
        [0.2366, 0.2392, 0.2463, 0.2779],
        [0.2404, 0.2365, 0.2420, 0.2811]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2404, 0.2461, 0.2733, 0.2401],
        [0.2434, 0.2438, 0.2653, 0.2474],
        [0.2401, 0.2414, 0.2480, 0.2705]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2150, 0.2201, 0.3206, 0.2443],
        [0.2151, 0.2219, 0.3157, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2297, 0.3086, 0.2325],
        [0.2262, 0.2234, 0.3080, 0.2424],
        [0.2541, 0.2312, 0.2448, 0.2699]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2323, 0.2387, 0.3220, 0.2069],
        [0.2365, 0.2192, 0.2736, 0.2707],
        [0.2311, 0.2274, 0.2626, 0.2789]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2344, 0.2634, 0.2574],
        [0.2375, 0.2323, 0.2758, 0.2544],
        [0.2384, 0.2430, 0.2733, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2363, 0.2335, 0.2880, 0.2422],
        [0.2290, 0.2178, 0.2868, 0.2664],
        [0.2283, 0.2300, 0.2827, 0.2590]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2423, 0.2278, 0.2891, 0.2408],
        [0.2316, 0.2363, 0.2816, 0.2505],
        [0.2303, 0.2265, 0.2733, 0.2699]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:17:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.6052 (0.8107)	Prec@(1,5) (71.9%, 97.7%)	
08/11 01:18:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 1.0161 (0.8021)	Prec@(1,5) (72.2%, 97.8%)	
08/11 01:19:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.5925 (0.8073)	Prec@(1,5) (71.8%, 97.7%)	
08/11 01:20:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.7297 (0.8002)	Prec@(1,5) (72.0%, 97.8%)	
08/11 01:21:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.7409 (0.7986)	Prec@(1,5) (71.9%, 97.9%)	
08/11 01:22:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.8672 (0.8008)	Prec@(1,5) (71.7%, 97.9%)	
08/11 01:22:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.9156 (0.8028)	Prec@(1,5) (71.7%, 97.9%)	
08/11 01:23:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.7716 (0.8059)	Prec@(1,5) (71.7%, 97.9%)	
08/11 01:23:41AM searchStage_trainer.py:210 [INFO] Train: [  9/49] Final Prec@1 71.6640%
08/11 01:23:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.8904	Prec@(1,5) (68.2%, 98.0%)
08/11 01:23:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.8549	Prec@(1,5) (69.6%, 97.9%)
08/11 01:24:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.8571	Prec@(1,5) (69.6%, 97.9%)
08/11 01:24:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.8579	Prec@(1,5) (69.7%, 97.8%)
08/11 01:24:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.8602	Prec@(1,5) (69.7%, 97.8%)
08/11 01:24:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.8612	Prec@(1,5) (69.7%, 97.7%)
08/11 01:24:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.8587	Prec@(1,5) (69.8%, 97.8%)
08/11 01:24:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.8649	Prec@(1,5) (69.7%, 97.7%)
08/11 01:24:48AM searchStage_trainer.py:249 [INFO] Valid: [  9/49] Final Prec@1 69.7000%
08/11 01:24:48AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:24:48AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 69.7000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1909, 0.1714, 0.3879, 0.2498],
        [0.1865, 0.1643, 0.4058, 0.2433]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2809, 0.1870, 0.2843, 0.2478],
        [0.2787, 0.1908, 0.2846, 0.2459],
        [0.2639, 0.2151, 0.2672, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.1813, 0.3587, 0.2149],
        [0.2420, 0.1956, 0.2994, 0.2630],
        [0.2336, 0.2188, 0.2650, 0.2826]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2568, 0.2244, 0.2862, 0.2326],
        [0.2403, 0.2334, 0.2633, 0.2630],
        [0.2412, 0.2300, 0.2669, 0.2619]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2190, 0.2935, 0.2290],
        [0.2762, 0.2017, 0.2794, 0.2426],
        [0.2456, 0.2194, 0.2334, 0.3017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2721, 0.2142, 0.2918, 0.2220],
        [0.2530, 0.2260, 0.2528, 0.2682],
        [0.2360, 0.2324, 0.2557, 0.2759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2258, 0.2463, 0.2826, 0.2453],
        [0.2215, 0.2478, 0.2833, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2443, 0.2810, 0.2281],
        [0.2481, 0.2390, 0.2793, 0.2335],
        [0.2471, 0.2381, 0.2324, 0.2823]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2601, 0.2803, 0.2094],
        [0.2433, 0.2410, 0.2612, 0.2545],
        [0.2299, 0.2365, 0.2387, 0.2948]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2550, 0.2357, 0.2746, 0.2347],
        [0.2371, 0.2443, 0.2658, 0.2528],
        [0.2526, 0.2359, 0.2421, 0.2694]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2434, 0.2471, 0.2985, 0.2110],
        [0.2356, 0.2371, 0.2469, 0.2804],
        [0.2377, 0.2382, 0.2427, 0.2814]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2407, 0.2443, 0.2739, 0.2411],
        [0.2430, 0.2442, 0.2662, 0.2466],
        [0.2378, 0.2423, 0.2496, 0.2703]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2099, 0.2161, 0.3289, 0.2451],
        [0.2114, 0.2190, 0.3245, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2252, 0.2284, 0.3167, 0.2297],
        [0.2212, 0.2211, 0.3162, 0.2414],
        [0.2538, 0.2296, 0.2437, 0.2728]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2306, 0.2355, 0.3310, 0.2029],
        [0.2335, 0.2152, 0.2746, 0.2767],
        [0.2284, 0.2259, 0.2664, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2426, 0.2310, 0.2642, 0.2621],
        [0.2348, 0.2293, 0.2797, 0.2561],
        [0.2383, 0.2423, 0.2791, 0.2403]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2311, 0.2309, 0.2962, 0.2419],
        [0.2243, 0.2126, 0.2931, 0.2699],
        [0.2244, 0.2267, 0.2904, 0.2585]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2402, 0.2245, 0.2940, 0.2413],
        [0.2296, 0.2336, 0.2876, 0.2492],
        [0.2293, 0.2244, 0.2746, 0.2717]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:25:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.7262 (0.7478)	Prec@(1,5) (73.3%, 98.3%)	
08/11 01:26:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 1.2339 (0.7646)	Prec@(1,5) (72.8%, 98.3%)	
08/11 01:27:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.5563 (0.7761)	Prec@(1,5) (72.4%, 98.2%)	
08/11 01:28:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.6152 (0.7716)	Prec@(1,5) (72.7%, 98.1%)	
08/11 01:29:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.7029 (0.7681)	Prec@(1,5) (72.7%, 98.1%)	
08/11 01:29:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.5519 (0.7666)	Prec@(1,5) (72.9%, 98.1%)	
08/11 01:30:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.6016 (0.7611)	Prec@(1,5) (73.0%, 98.2%)	
08/11 01:31:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.8842 (0.7630)	Prec@(1,5) (73.1%, 98.2%)	
08/11 01:31:34AM searchStage_trainer.py:210 [INFO] Train: [ 10/49] Final Prec@1 73.1040%
08/11 01:31:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.7699	Prec@(1,5) (72.7%, 98.0%)
08/11 01:31:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.7849	Prec@(1,5) (72.8%, 97.9%)
08/11 01:32:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.7887	Prec@(1,5) (72.3%, 97.9%)
08/11 01:32:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.7830	Prec@(1,5) (72.6%, 97.9%)
08/11 01:32:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.7781	Prec@(1,5) (73.0%, 97.9%)
08/11 01:32:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.7716	Prec@(1,5) (73.1%, 97.9%)
08/11 01:32:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.7720	Prec@(1,5) (73.1%, 97.9%)
08/11 01:32:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.7708	Prec@(1,5) (73.1%, 97.9%)
08/11 01:32:41AM searchStage_trainer.py:249 [INFO] Valid: [ 10/49] Final Prec@1 73.1360%
08/11 01:32:41AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:32:41AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 73.1360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1837, 0.1649, 0.4009, 0.2505],
        [0.1789, 0.1586, 0.4229, 0.2396]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2791, 0.1839, 0.2903, 0.2467],
        [0.2772, 0.1875, 0.2920, 0.2433],
        [0.2601, 0.2130, 0.2713, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2376, 0.1753, 0.3735, 0.2136],
        [0.2406, 0.1910, 0.3064, 0.2621],
        [0.2309, 0.2179, 0.2674, 0.2839]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2260, 0.2919, 0.2245],
        [0.2385, 0.2329, 0.2630, 0.2656],
        [0.2372, 0.2270, 0.2669, 0.2688]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2581, 0.2161, 0.2989, 0.2269],
        [0.2796, 0.1970, 0.2830, 0.2404],
        [0.2421, 0.2163, 0.2328, 0.3087]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2765, 0.2102, 0.2950, 0.2182],
        [0.2509, 0.2258, 0.2543, 0.2690],
        [0.2342, 0.2302, 0.2549, 0.2807]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2257, 0.2457, 0.2823, 0.2463],
        [0.2221, 0.2475, 0.2836, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2444, 0.2813, 0.2282],
        [0.2489, 0.2376, 0.2792, 0.2342],
        [0.2471, 0.2392, 0.2318, 0.2819]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2619, 0.2823, 0.2067],
        [0.2400, 0.2428, 0.2605, 0.2567],
        [0.2295, 0.2354, 0.2388, 0.2963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2358, 0.2740, 0.2369],
        [0.2363, 0.2452, 0.2646, 0.2539],
        [0.2545, 0.2366, 0.2436, 0.2653]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2441, 0.2985, 0.2103],
        [0.2389, 0.2353, 0.2460, 0.2799],
        [0.2370, 0.2384, 0.2415, 0.2830]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2434, 0.2751, 0.2397],
        [0.2430, 0.2433, 0.2672, 0.2465],
        [0.2367, 0.2422, 0.2489, 0.2722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2101, 0.2118, 0.3328, 0.2453],
        [0.2122, 0.2144, 0.3286, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2255, 0.2266, 0.3224, 0.2254],
        [0.2213, 0.2185, 0.3209, 0.2394],
        [0.2519, 0.2268, 0.2423, 0.2790]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2303, 0.2344, 0.3383, 0.1970],
        [0.2310, 0.2121, 0.2750, 0.2819],
        [0.2247, 0.2245, 0.2675, 0.2833]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2287, 0.2660, 0.2635],
        [0.2336, 0.2258, 0.2819, 0.2587],
        [0.2366, 0.2412, 0.2843, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2272, 0.3038, 0.2398],
        [0.2208, 0.2087, 0.2986, 0.2719],
        [0.2198, 0.2226, 0.2971, 0.2605]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2403, 0.2218, 0.3003, 0.2375],
        [0.2267, 0.2303, 0.2930, 0.2499],
        [0.2264, 0.2194, 0.2775, 0.2767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:33:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.8030 (0.7337)	Prec@(1,5) (73.5%, 98.3%)	
08/11 01:34:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.6589 (0.7411)	Prec@(1,5) (74.0%, 98.3%)	
08/11 01:35:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.7816 (0.7288)	Prec@(1,5) (74.4%, 98.4%)	
08/11 01:36:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.6606 (0.7269)	Prec@(1,5) (74.4%, 98.4%)	
08/11 01:37:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.7946 (0.7182)	Prec@(1,5) (74.7%, 98.4%)	
08/11 01:37:52AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.6063 (0.7172)	Prec@(1,5) (74.7%, 98.4%)	
08/11 01:38:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.7095 (0.7171)	Prec@(1,5) (74.8%, 98.4%)	
08/11 01:39:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.9477 (0.7192)	Prec@(1,5) (74.8%, 98.4%)	
08/11 01:39:27AM searchStage_trainer.py:210 [INFO] Train: [ 11/49] Final Prec@1 74.8040%
08/11 01:39:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.8320	Prec@(1,5) (71.3%, 97.8%)
08/11 01:39:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.8312	Prec@(1,5) (71.5%, 97.8%)
08/11 01:39:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.8228	Prec@(1,5) (71.7%, 97.8%)
08/11 01:40:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.8074	Prec@(1,5) (72.2%, 97.9%)
08/11 01:40:09AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.8047	Prec@(1,5) (72.2%, 97.9%)
08/11 01:40:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.8050	Prec@(1,5) (72.0%, 97.9%)
08/11 01:40:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.8050	Prec@(1,5) (72.0%, 97.9%)
08/11 01:40:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.8038	Prec@(1,5) (72.1%, 97.9%)
08/11 01:40:34AM searchStage_trainer.py:249 [INFO] Valid: [ 11/49] Final Prec@1 72.0640%
08/11 01:40:34AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:40:34AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 73.1360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1758, 0.1580, 0.4153, 0.2509],
        [0.1728, 0.1527, 0.4387, 0.2358]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.1804, 0.2932, 0.2490],
        [0.2760, 0.1855, 0.2973, 0.2413],
        [0.2601, 0.2094, 0.2759, 0.2545]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2331, 0.1711, 0.3878, 0.2080],
        [0.2397, 0.1862, 0.3120, 0.2621],
        [0.2251, 0.2152, 0.2675, 0.2922]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2261, 0.2920, 0.2211],
        [0.2376, 0.2293, 0.2631, 0.2699],
        [0.2371, 0.2250, 0.2686, 0.2692]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2560, 0.2124, 0.3059, 0.2257],
        [0.2843, 0.1920, 0.2842, 0.2395],
        [0.2416, 0.2135, 0.2330, 0.3119]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2808, 0.2078, 0.2963, 0.2151],
        [0.2499, 0.2247, 0.2531, 0.2723],
        [0.2315, 0.2300, 0.2567, 0.2818]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2267, 0.2455, 0.2840, 0.2439],
        [0.2203, 0.2466, 0.2838, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2443, 0.2828, 0.2256],
        [0.2498, 0.2361, 0.2809, 0.2333],
        [0.2436, 0.2397, 0.2312, 0.2855]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2620, 0.2867, 0.2035],
        [0.2398, 0.2416, 0.2619, 0.2566],
        [0.2288, 0.2323, 0.2375, 0.3013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2354, 0.2768, 0.2331],
        [0.2380, 0.2422, 0.2639, 0.2559],
        [0.2565, 0.2337, 0.2416, 0.2682]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2414, 0.3001, 0.2077],
        [0.2416, 0.2334, 0.2461, 0.2789],
        [0.2359, 0.2365, 0.2390, 0.2886]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2415, 0.2761, 0.2392],
        [0.2441, 0.2403, 0.2659, 0.2496],
        [0.2363, 0.2435, 0.2505, 0.2698]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2082, 0.2070, 0.3378, 0.2470],
        [0.2113, 0.2117, 0.3346, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2233, 0.2222, 0.3316, 0.2229],
        [0.2185, 0.2139, 0.3288, 0.2388],
        [0.2530, 0.2225, 0.2425, 0.2820]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2267, 0.2308, 0.3486, 0.1939],
        [0.2284, 0.2093, 0.2767, 0.2856],
        [0.2229, 0.2216, 0.2708, 0.2847]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2392, 0.2272, 0.2689, 0.2647],
        [0.2313, 0.2236, 0.2849, 0.2602],
        [0.2354, 0.2398, 0.2886, 0.2363]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2231, 0.2232, 0.3102, 0.2435],
        [0.2154, 0.2044, 0.3088, 0.2714],
        [0.2155, 0.2206, 0.3063, 0.2576]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2375, 0.2185, 0.3075, 0.2365],
        [0.2221, 0.2277, 0.3003, 0.2500],
        [0.2232, 0.2169, 0.2822, 0.2778]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:41:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.7397 (0.6871)	Prec@(1,5) (75.7%, 98.3%)	
08/11 01:42:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.8048 (0.7029)	Prec@(1,5) (75.7%, 98.3%)	
08/11 01:43:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.7849 (0.6881)	Prec@(1,5) (75.9%, 98.4%)	
08/11 01:44:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.7114 (0.6892)	Prec@(1,5) (76.1%, 98.4%)	
08/11 01:44:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.5946 (0.6863)	Prec@(1,5) (76.1%, 98.4%)	
08/11 01:45:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.6872 (0.6891)	Prec@(1,5) (76.1%, 98.4%)	
08/11 01:46:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.9013 (0.6868)	Prec@(1,5) (76.2%, 98.4%)	
08/11 01:47:18AM searchStage_trainer.py:200 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.5499 (0.6837)	Prec@(1,5) (76.2%, 98.4%)	
08/11 01:47:19AM searchStage_trainer.py:210 [INFO] Train: [ 12/49] Final Prec@1 76.1960%
08/11 01:47:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.7378	Prec@(1,5) (73.4%, 97.8%)
08/11 01:47:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7582	Prec@(1,5) (73.1%, 97.9%)
08/11 01:47:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.7629	Prec@(1,5) (73.1%, 98.0%)
08/11 01:47:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.7671	Prec@(1,5) (73.1%, 97.9%)
08/11 01:48:02AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.7733	Prec@(1,5) (73.0%, 97.9%)
08/11 01:48:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.7740	Prec@(1,5) (73.1%, 97.9%)
08/11 01:48:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.7719	Prec@(1,5) (73.1%, 97.9%)
08/11 01:48:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.7702	Prec@(1,5) (73.2%, 97.9%)
08/11 01:48:26AM searchStage_trainer.py:249 [INFO] Valid: [ 12/49] Final Prec@1 73.1880%
08/11 01:48:26AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:48:26AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 73.1880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1695, 0.1509, 0.4290, 0.2506],
        [0.1678, 0.1461, 0.4535, 0.2326]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2750, 0.1778, 0.2992, 0.2481],
        [0.2727, 0.1820, 0.3041, 0.2412],
        [0.2570, 0.2061, 0.2832, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2250, 0.1652, 0.4032, 0.2066],
        [0.2360, 0.1812, 0.3206, 0.2621],
        [0.2248, 0.2139, 0.2687, 0.2926]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2252, 0.2931, 0.2209],
        [0.2371, 0.2279, 0.2640, 0.2710],
        [0.2336, 0.2262, 0.2722, 0.2679]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2571, 0.2067, 0.3088, 0.2274],
        [0.2898, 0.1873, 0.2859, 0.2370],
        [0.2428, 0.2122, 0.2322, 0.3128]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.2033, 0.3023, 0.2126],
        [0.2504, 0.2245, 0.2525, 0.2726],
        [0.2308, 0.2279, 0.2561, 0.2853]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2259, 0.2460, 0.2863, 0.2418],
        [0.2185, 0.2455, 0.2846, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2421, 0.2844, 0.2258],
        [0.2473, 0.2341, 0.2835, 0.2351],
        [0.2444, 0.2410, 0.2309, 0.2836]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2623, 0.2927, 0.1987],
        [0.2368, 0.2411, 0.2636, 0.2585],
        [0.2277, 0.2294, 0.2362, 0.3068]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2367, 0.2794, 0.2317],
        [0.2380, 0.2418, 0.2656, 0.2546],
        [0.2567, 0.2316, 0.2404, 0.2713]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2418, 0.3035, 0.2044],
        [0.2384, 0.2327, 0.2467, 0.2823],
        [0.2350, 0.2350, 0.2387, 0.2914]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2415, 0.2424, 0.2767, 0.2394],
        [0.2426, 0.2417, 0.2676, 0.2481],
        [0.2347, 0.2436, 0.2511, 0.2705]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2070, 0.2038, 0.3415, 0.2476],
        [0.2096, 0.2092, 0.3399, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2234, 0.2193, 0.3374, 0.2198],
        [0.2178, 0.2106, 0.3333, 0.2382],
        [0.2536, 0.2193, 0.2411, 0.2861]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2244, 0.2280, 0.3579, 0.1897],
        [0.2251, 0.2046, 0.2771, 0.2932],
        [0.2217, 0.2207, 0.2730, 0.2846]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2375, 0.2235, 0.2699, 0.2691],
        [0.2282, 0.2216, 0.2891, 0.2611],
        [0.2355, 0.2385, 0.2942, 0.2318]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2192, 0.2190, 0.3172, 0.2446],
        [0.2122, 0.1990, 0.3168, 0.2720],
        [0.2116, 0.2161, 0.3156, 0.2568]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2349, 0.2164, 0.3133, 0.2355],
        [0.2192, 0.2246, 0.3059, 0.2504],
        [0.2232, 0.2149, 0.2833, 0.2786]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:49:18AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.5998 (0.6773)	Prec@(1,5) (76.3%, 98.5%)	
08/11 01:50:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.7169 (0.6761)	Prec@(1,5) (76.3%, 98.6%)	
08/11 01:51:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.6001 (0.6610)	Prec@(1,5) (76.9%, 98.7%)	
08/11 01:51:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.8361 (0.6660)	Prec@(1,5) (76.7%, 98.7%)	
08/11 01:52:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.4341 (0.6627)	Prec@(1,5) (76.8%, 98.6%)	
08/11 01:53:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.8129 (0.6592)	Prec@(1,5) (77.1%, 98.6%)	
08/11 01:54:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.6175 (0.6584)	Prec@(1,5) (77.1%, 98.5%)	
08/11 01:55:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.5184 (0.6578)	Prec@(1,5) (77.1%, 98.6%)	
08/11 01:55:12AM searchStage_trainer.py:210 [INFO] Train: [ 13/49] Final Prec@1 77.1200%
08/11 01:55:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.7021	Prec@(1,5) (76.7%, 98.2%)
08/11 01:55:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.7169	Prec@(1,5) (75.5%, 98.2%)
08/11 01:55:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.7192	Prec@(1,5) (75.3%, 98.3%)
08/11 01:55:46AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.7034	Prec@(1,5) (75.9%, 98.3%)
08/11 01:55:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.7039	Prec@(1,5) (75.9%, 98.3%)
08/11 01:56:03AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.7086	Prec@(1,5) (75.7%, 98.3%)
08/11 01:56:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.7121	Prec@(1,5) (75.6%, 98.3%)
08/11 01:56:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.7136	Prec@(1,5) (75.4%, 98.3%)
08/11 01:56:19AM searchStage_trainer.py:249 [INFO] Valid: [ 13/49] Final Prec@1 75.4160%
08/11 01:56:19AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:56:19AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 75.4160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1645, 0.1471, 0.4410, 0.2473],
        [0.1627, 0.1412, 0.4636, 0.2325]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2747, 0.1757, 0.3032, 0.2464],
        [0.2718, 0.1782, 0.3087, 0.2413],
        [0.2551, 0.2028, 0.2881, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2184, 0.1593, 0.4183, 0.2041],
        [0.2296, 0.1773, 0.3302, 0.2629],
        [0.2208, 0.2127, 0.2726, 0.2940]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2597, 0.2246, 0.2978, 0.2179],
        [0.2353, 0.2237, 0.2664, 0.2746],
        [0.2331, 0.2245, 0.2742, 0.2682]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2041, 0.3133, 0.2230],
        [0.2909, 0.1839, 0.2890, 0.2361],
        [0.2414, 0.2073, 0.2295, 0.3219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2818, 0.1989, 0.3096, 0.2098],
        [0.2486, 0.2233, 0.2542, 0.2740],
        [0.2283, 0.2264, 0.2574, 0.2879]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2248, 0.2475, 0.2868, 0.2409],
        [0.2173, 0.2458, 0.2846, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2395, 0.2886, 0.2246],
        [0.2465, 0.2312, 0.2885, 0.2338],
        [0.2447, 0.2400, 0.2294, 0.2859]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2630, 0.2963, 0.1952],
        [0.2351, 0.2424, 0.2644, 0.2580],
        [0.2260, 0.2265, 0.2342, 0.3132]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2367, 0.2820, 0.2307],
        [0.2386, 0.2397, 0.2676, 0.2541],
        [0.2556, 0.2311, 0.2400, 0.2733]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2394, 0.3040, 0.2045],
        [0.2374, 0.2322, 0.2473, 0.2831],
        [0.2361, 0.2349, 0.2383, 0.2907]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2403, 0.2419, 0.2769, 0.2410],
        [0.2416, 0.2425, 0.2680, 0.2480],
        [0.2334, 0.2455, 0.2524, 0.2686]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2058, 0.1987, 0.3459, 0.2497],
        [0.2099, 0.2060, 0.3454, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2212, 0.2175, 0.3448, 0.2165],
        [0.2156, 0.2086, 0.3400, 0.2358],
        [0.2513, 0.2158, 0.2411, 0.2918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2244, 0.3694, 0.1864],
        [0.2214, 0.2018, 0.2810, 0.2958],
        [0.2171, 0.2186, 0.2771, 0.2872]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2355, 0.2217, 0.2731, 0.2697],
        [0.2242, 0.2189, 0.2943, 0.2627],
        [0.2328, 0.2363, 0.3005, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2145, 0.2163, 0.3259, 0.2433],
        [0.2071, 0.1942, 0.3242, 0.2744],
        [0.2065, 0.2125, 0.3245, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2318, 0.2128, 0.3206, 0.2348],
        [0.2159, 0.2221, 0.3131, 0.2489],
        [0.2211, 0.2119, 0.2858, 0.2812]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:57:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.7661 (0.6356)	Prec@(1,5) (78.0%, 98.9%)	
08/11 01:58:02AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.4874 (0.6287)	Prec@(1,5) (78.4%, 98.8%)	
08/11 01:58:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.3812 (0.6233)	Prec@(1,5) (78.7%, 98.8%)	
08/11 01:59:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.5251 (0.6204)	Prec@(1,5) (78.8%, 98.9%)	
08/11 02:00:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.6441 (0.6311)	Prec@(1,5) (78.4%, 98.9%)	
08/11 02:01:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.4787 (0.6320)	Prec@(1,5) (78.2%, 98.8%)	
08/11 02:02:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.8033 (0.6277)	Prec@(1,5) (78.4%, 98.8%)	
08/11 02:03:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.5164 (0.6301)	Prec@(1,5) (78.3%, 98.8%)	
08/11 02:03:05AM searchStage_trainer.py:210 [INFO] Train: [ 14/49] Final Prec@1 78.3280%
08/11 02:03:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.7064	Prec@(1,5) (75.9%, 98.6%)
08/11 02:03:22AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.7216	Prec@(1,5) (75.4%, 98.3%)
08/11 02:03:30AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.7300	Prec@(1,5) (74.9%, 98.2%)
08/11 02:03:39AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.7378	Prec@(1,5) (74.8%, 98.3%)
08/11 02:03:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.7375	Prec@(1,5) (74.6%, 98.2%)
08/11 02:03:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.7345	Prec@(1,5) (74.6%, 98.2%)
08/11 02:04:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.7371	Prec@(1,5) (74.4%, 98.2%)
08/11 02:04:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.7358	Prec@(1,5) (74.5%, 98.2%)
08/11 02:04:12AM searchStage_trainer.py:249 [INFO] Valid: [ 14/49] Final Prec@1 74.5440%
08/11 02:04:12AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:04:12AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 75.4160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1588, 0.1429, 0.4511, 0.2473],
        [0.1574, 0.1380, 0.4754, 0.2291]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2732, 0.1738, 0.3081, 0.2450],
        [0.2696, 0.1749, 0.3124, 0.2431],
        [0.2535, 0.1988, 0.2953, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2119, 0.1543, 0.4301, 0.2037],
        [0.2254, 0.1732, 0.3400, 0.2614],
        [0.2194, 0.2119, 0.2752, 0.2936]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.2250, 0.3004, 0.2156],
        [0.2338, 0.2232, 0.2673, 0.2757],
        [0.2340, 0.2234, 0.2727, 0.2700]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2615, 0.2006, 0.3143, 0.2236],
        [0.2946, 0.1798, 0.2935, 0.2321],
        [0.2397, 0.2048, 0.2286, 0.3270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2812, 0.1961, 0.3157, 0.2070],
        [0.2485, 0.2229, 0.2540, 0.2745],
        [0.2256, 0.2245, 0.2583, 0.2916]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2237, 0.2461, 0.2882, 0.2420],
        [0.2185, 0.2441, 0.2861, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2397, 0.2932, 0.2221],
        [0.2448, 0.2299, 0.2935, 0.2318],
        [0.2449, 0.2371, 0.2280, 0.2900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2650, 0.2978, 0.1896],
        [0.2326, 0.2413, 0.2640, 0.2621],
        [0.2234, 0.2245, 0.2326, 0.3195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2384, 0.2854, 0.2273],
        [0.2380, 0.2390, 0.2689, 0.2541],
        [0.2531, 0.2308, 0.2382, 0.2778]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2395, 0.3081, 0.2023],
        [0.2350, 0.2325, 0.2491, 0.2834],
        [0.2340, 0.2331, 0.2383, 0.2945]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2389, 0.2424, 0.2786, 0.2401],
        [0.2374, 0.2441, 0.2689, 0.2496],
        [0.2303, 0.2481, 0.2542, 0.2674]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2019, 0.1951, 0.3535, 0.2495],
        [0.2071, 0.2033, 0.3520, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2171, 0.2155, 0.3525, 0.2149],
        [0.2133, 0.2058, 0.3447, 0.2362],
        [0.2518, 0.2142, 0.2409, 0.2930]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2147, 0.2217, 0.3806, 0.1829],
        [0.2167, 0.1984, 0.2821, 0.3028],
        [0.2140, 0.2172, 0.2822, 0.2867]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2335, 0.2199, 0.2759, 0.2706],
        [0.2200, 0.2169, 0.2991, 0.2640],
        [0.2317, 0.2354, 0.3044, 0.2286]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2095, 0.2139, 0.3349, 0.2417],
        [0.2027, 0.1894, 0.3294, 0.2784],
        [0.2028, 0.2101, 0.3322, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2284, 0.2105, 0.3289, 0.2323],
        [0.2112, 0.2193, 0.3211, 0.2484],
        [0.2182, 0.2086, 0.2885, 0.2847]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:05:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.3951 (0.6143)	Prec@(1,5) (78.6%, 98.8%)	
08/11 02:05:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.6030 (0.5953)	Prec@(1,5) (79.1%, 98.9%)	
08/11 02:06:47AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.6987 (0.5968)	Prec@(1,5) (79.3%, 98.9%)	
08/11 02:07:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.5038 (0.5994)	Prec@(1,5) (79.4%, 98.8%)	
08/11 02:08:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.7478 (0.5992)	Prec@(1,5) (79.4%, 98.8%)	
08/11 02:09:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.8715 (0.6011)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:10:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.5231 (0.6044)	Prec@(1,5) (79.2%, 98.8%)	
08/11 02:10:56AM searchStage_trainer.py:200 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.6301 (0.6036)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:10:57AM searchStage_trainer.py:210 [INFO] Train: [ 15/49] Final Prec@1 79.2960%
08/11 02:11:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.7084	Prec@(1,5) (75.9%, 98.6%)
08/11 02:11:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.7237	Prec@(1,5) (75.5%, 98.6%)
08/11 02:11:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.7109	Prec@(1,5) (75.7%, 98.7%)
08/11 02:11:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.7096	Prec@(1,5) (75.5%, 98.7%)
08/11 02:11:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.7173	Prec@(1,5) (75.2%, 98.6%)
08/11 02:11:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.7153	Prec@(1,5) (75.3%, 98.6%)
08/11 02:11:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.7133	Prec@(1,5) (75.2%, 98.7%)
08/11 02:12:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.7119	Prec@(1,5) (75.2%, 98.6%)
08/11 02:12:04AM searchStage_trainer.py:249 [INFO] Valid: [ 15/49] Final Prec@1 75.2240%
08/11 02:12:04AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:12:04AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 75.4160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1531, 0.1397, 0.4632, 0.2440],
        [0.1517, 0.1338, 0.4861, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2740, 0.1717, 0.3106, 0.2437],
        [0.2707, 0.1724, 0.3165, 0.2405],
        [0.2513, 0.1962, 0.2973, 0.2552]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2044, 0.1488, 0.4454, 0.2015],
        [0.2205, 0.1696, 0.3488, 0.2610],
        [0.2164, 0.2133, 0.2753, 0.2950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2567, 0.2254, 0.3048, 0.2132],
        [0.2322, 0.2226, 0.2694, 0.2758],
        [0.2321, 0.2201, 0.2755, 0.2722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.1985, 0.3188, 0.2219],
        [0.2980, 0.1757, 0.2967, 0.2296],
        [0.2374, 0.2007, 0.2280, 0.3339]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2845, 0.1922, 0.3217, 0.2017],
        [0.2482, 0.2219, 0.2540, 0.2759],
        [0.2213, 0.2219, 0.2579, 0.2989]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2245, 0.2465, 0.2859, 0.2431],
        [0.2208, 0.2442, 0.2842, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2386, 0.2931, 0.2208],
        [0.2460, 0.2283, 0.2943, 0.2314],
        [0.2446, 0.2362, 0.2268, 0.2925]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2643, 0.2989, 0.1895],
        [0.2339, 0.2405, 0.2617, 0.2639],
        [0.2224, 0.2254, 0.2339, 0.3183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2367, 0.2845, 0.2255],
        [0.2389, 0.2374, 0.2680, 0.2556],
        [0.2538, 0.2298, 0.2379, 0.2785]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2383, 0.3099, 0.2002],
        [0.2337, 0.2317, 0.2488, 0.2858],
        [0.2335, 0.2325, 0.2378, 0.2962]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2399, 0.2410, 0.2770, 0.2422],
        [0.2390, 0.2452, 0.2709, 0.2450],
        [0.2284, 0.2480, 0.2543, 0.2693]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1995, 0.1924, 0.3585, 0.2497],
        [0.2047, 0.2011, 0.3579, 0.2364]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2123, 0.2125, 0.3628, 0.2124],
        [0.2094, 0.2026, 0.3535, 0.2345],
        [0.2505, 0.2118, 0.2409, 0.2967]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2093, 0.2181, 0.3924, 0.1802],
        [0.2130, 0.1959, 0.2858, 0.3053],
        [0.2098, 0.2157, 0.2863, 0.2883]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2316, 0.2169, 0.2796, 0.2719],
        [0.2160, 0.2129, 0.3041, 0.2670],
        [0.2297, 0.2340, 0.3108, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2026, 0.2083, 0.3433, 0.2458],
        [0.1963, 0.1852, 0.3406, 0.2779],
        [0.1986, 0.2077, 0.3440, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2245, 0.2068, 0.3385, 0.2302],
        [0.2079, 0.2137, 0.3285, 0.2499],
        [0.2177, 0.2065, 0.2904, 0.2854]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:12:56AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.6658 (0.6042)	Prec@(1,5) (79.2%, 98.7%)	
08/11 02:13:47AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.3014 (0.5753)	Prec@(1,5) (79.9%, 99.0%)	
08/11 02:14:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.5943 (0.5781)	Prec@(1,5) (79.8%, 99.0%)	
08/11 02:15:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.7206 (0.5694)	Prec@(1,5) (80.0%, 99.0%)	
08/11 02:16:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.5243 (0.5709)	Prec@(1,5) (80.0%, 99.0%)	
08/11 02:17:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.6033 (0.5745)	Prec@(1,5) (80.0%, 99.0%)	
08/11 02:18:05AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.6062 (0.5779)	Prec@(1,5) (79.9%, 99.0%)	
08/11 02:18:47AM searchStage_trainer.py:200 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.5517 (0.5782)	Prec@(1,5) (79.9%, 99.0%)	
08/11 02:18:47AM searchStage_trainer.py:210 [INFO] Train: [ 16/49] Final Prec@1 79.8800%
08/11 02:18:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.6535	Prec@(1,5) (77.8%, 98.7%)
08/11 02:19:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6454	Prec@(1,5) (77.6%, 98.5%)
08/11 02:19:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.6591	Prec@(1,5) (77.5%, 98.5%)
08/11 02:19:22AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.6581	Prec@(1,5) (77.5%, 98.5%)
08/11 02:19:30AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6562	Prec@(1,5) (77.5%, 98.5%)
08/11 02:19:39AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6615	Prec@(1,5) (77.4%, 98.5%)
08/11 02:19:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6627	Prec@(1,5) (77.5%, 98.5%)
08/11 02:19:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6607	Prec@(1,5) (77.5%, 98.5%)
08/11 02:19:54AM searchStage_trainer.py:249 [INFO] Valid: [ 16/49] Final Prec@1 77.5120%
08/11 02:19:54AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:19:54AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 77.5120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1462, 0.1357, 0.4750, 0.2431],
        [0.1458, 0.1308, 0.4984, 0.2250]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2734, 0.1707, 0.3142, 0.2417],
        [0.2687, 0.1690, 0.3187, 0.2435],
        [0.2485, 0.1943, 0.3042, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1992, 0.1450, 0.4549, 0.2010],
        [0.2148, 0.1646, 0.3611, 0.2595],
        [0.2133, 0.2119, 0.2798, 0.2950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2227, 0.3097, 0.2121],
        [0.2316, 0.2212, 0.2712, 0.2759],
        [0.2299, 0.2182, 0.2791, 0.2729]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.1966, 0.3268, 0.2194],
        [0.2975, 0.1714, 0.3025, 0.2286],
        [0.2324, 0.1998, 0.2296, 0.3382]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.1875, 0.3332, 0.1970],
        [0.2450, 0.2234, 0.2540, 0.2776],
        [0.2159, 0.2194, 0.2603, 0.3044]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2211, 0.2489, 0.2865, 0.2435],
        [0.2186, 0.2470, 0.2844, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2411, 0.2968, 0.2151],
        [0.2435, 0.2287, 0.2995, 0.2283],
        [0.2422, 0.2324, 0.2239, 0.3015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2668, 0.3004, 0.1852],
        [0.2350, 0.2386, 0.2599, 0.2665],
        [0.2202, 0.2231, 0.2323, 0.3244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2390, 0.2872, 0.2220],
        [0.2401, 0.2358, 0.2682, 0.2560],
        [0.2514, 0.2294, 0.2363, 0.2829]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2370, 0.3117, 0.1986],
        [0.2341, 0.2295, 0.2479, 0.2884],
        [0.2341, 0.2317, 0.2372, 0.2970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2378, 0.2415, 0.2795, 0.2412],
        [0.2377, 0.2462, 0.2723, 0.2437],
        [0.2275, 0.2471, 0.2541, 0.2713]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1988, 0.1908, 0.3616, 0.2488],
        [0.2025, 0.1998, 0.3612, 0.2365]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2093, 0.2099, 0.3721, 0.2087],
        [0.2060, 0.1982, 0.3602, 0.2356],
        [0.2487, 0.2111, 0.2407, 0.2995]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2047, 0.2125, 0.4042, 0.1786],
        [0.2093, 0.1938, 0.2905, 0.3063],
        [0.2078, 0.2144, 0.2890, 0.2888]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2140, 0.2846, 0.2722],
        [0.2117, 0.2099, 0.3103, 0.2682],
        [0.2267, 0.2313, 0.3179, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1959, 0.2038, 0.3554, 0.2449],
        [0.1894, 0.1795, 0.3533, 0.2779],
        [0.1920, 0.2015, 0.3573, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2196, 0.2022, 0.3489, 0.2293],
        [0.2039, 0.2097, 0.3384, 0.2479],
        [0.2167, 0.2040, 0.2910, 0.2883]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:20:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.4252 (0.5437)	Prec@(1,5) (81.5%, 99.0%)	
08/11 02:21:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.5858 (0.5432)	Prec@(1,5) (81.4%, 99.0%)	
08/11 02:22:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.6090 (0.5353)	Prec@(1,5) (81.5%, 99.1%)	
08/11 02:23:20AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.5273 (0.5393)	Prec@(1,5) (81.4%, 99.1%)	
08/11 02:24:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.5701 (0.5415)	Prec@(1,5) (81.2%, 99.1%)	
08/11 02:25:03AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.5836 (0.5430)	Prec@(1,5) (81.3%, 99.0%)	
08/11 02:25:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.6289 (0.5456)	Prec@(1,5) (81.3%, 99.0%)	
08/11 02:26:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.3934 (0.5494)	Prec@(1,5) (81.2%, 99.0%)	
08/11 02:26:37AM searchStage_trainer.py:210 [INFO] Train: [ 17/49] Final Prec@1 81.2000%
08/11 02:26:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6626	Prec@(1,5) (76.9%, 98.1%)
08/11 02:26:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6669	Prec@(1,5) (77.2%, 98.3%)
08/11 02:27:02AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6666	Prec@(1,5) (77.2%, 98.3%)
08/11 02:27:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6677	Prec@(1,5) (77.1%, 98.3%)
08/11 02:27:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6707	Prec@(1,5) (76.9%, 98.3%)
08/11 02:27:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6685	Prec@(1,5) (77.0%, 98.3%)
08/11 02:27:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6648	Prec@(1,5) (77.1%, 98.3%)
08/11 02:27:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6664	Prec@(1,5) (77.1%, 98.3%)
08/11 02:27:43AM searchStage_trainer.py:249 [INFO] Valid: [ 17/49] Final Prec@1 77.0960%
08/11 02:27:43AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:27:44AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 77.5120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1418, 0.1335, 0.4814, 0.2433],
        [0.1422, 0.1290, 0.5070, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2728, 0.1681, 0.3146, 0.2445],
        [0.2684, 0.1671, 0.3218, 0.2427],
        [0.2476, 0.1913, 0.3106, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1942, 0.1406, 0.4673, 0.1980],
        [0.2118, 0.1604, 0.3665, 0.2614],
        [0.2106, 0.2112, 0.2824, 0.2958]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2228, 0.3117, 0.2099],
        [0.2286, 0.2204, 0.2725, 0.2785],
        [0.2276, 0.2182, 0.2814, 0.2728]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.1943, 0.3307, 0.2162],
        [0.2972, 0.1676, 0.3071, 0.2281],
        [0.2291, 0.1970, 0.2294, 0.3445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2857, 0.1860, 0.3349, 0.1934],
        [0.2434, 0.2246, 0.2534, 0.2785],
        [0.2123, 0.2170, 0.2606, 0.3100]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2210, 0.2501, 0.2863, 0.2426],
        [0.2192, 0.2465, 0.2830, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2384, 0.2982, 0.2183],
        [0.2417, 0.2269, 0.3010, 0.2305],
        [0.2436, 0.2348, 0.2256, 0.2960]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2675, 0.3013, 0.1846],
        [0.2326, 0.2395, 0.2601, 0.2678],
        [0.2179, 0.2238, 0.2338, 0.3245]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2380, 0.2879, 0.2213],
        [0.2377, 0.2371, 0.2697, 0.2555],
        [0.2503, 0.2286, 0.2371, 0.2841]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2355, 0.3147, 0.1980],
        [0.2320, 0.2283, 0.2487, 0.2910],
        [0.2347, 0.2316, 0.2377, 0.2960]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2350, 0.2419, 0.2817, 0.2414],
        [0.2379, 0.2457, 0.2743, 0.2421],
        [0.2255, 0.2467, 0.2552, 0.2726]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1942, 0.1873, 0.3712, 0.2473],
        [0.1976, 0.1967, 0.3698, 0.2359]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2038, 0.2048, 0.3846, 0.2069],
        [0.2002, 0.1931, 0.3710, 0.2357],
        [0.2479, 0.2108, 0.2412, 0.3001]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1988, 0.2059, 0.4190, 0.1764],
        [0.2049, 0.1910, 0.2965, 0.3076],
        [0.2037, 0.2113, 0.2956, 0.2895]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2243, 0.2088, 0.2890, 0.2780],
        [0.2067, 0.2072, 0.3192, 0.2669],
        [0.2238, 0.2300, 0.3274, 0.2188]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1883, 0.1978, 0.3664, 0.2475],
        [0.1815, 0.1729, 0.3672, 0.2783],
        [0.1873, 0.1975, 0.3715, 0.2438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2138, 0.1970, 0.3599, 0.2294],
        [0.1988, 0.2049, 0.3511, 0.2452],
        [0.2155, 0.2020, 0.2925, 0.2900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:28:35AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.4120 (0.5057)	Prec@(1,5) (82.5%, 99.2%)	
08/11 02:29:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.4431 (0.5200)	Prec@(1,5) (82.1%, 99.2%)	
08/11 02:30:18AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.3802 (0.5337)	Prec@(1,5) (81.7%, 99.1%)	
08/11 02:31:10AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.4606 (0.5328)	Prec@(1,5) (81.6%, 99.1%)	
08/11 02:32:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.6774 (0.5318)	Prec@(1,5) (81.7%, 99.1%)	
08/11 02:32:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.4735 (0.5337)	Prec@(1,5) (81.6%, 99.1%)	
08/11 02:33:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.5253 (0.5349)	Prec@(1,5) (81.5%, 99.1%)	
08/11 02:34:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.2878 (0.5305)	Prec@(1,5) (81.6%, 99.1%)	
08/11 02:34:27AM searchStage_trainer.py:210 [INFO] Train: [ 18/49] Final Prec@1 81.6480%
08/11 02:34:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6374	Prec@(1,5) (78.5%, 98.8%)
08/11 02:34:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.6218	Prec@(1,5) (78.8%, 98.8%)
08/11 02:34:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.6258	Prec@(1,5) (78.7%, 98.7%)
08/11 02:35:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.6153	Prec@(1,5) (78.8%, 98.8%)
08/11 02:35:09AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.6194	Prec@(1,5) (78.8%, 98.8%)
08/11 02:35:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.6203	Prec@(1,5) (78.8%, 98.7%)
08/11 02:35:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.6242	Prec@(1,5) (78.7%, 98.8%)
08/11 02:35:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.6215	Prec@(1,5) (78.8%, 98.8%)
08/11 02:35:33AM searchStage_trainer.py:249 [INFO] Valid: [ 18/49] Final Prec@1 78.7600%
08/11 02:35:34AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:35:34AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 78.7600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1384, 0.1322, 0.4889, 0.2405],
        [0.1390, 0.1273, 0.5122, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2711, 0.1675, 0.3161, 0.2453],
        [0.2652, 0.1665, 0.3248, 0.2434],
        [0.2449, 0.1914, 0.3153, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1878, 0.1379, 0.4780, 0.1963],
        [0.2089, 0.1581, 0.3727, 0.2603],
        [0.2087, 0.2108, 0.2831, 0.2973]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2204, 0.3144, 0.2098],
        [0.2278, 0.2210, 0.2721, 0.2791],
        [0.2264, 0.2183, 0.2834, 0.2719]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.1921, 0.3340, 0.2152],
        [0.2966, 0.1644, 0.3154, 0.2236],
        [0.2236, 0.1937, 0.2287, 0.3540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2868, 0.1830, 0.3421, 0.1881],
        [0.2415, 0.2236, 0.2535, 0.2813],
        [0.2083, 0.2127, 0.2619, 0.3171]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2193, 0.2516, 0.2858, 0.2433],
        [0.2194, 0.2478, 0.2822, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.2414, 0.3022, 0.2143],
        [0.2388, 0.2285, 0.3040, 0.2287],
        [0.2422, 0.2329, 0.2231, 0.3018]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2678, 0.3056, 0.1826],
        [0.2290, 0.2385, 0.2599, 0.2726],
        [0.2175, 0.2240, 0.2354, 0.3231]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2404, 0.2907, 0.2174],
        [0.2366, 0.2363, 0.2698, 0.2573],
        [0.2491, 0.2272, 0.2361, 0.2875]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2370, 0.3178, 0.1968],
        [0.2300, 0.2265, 0.2505, 0.2930],
        [0.2353, 0.2309, 0.2376, 0.2962]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2360, 0.2416, 0.2832, 0.2392],
        [0.2384, 0.2448, 0.2752, 0.2417],
        [0.2247, 0.2454, 0.2545, 0.2754]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1906, 0.1837, 0.3787, 0.2471],
        [0.1939, 0.1940, 0.3777, 0.2343]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1991, 0.2011, 0.3964, 0.2035],
        [0.1946, 0.1891, 0.3817, 0.2346],
        [0.2450, 0.2089, 0.2423, 0.3038]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1908, 0.1995, 0.4377, 0.1721],
        [0.1985, 0.1851, 0.3008, 0.3155],
        [0.1993, 0.2092, 0.3036, 0.2879]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2213, 0.2056, 0.2934, 0.2797],
        [0.2004, 0.2042, 0.3290, 0.2664],
        [0.2200, 0.2268, 0.3367, 0.2165]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1810, 0.1917, 0.3796, 0.2477],
        [0.1730, 0.1656, 0.3816, 0.2798],
        [0.1806, 0.1925, 0.3875, 0.2394]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2066, 0.1926, 0.3741, 0.2268],
        [0.1915, 0.1998, 0.3633, 0.2454],
        [0.2136, 0.2001, 0.2956, 0.2907]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:36:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.4423 (0.5103)	Prec@(1,5) (81.9%, 99.2%)	
08/11 02:37:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.4504 (0.5038)	Prec@(1,5) (82.5%, 99.1%)	
08/11 02:38:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.3290 (0.5022)	Prec@(1,5) (82.7%, 99.1%)	
08/11 02:38:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4874 (0.5015)	Prec@(1,5) (82.6%, 99.1%)	
08/11 02:39:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.3929 (0.5079)	Prec@(1,5) (82.4%, 99.1%)	
08/11 02:40:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.3740 (0.5126)	Prec@(1,5) (82.3%, 99.2%)	
08/11 02:41:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.3500 (0.5145)	Prec@(1,5) (82.2%, 99.2%)	
08/11 02:42:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.5894 (0.5116)	Prec@(1,5) (82.3%, 99.2%)	
08/11 02:42:16AM searchStage_trainer.py:210 [INFO] Train: [ 19/49] Final Prec@1 82.2680%
08/11 02:42:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6489	Prec@(1,5) (78.0%, 99.0%)
08/11 02:42:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6647	Prec@(1,5) (77.6%, 98.8%)
08/11 02:42:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6702	Prec@(1,5) (77.7%, 98.8%)
08/11 02:42:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6769	Prec@(1,5) (77.5%, 98.8%)
08/11 02:42:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6738	Prec@(1,5) (77.6%, 98.7%)
08/11 02:43:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6686	Prec@(1,5) (77.7%, 98.8%)
08/11 02:43:16AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6701	Prec@(1,5) (77.7%, 98.8%)
08/11 02:43:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6681	Prec@(1,5) (77.8%, 98.8%)
08/11 02:43:23AM searchStage_trainer.py:249 [INFO] Valid: [ 19/49] Final Prec@1 77.8080%
08/11 02:43:23AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:43:23AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 78.7600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1350, 0.1318, 0.4975, 0.2357],
        [0.1349, 0.1259, 0.5161, 0.2231]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2738, 0.1651, 0.3173, 0.2439],
        [0.2659, 0.1631, 0.3268, 0.2442],
        [0.2455, 0.1870, 0.3191, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1835, 0.1369, 0.4841, 0.1955],
        [0.2048, 0.1550, 0.3796, 0.2607],
        [0.2085, 0.2097, 0.2854, 0.2964]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2201, 0.3162, 0.2102],
        [0.2271, 0.2228, 0.2748, 0.2752],
        [0.2228, 0.2170, 0.2863, 0.2740]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2590, 0.1907, 0.3367, 0.2135],
        [0.3011, 0.1607, 0.3168, 0.2215],
        [0.2200, 0.1907, 0.2279, 0.3615]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2875, 0.1798, 0.3478, 0.1849],
        [0.2375, 0.2234, 0.2555, 0.2835],
        [0.2056, 0.2109, 0.2631, 0.3204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2180, 0.2536, 0.2849, 0.2435],
        [0.2207, 0.2492, 0.2794, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2436, 0.2411, 0.3032, 0.2121],
        [0.2383, 0.2275, 0.3061, 0.2282],
        [0.2411, 0.2320, 0.2215, 0.3054]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.2696, 0.3095, 0.1775],
        [0.2250, 0.2382, 0.2596, 0.2772],
        [0.2155, 0.2219, 0.2342, 0.3285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2417, 0.2930, 0.2155],
        [0.2355, 0.2328, 0.2696, 0.2621],
        [0.2465, 0.2287, 0.2394, 0.2854]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2382, 0.3221, 0.1934],
        [0.2305, 0.2257, 0.2508, 0.2930],
        [0.2354, 0.2263, 0.2357, 0.3026]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2337, 0.2431, 0.2865, 0.2367],
        [0.2355, 0.2446, 0.2774, 0.2424],
        [0.2221, 0.2456, 0.2557, 0.2767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1865, 0.1825, 0.3884, 0.2427],
        [0.1878, 0.1912, 0.3848, 0.2362]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1954, 0.1983, 0.4071, 0.1992],
        [0.1894, 0.1852, 0.3906, 0.2349],
        [0.2432, 0.2074, 0.2414, 0.3080]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1830, 0.1938, 0.4541, 0.1691],
        [0.1932, 0.1820, 0.3044, 0.3204],
        [0.1962, 0.2089, 0.3083, 0.2865]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2162, 0.2025, 0.2986, 0.2828],
        [0.1969, 0.2016, 0.3374, 0.2642],
        [0.2166, 0.2256, 0.3440, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1742, 0.1871, 0.3907, 0.2480],
        [0.1652, 0.1603, 0.3946, 0.2799],
        [0.1749, 0.1887, 0.4011, 0.2353]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2004, 0.1883, 0.3857, 0.2256],
        [0.1861, 0.1948, 0.3738, 0.2453],
        [0.2144, 0.1995, 0.2956, 0.2904]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:44:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.4909 (0.4732)	Prec@(1,5) (83.2%, 99.5%)	
08/11 02:45:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.6949 (0.4750)	Prec@(1,5) (83.2%, 99.4%)	
08/11 02:45:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4684 (0.4709)	Prec@(1,5) (83.4%, 99.4%)	
08/11 02:46:49AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.5036 (0.4741)	Prec@(1,5) (83.5%, 99.3%)	
08/11 02:47:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4806 (0.4802)	Prec@(1,5) (83.2%, 99.3%)	
08/11 02:48:32AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.3554 (0.4834)	Prec@(1,5) (83.0%, 99.3%)	
08/11 02:49:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.5389 (0.4854)	Prec@(1,5) (83.0%, 99.3%)	
08/11 02:50:05AM searchStage_trainer.py:200 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.3982 (0.4887)	Prec@(1,5) (82.9%, 99.3%)	
08/11 02:50:05AM searchStage_trainer.py:210 [INFO] Train: [ 20/49] Final Prec@1 82.9360%
08/11 02:50:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5716	Prec@(1,5) (79.0%, 99.1%)
08/11 02:50:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5759	Prec@(1,5) (79.7%, 99.2%)
08/11 02:50:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5839	Prec@(1,5) (79.7%, 99.1%)
08/11 02:50:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.5914	Prec@(1,5) (79.5%, 99.0%)
08/11 02:50:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.5846	Prec@(1,5) (79.8%, 99.0%)
08/11 02:50:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.5867	Prec@(1,5) (79.8%, 99.0%)
08/11 02:51:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.5875	Prec@(1,5) (79.8%, 99.0%)
08/11 02:51:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.5912	Prec@(1,5) (79.7%, 99.0%)
08/11 02:51:12AM searchStage_trainer.py:249 [INFO] Valid: [ 20/49] Final Prec@1 79.6760%
08/11 02:51:12AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:51:12AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 79.6760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1331, 0.1316, 0.5001, 0.2352],
        [0.1323, 0.1247, 0.5217, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.1637, 0.3216, 0.2442],
        [0.2646, 0.1622, 0.3309, 0.2422],
        [0.2412, 0.1864, 0.3237, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1780, 0.1345, 0.4939, 0.1936],
        [0.2003, 0.1527, 0.3850, 0.2620],
        [0.2056, 0.2107, 0.2880, 0.2957]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2179, 0.3209, 0.2126],
        [0.2265, 0.2223, 0.2801, 0.2711],
        [0.2228, 0.2171, 0.2865, 0.2736]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.1881, 0.3438, 0.2122],
        [0.3025, 0.1569, 0.3200, 0.2206],
        [0.2173, 0.1902, 0.2281, 0.3644]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2856, 0.1774, 0.3538, 0.1832],
        [0.2344, 0.2233, 0.2575, 0.2848],
        [0.2045, 0.2108, 0.2633, 0.3215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2196, 0.2505, 0.2840, 0.2460],
        [0.2223, 0.2489, 0.2800, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2430, 0.2422, 0.3048, 0.2099],
        [0.2367, 0.2274, 0.3087, 0.2272],
        [0.2365, 0.2322, 0.2223, 0.3090]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2718, 0.3112, 0.1728],
        [0.2224, 0.2387, 0.2606, 0.2782],
        [0.2111, 0.2190, 0.2318, 0.3381]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2414, 0.2943, 0.2144],
        [0.2332, 0.2342, 0.2721, 0.2605],
        [0.2469, 0.2262, 0.2386, 0.2883]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2388, 0.3247, 0.1919],
        [0.2282, 0.2255, 0.2524, 0.2940],
        [0.2351, 0.2253, 0.2355, 0.3041]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2335, 0.2432, 0.2881, 0.2352],
        [0.2355, 0.2452, 0.2798, 0.2394],
        [0.2192, 0.2439, 0.2557, 0.2812]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1831, 0.1802, 0.3955, 0.2411],
        [0.1838, 0.1893, 0.3910, 0.2358]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1898, 0.1962, 0.4164, 0.1975],
        [0.1848, 0.1829, 0.3980, 0.2342],
        [0.2393, 0.2072, 0.2444, 0.3091]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1763, 0.1879, 0.4686, 0.1673],
        [0.1870, 0.1785, 0.3089, 0.3256],
        [0.1926, 0.2087, 0.3163, 0.2824]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2110, 0.1990, 0.3025, 0.2875],
        [0.1918, 0.1989, 0.3455, 0.2638],
        [0.2147, 0.2253, 0.3512, 0.2088]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1664, 0.1824, 0.4027, 0.2485],
        [0.1568, 0.1549, 0.4090, 0.2793],
        [0.1685, 0.1855, 0.4156, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1939, 0.1836, 0.3958, 0.2267],
        [0.1808, 0.1907, 0.3856, 0.2429],
        [0.2152, 0.2002, 0.2957, 0.2889]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:52:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.2752 (0.4468)	Prec@(1,5) (84.2%, 99.4%)	
08/11 02:52:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.4814 (0.4515)	Prec@(1,5) (84.5%, 99.3%)	
08/11 02:53:46AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.4891 (0.4569)	Prec@(1,5) (84.1%, 99.4%)	
08/11 02:54:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.4724 (0.4593)	Prec@(1,5) (84.0%, 99.4%)	
08/11 02:55:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.4386 (0.4610)	Prec@(1,5) (83.8%, 99.4%)	
08/11 02:56:21AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.5289 (0.4616)	Prec@(1,5) (83.8%, 99.4%)	
08/11 02:57:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.4744 (0.4663)	Prec@(1,5) (83.7%, 99.4%)	
08/11 02:57:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.5044 (0.4681)	Prec@(1,5) (83.7%, 99.4%)	
08/11 02:57:55AM searchStage_trainer.py:210 [INFO] Train: [ 21/49] Final Prec@1 83.7440%
08/11 02:58:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.5690	Prec@(1,5) (80.4%, 98.8%)
08/11 02:58:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.5814	Prec@(1,5) (80.3%, 98.8%)
08/11 02:58:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.5683	Prec@(1,5) (80.6%, 98.8%)
08/11 02:58:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.5679	Prec@(1,5) (80.7%, 98.8%)
08/11 02:58:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.5754	Prec@(1,5) (80.3%, 98.8%)
08/11 02:58:46AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.5825	Prec@(1,5) (80.1%, 98.7%)
08/11 02:58:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.5834	Prec@(1,5) (80.0%, 98.7%)
08/11 02:59:02AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5839	Prec@(1,5) (80.0%, 98.8%)
08/11 02:59:02AM searchStage_trainer.py:249 [INFO] Valid: [ 21/49] Final Prec@1 79.9880%
08/11 02:59:02AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:59:02AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 79.9880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1307, 0.1297, 0.5058, 0.2339],
        [0.1301, 0.1234, 0.5264, 0.2201]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2704, 0.1627, 0.3224, 0.2445],
        [0.2621, 0.1602, 0.3334, 0.2443],
        [0.2404, 0.1847, 0.3292, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1739, 0.1316, 0.5011, 0.1934],
        [0.1975, 0.1510, 0.3904, 0.2611],
        [0.2042, 0.2118, 0.2897, 0.2943]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2152, 0.3217, 0.2132],
        [0.2270, 0.2221, 0.2800, 0.2708],
        [0.2235, 0.2149, 0.2885, 0.2731]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.1845, 0.3505, 0.2109],
        [0.3028, 0.1539, 0.3232, 0.2200],
        [0.2146, 0.1897, 0.2294, 0.3663]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2872, 0.1755, 0.3576, 0.1797],
        [0.2331, 0.2226, 0.2585, 0.2858],
        [0.2009, 0.2077, 0.2636, 0.3278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2181, 0.2499, 0.2829, 0.2492],
        [0.2235, 0.2510, 0.2797, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2413, 0.3081, 0.2074],
        [0.2343, 0.2281, 0.3134, 0.2243],
        [0.2338, 0.2299, 0.2210, 0.3153]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2398, 0.2763, 0.3143, 0.1695],
        [0.2206, 0.2387, 0.2592, 0.2816],
        [0.2102, 0.2178, 0.2301, 0.3419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2437, 0.2952, 0.2096],
        [0.2311, 0.2325, 0.2698, 0.2666],
        [0.2437, 0.2276, 0.2397, 0.2890]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2426, 0.2372, 0.3263, 0.1939],
        [0.2264, 0.2256, 0.2547, 0.2934],
        [0.2376, 0.2252, 0.2363, 0.3009]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2289, 0.2468, 0.2933, 0.2310],
        [0.2322, 0.2467, 0.2822, 0.2389],
        [0.2155, 0.2434, 0.2556, 0.2856]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1776, 0.1778, 0.4049, 0.2398],
        [0.1783, 0.1867, 0.4007, 0.2342]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1833, 0.1910, 0.4320, 0.1936],
        [0.1787, 0.1772, 0.4103, 0.2338],
        [0.2355, 0.2064, 0.2460, 0.3120]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1686, 0.1817, 0.4879, 0.1618],
        [0.1805, 0.1751, 0.3156, 0.3288],
        [0.1866, 0.2044, 0.3232, 0.2859]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2056, 0.1962, 0.3096, 0.2886],
        [0.1851, 0.1955, 0.3553, 0.2641],
        [0.2086, 0.2231, 0.3626, 0.2057]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1596, 0.1767, 0.4150, 0.2487],
        [0.1487, 0.1486, 0.4258, 0.2769],
        [0.1616, 0.1795, 0.4327, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1871, 0.1783, 0.4101, 0.2245],
        [0.1759, 0.1864, 0.3999, 0.2379],
        [0.2125, 0.1970, 0.2961, 0.2944]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:59:54AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.3714 (0.4456)	Prec@(1,5) (84.8%, 99.4%)	
08/11 03:00:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.4223 (0.4456)	Prec@(1,5) (84.8%, 99.5%)	
08/11 03:01:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.2926 (0.4571)	Prec@(1,5) (84.2%, 99.4%)	
08/11 03:02:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.6175 (0.4572)	Prec@(1,5) (84.2%, 99.4%)	
08/11 03:03:19AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.4454 (0.4574)	Prec@(1,5) (84.0%, 99.4%)	
08/11 03:04:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.3000 (0.4561)	Prec@(1,5) (84.2%, 99.4%)	
08/11 03:05:02AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.5871 (0.4528)	Prec@(1,5) (84.2%, 99.4%)	
08/11 03:05:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.4573 (0.4503)	Prec@(1,5) (84.3%, 99.4%)	
08/11 03:05:44AM searchStage_trainer.py:210 [INFO] Train: [ 22/49] Final Prec@1 84.2800%
08/11 03:05:53AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.5783	Prec@(1,5) (81.2%, 98.8%)
08/11 03:06:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.5727	Prec@(1,5) (81.3%, 98.8%)
08/11 03:06:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.5752	Prec@(1,5) (81.2%, 98.9%)
08/11 03:06:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.5760	Prec@(1,5) (80.9%, 98.9%)
08/11 03:06:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.5707	Prec@(1,5) (80.9%, 98.8%)
08/11 03:06:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.5732	Prec@(1,5) (80.8%, 98.8%)
08/11 03:06:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.5747	Prec@(1,5) (80.7%, 98.8%)
08/11 03:06:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.5753	Prec@(1,5) (80.6%, 98.8%)
08/11 03:06:51AM searchStage_trainer.py:249 [INFO] Valid: [ 22/49] Final Prec@1 80.5800%
08/11 03:06:51AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:06:51AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 80.5800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1278, 0.1293, 0.5141, 0.2289],
        [0.1276, 0.1229, 0.5271, 0.2224]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2738, 0.1593, 0.3196, 0.2473],
        [0.2655, 0.1587, 0.3333, 0.2426],
        [0.2403, 0.1829, 0.3320, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1687, 0.1275, 0.5118, 0.1920],
        [0.1954, 0.1464, 0.3986, 0.2596],
        [0.2030, 0.2112, 0.2908, 0.2950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2131, 0.3273, 0.2123],
        [0.2261, 0.2231, 0.2805, 0.2703],
        [0.2224, 0.2129, 0.2909, 0.2737]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.1818, 0.3534, 0.2120],
        [0.3068, 0.1513, 0.3268, 0.2150],
        [0.2099, 0.1867, 0.2307, 0.3727]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2903, 0.1709, 0.3632, 0.1756],
        [0.2311, 0.2192, 0.2578, 0.2919],
        [0.1994, 0.2060, 0.2647, 0.3299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2187, 0.2503, 0.2833, 0.2478],
        [0.2230, 0.2495, 0.2803, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2385, 0.3082, 0.2079],
        [0.2341, 0.2252, 0.3147, 0.2260],
        [0.2342, 0.2305, 0.2217, 0.3135]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2381, 0.2730, 0.3193, 0.1696],
        [0.2208, 0.2382, 0.2604, 0.2807],
        [0.2104, 0.2158, 0.2307, 0.3431]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2451, 0.2956, 0.2067],
        [0.2307, 0.2324, 0.2691, 0.2678],
        [0.2404, 0.2273, 0.2401, 0.2921]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2352, 0.3274, 0.1948],
        [0.2241, 0.2241, 0.2572, 0.2946],
        [0.2404, 0.2247, 0.2368, 0.2980]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2275, 0.2458, 0.2955, 0.2312],
        [0.2305, 0.2466, 0.2868, 0.2361],
        [0.2131, 0.2433, 0.2558, 0.2877]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1738, 0.1741, 0.4186, 0.2334],
        [0.1726, 0.1810, 0.4087, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1766, 0.1849, 0.4482, 0.1903],
        [0.1715, 0.1721, 0.4260, 0.2303],
        [0.2320, 0.2050, 0.2464, 0.3165]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1601, 0.1718, 0.5086, 0.1596],
        [0.1740, 0.1711, 0.3207, 0.3343],
        [0.1842, 0.2029, 0.3322, 0.2807]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2015, 0.1915, 0.3138, 0.2932],
        [0.1831, 0.1932, 0.3659, 0.2578],
        [0.2052, 0.2195, 0.3716, 0.2037]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1531, 0.1690, 0.4266, 0.2513],
        [0.1418, 0.1416, 0.4425, 0.2741],
        [0.1556, 0.1746, 0.4505, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1799, 0.1718, 0.4243, 0.2240],
        [0.1702, 0.1808, 0.4148, 0.2343],
        [0.2127, 0.1952, 0.2971, 0.2951]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:07:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.2039 (0.4397)	Prec@(1,5) (85.0%, 99.3%)	
08/11 03:08:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.3393 (0.4176)	Prec@(1,5) (85.4%, 99.5%)	
08/11 03:09:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.5653 (0.4219)	Prec@(1,5) (85.3%, 99.4%)	
08/11 03:10:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.4176 (0.4285)	Prec@(1,5) (85.1%, 99.4%)	
08/11 03:11:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4210 (0.4277)	Prec@(1,5) (85.1%, 99.4%)	
08/11 03:12:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.5841 (0.4258)	Prec@(1,5) (85.1%, 99.4%)	
08/11 03:12:52AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.4428 (0.4227)	Prec@(1,5) (85.1%, 99.5%)	
08/11 03:13:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.5131 (0.4253)	Prec@(1,5) (85.0%, 99.5%)	
08/11 03:13:34AM searchStage_trainer.py:210 [INFO] Train: [ 23/49] Final Prec@1 85.0480%
08/11 03:13:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5681	Prec@(1,5) (80.3%, 99.2%)
08/11 03:13:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5802	Prec@(1,5) (79.9%, 98.9%)
08/11 03:14:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5777	Prec@(1,5) (80.1%, 99.0%)
08/11 03:14:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5778	Prec@(1,5) (79.9%, 98.9%)
08/11 03:14:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5843	Prec@(1,5) (79.7%, 99.0%)
08/11 03:14:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5865	Prec@(1,5) (79.6%, 99.0%)
08/11 03:14:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5836	Prec@(1,5) (79.8%, 99.0%)
08/11 03:14:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5787	Prec@(1,5) (80.0%, 99.0%)
08/11 03:14:41AM searchStage_trainer.py:249 [INFO] Valid: [ 23/49] Final Prec@1 79.9720%
08/11 03:14:41AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:14:41AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 80.5800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1242, 0.1272, 0.5206, 0.2280],
        [0.1238, 0.1213, 0.5351, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2739, 0.1582, 0.3192, 0.2488],
        [0.2644, 0.1580, 0.3344, 0.2433],
        [0.2387, 0.1810, 0.3379, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1648, 0.1248, 0.5194, 0.1911],
        [0.1910, 0.1439, 0.4050, 0.2601],
        [0.2017, 0.2113, 0.2937, 0.2933]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2135, 0.3294, 0.2096],
        [0.2247, 0.2215, 0.2819, 0.2719],
        [0.2208, 0.2103, 0.2938, 0.2751]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.1807, 0.3576, 0.2091],
        [0.3101, 0.1479, 0.3276, 0.2144],
        [0.2065, 0.1840, 0.2294, 0.3802]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2895, 0.1679, 0.3694, 0.1732],
        [0.2309, 0.2176, 0.2554, 0.2960],
        [0.2009, 0.2048, 0.2631, 0.3312]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2179, 0.2499, 0.2819, 0.2502],
        [0.2240, 0.2502, 0.2808, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2420, 0.2402, 0.3117, 0.2060],
        [0.2307, 0.2265, 0.3180, 0.2249],
        [0.2312, 0.2304, 0.2216, 0.3167]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2383, 0.2743, 0.3197, 0.1677],
        [0.2213, 0.2373, 0.2591, 0.2822],
        [0.2084, 0.2147, 0.2300, 0.3468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2470, 0.2967, 0.2028],
        [0.2292, 0.2296, 0.2702, 0.2710],
        [0.2378, 0.2263, 0.2409, 0.2950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2406, 0.2351, 0.3289, 0.1955],
        [0.2251, 0.2242, 0.2583, 0.2924],
        [0.2412, 0.2234, 0.2367, 0.2987]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2238, 0.2466, 0.2975, 0.2321],
        [0.2291, 0.2483, 0.2907, 0.2319],
        [0.2109, 0.2432, 0.2556, 0.2903]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1701, 0.1723, 0.4261, 0.2315],
        [0.1685, 0.1792, 0.4151, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1700, 0.1800, 0.4638, 0.1862],
        [0.1655, 0.1675, 0.4410, 0.2260],
        [0.2281, 0.2031, 0.2453, 0.3235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1521, 0.1646, 0.5289, 0.1543],
        [0.1674, 0.1679, 0.3266, 0.3382],
        [0.1795, 0.1996, 0.3377, 0.2832]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1984, 0.1906, 0.3213, 0.2897],
        [0.1770, 0.1903, 0.3762, 0.2565],
        [0.2002, 0.2147, 0.3803, 0.2047]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1465, 0.1626, 0.4384, 0.2525],
        [0.1343, 0.1352, 0.4585, 0.2721],
        [0.1501, 0.1698, 0.4672, 0.2129]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1719, 0.1665, 0.4403, 0.2214],
        [0.1629, 0.1754, 0.4303, 0.2314],
        [0.2124, 0.1934, 0.2972, 0.2970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:15:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.3806 (0.3797)	Prec@(1,5) (86.9%, 99.6%)	
08/11 03:16:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.3512 (0.3977)	Prec@(1,5) (86.1%, 99.5%)	
08/11 03:17:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.4877 (0.3999)	Prec@(1,5) (86.1%, 99.5%)	
08/11 03:18:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.6504 (0.3988)	Prec@(1,5) (86.2%, 99.5%)	
08/11 03:18:58AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.6189 (0.4070)	Prec@(1,5) (85.9%, 99.4%)	
08/11 03:19:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.4342 (0.4143)	Prec@(1,5) (85.6%, 99.5%)	
08/11 03:20:41AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.3434 (0.4121)	Prec@(1,5) (85.6%, 99.5%)	
08/11 03:21:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.3997 (0.4106)	Prec@(1,5) (85.7%, 99.4%)	
08/11 03:21:23AM searchStage_trainer.py:210 [INFO] Train: [ 24/49] Final Prec@1 85.6680%
08/11 03:21:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.5736	Prec@(1,5) (80.9%, 98.9%)
08/11 03:21:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5464	Prec@(1,5) (81.6%, 98.9%)
08/11 03:21:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.5357	Prec@(1,5) (81.9%, 99.1%)
08/11 03:21:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5332	Prec@(1,5) (82.0%, 99.1%)
08/11 03:22:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5445	Prec@(1,5) (81.6%, 99.1%)
08/11 03:22:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5506	Prec@(1,5) (81.4%, 99.0%)
08/11 03:22:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5506	Prec@(1,5) (81.4%, 99.0%)
08/11 03:22:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5521	Prec@(1,5) (81.3%, 99.0%)
08/11 03:22:29AM searchStage_trainer.py:249 [INFO] Valid: [ 24/49] Final Prec@1 81.3200%
08/11 03:22:29AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:22:30AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.3200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1230, 0.1261, 0.5230, 0.2280],
        [0.1233, 0.1217, 0.5367, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2730, 0.1566, 0.3219, 0.2485],
        [0.2622, 0.1558, 0.3381, 0.2439],
        [0.2374, 0.1786, 0.3429, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1613, 0.1231, 0.5256, 0.1900],
        [0.1860, 0.1423, 0.4099, 0.2618],
        [0.1995, 0.2122, 0.2977, 0.2907]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2434, 0.2138, 0.3353, 0.2075],
        [0.2207, 0.2225, 0.2840, 0.2728],
        [0.2160, 0.2088, 0.3003, 0.2749]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.1804, 0.3589, 0.2075],
        [0.3122, 0.1449, 0.3294, 0.2135],
        [0.2037, 0.1820, 0.2289, 0.3854]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2938, 0.1658, 0.3706, 0.1697],
        [0.2260, 0.2170, 0.2565, 0.3005],
        [0.1995, 0.2027, 0.2633, 0.3345]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2159, 0.2519, 0.2827, 0.2495],
        [0.2228, 0.2497, 0.2822, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2409, 0.2415, 0.3145, 0.2031],
        [0.2292, 0.2272, 0.3213, 0.2223],
        [0.2294, 0.2277, 0.2195, 0.3235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2366, 0.2763, 0.3209, 0.1662],
        [0.2219, 0.2364, 0.2564, 0.2852],
        [0.2079, 0.2143, 0.2297, 0.3481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2477, 0.2977, 0.2008],
        [0.2254, 0.2307, 0.2734, 0.2705],
        [0.2369, 0.2242, 0.2406, 0.2984]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2388, 0.2349, 0.3318, 0.1945],
        [0.2231, 0.2249, 0.2597, 0.2923],
        [0.2410, 0.2227, 0.2365, 0.2998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2187, 0.2505, 0.3024, 0.2284],
        [0.2250, 0.2489, 0.2925, 0.2335],
        [0.2082, 0.2442, 0.2562, 0.2914]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1646, 0.1688, 0.4391, 0.2275],
        [0.1640, 0.1748, 0.4231, 0.2381]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1630, 0.1740, 0.4799, 0.1831],
        [0.1594, 0.1623, 0.4554, 0.2229],
        [0.2252, 0.2011, 0.2461, 0.3276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1432, 0.1553, 0.5511, 0.1504],
        [0.1611, 0.1616, 0.3321, 0.3453],
        [0.1757, 0.1981, 0.3469, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1926, 0.1863, 0.3303, 0.2907],
        [0.1719, 0.1857, 0.3899, 0.2525],
        [0.1943, 0.2103, 0.3925, 0.2029]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1397, 0.1546, 0.4512, 0.2545],
        [0.1271, 0.1282, 0.4777, 0.2669],
        [0.1437, 0.1635, 0.4863, 0.2065]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1642, 0.1591, 0.4568, 0.2199],
        [0.1565, 0.1685, 0.4477, 0.2273],
        [0.2125, 0.1917, 0.2977, 0.2981]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:23:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.3322 (0.4120)	Prec@(1,5) (86.1%, 99.4%)	
08/11 03:24:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.5213 (0.3929)	Prec@(1,5) (86.6%, 99.5%)	
08/11 03:25:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.3522 (0.3916)	Prec@(1,5) (86.7%, 99.5%)	
08/11 03:25:56AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.3624 (0.3834)	Prec@(1,5) (86.9%, 99.5%)	
08/11 03:26:47AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.2531 (0.3831)	Prec@(1,5) (87.0%, 99.4%)	
08/11 03:27:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.3413 (0.3929)	Prec@(1,5) (86.6%, 99.4%)	
08/11 03:28:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.2465 (0.3931)	Prec@(1,5) (86.6%, 99.4%)	
08/11 03:29:12AM searchStage_trainer.py:200 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.5184 (0.3980)	Prec@(1,5) (86.3%, 99.5%)	
08/11 03:29:12AM searchStage_trainer.py:210 [INFO] Train: [ 25/49] Final Prec@1 86.3360%
08/11 03:29:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5287	Prec@(1,5) (82.5%, 99.1%)
08/11 03:29:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5305	Prec@(1,5) (82.3%, 99.0%)
08/11 03:29:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5277	Prec@(1,5) (82.2%, 99.1%)
08/11 03:29:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5275	Prec@(1,5) (82.3%, 99.1%)
08/11 03:29:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5355	Prec@(1,5) (81.9%, 99.0%)
08/11 03:30:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5354	Prec@(1,5) (81.9%, 99.0%)
08/11 03:30:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5401	Prec@(1,5) (81.8%, 99.0%)
08/11 03:30:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5443	Prec@(1,5) (81.6%, 99.0%)
08/11 03:30:18AM searchStage_trainer.py:249 [INFO] Valid: [ 25/49] Final Prec@1 81.6160%
08/11 03:30:18AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:30:19AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.6160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1211, 0.1252, 0.5275, 0.2262],
        [0.1209, 0.1212, 0.5402, 0.2177]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2713, 0.1565, 0.3264, 0.2459],
        [0.2583, 0.1551, 0.3415, 0.2450],
        [0.2332, 0.1771, 0.3488, 0.2409]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1568, 0.1204, 0.5322, 0.1905],
        [0.1848, 0.1404, 0.4140, 0.2609],
        [0.1976, 0.2142, 0.3005, 0.2876]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2132, 0.3375, 0.2051],
        [0.2172, 0.2242, 0.2881, 0.2705],
        [0.2112, 0.2069, 0.3030, 0.2789]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.1804, 0.3642, 0.2049],
        [0.3109, 0.1443, 0.3334, 0.2114],
        [0.1982, 0.1795, 0.2277, 0.3946]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2953, 0.1628, 0.3739, 0.1680],
        [0.2264, 0.2152, 0.2553, 0.3031],
        [0.1973, 0.2014, 0.2657, 0.3356]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2145, 0.2528, 0.2829, 0.2498],
        [0.2201, 0.2507, 0.2846, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2370, 0.2423, 0.3196, 0.2011],
        [0.2260, 0.2278, 0.3254, 0.2207],
        [0.2285, 0.2257, 0.2181, 0.3277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2340, 0.2779, 0.3245, 0.1635],
        [0.2184, 0.2346, 0.2575, 0.2895],
        [0.2067, 0.2132, 0.2302, 0.3499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2470, 0.2970, 0.2032],
        [0.2253, 0.2320, 0.2771, 0.2655],
        [0.2344, 0.2242, 0.2415, 0.2998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2373, 0.2342, 0.3346, 0.1939],
        [0.2229, 0.2226, 0.2617, 0.2927],
        [0.2423, 0.2208, 0.2364, 0.3004]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2175, 0.2502, 0.3028, 0.2295],
        [0.2229, 0.2487, 0.2953, 0.2331],
        [0.2072, 0.2453, 0.2577, 0.2898]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1620, 0.1652, 0.4478, 0.2250],
        [0.1615, 0.1709, 0.4292, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1592, 0.1685, 0.4933, 0.1791],
        [0.1556, 0.1573, 0.4669, 0.2202],
        [0.2232, 0.1980, 0.2449, 0.3339]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1369, 0.1478, 0.5687, 0.1466],
        [0.1564, 0.1571, 0.3373, 0.3492],
        [0.1731, 0.1952, 0.3526, 0.2791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1871, 0.1833, 0.3401, 0.2895],
        [0.1681, 0.1810, 0.4009, 0.2501],
        [0.1907, 0.2066, 0.4003, 0.2024]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1341, 0.1481, 0.4644, 0.2534],
        [0.1209, 0.1221, 0.4940, 0.2630],
        [0.1371, 0.1576, 0.5033, 0.2020]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1584, 0.1542, 0.4727, 0.2147],
        [0.1516, 0.1634, 0.4608, 0.2241],
        [0.2093, 0.1884, 0.2991, 0.3032]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:31:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.4038 (0.3673)	Prec@(1,5) (87.0%, 99.4%)	
08/11 03:32:02AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.2358 (0.3671)	Prec@(1,5) (87.1%, 99.4%)	
08/11 03:32:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.2690 (0.3680)	Prec@(1,5) (86.9%, 99.5%)	
08/11 03:33:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.3013 (0.3667)	Prec@(1,5) (87.1%, 99.5%)	
08/11 03:34:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.3889 (0.3697)	Prec@(1,5) (87.0%, 99.5%)	
08/11 03:35:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.4107 (0.3707)	Prec@(1,5) (86.9%, 99.5%)	
08/11 03:36:20AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.3412 (0.3739)	Prec@(1,5) (86.8%, 99.5%)	
08/11 03:37:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.4343 (0.3756)	Prec@(1,5) (86.7%, 99.5%)	
08/11 03:37:02AM searchStage_trainer.py:210 [INFO] Train: [ 26/49] Final Prec@1 86.6920%
08/11 03:37:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5317	Prec@(1,5) (82.4%, 98.9%)
08/11 03:37:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5276	Prec@(1,5) (82.3%, 98.9%)
08/11 03:37:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5343	Prec@(1,5) (82.1%, 98.9%)
08/11 03:37:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5343	Prec@(1,5) (82.1%, 99.0%)
08/11 03:37:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5345	Prec@(1,5) (82.1%, 99.0%)
08/11 03:37:53AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5335	Prec@(1,5) (82.1%, 98.9%)
08/11 03:38:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5323	Prec@(1,5) (82.1%, 99.0%)
08/11 03:38:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5319	Prec@(1,5) (82.1%, 99.0%)
08/11 03:38:08AM searchStage_trainer.py:249 [INFO] Valid: [ 26/49] Final Prec@1 82.1440%
08/11 03:38:08AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:38:08AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.1440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1181, 0.1234, 0.5348, 0.2238],
        [0.1188, 0.1202, 0.5438, 0.2172]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2715, 0.1553, 0.3287, 0.2444],
        [0.2576, 0.1534, 0.3426, 0.2465],
        [0.2320, 0.1737, 0.3542, 0.2401]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1535, 0.1188, 0.5371, 0.1907],
        [0.1812, 0.1387, 0.4212, 0.2589],
        [0.1940, 0.2138, 0.3060, 0.2863]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2125, 0.3378, 0.2052],
        [0.2160, 0.2255, 0.2900, 0.2685],
        [0.2073, 0.2060, 0.3069, 0.2798]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.1781, 0.3668, 0.2056],
        [0.3100, 0.1432, 0.3392, 0.2077],
        [0.1956, 0.1772, 0.2275, 0.3998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2927, 0.1620, 0.3801, 0.1652],
        [0.2241, 0.2152, 0.2571, 0.3035],
        [0.1965, 0.1977, 0.2640, 0.3419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2141, 0.2550, 0.2821, 0.2489],
        [0.2189, 0.2523, 0.2833, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2343, 0.2430, 0.3209, 0.2017],
        [0.2234, 0.2312, 0.3279, 0.2174],
        [0.2256, 0.2253, 0.2184, 0.3307]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2324, 0.2814, 0.3260, 0.1602],
        [0.2148, 0.2326, 0.2564, 0.2962],
        [0.2052, 0.2125, 0.2308, 0.3516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2467, 0.2985, 0.2066],
        [0.2245, 0.2334, 0.2788, 0.2633],
        [0.2338, 0.2257, 0.2443, 0.2962]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2338, 0.2358, 0.3396, 0.1908],
        [0.2216, 0.2209, 0.2626, 0.2949],
        [0.2412, 0.2195, 0.2362, 0.3031]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2125, 0.2522, 0.3073, 0.2280],
        [0.2189, 0.2480, 0.2974, 0.2356],
        [0.2046, 0.2480, 0.2602, 0.2872]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1560, 0.1600, 0.4629, 0.2211],
        [0.1571, 0.1655, 0.4390, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1529, 0.1619, 0.5105, 0.1747],
        [0.1491, 0.1517, 0.4835, 0.2157],
        [0.2202, 0.1941, 0.2444, 0.3413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1290, 0.1393, 0.5899, 0.1418],
        [0.1505, 0.1517, 0.3404, 0.3575],
        [0.1693, 0.1934, 0.3606, 0.2767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1818, 0.1793, 0.3484, 0.2905],
        [0.1634, 0.1767, 0.4115, 0.2485],
        [0.1870, 0.2035, 0.4103, 0.1992]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1270, 0.1416, 0.4795, 0.2520],
        [0.1145, 0.1158, 0.5127, 0.2571],
        [0.1305, 0.1513, 0.5206, 0.1976]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1521, 0.1486, 0.4880, 0.2114],
        [0.1471, 0.1588, 0.4746, 0.2195],
        [0.2073, 0.1859, 0.2994, 0.3075]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:39:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.5423 (0.3747)	Prec@(1,5) (86.8%, 99.7%)	
08/11 03:39:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.2982 (0.3559)	Prec@(1,5) (87.4%, 99.6%)	
08/11 03:40:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.2395 (0.3572)	Prec@(1,5) (87.5%, 99.6%)	
08/11 03:41:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.5256 (0.3554)	Prec@(1,5) (87.5%, 99.6%)	
08/11 03:42:26AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.3377 (0.3504)	Prec@(1,5) (87.7%, 99.6%)	
08/11 03:43:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.2338 (0.3544)	Prec@(1,5) (87.6%, 99.6%)	
08/11 03:44:09AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.3537 (0.3538)	Prec@(1,5) (87.7%, 99.6%)	
08/11 03:44:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.3720 (0.3573)	Prec@(1,5) (87.6%, 99.6%)	
08/11 03:44:51AM searchStage_trainer.py:210 [INFO] Train: [ 27/49] Final Prec@1 87.6000%
08/11 03:44:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5435	Prec@(1,5) (82.3%, 98.9%)
08/11 03:45:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5245	Prec@(1,5) (82.9%, 99.0%)
08/11 03:45:16AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5314	Prec@(1,5) (82.4%, 99.1%)
08/11 03:45:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5263	Prec@(1,5) (82.5%, 99.1%)
08/11 03:45:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5237	Prec@(1,5) (82.4%, 99.1%)
08/11 03:45:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5241	Prec@(1,5) (82.5%, 99.1%)
08/11 03:45:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5273	Prec@(1,5) (82.4%, 99.1%)
08/11 03:45:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5264	Prec@(1,5) (82.5%, 99.1%)
08/11 03:45:57AM searchStage_trainer.py:249 [INFO] Valid: [ 27/49] Final Prec@1 82.4680%
08/11 03:45:57AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:45:57AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.4680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1150, 0.1213, 0.5420, 0.2217],
        [0.1176, 0.1204, 0.5453, 0.2167]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2711, 0.1537, 0.3292, 0.2460],
        [0.2585, 0.1531, 0.3433, 0.2452],
        [0.2296, 0.1723, 0.3590, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1482, 0.1168, 0.5457, 0.1893],
        [0.1761, 0.1359, 0.4298, 0.2581],
        [0.1921, 0.2143, 0.3079, 0.2857]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2412, 0.2120, 0.3418, 0.2051],
        [0.2121, 0.2279, 0.2920, 0.2680],
        [0.2068, 0.2051, 0.3090, 0.2791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.1741, 0.3732, 0.2061],
        [0.3084, 0.1408, 0.3465, 0.2043],
        [0.1922, 0.1766, 0.2276, 0.4036]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2900, 0.1593, 0.3868, 0.1639],
        [0.2238, 0.2160, 0.2578, 0.3025],
        [0.1943, 0.1965, 0.2635, 0.3457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2132, 0.2580, 0.2819, 0.2469],
        [0.2167, 0.2536, 0.2823, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2330, 0.2442, 0.3221, 0.2008],
        [0.2228, 0.2331, 0.3282, 0.2159],
        [0.2237, 0.2241, 0.2177, 0.3345]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2288, 0.2823, 0.3314, 0.1574],
        [0.2122, 0.2324, 0.2558, 0.2996],
        [0.2025, 0.2116, 0.2308, 0.3551]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2473, 0.2980, 0.2069],
        [0.2234, 0.2333, 0.2781, 0.2652],
        [0.2302, 0.2285, 0.2481, 0.2932]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2305, 0.2356, 0.3442, 0.1897],
        [0.2191, 0.2204, 0.2652, 0.2954],
        [0.2378, 0.2206, 0.2384, 0.3032]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2091, 0.2533, 0.3112, 0.2264],
        [0.2151, 0.2477, 0.2997, 0.2376],
        [0.2027, 0.2496, 0.2623, 0.2854]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1500, 0.1541, 0.4765, 0.2195],
        [0.1524, 0.1604, 0.4517, 0.2355]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1455, 0.1553, 0.5295, 0.1697],
        [0.1420, 0.1463, 0.5003, 0.2114],
        [0.2154, 0.1903, 0.2456, 0.3487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1207, 0.1308, 0.6115, 0.1370],
        [0.1437, 0.1473, 0.3466, 0.3624],
        [0.1655, 0.1897, 0.3695, 0.2753]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1760, 0.1767, 0.3596, 0.2877],
        [0.1576, 0.1714, 0.4234, 0.2476],
        [0.1813, 0.1987, 0.4224, 0.1975]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1200, 0.1350, 0.4957, 0.2493],
        [0.1070, 0.1091, 0.5304, 0.2535],
        [0.1234, 0.1448, 0.5401, 0.1917]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1448, 0.1424, 0.5063, 0.2066],
        [0.1405, 0.1540, 0.4933, 0.2122],
        [0.2025, 0.1814, 0.3008, 0.3153]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:46:49AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.3096 (0.3049)	Prec@(1,5) (89.5%, 99.8%)	
08/11 03:47:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.2308 (0.3199)	Prec@(1,5) (89.0%, 99.8%)	
08/11 03:48:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.4487 (0.3290)	Prec@(1,5) (88.7%, 99.8%)	
08/11 03:49:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.3717 (0.3349)	Prec@(1,5) (88.5%, 99.8%)	
08/11 03:50:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.3184 (0.3423)	Prec@(1,5) (88.3%, 99.8%)	
08/11 03:51:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.2519 (0.3413)	Prec@(1,5) (88.2%, 99.7%)	
08/11 03:51:58AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.3346 (0.3420)	Prec@(1,5) (88.2%, 99.7%)	
08/11 03:52:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.5880 (0.3452)	Prec@(1,5) (88.0%, 99.7%)	
08/11 03:52:39AM searchStage_trainer.py:210 [INFO] Train: [ 28/49] Final Prec@1 88.0360%
08/11 03:52:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.5517	Prec@(1,5) (81.5%, 98.8%)
08/11 03:52:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.5466	Prec@(1,5) (81.7%, 99.0%)
08/11 03:53:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5573	Prec@(1,5) (81.3%, 98.9%)
08/11 03:53:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5570	Prec@(1,5) (81.3%, 98.9%)
08/11 03:53:22AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5499	Prec@(1,5) (81.5%, 99.0%)
08/11 03:53:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5550	Prec@(1,5) (81.4%, 99.0%)
08/11 03:53:39AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5510	Prec@(1,5) (81.6%, 99.0%)
08/11 03:53:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5504	Prec@(1,5) (81.6%, 99.0%)
08/11 03:53:46AM searchStage_trainer.py:249 [INFO] Valid: [ 28/49] Final Prec@1 81.5720%
08/11 03:53:46AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:53:46AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.4680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1141, 0.1205, 0.5439, 0.2214],
        [0.1172, 0.1200, 0.5473, 0.2155]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2697, 0.1523, 0.3328, 0.2452],
        [0.2542, 0.1509, 0.3471, 0.2478],
        [0.2277, 0.1713, 0.3647, 0.2363]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1452, 0.1160, 0.5513, 0.1875],
        [0.1741, 0.1351, 0.4325, 0.2583],
        [0.1908, 0.2134, 0.3093, 0.2865]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2414, 0.2101, 0.3424, 0.2061],
        [0.2114, 0.2289, 0.2936, 0.2661],
        [0.2063, 0.2049, 0.3096, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.1725, 0.3765, 0.2043],
        [0.3072, 0.1385, 0.3518, 0.2025],
        [0.1880, 0.1740, 0.2275, 0.4105]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2898, 0.1562, 0.3921, 0.1618],
        [0.2240, 0.2153, 0.2567, 0.3040],
        [0.1921, 0.1940, 0.2648, 0.3490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2128, 0.2612, 0.2797, 0.2463],
        [0.2159, 0.2563, 0.2797, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2296, 0.2447, 0.3255, 0.2003],
        [0.2180, 0.2350, 0.3331, 0.2138],
        [0.2224, 0.2236, 0.2166, 0.3375]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2263, 0.2829, 0.3348, 0.1560],
        [0.2110, 0.2319, 0.2555, 0.3016],
        [0.2018, 0.2107, 0.2303, 0.3572]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2508, 0.3016, 0.2039],
        [0.2229, 0.2321, 0.2781, 0.2669],
        [0.2286, 0.2278, 0.2483, 0.2953]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2286, 0.2349, 0.3478, 0.1887],
        [0.2159, 0.2197, 0.2680, 0.2965],
        [0.2386, 0.2189, 0.2395, 0.3030]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2056, 0.2557, 0.3155, 0.2231],
        [0.2130, 0.2478, 0.3024, 0.2367],
        [0.1990, 0.2489, 0.2636, 0.2885]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1454, 0.1498, 0.4889, 0.2159],
        [0.1490, 0.1564, 0.4592, 0.2354]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1387, 0.1488, 0.5478, 0.1647],
        [0.1353, 0.1409, 0.5182, 0.2056],
        [0.2113, 0.1862, 0.2434, 0.3590]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1122, 0.1217, 0.6339, 0.1322],
        [0.1391, 0.1439, 0.3536, 0.3634],
        [0.1616, 0.1860, 0.3763, 0.2761]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1711, 0.1740, 0.3718, 0.2831],
        [0.1531, 0.1663, 0.4355, 0.2451],
        [0.1760, 0.1935, 0.4323, 0.1982]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1131, 0.1286, 0.5131, 0.2451],
        [0.0993, 0.1029, 0.5503, 0.2475],
        [0.1161, 0.1386, 0.5578, 0.1875]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1365, 0.1358, 0.5283, 0.1994],
        [0.1336, 0.1474, 0.5120, 0.2070],
        [0.1986, 0.1784, 0.2983, 0.3247]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:54:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.2532 (0.3186)	Prec@(1,5) (89.0%, 99.7%)	
08/11 03:55:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.3420 (0.3175)	Prec@(1,5) (88.9%, 99.7%)	
08/11 03:56:21AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.3360 (0.3216)	Prec@(1,5) (88.9%, 99.6%)	
08/11 03:57:12AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.3417 (0.3184)	Prec@(1,5) (88.8%, 99.7%)	
08/11 03:58:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.3947 (0.3181)	Prec@(1,5) (88.7%, 99.7%)	
08/11 03:58:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2857 (0.3214)	Prec@(1,5) (88.7%, 99.7%)	
08/11 03:59:47AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.2479 (0.3237)	Prec@(1,5) (88.6%, 99.7%)	
08/11 04:00:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.3912 (0.3267)	Prec@(1,5) (88.6%, 99.6%)	
08/11 04:00:29AM searchStage_trainer.py:210 [INFO] Train: [ 29/49] Final Prec@1 88.5720%
08/11 04:00:37AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.4959	Prec@(1,5) (82.8%, 99.2%)
08/11 04:00:46AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.4882	Prec@(1,5) (83.2%, 99.2%)
08/11 04:00:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.4968	Prec@(1,5) (83.3%, 99.1%)
08/11 04:01:03AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5001	Prec@(1,5) (83.3%, 99.1%)
08/11 04:01:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.4994	Prec@(1,5) (83.4%, 99.1%)
08/11 04:01:20AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5009	Prec@(1,5) (83.4%, 99.1%)
08/11 04:01:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5025	Prec@(1,5) (83.3%, 99.1%)
08/11 04:01:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5009	Prec@(1,5) (83.3%, 99.2%)
08/11 04:01:35AM searchStage_trainer.py:249 [INFO] Valid: [ 29/49] Final Prec@1 83.2960%
08/11 04:01:35AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:01:35AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 83.2960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1125, 0.1195, 0.5490, 0.2190],
        [0.1151, 0.1186, 0.5508, 0.2154]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.1521, 0.3331, 0.2458],
        [0.2516, 0.1505, 0.3490, 0.2488],
        [0.2269, 0.1714, 0.3671, 0.2345]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1419, 0.1142, 0.5562, 0.1876],
        [0.1721, 0.1339, 0.4377, 0.2563],
        [0.1909, 0.2146, 0.3089, 0.2856]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2409, 0.2079, 0.3442, 0.2069],
        [0.2102, 0.2314, 0.2968, 0.2615],
        [0.2047, 0.2015, 0.3119, 0.2819]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.1713, 0.3811, 0.2017],
        [0.3071, 0.1361, 0.3547, 0.2021],
        [0.1845, 0.1716, 0.2274, 0.4165]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2876, 0.1523, 0.4006, 0.1595],
        [0.2221, 0.2137, 0.2594, 0.3048],
        [0.1902, 0.1922, 0.2648, 0.3528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2117, 0.2636, 0.2793, 0.2454],
        [0.2144, 0.2575, 0.2792, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2263, 0.2455, 0.3310, 0.1971],
        [0.2129, 0.2354, 0.3396, 0.2121],
        [0.2198, 0.2213, 0.2156, 0.3434]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2234, 0.2838, 0.3396, 0.1531],
        [0.2103, 0.2320, 0.2555, 0.3022],
        [0.1988, 0.2088, 0.2274, 0.3649]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2437, 0.2514, 0.3031, 0.2018],
        [0.2220, 0.2306, 0.2784, 0.2691],
        [0.2279, 0.2272, 0.2488, 0.2961]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2236, 0.2329, 0.3532, 0.1903],
        [0.2144, 0.2206, 0.2720, 0.2931],
        [0.2397, 0.2181, 0.2406, 0.3017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2041, 0.2570, 0.3200, 0.2190],
        [0.2129, 0.2469, 0.3027, 0.2375],
        [0.1981, 0.2471, 0.2627, 0.2921]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1409, 0.1462, 0.5006, 0.2123],
        [0.1448, 0.1522, 0.4680, 0.2349]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1330, 0.1427, 0.5623, 0.1620],
        [0.1305, 0.1359, 0.5324, 0.2013],
        [0.2094, 0.1837, 0.2422, 0.3647]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1043, 0.1131, 0.6555, 0.1271],
        [0.1336, 0.1404, 0.3588, 0.3672],
        [0.1580, 0.1822, 0.3838, 0.2759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1655, 0.1702, 0.3814, 0.2828],
        [0.1478, 0.1625, 0.4490, 0.2407],
        [0.1719, 0.1902, 0.4414, 0.1965]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1074, 0.1227, 0.5286, 0.2412],
        [0.0935, 0.0973, 0.5649, 0.2443],
        [0.1101, 0.1335, 0.5741, 0.1822]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1290, 0.1288, 0.5449, 0.1973],
        [0.1269, 0.1418, 0.5307, 0.2006],
        [0.1970, 0.1769, 0.2997, 0.3264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:02:27AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.4140 (0.2936)	Prec@(1,5) (89.7%, 99.7%)	
08/11 04:03:18AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.3836 (0.2990)	Prec@(1,5) (89.3%, 99.7%)	
08/11 04:04:10AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.3540 (0.3043)	Prec@(1,5) (89.1%, 99.7%)	
08/11 04:05:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.1759 (0.3080)	Prec@(1,5) (89.0%, 99.7%)	
08/11 04:05:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.3392 (0.3040)	Prec@(1,5) (89.3%, 99.7%)	
08/11 04:06:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.3673 (0.3063)	Prec@(1,5) (89.3%, 99.7%)	
08/11 04:07:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2503 (0.3050)	Prec@(1,5) (89.4%, 99.7%)	
08/11 04:08:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.3693 (0.3087)	Prec@(1,5) (89.3%, 99.7%)	
08/11 04:08:18AM searchStage_trainer.py:210 [INFO] Train: [ 30/49] Final Prec@1 89.3280%
08/11 04:08:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5353	Prec@(1,5) (82.9%, 99.2%)
08/11 04:08:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5272	Prec@(1,5) (82.7%, 99.1%)
08/11 04:08:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5314	Prec@(1,5) (82.7%, 99.1%)
08/11 04:08:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5408	Prec@(1,5) (82.5%, 99.1%)
08/11 04:09:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5406	Prec@(1,5) (82.5%, 99.1%)
08/11 04:09:09AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5411	Prec@(1,5) (82.5%, 99.1%)
08/11 04:09:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5404	Prec@(1,5) (82.5%, 99.1%)
08/11 04:09:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5407	Prec@(1,5) (82.5%, 99.1%)
08/11 04:09:24AM searchStage_trainer.py:249 [INFO] Valid: [ 30/49] Final Prec@1 82.5400%
08/11 04:09:24AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:09:24AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 83.2960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1120, 0.1201, 0.5524, 0.2156],
        [0.1150, 0.1193, 0.5477, 0.2180]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2716, 0.1510, 0.3318, 0.2456],
        [0.2532, 0.1501, 0.3514, 0.2453],
        [0.2242, 0.1690, 0.3698, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1377, 0.1120, 0.5617, 0.1885],
        [0.1687, 0.1330, 0.4445, 0.2539],
        [0.1894, 0.2168, 0.3108, 0.2830]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2408, 0.2068, 0.3463, 0.2061],
        [0.2085, 0.2317, 0.2991, 0.2607],
        [0.2025, 0.1983, 0.3165, 0.2826]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.1691, 0.3851, 0.1999],
        [0.3061, 0.1336, 0.3589, 0.2014],
        [0.1794, 0.1702, 0.2300, 0.4203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2883, 0.1494, 0.4037, 0.1585],
        [0.2205, 0.2135, 0.2599, 0.3060],
        [0.1888, 0.1923, 0.2656, 0.3533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2102, 0.2669, 0.2802, 0.2426],
        [0.2102, 0.2588, 0.2799, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2239, 0.2451, 0.3362, 0.1948],
        [0.2087, 0.2364, 0.3451, 0.2098],
        [0.2170, 0.2189, 0.2148, 0.3493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2239, 0.2833, 0.3411, 0.1517],
        [0.2078, 0.2298, 0.2550, 0.3074],
        [0.1982, 0.2098, 0.2276, 0.3644]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2423, 0.2526, 0.3034, 0.2017],
        [0.2215, 0.2296, 0.2809, 0.2680],
        [0.2273, 0.2263, 0.2493, 0.2972]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2216, 0.2312, 0.3592, 0.1879],
        [0.2112, 0.2184, 0.2744, 0.2961],
        [0.2392, 0.2176, 0.2415, 0.3018]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2002, 0.2592, 0.3250, 0.2156],
        [0.2098, 0.2464, 0.3049, 0.2390],
        [0.1946, 0.2475, 0.2650, 0.2928]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1355, 0.1420, 0.5117, 0.2108],
        [0.1411, 0.1488, 0.4781, 0.2319]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1360, 0.5822, 0.1569],
        [0.1233, 0.1303, 0.5506, 0.1958],
        [0.2044, 0.1801, 0.2415, 0.3739]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0972, 0.1060, 0.6751, 0.1216],
        [0.1278, 0.1357, 0.3617, 0.3748],
        [0.1548, 0.1787, 0.3902, 0.2764]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1584, 0.1666, 0.3952, 0.2798],
        [0.1428, 0.1579, 0.4647, 0.2346],
        [0.1669, 0.1860, 0.4500, 0.1972]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1009, 0.1161, 0.5455, 0.2375],
        [0.0874, 0.0916, 0.5826, 0.2384],
        [0.1046, 0.1278, 0.5902, 0.1774]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1216, 0.1226, 0.5644, 0.1914],
        [0.1205, 0.1353, 0.5497, 0.1946],
        [0.1944, 0.1735, 0.2974, 0.3347]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:10:17AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.2174 (0.2810)	Prec@(1,5) (90.1%, 99.8%)	
08/11 04:11:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.2491 (0.2770)	Prec@(1,5) (90.2%, 99.8%)	
08/11 04:11:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.3104 (0.2760)	Prec@(1,5) (90.2%, 99.8%)	
08/11 04:12:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2489 (0.2805)	Prec@(1,5) (90.0%, 99.8%)	
08/11 04:13:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.4037 (0.2847)	Prec@(1,5) (90.0%, 99.8%)	
08/11 04:14:33AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.2646 (0.2906)	Prec@(1,5) (89.9%, 99.7%)	
08/11 04:15:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.2673 (0.2948)	Prec@(1,5) (89.8%, 99.7%)	
08/11 04:16:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.2764 (0.2993)	Prec@(1,5) (89.7%, 99.7%)	
08/11 04:16:07AM searchStage_trainer.py:210 [INFO] Train: [ 31/49] Final Prec@1 89.7280%
08/11 04:16:16AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5245	Prec@(1,5) (82.0%, 99.2%)
08/11 04:16:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5254	Prec@(1,5) (82.3%, 99.3%)
08/11 04:16:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5212	Prec@(1,5) (82.7%, 99.3%)
08/11 04:16:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5124	Prec@(1,5) (82.9%, 99.3%)
08/11 04:16:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5158	Prec@(1,5) (82.8%, 99.2%)
08/11 04:16:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5174	Prec@(1,5) (82.8%, 99.3%)
08/11 04:17:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5158	Prec@(1,5) (82.9%, 99.3%)
08/11 04:17:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5163	Prec@(1,5) (82.9%, 99.3%)
08/11 04:17:13AM searchStage_trainer.py:249 [INFO] Valid: [ 31/49] Final Prec@1 82.8440%
08/11 04:17:13AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:17:13AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 83.2960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1121, 0.1202, 0.5527, 0.2150],
        [0.1147, 0.1198, 0.5477, 0.2177]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.1489, 0.3332, 0.2476],
        [0.2525, 0.1482, 0.3548, 0.2445],
        [0.2225, 0.1661, 0.3766, 0.2347]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1331, 0.1091, 0.5699, 0.1878],
        [0.1657, 0.1307, 0.4523, 0.2513],
        [0.1884, 0.2162, 0.3124, 0.2830]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2030, 0.3460, 0.2085],
        [0.2118, 0.2319, 0.2985, 0.2578],
        [0.2013, 0.1966, 0.3194, 0.2826]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.1659, 0.3866, 0.1994],
        [0.3068, 0.1298, 0.3641, 0.1992],
        [0.1773, 0.1682, 0.2285, 0.4261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2932, 0.1483, 0.4032, 0.1553],
        [0.2184, 0.2116, 0.2587, 0.3112],
        [0.1861, 0.1905, 0.2661, 0.3573]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2093, 0.2665, 0.2833, 0.2409],
        [0.2092, 0.2571, 0.2811, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2215, 0.2446, 0.3414, 0.1925],
        [0.2046, 0.2351, 0.3510, 0.2093],
        [0.2159, 0.2179, 0.2131, 0.3531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2824, 0.3469, 0.1510],
        [0.2074, 0.2300, 0.2561, 0.3065],
        [0.1973, 0.2088, 0.2268, 0.3672]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.2547, 0.3058, 0.1978],
        [0.2203, 0.2286, 0.2809, 0.2702],
        [0.2258, 0.2236, 0.2497, 0.3009]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2187, 0.2301, 0.3643, 0.1870],
        [0.2096, 0.2179, 0.2776, 0.2949],
        [0.2368, 0.2170, 0.2434, 0.3028]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1985, 0.2595, 0.3291, 0.2129],
        [0.2086, 0.2447, 0.3072, 0.2396],
        [0.1921, 0.2473, 0.2660, 0.2946]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1306, 0.1372, 0.5232, 0.2090],
        [0.1369, 0.1445, 0.4899, 0.2287]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1191, 0.1291, 0.5989, 0.1529],
        [0.1183, 0.1243, 0.5683, 0.1891],
        [0.2005, 0.1760, 0.2387, 0.3847]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0901, 0.0984, 0.6951, 0.1164],
        [0.1228, 0.1316, 0.3645, 0.3812],
        [0.1519, 0.1740, 0.3976, 0.2765]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1529, 0.1630, 0.4109, 0.2732],
        [0.1368, 0.1518, 0.4797, 0.2317],
        [0.1629, 0.1807, 0.4581, 0.1984]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0950, 0.1099, 0.5650, 0.2302],
        [0.0820, 0.0864, 0.5998, 0.2317],
        [0.0987, 0.1214, 0.6043, 0.1755]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1146, 0.1164, 0.5846, 0.1844],
        [0.1142, 0.1288, 0.5685, 0.1885],
        [0.1921, 0.1694, 0.2939, 0.3446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:18:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.1761 (0.2686)	Prec@(1,5) (90.2%, 100.0%)	
08/11 04:18:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.1746 (0.2637)	Prec@(1,5) (90.7%, 99.9%)	
08/11 04:19:49AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.2375 (0.2623)	Prec@(1,5) (90.9%, 99.9%)	
08/11 04:20:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.3041 (0.2647)	Prec@(1,5) (90.7%, 99.9%)	
08/11 04:21:32AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.1674 (0.2718)	Prec@(1,5) (90.4%, 99.8%)	
08/11 04:22:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.2210 (0.2734)	Prec@(1,5) (90.4%, 99.8%)	
08/11 04:23:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.2228 (0.2762)	Prec@(1,5) (90.3%, 99.8%)	
08/11 04:23:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.1914 (0.2775)	Prec@(1,5) (90.3%, 99.8%)	
08/11 04:23:58AM searchStage_trainer.py:210 [INFO] Train: [ 32/49] Final Prec@1 90.2640%
08/11 04:24:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.4628	Prec@(1,5) (85.0%, 99.3%)
08/11 04:24:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.4714	Prec@(1,5) (84.8%, 99.2%)
08/11 04:24:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.4880	Prec@(1,5) (84.5%, 99.2%)
08/11 04:24:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.4846	Prec@(1,5) (84.4%, 99.1%)
08/11 04:24:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.4865	Prec@(1,5) (84.2%, 99.1%)
08/11 04:24:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.4874	Prec@(1,5) (84.3%, 99.2%)
08/11 04:24:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.4859	Prec@(1,5) (84.3%, 99.1%)
08/11 04:25:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.4841	Prec@(1,5) (84.3%, 99.1%)
08/11 04:25:04AM searchStage_trainer.py:249 [INFO] Valid: [ 32/49] Final Prec@1 84.2960%
08/11 04:25:04AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:25:04AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.2960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1104, 0.1193, 0.5565, 0.2138],
        [0.1143, 0.1200, 0.5483, 0.2174]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2746, 0.1485, 0.3314, 0.2455],
        [0.2533, 0.1476, 0.3555, 0.2436],
        [0.2197, 0.1624, 0.3813, 0.2366]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1288, 0.1067, 0.5769, 0.1876],
        [0.1631, 0.1278, 0.4605, 0.2485],
        [0.1898, 0.2161, 0.3116, 0.2826]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2412, 0.2008, 0.3505, 0.2074],
        [0.2109, 0.2305, 0.3023, 0.2563],
        [0.2000, 0.1939, 0.3218, 0.2843]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.1640, 0.3871, 0.1995],
        [0.3060, 0.1290, 0.3666, 0.1984],
        [0.1749, 0.1682, 0.2303, 0.4266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2908, 0.1450, 0.4108, 0.1535],
        [0.2177, 0.2107, 0.2593, 0.3124],
        [0.1851, 0.1885, 0.2660, 0.3603]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2085, 0.2676, 0.2839, 0.2400],
        [0.2084, 0.2563, 0.2820, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2191, 0.2422, 0.3476, 0.1910],
        [0.2026, 0.2345, 0.3582, 0.2047],
        [0.2144, 0.2145, 0.2097, 0.3614]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2182, 0.2830, 0.3488, 0.1500],
        [0.2038, 0.2298, 0.2572, 0.3092],
        [0.1974, 0.2089, 0.2261, 0.3675]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2390, 0.2548, 0.3079, 0.1983],
        [0.2189, 0.2280, 0.2840, 0.2690],
        [0.2248, 0.2243, 0.2504, 0.3006]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2161, 0.2296, 0.3697, 0.1846],
        [0.2067, 0.2164, 0.2794, 0.2976],
        [0.2368, 0.2159, 0.2438, 0.3035]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1948, 0.2614, 0.3331, 0.2107],
        [0.2057, 0.2438, 0.3096, 0.2409],
        [0.1894, 0.2483, 0.2681, 0.2942]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1264, 0.1329, 0.5343, 0.2064],
        [0.1335, 0.1399, 0.4998, 0.2268]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1134, 0.1233, 0.6141, 0.1492],
        [0.1138, 0.1191, 0.5826, 0.1845],
        [0.1968, 0.1734, 0.2372, 0.3927]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0837, 0.0914, 0.7137, 0.1111],
        [0.1182, 0.1276, 0.3639, 0.3903],
        [0.1493, 0.1706, 0.4036, 0.2764]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1463, 0.1577, 0.4239, 0.2721],
        [0.1320, 0.1481, 0.4937, 0.2263],
        [0.1589, 0.1779, 0.4662, 0.1970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0898, 0.1046, 0.5821, 0.2234],
        [0.0771, 0.0818, 0.6127, 0.2284],
        [0.0943, 0.1169, 0.6167, 0.1722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1077, 0.1096, 0.5997, 0.1829],
        [0.1086, 0.1229, 0.5862, 0.1823],
        [0.1924, 0.1688, 0.2949, 0.3438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:25:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.3358 (0.2553)	Prec@(1,5) (91.0%, 99.7%)	
08/11 04:26:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.1242 (0.2489)	Prec@(1,5) (91.4%, 99.8%)	
08/11 04:27:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.1462 (0.2467)	Prec@(1,5) (91.3%, 99.8%)	
08/11 04:28:32AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.2310 (0.2496)	Prec@(1,5) (91.3%, 99.8%)	
08/11 04:29:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1204 (0.2501)	Prec@(1,5) (91.2%, 99.8%)	
08/11 04:30:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.2567 (0.2557)	Prec@(1,5) (91.0%, 99.8%)	
08/11 04:31:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.1514 (0.2571)	Prec@(1,5) (90.9%, 99.8%)	
08/11 04:31:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.3394 (0.2604)	Prec@(1,5) (90.8%, 99.8%)	
08/11 04:31:49AM searchStage_trainer.py:210 [INFO] Train: [ 33/49] Final Prec@1 90.8040%
08/11 04:31:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.4704	Prec@(1,5) (84.7%, 99.3%)
08/11 04:32:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.4617	Prec@(1,5) (84.7%, 99.4%)
08/11 04:32:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.4699	Prec@(1,5) (84.6%, 99.3%)
08/11 04:32:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.4793	Prec@(1,5) (84.5%, 99.2%)
08/11 04:32:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.4808	Prec@(1,5) (84.5%, 99.2%)
08/11 04:32:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.4818	Prec@(1,5) (84.4%, 99.2%)
08/11 04:32:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.4845	Prec@(1,5) (84.3%, 99.2%)
08/11 04:32:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.4832	Prec@(1,5) (84.3%, 99.2%)
08/11 04:32:55AM searchStage_trainer.py:249 [INFO] Valid: [ 33/49] Final Prec@1 84.3240%
08/11 04:32:55AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:32:56AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1089, 0.1184, 0.5607, 0.2120],
        [0.1118, 0.1188, 0.5525, 0.2169]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2735, 0.1489, 0.3331, 0.2445],
        [0.2510, 0.1461, 0.3595, 0.2435],
        [0.2180, 0.1604, 0.3852, 0.2365]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1268, 0.1048, 0.5782, 0.1902],
        [0.1601, 0.1258, 0.4708, 0.2433],
        [0.1922, 0.2156, 0.3114, 0.2807]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2398, 0.1985, 0.3544, 0.2073],
        [0.2101, 0.2318, 0.3030, 0.2551],
        [0.1991, 0.1931, 0.3231, 0.2848]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.1635, 0.3918, 0.1982],
        [0.3056, 0.1277, 0.3703, 0.1964],
        [0.1713, 0.1660, 0.2299, 0.4328]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2911, 0.1418, 0.4153, 0.1519],
        [0.2189, 0.2106, 0.2573, 0.3132],
        [0.1832, 0.1864, 0.2663, 0.3642]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2075, 0.2697, 0.2863, 0.2365],
        [0.2070, 0.2535, 0.2825, 0.2570]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2186, 0.2421, 0.3485, 0.1908],
        [0.2000, 0.2348, 0.3615, 0.2038],
        [0.2135, 0.2133, 0.2097, 0.3635]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2135, 0.2846, 0.3535, 0.1484],
        [0.2010, 0.2282, 0.2574, 0.3133],
        [0.1959, 0.2096, 0.2279, 0.3666]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2374, 0.2573, 0.3086, 0.1967],
        [0.2186, 0.2278, 0.2833, 0.2703],
        [0.2219, 0.2243, 0.2524, 0.3015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2137, 0.2284, 0.3727, 0.1852],
        [0.2050, 0.2162, 0.2827, 0.2962],
        [0.2367, 0.2155, 0.2454, 0.3024]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1928, 0.2616, 0.3369, 0.2088],
        [0.2032, 0.2448, 0.3127, 0.2393],
        [0.1875, 0.2477, 0.2679, 0.2970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1221, 0.1280, 0.5467, 0.2032],
        [0.1304, 0.1346, 0.5101, 0.2249]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1090, 0.1175, 0.6273, 0.1461],
        [0.1099, 0.1139, 0.5970, 0.1792],
        [0.1926, 0.1698, 0.2363, 0.4013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0781, 0.0841, 0.7320, 0.1058],
        [0.1151, 0.1235, 0.3623, 0.3991],
        [0.1465, 0.1661, 0.4092, 0.2782]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1426, 0.1527, 0.4398, 0.2650],
        [0.1281, 0.1431, 0.5070, 0.2218],
        [0.1544, 0.1724, 0.4735, 0.1997]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0849, 0.0988, 0.6035, 0.2129],
        [0.0725, 0.0764, 0.6242, 0.2269],
        [0.0898, 0.1111, 0.6289, 0.1702]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1014, 0.1033, 0.6165, 0.1788],
        [0.1034, 0.1168, 0.6043, 0.1755],
        [0.1914, 0.1660, 0.2930, 0.3496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:33:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1175 (0.2254)	Prec@(1,5) (91.7%, 100.0%)	
08/11 04:34:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.3824 (0.2349)	Prec@(1,5) (91.5%, 99.9%)	
08/11 04:35:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.3321 (0.2336)	Prec@(1,5) (91.7%, 99.8%)	
08/11 04:36:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.2452 (0.2310)	Prec@(1,5) (91.8%, 99.8%)	
08/11 04:37:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.3762 (0.2379)	Prec@(1,5) (91.6%, 99.8%)	
08/11 04:38:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.2420 (0.2377)	Prec@(1,5) (91.6%, 99.8%)	
08/11 04:38:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.3525 (0.2381)	Prec@(1,5) (91.6%, 99.8%)	
08/11 04:39:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.1816 (0.2388)	Prec@(1,5) (91.6%, 99.8%)	
08/11 04:39:41AM searchStage_trainer.py:210 [INFO] Train: [ 34/49] Final Prec@1 91.6080%
08/11 04:39:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.4692	Prec@(1,5) (84.6%, 99.3%)
08/11 04:39:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.4919	Prec@(1,5) (83.9%, 99.3%)
08/11 04:40:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.4769	Prec@(1,5) (84.6%, 99.3%)
08/11 04:40:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.4890	Prec@(1,5) (84.3%, 99.3%)
08/11 04:40:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.4854	Prec@(1,5) (84.3%, 99.3%)
08/11 04:40:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.4862	Prec@(1,5) (84.3%, 99.3%)
08/11 04:40:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.4898	Prec@(1,5) (84.2%, 99.4%)
08/11 04:40:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.4940	Prec@(1,5) (84.0%, 99.3%)
08/11 04:40:47AM searchStage_trainer.py:249 [INFO] Valid: [ 34/49] Final Prec@1 84.0360%
08/11 04:40:47AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:40:47AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1080, 0.1183, 0.5620, 0.2117],
        [0.1106, 0.1185, 0.5554, 0.2156]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2744, 0.1479, 0.3335, 0.2442],
        [0.2505, 0.1446, 0.3625, 0.2425],
        [0.2161, 0.1570, 0.3903, 0.2366]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1267, 0.1029, 0.5784, 0.1921],
        [0.1591, 0.1242, 0.4768, 0.2399],
        [0.1976, 0.2166, 0.3056, 0.2803]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2382, 0.1972, 0.3570, 0.2076],
        [0.2102, 0.2332, 0.3035, 0.2531],
        [0.1990, 0.1929, 0.3219, 0.2862]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.1621, 0.3943, 0.1953],
        [0.3074, 0.1256, 0.3706, 0.1964],
        [0.1684, 0.1633, 0.2278, 0.4405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2927, 0.1401, 0.4168, 0.1504],
        [0.2190, 0.2079, 0.2582, 0.3149],
        [0.1832, 0.1850, 0.2647, 0.3670]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2095, 0.2696, 0.2846, 0.2363],
        [0.2075, 0.2518, 0.2829, 0.2577]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2162, 0.2412, 0.3513, 0.1913],
        [0.1962, 0.2347, 0.3668, 0.2023],
        [0.2130, 0.2134, 0.2091, 0.3645]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2122, 0.2849, 0.3540, 0.1490],
        [0.1988, 0.2279, 0.2580, 0.3153],
        [0.1949, 0.2118, 0.2299, 0.3635]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2338, 0.2584, 0.3104, 0.1975],
        [0.2175, 0.2283, 0.2837, 0.2705],
        [0.2216, 0.2253, 0.2540, 0.2991]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2118, 0.2257, 0.3772, 0.1853],
        [0.2035, 0.2148, 0.2841, 0.2976],
        [0.2370, 0.2159, 0.2475, 0.2996]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1896, 0.2622, 0.3383, 0.2099],
        [0.2012, 0.2479, 0.3177, 0.2332],
        [0.1840, 0.2480, 0.2673, 0.3007]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1166, 0.1222, 0.5617, 0.1995],
        [0.1248, 0.1296, 0.5237, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1025, 0.1110, 0.6446, 0.1420],
        [0.1038, 0.1078, 0.6150, 0.1734],
        [0.1886, 0.1660, 0.2348, 0.4106]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0722, 0.0777, 0.7504, 0.0997],
        [0.1097, 0.1184, 0.3585, 0.4134],
        [0.1432, 0.1624, 0.4144, 0.2800]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1360, 0.1472, 0.4570, 0.2598],
        [0.1218, 0.1375, 0.5254, 0.2154],
        [0.1497, 0.1676, 0.4820, 0.2007]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0787, 0.0925, 0.6242, 0.2045],
        [0.0675, 0.0715, 0.6411, 0.2199],
        [0.0855, 0.1053, 0.6406, 0.1686]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0944, 0.0966, 0.6375, 0.1716],
        [0.0966, 0.1093, 0.6251, 0.1690],
        [0.1893, 0.1621, 0.2904, 0.3581]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:41:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.2302 (0.2142)	Prec@(1,5) (92.1%, 99.8%)	
08/11 04:42:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.2218 (0.2240)	Prec@(1,5) (92.1%, 99.8%)	
08/11 04:43:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.3771 (0.2211)	Prec@(1,5) (92.2%, 99.8%)	
08/11 04:44:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.1685 (0.2279)	Prec@(1,5) (91.9%, 99.9%)	
08/11 04:45:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1193 (0.2293)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:45:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.2688 (0.2304)	Prec@(1,5) (91.8%, 99.8%)	
08/11 04:46:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.2325 (0.2298)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:47:32AM searchStage_trainer.py:200 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.2020 (0.2305)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:47:32AM searchStage_trainer.py:210 [INFO] Train: [ 35/49] Final Prec@1 91.9560%
08/11 04:47:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.4595	Prec@(1,5) (85.4%, 99.4%)
08/11 04:47:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.4801	Prec@(1,5) (84.7%, 99.3%)
08/11 04:47:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.4860	Prec@(1,5) (84.4%, 99.3%)
08/11 04:48:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.4872	Prec@(1,5) (84.5%, 99.2%)
08/11 04:48:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.4973	Prec@(1,5) (84.3%, 99.1%)
08/11 04:48:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.4955	Prec@(1,5) (84.3%, 99.2%)
08/11 04:48:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.4938	Prec@(1,5) (84.4%, 99.2%)
08/11 04:48:39AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.4920	Prec@(1,5) (84.4%, 99.2%)
08/11 04:48:39AM searchStage_trainer.py:249 [INFO] Valid: [ 35/49] Final Prec@1 84.3960%
08/11 04:48:39AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:48:39AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1085, 0.1186, 0.5595, 0.2135],
        [0.1113, 0.1192, 0.5559, 0.2136]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2749, 0.1474, 0.3343, 0.2433],
        [0.2507, 0.1436, 0.3646, 0.2410],
        [0.2154, 0.1545, 0.3921, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1238, 0.1017, 0.5825, 0.1920],
        [0.1581, 0.1227, 0.4834, 0.2358],
        [0.1959, 0.2165, 0.3057, 0.2819]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.1964, 0.3547, 0.2068],
        [0.2093, 0.2312, 0.3031, 0.2564],
        [0.1992, 0.1918, 0.3248, 0.2843]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.1611, 0.4001, 0.1933],
        [0.3053, 0.1230, 0.3762, 0.1955],
        [0.1662, 0.1614, 0.2268, 0.4456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2916, 0.1376, 0.4201, 0.1507],
        [0.2207, 0.2077, 0.2587, 0.3129],
        [0.1838, 0.1842, 0.2629, 0.3690]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2081, 0.2713, 0.2859, 0.2347],
        [0.2049, 0.2525, 0.2836, 0.2591]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2122, 0.2424, 0.3547, 0.1906],
        [0.1930, 0.2356, 0.3718, 0.1995],
        [0.2102, 0.2127, 0.2082, 0.3689]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2116, 0.2855, 0.3541, 0.1488],
        [0.1972, 0.2274, 0.2589, 0.3166],
        [0.1926, 0.2123, 0.2318, 0.3633]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2315, 0.2600, 0.3126, 0.1959],
        [0.2152, 0.2280, 0.2855, 0.2713],
        [0.2192, 0.2252, 0.2561, 0.2996]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2103, 0.2242, 0.3800, 0.1855],
        [0.2022, 0.2134, 0.2861, 0.2982],
        [0.2361, 0.2161, 0.2505, 0.2974]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1868, 0.2628, 0.3437, 0.2067],
        [0.1996, 0.2462, 0.3195, 0.2347],
        [0.1817, 0.2482, 0.2683, 0.3017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1127, 0.1187, 0.5697, 0.1989],
        [0.1217, 0.1267, 0.5342, 0.2175]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0978, 0.1060, 0.6566, 0.1396],
        [0.0994, 0.1033, 0.6272, 0.1700],
        [0.1866, 0.1647, 0.2338, 0.4149]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0671, 0.0718, 0.7662, 0.0948],
        [0.1060, 0.1149, 0.3566, 0.4226],
        [0.1397, 0.1598, 0.4182, 0.2823]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1306, 0.1420, 0.4722, 0.2552],
        [0.1161, 0.1325, 0.5429, 0.2085],
        [0.1449, 0.1633, 0.4899, 0.2019]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0741, 0.0871, 0.6448, 0.1939],
        [0.0629, 0.0672, 0.6545, 0.2154],
        [0.0812, 0.1001, 0.6501, 0.1686]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0882, 0.0907, 0.6573, 0.1638],
        [0.0904, 0.1024, 0.6431, 0.1641],
        [0.1871, 0.1588, 0.2876, 0.3665]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:49:32AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.3140 (0.1980)	Prec@(1,5) (93.3%, 99.9%)	
08/11 04:50:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.2886 (0.1913)	Prec@(1,5) (93.5%, 99.9%)	
08/11 04:51:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1467 (0.1943)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:52:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1118 (0.1976)	Prec@(1,5) (93.1%, 99.9%)	
08/11 04:52:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.1828 (0.1972)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:53:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.1393 (0.1971)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:54:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.1000 (0.2005)	Prec@(1,5) (93.0%, 99.9%)	
08/11 04:55:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.1315 (0.2048)	Prec@(1,5) (92.9%, 99.9%)	
08/11 04:55:24AM searchStage_trainer.py:210 [INFO] Train: [ 36/49] Final Prec@1 92.9160%
08/11 04:55:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.4841	Prec@(1,5) (84.9%, 99.3%)
08/11 04:55:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.4759	Prec@(1,5) (84.9%, 99.5%)
08/11 04:55:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.4778	Prec@(1,5) (84.9%, 99.4%)
08/11 04:55:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.4854	Prec@(1,5) (84.8%, 99.4%)
08/11 04:56:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.4808	Prec@(1,5) (85.0%, 99.4%)
08/11 04:56:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.4884	Prec@(1,5) (84.8%, 99.4%)
08/11 04:56:24AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.4926	Prec@(1,5) (84.6%, 99.4%)
08/11 04:56:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.4915	Prec@(1,5) (84.6%, 99.4%)
08/11 04:56:31AM searchStage_trainer.py:249 [INFO] Valid: [ 36/49] Final Prec@1 84.5960%
08/11 04:56:31AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:56:31AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1083, 0.1193, 0.5592, 0.2132],
        [0.1115, 0.1202, 0.5548, 0.2135]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2740, 0.1463, 0.3382, 0.2415],
        [0.2493, 0.1416, 0.3679, 0.2411],
        [0.2153, 0.1520, 0.3943, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1233, 0.1020, 0.5809, 0.1938],
        [0.1561, 0.1217, 0.4858, 0.2364],
        [0.1957, 0.2189, 0.3083, 0.2771]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2413, 0.1956, 0.3551, 0.2080],
        [0.2096, 0.2296, 0.3048, 0.2560],
        [0.1987, 0.1917, 0.3269, 0.2827]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2427, 0.1595, 0.4056, 0.1922],
        [0.3061, 0.1225, 0.3784, 0.1930],
        [0.1629, 0.1586, 0.2254, 0.4531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2903, 0.1381, 0.4230, 0.1486],
        [0.2214, 0.2054, 0.2567, 0.3165],
        [0.1822, 0.1822, 0.2639, 0.3716]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2049, 0.2744, 0.2887, 0.2320],
        [0.2017, 0.2515, 0.2858, 0.2610]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2097, 0.2430, 0.3585, 0.1889],
        [0.1885, 0.2370, 0.3768, 0.1977],
        [0.2073, 0.2114, 0.2075, 0.3737]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2089, 0.2864, 0.3561, 0.1486],
        [0.1953, 0.2277, 0.2594, 0.3176],
        [0.1908, 0.2135, 0.2328, 0.3629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2287, 0.2621, 0.3146, 0.1947],
        [0.2146, 0.2276, 0.2852, 0.2725],
        [0.2156, 0.2256, 0.2594, 0.2994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2069, 0.2226, 0.3861, 0.1845],
        [0.1988, 0.2131, 0.2905, 0.2976],
        [0.2354, 0.2150, 0.2521, 0.2975]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1833, 0.2626, 0.3477, 0.2063],
        [0.1967, 0.2439, 0.3239, 0.2355],
        [0.1794, 0.2500, 0.2717, 0.2989]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1074, 0.1134, 0.5825, 0.1966],
        [0.1168, 0.1219, 0.5485, 0.2128]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0913, 0.0996, 0.6742, 0.1349],
        [0.0930, 0.0972, 0.6460, 0.1637],
        [0.1820, 0.1605, 0.2321, 0.4253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0613, 0.0657, 0.7838, 0.0892],
        [0.1011, 0.1110, 0.3576, 0.4303],
        [0.1354, 0.1552, 0.4229, 0.2865]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1232, 0.1371, 0.4941, 0.2457],
        [0.1096, 0.1256, 0.5608, 0.2040],
        [0.1396, 0.1573, 0.4994, 0.2037]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0689, 0.0812, 0.6683, 0.1816],
        [0.0585, 0.0625, 0.6698, 0.2093],
        [0.0766, 0.0940, 0.6593, 0.1702]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0821, 0.0842, 0.6777, 0.1560],
        [0.0842, 0.0948, 0.6608, 0.1603],
        [0.1853, 0.1555, 0.2879, 0.3713]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:57:24AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.2368 (0.1792)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:58:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.0735 (0.1860)	Prec@(1,5) (93.4%, 99.9%)	
08/11 04:59:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.1935 (0.1920)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:59:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1603 (0.1909)	Prec@(1,5) (93.4%, 99.9%)	
08/11 05:00:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.0709 (0.1897)	Prec@(1,5) (93.4%, 99.9%)	
08/11 05:01:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.3894 (0.1919)	Prec@(1,5) (93.3%, 99.9%)	
08/11 05:02:35AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.1573 (0.1942)	Prec@(1,5) (93.3%, 99.9%)	
08/11 05:03:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.2341 (0.1961)	Prec@(1,5) (93.2%, 99.9%)	
08/11 05:03:16AM searchStage_trainer.py:210 [INFO] Train: [ 37/49] Final Prec@1 93.1960%
08/11 05:03:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5272	Prec@(1,5) (84.2%, 99.2%)
08/11 05:03:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.5378	Prec@(1,5) (83.9%, 99.3%)
08/11 05:03:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5303	Prec@(1,5) (84.0%, 99.3%)
08/11 05:03:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5205	Prec@(1,5) (84.3%, 99.3%)
08/11 05:03:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5202	Prec@(1,5) (84.2%, 99.2%)
08/11 05:04:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5142	Prec@(1,5) (84.4%, 99.3%)
08/11 05:04:16AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5139	Prec@(1,5) (84.3%, 99.3%)
08/11 05:04:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5103	Prec@(1,5) (84.3%, 99.3%)
08/11 05:04:23AM searchStage_trainer.py:249 [INFO] Valid: [ 37/49] Final Prec@1 84.3080%
08/11 05:04:23AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:04:23AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1078, 0.1195, 0.5593, 0.2134],
        [0.1108, 0.1200, 0.5569, 0.2123]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.1455, 0.3399, 0.2423],
        [0.2474, 0.1406, 0.3721, 0.2399],
        [0.2124, 0.1509, 0.3992, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1214, 0.1018, 0.5827, 0.1941],
        [0.1544, 0.1200, 0.4916, 0.2340],
        [0.1938, 0.2194, 0.3099, 0.2770]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2401, 0.1950, 0.3572, 0.2077],
        [0.2054, 0.2313, 0.3074, 0.2558],
        [0.1968, 0.1910, 0.3307, 0.2816]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2406, 0.1595, 0.4073, 0.1926],
        [0.3096, 0.1214, 0.3791, 0.1900],
        [0.1599, 0.1561, 0.2237, 0.4603]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2940, 0.1368, 0.4226, 0.1465],
        [0.2201, 0.2033, 0.2548, 0.3218],
        [0.1803, 0.1816, 0.2656, 0.3726]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2051, 0.2755, 0.2898, 0.2295],
        [0.2015, 0.2500, 0.2844, 0.2641]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2059, 0.2428, 0.3643, 0.1870],
        [0.1853, 0.2354, 0.3823, 0.1970],
        [0.2053, 0.2106, 0.2071, 0.3769]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2075, 0.2892, 0.3574, 0.1459],
        [0.1934, 0.2262, 0.2571, 0.3233],
        [0.1880, 0.2139, 0.2325, 0.3656]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2632, 0.3137, 0.1937],
        [0.2143, 0.2280, 0.2868, 0.2709],
        [0.2147, 0.2236, 0.2586, 0.3031]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2033, 0.2201, 0.3922, 0.1843],
        [0.1948, 0.2125, 0.2943, 0.2984],
        [0.2333, 0.2160, 0.2568, 0.2939]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1801, 0.2633, 0.3537, 0.2029],
        [0.1955, 0.2435, 0.3262, 0.2347],
        [0.1773, 0.2480, 0.2720, 0.3027]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1026, 0.1087, 0.5944, 0.1944],
        [0.1120, 0.1169, 0.5631, 0.2079]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0861, 0.0951, 0.6880, 0.1308],
        [0.0882, 0.0929, 0.6598, 0.1590],
        [0.1768, 0.1568, 0.2317, 0.4347]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0565, 0.0605, 0.7980, 0.0850],
        [0.0969, 0.1065, 0.3567, 0.4399],
        [0.1334, 0.1524, 0.4270, 0.2872]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1182, 0.1319, 0.5136, 0.2363],
        [0.1051, 0.1195, 0.5767, 0.1987],
        [0.1348, 0.1518, 0.5057, 0.2077]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0641, 0.0757, 0.6898, 0.1704],
        [0.0546, 0.0582, 0.6820, 0.2052],
        [0.0725, 0.0890, 0.6678, 0.1707]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0770, 0.0788, 0.6942, 0.1501],
        [0.0792, 0.0887, 0.6782, 0.1539],
        [0.1837, 0.1512, 0.2850, 0.3800]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:05:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.2341 (0.1718)	Prec@(1,5) (94.9%, 100.0%)	
08/11 05:06:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.1739 (0.1633)	Prec@(1,5) (94.7%, 100.0%)	
08/11 05:06:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.2433 (0.1665)	Prec@(1,5) (94.7%, 99.9%)	
08/11 05:07:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.2221 (0.1685)	Prec@(1,5) (94.4%, 99.9%)	
08/11 05:08:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.2177 (0.1723)	Prec@(1,5) (94.1%, 99.9%)	
08/11 05:09:35AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.2298 (0.1741)	Prec@(1,5) (94.1%, 99.9%)	
08/11 05:10:27AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.4018 (0.1781)	Prec@(1,5) (94.0%, 99.9%)	
08/11 05:11:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.2365 (0.1781)	Prec@(1,5) (93.9%, 99.9%)	
08/11 05:11:09AM searchStage_trainer.py:210 [INFO] Train: [ 38/49] Final Prec@1 93.9240%
08/11 05:11:17AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.4935	Prec@(1,5) (85.1%, 99.3%)
08/11 05:11:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.4954	Prec@(1,5) (85.2%, 99.4%)
08/11 05:11:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.4967	Prec@(1,5) (85.1%, 99.4%)
08/11 05:11:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.4964	Prec@(1,5) (85.0%, 99.4%)
08/11 05:11:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.4990	Prec@(1,5) (84.9%, 99.4%)
08/11 05:12:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.4973	Prec@(1,5) (84.9%, 99.4%)
08/11 05:12:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.4919	Prec@(1,5) (85.1%, 99.4%)
08/11 05:12:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.4880	Prec@(1,5) (85.1%, 99.4%)
08/11 05:12:15AM searchStage_trainer.py:249 [INFO] Valid: [ 38/49] Final Prec@1 85.1000%
08/11 05:12:15AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:12:15AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.1000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1075, 0.1207, 0.5608, 0.2110],
        [0.1101, 0.1211, 0.5548, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.1450, 0.3409, 0.2432],
        [0.2477, 0.1397, 0.3750, 0.2376],
        [0.2086, 0.1501, 0.4035, 0.2378]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1202, 0.1021, 0.5841, 0.1936],
        [0.1538, 0.1188, 0.4961, 0.2313],
        [0.1922, 0.2185, 0.3102, 0.2792]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2386, 0.1945, 0.3595, 0.2074],
        [0.2072, 0.2308, 0.3064, 0.2556],
        [0.1956, 0.1915, 0.3309, 0.2820]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2396, 0.1583, 0.4095, 0.1926],
        [0.3062, 0.1204, 0.3847, 0.1886],
        [0.1589, 0.1558, 0.2233, 0.4619]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2926, 0.1370, 0.4253, 0.1452],
        [0.2186, 0.2009, 0.2564, 0.3241],
        [0.1795, 0.1809, 0.2657, 0.3739]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2028, 0.2775, 0.2925, 0.2273],
        [0.2001, 0.2503, 0.2833, 0.2663]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2008, 0.2418, 0.3712, 0.1862],
        [0.1806, 0.2340, 0.3880, 0.1973],
        [0.2041, 0.2115, 0.2083, 0.3760]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2056, 0.2899, 0.3605, 0.1440],
        [0.1902, 0.2249, 0.2579, 0.3270],
        [0.1878, 0.2128, 0.2320, 0.3673]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2282, 0.2626, 0.3146, 0.1946],
        [0.2143, 0.2270, 0.2906, 0.2681],
        [0.2131, 0.2227, 0.2601, 0.3041]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2012, 0.2172, 0.3971, 0.1844],
        [0.1926, 0.2109, 0.2991, 0.2973],
        [0.2314, 0.2154, 0.2610, 0.2922]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1764, 0.2634, 0.3588, 0.2013],
        [0.1916, 0.2453, 0.3348, 0.2284],
        [0.1742, 0.2456, 0.2711, 0.3091]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0974, 0.1032, 0.6116, 0.1878],
        [0.1072, 0.1111, 0.5747, 0.2069]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0818, 0.0898, 0.7003, 0.1281],
        [0.0844, 0.0881, 0.6739, 0.1536],
        [0.1739, 0.1532, 0.2292, 0.4438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0518, 0.0552, 0.8131, 0.0799],
        [0.0937, 0.1022, 0.3526, 0.4516],
        [0.1300, 0.1482, 0.4303, 0.2915]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1129, 0.1253, 0.5314, 0.2304],
        [0.1014, 0.1148, 0.5918, 0.1920],
        [0.1295, 0.1466, 0.5148, 0.2092]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0603, 0.0710, 0.7094, 0.1593],
        [0.0512, 0.0545, 0.6906, 0.2037],
        [0.0699, 0.0850, 0.6732, 0.1719]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0710, 0.0723, 0.7127, 0.1440],
        [0.0733, 0.0818, 0.6974, 0.1475],
        [0.1828, 0.1482, 0.2833, 0.3857]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:13:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.1528 (0.1644)	Prec@(1,5) (94.4%, 100.0%)	
08/11 05:13:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.1456 (0.1621)	Prec@(1,5) (94.4%, 99.9%)	
08/11 05:14:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.1639 (0.1599)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:15:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0522 (0.1584)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:16:35AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0768 (0.1603)	Prec@(1,5) (94.4%, 99.9%)	
08/11 05:17:27AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.2650 (0.1642)	Prec@(1,5) (94.2%, 99.9%)	
08/11 05:18:19AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.1632 (0.1663)	Prec@(1,5) (94.1%, 99.9%)	
08/11 05:19:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.1504 (0.1683)	Prec@(1,5) (94.1%, 99.9%)	
08/11 05:19:01AM searchStage_trainer.py:210 [INFO] Train: [ 39/49] Final Prec@1 94.0880%
08/11 05:19:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.4835	Prec@(1,5) (85.0%, 99.3%)
08/11 05:19:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.4851	Prec@(1,5) (85.0%, 99.2%)
08/11 05:19:27AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.4746	Prec@(1,5) (85.4%, 99.2%)
08/11 05:19:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.4784	Prec@(1,5) (85.4%, 99.3%)
08/11 05:19:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.4841	Prec@(1,5) (85.1%, 99.3%)
08/11 05:19:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.4830	Prec@(1,5) (85.1%, 99.3%)
08/11 05:20:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.4824	Prec@(1,5) (85.1%, 99.3%)
08/11 05:20:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.4803	Prec@(1,5) (85.2%, 99.3%)
08/11 05:20:08AM searchStage_trainer.py:249 [INFO] Valid: [ 39/49] Final Prec@1 85.2520%
08/11 05:20:08AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:20:08AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.2520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1067, 0.1195, 0.5654, 0.2084],
        [0.1100, 0.1193, 0.5553, 0.2154]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2732, 0.1443, 0.3401, 0.2425],
        [0.2491, 0.1371, 0.3755, 0.2384],
        [0.2093, 0.1485, 0.4047, 0.2375]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1181, 0.1009, 0.5883, 0.1926],
        [0.1534, 0.1184, 0.4984, 0.2298],
        [0.1920, 0.2186, 0.3088, 0.2805]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2395, 0.1916, 0.3604, 0.2086],
        [0.2065, 0.2313, 0.3098, 0.2524],
        [0.1943, 0.1895, 0.3334, 0.2828]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2409, 0.1578, 0.4115, 0.1898],
        [0.3063, 0.1197, 0.3850, 0.1889],
        [0.1553, 0.1536, 0.2219, 0.4692]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2948, 0.1363, 0.4258, 0.1431],
        [0.2157, 0.1981, 0.2548, 0.3314],
        [0.1804, 0.1805, 0.2657, 0.3734]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2011, 0.2793, 0.2954, 0.2242],
        [0.1980, 0.2493, 0.2832, 0.2695]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1972, 0.2393, 0.3774, 0.1860],
        [0.1760, 0.2329, 0.3951, 0.1960],
        [0.2037, 0.2115, 0.2081, 0.3766]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2032, 0.2921, 0.3637, 0.1411],
        [0.1895, 0.2238, 0.2577, 0.3291],
        [0.1860, 0.2100, 0.2296, 0.3744]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2275, 0.2644, 0.3152, 0.1930],
        [0.2117, 0.2255, 0.2922, 0.2707],
        [0.2113, 0.2232, 0.2623, 0.3032]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1963, 0.2158, 0.4058, 0.1821],
        [0.1904, 0.2108, 0.3044, 0.2944],
        [0.2288, 0.2130, 0.2626, 0.2956]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1747, 0.2631, 0.3630, 0.1992],
        [0.1885, 0.2434, 0.3410, 0.2271],
        [0.1727, 0.2453, 0.2706, 0.3115]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0932, 0.1003, 0.6223, 0.1841],
        [0.1039, 0.1081, 0.5830, 0.2049]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0769, 0.0853, 0.7126, 0.1252],
        [0.0795, 0.0839, 0.6886, 0.1480],
        [0.1687, 0.1505, 0.2291, 0.4517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0478, 0.0510, 0.8256, 0.0756],
        [0.0905, 0.0994, 0.3518, 0.4584],
        [0.1261, 0.1442, 0.4325, 0.2971]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1068, 0.1198, 0.5523, 0.2211],
        [0.0964, 0.1089, 0.6085, 0.1861],
        [0.1234, 0.1404, 0.5243, 0.2118]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0566, 0.0663, 0.7280, 0.1491],
        [0.0480, 0.0514, 0.7033, 0.1973],
        [0.0664, 0.0802, 0.6780, 0.1753]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0656, 0.0670, 0.7304, 0.1371],
        [0.0682, 0.0757, 0.7138, 0.1422],
        [0.1798, 0.1447, 0.2827, 0.3927]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:21:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.1513 (0.1479)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:21:52AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.1406 (0.1489)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:22:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.1060 (0.1475)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:23:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.1035 (0.1464)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:24:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.1027 (0.1461)	Prec@(1,5) (95.0%, 99.9%)	
08/11 05:25:20AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.1491 (0.1462)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:26:12AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.1321 (0.1460)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:26:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.2026 (0.1473)	Prec@(1,5) (94.9%, 99.9%)	
08/11 05:26:54AM searchStage_trainer.py:210 [INFO] Train: [ 40/49] Final Prec@1 94.8880%
08/11 05:27:02AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.4647	Prec@(1,5) (85.5%, 99.5%)
08/11 05:27:11AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.4652	Prec@(1,5) (86.1%, 99.5%)
08/11 05:27:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.4697	Prec@(1,5) (85.8%, 99.5%)
08/11 05:27:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.4790	Prec@(1,5) (85.6%, 99.4%)
08/11 05:27:36AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.4878	Prec@(1,5) (85.5%, 99.4%)
08/11 05:27:44AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.4902	Prec@(1,5) (85.5%, 99.4%)
08/11 05:27:53AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.4943	Prec@(1,5) (85.4%, 99.4%)
08/11 05:28:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.4937	Prec@(1,5) (85.5%, 99.4%)
08/11 05:28:00AM searchStage_trainer.py:249 [INFO] Valid: [ 40/49] Final Prec@1 85.4480%
08/11 05:28:00AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:28:00AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.4480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1051, 0.1192, 0.5695, 0.2062],
        [0.1088, 0.1185, 0.5567, 0.2160]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2745, 0.1427, 0.3424, 0.2404],
        [0.2492, 0.1354, 0.3750, 0.2404],
        [0.2094, 0.1472, 0.4064, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1160, 0.0990, 0.5932, 0.1917],
        [0.1527, 0.1162, 0.5053, 0.2258],
        [0.1914, 0.2187, 0.3061, 0.2838]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2359, 0.1898, 0.3654, 0.2089],
        [0.2062, 0.2335, 0.3123, 0.2480],
        [0.1913, 0.1877, 0.3360, 0.2850]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.1558, 0.4142, 0.1879],
        [0.3050, 0.1166, 0.3902, 0.1882],
        [0.1516, 0.1516, 0.2222, 0.4746]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2937, 0.1355, 0.4313, 0.1395],
        [0.2130, 0.1941, 0.2544, 0.3385],
        [0.1792, 0.1779, 0.2658, 0.3771]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2005, 0.2816, 0.2943, 0.2236],
        [0.1965, 0.2505, 0.2828, 0.2703]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1932, 0.2374, 0.3818, 0.1877],
        [0.1731, 0.2325, 0.4009, 0.1935],
        [0.2039, 0.2115, 0.2076, 0.3769]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2022, 0.2912, 0.3672, 0.1394],
        [0.1875, 0.2203, 0.2540, 0.3382],
        [0.1861, 0.2115, 0.2315, 0.3709]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2253, 0.2635, 0.3178, 0.1934],
        [0.2096, 0.2245, 0.2943, 0.2716],
        [0.2114, 0.2239, 0.2645, 0.3002]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1938, 0.2130, 0.4122, 0.1810],
        [0.1876, 0.2115, 0.3089, 0.2921],
        [0.2280, 0.2117, 0.2628, 0.2976]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1724, 0.2617, 0.3681, 0.1977],
        [0.1857, 0.2414, 0.3450, 0.2279],
        [0.1705, 0.2473, 0.2719, 0.3103]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0896, 0.0963, 0.6344, 0.1797],
        [0.1002, 0.1039, 0.5929, 0.2030]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0731, 0.0818, 0.7228, 0.1223],
        [0.0759, 0.0805, 0.6991, 0.1445],
        [0.1649, 0.1475, 0.2285, 0.4592]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0444, 0.0474, 0.8364, 0.0718],
        [0.0872, 0.0962, 0.3496, 0.4670],
        [0.1226, 0.1401, 0.4352, 0.3021]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1007, 0.1138, 0.5709, 0.2145],
        [0.0925, 0.1046, 0.6234, 0.1795],
        [0.1188, 0.1353, 0.5323, 0.2136]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0533, 0.0625, 0.7450, 0.1391],
        [0.0455, 0.0487, 0.7122, 0.1937],
        [0.0635, 0.0764, 0.6810, 0.1791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0606, 0.0619, 0.7458, 0.1316],
        [0.0634, 0.0703, 0.7311, 0.1352],
        [0.1781, 0.1409, 0.2804, 0.4006]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:28:53AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.1473 (0.1246)	Prec@(1,5) (95.7%, 100.0%)	
08/11 05:29:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.1162 (0.1270)	Prec@(1,5) (95.6%, 100.0%)	
08/11 05:30:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.2035 (0.1291)	Prec@(1,5) (95.5%, 100.0%)	
08/11 05:31:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.2346 (0.1299)	Prec@(1,5) (95.6%, 100.0%)	
08/11 05:32:20AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.1502 (0.1297)	Prec@(1,5) (95.5%, 99.9%)	
08/11 05:33:12AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.1791 (0.1296)	Prec@(1,5) (95.5%, 99.9%)	
08/11 05:34:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.1391 (0.1306)	Prec@(1,5) (95.5%, 99.9%)	
08/11 05:34:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.1308 (0.1334)	Prec@(1,5) (95.3%, 99.9%)	
08/11 05:34:46AM searchStage_trainer.py:210 [INFO] Train: [ 41/49] Final Prec@1 95.3440%
08/11 05:34:55AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.4807	Prec@(1,5) (86.5%, 99.4%)
08/11 05:35:03AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.4768	Prec@(1,5) (86.3%, 99.3%)
08/11 05:35:12AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.4857	Prec@(1,5) (86.1%, 99.4%)
08/11 05:35:20AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.4844	Prec@(1,5) (86.0%, 99.4%)
08/11 05:35:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.4808	Prec@(1,5) (86.1%, 99.4%)
08/11 05:35:37AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.4880	Prec@(1,5) (86.0%, 99.3%)
08/11 05:35:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.4859	Prec@(1,5) (85.9%, 99.4%)
08/11 05:35:52AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.4910	Prec@(1,5) (85.8%, 99.3%)
08/11 05:35:52AM searchStage_trainer.py:249 [INFO] Valid: [ 41/49] Final Prec@1 85.8360%
08/11 05:35:53AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:35:53AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.8360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1053, 0.1191, 0.5704, 0.2053],
        [0.1081, 0.1176, 0.5582, 0.2161]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2754, 0.1420, 0.3454, 0.2372],
        [0.2487, 0.1329, 0.3748, 0.2436],
        [0.2076, 0.1456, 0.4110, 0.2358]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1148, 0.0988, 0.5969, 0.1895],
        [0.1496, 0.1149, 0.5113, 0.2241],
        [0.1893, 0.2172, 0.3064, 0.2870]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2354, 0.1873, 0.3678, 0.2094],
        [0.2065, 0.2316, 0.3126, 0.2493],
        [0.1913, 0.1863, 0.3403, 0.2821]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.1551, 0.4129, 0.1873],
        [0.3051, 0.1156, 0.3911, 0.1882],
        [0.1503, 0.1508, 0.2220, 0.4770]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2884, 0.1339, 0.4408, 0.1368],
        [0.2090, 0.1906, 0.2555, 0.3449],
        [0.1781, 0.1781, 0.2674, 0.3764]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1981, 0.2826, 0.2966, 0.2227],
        [0.1933, 0.2507, 0.2856, 0.2704]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1869, 0.2340, 0.3908, 0.1883],
        [0.1693, 0.2304, 0.4089, 0.1914],
        [0.2027, 0.2122, 0.2087, 0.3763]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1984, 0.2909, 0.3728, 0.1379],
        [0.1859, 0.2190, 0.2539, 0.3412],
        [0.1844, 0.2118, 0.2316, 0.3722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2236, 0.2658, 0.3195, 0.1911],
        [0.2077, 0.2241, 0.2944, 0.2739],
        [0.2111, 0.2229, 0.2651, 0.3009]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1903, 0.2103, 0.4201, 0.1793],
        [0.1862, 0.2106, 0.3136, 0.2896],
        [0.2267, 0.2100, 0.2631, 0.3002]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1689, 0.2621, 0.3732, 0.1958],
        [0.1834, 0.2406, 0.3488, 0.2272],
        [0.1687, 0.2474, 0.2724, 0.3116]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0858, 0.0925, 0.6461, 0.1757],
        [0.0971, 0.1001, 0.6020, 0.2008]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0701, 0.0781, 0.7327, 0.1191],
        [0.0734, 0.0775, 0.7091, 0.1400],
        [0.1611, 0.1429, 0.2239, 0.4722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0418, 0.0444, 0.8456, 0.0682],
        [0.0839, 0.0921, 0.3451, 0.4790],
        [0.1192, 0.1368, 0.4371, 0.3069]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0950, 0.1080, 0.5927, 0.2044],
        [0.0883, 0.1000, 0.6360, 0.1756],
        [0.1143, 0.1300, 0.5391, 0.2167]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0500, 0.0586, 0.7620, 0.1294],
        [0.0429, 0.0462, 0.7213, 0.1895],
        [0.0604, 0.0725, 0.6837, 0.1834]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0565, 0.0580, 0.7609, 0.1245],
        [0.0590, 0.0651, 0.7455, 0.1303],
        [0.1749, 0.1364, 0.2772, 0.4115]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:36:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.1286 (0.1191)	Prec@(1,5) (96.0%, 100.0%)	
08/11 05:37:37AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.1139 (0.1236)	Prec@(1,5) (95.9%, 100.0%)	
08/11 05:38:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.1846 (0.1220)	Prec@(1,5) (95.9%, 100.0%)	
08/11 05:39:21AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0902 (0.1201)	Prec@(1,5) (96.1%, 100.0%)	
08/11 05:40:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.1703 (0.1229)	Prec@(1,5) (95.9%, 100.0%)	
08/11 05:41:05AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.1926 (0.1262)	Prec@(1,5) (95.7%, 100.0%)	
08/11 05:41:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.2702 (0.1270)	Prec@(1,5) (95.7%, 100.0%)	
08/11 05:42:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.2638 (0.1287)	Prec@(1,5) (95.6%, 100.0%)	
08/11 05:42:39AM searchStage_trainer.py:210 [INFO] Train: [ 42/49] Final Prec@1 95.6240%
08/11 05:42:47AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5213	Prec@(1,5) (85.8%, 99.2%)
08/11 05:42:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5048	Prec@(1,5) (86.1%, 99.3%)
08/11 05:43:04AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5075	Prec@(1,5) (85.8%, 99.3%)
08/11 05:43:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5067	Prec@(1,5) (85.8%, 99.3%)
08/11 05:43:21AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5101	Prec@(1,5) (85.6%, 99.3%)
08/11 05:43:29AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5074	Prec@(1,5) (85.5%, 99.3%)
08/11 05:43:38AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5006	Prec@(1,5) (85.7%, 99.3%)
08/11 05:43:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5016	Prec@(1,5) (85.6%, 99.3%)
08/11 05:43:45AM searchStage_trainer.py:249 [INFO] Valid: [ 42/49] Final Prec@1 85.6160%
08/11 05:43:45AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:43:45AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.8360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1050, 0.1187, 0.5731, 0.2032],
        [0.1076, 0.1174, 0.5577, 0.2173]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.1404, 0.3451, 0.2370],
        [0.2496, 0.1322, 0.3761, 0.2421],
        [0.2072, 0.1435, 0.4123, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1141, 0.0974, 0.5998, 0.1887],
        [0.1488, 0.1132, 0.5183, 0.2197],
        [0.1877, 0.2158, 0.3050, 0.2915]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.1872, 0.3671, 0.2095],
        [0.2069, 0.2314, 0.3144, 0.2474],
        [0.1905, 0.1845, 0.3411, 0.2840]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.1543, 0.4131, 0.1868],
        [0.3075, 0.1139, 0.3906, 0.1880],
        [0.1485, 0.1507, 0.2213, 0.4795]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2907, 0.1321, 0.4432, 0.1340],
        [0.2078, 0.1884, 0.2547, 0.3491],
        [0.1747, 0.1749, 0.2686, 0.3818]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1968, 0.2835, 0.2985, 0.2212],
        [0.1932, 0.2490, 0.2857, 0.2721]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1830, 0.2330, 0.3956, 0.1884],
        [0.1660, 0.2322, 0.4120, 0.1898],
        [0.2020, 0.2116, 0.2089, 0.3774]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1946, 0.2938, 0.3756, 0.1361],
        [0.1849, 0.2172, 0.2526, 0.3454],
        [0.1825, 0.2117, 0.2320, 0.3738]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2682, 0.3216, 0.1905],
        [0.2048, 0.2256, 0.2977, 0.2719],
        [0.2088, 0.2225, 0.2663, 0.3024]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1868, 0.2076, 0.4270, 0.1785],
        [0.1832, 0.2102, 0.3192, 0.2874],
        [0.2230, 0.2099, 0.2671, 0.3000]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1647, 0.2601, 0.3799, 0.1954],
        [0.1805, 0.2398, 0.3533, 0.2264],
        [0.1667, 0.2486, 0.2747, 0.3100]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0815, 0.0876, 0.6594, 0.1715],
        [0.0926, 0.0955, 0.6149, 0.1970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0669, 0.0743, 0.7426, 0.1162],
        [0.0698, 0.0740, 0.7206, 0.1356],
        [0.1562, 0.1390, 0.2237, 0.4811]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0392, 0.0415, 0.8540, 0.0653],
        [0.0805, 0.0882, 0.3406, 0.4908],
        [0.1165, 0.1335, 0.4417, 0.3083]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0893, 0.1013, 0.6122, 0.1973],
        [0.0838, 0.0953, 0.6512, 0.1697],
        [0.1113, 0.1262, 0.5439, 0.2186]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0464, 0.0546, 0.7792, 0.1198],
        [0.0405, 0.0437, 0.7291, 0.1867],
        [0.0572, 0.0685, 0.6872, 0.1871]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0529, 0.0544, 0.7746, 0.1182],
        [0.0548, 0.0606, 0.7594, 0.1252],
        [0.1710, 0.1325, 0.2743, 0.4222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:44:38AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0466 (0.1169)	Prec@(1,5) (96.0%, 100.0%)	
08/11 05:45:29AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.1778 (0.1149)	Prec@(1,5) (96.2%, 100.0%)	
08/11 05:46:21AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.1144 (0.1116)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:47:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0997 (0.1144)	Prec@(1,5) (96.2%, 100.0%)	
08/11 05:48:05AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0353 (0.1120)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:48:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0997 (0.1119)	Prec@(1,5) (96.2%, 100.0%)	
08/11 05:49:49AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.1179 (0.1151)	Prec@(1,5) (96.1%, 100.0%)	
08/11 05:50:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0642 (0.1150)	Prec@(1,5) (96.1%, 100.0%)	
08/11 05:50:31AM searchStage_trainer.py:210 [INFO] Train: [ 43/49] Final Prec@1 96.1200%
08/11 05:50:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.4975	Prec@(1,5) (85.7%, 99.4%)
08/11 05:50:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5094	Prec@(1,5) (85.4%, 99.3%)
08/11 05:50:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5046	Prec@(1,5) (85.4%, 99.4%)
08/11 05:51:05AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.4977	Prec@(1,5) (85.6%, 99.4%)
08/11 05:51:13AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.4997	Prec@(1,5) (85.5%, 99.4%)
08/11 05:51:22AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5020	Prec@(1,5) (85.4%, 99.4%)
08/11 05:51:30AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.4978	Prec@(1,5) (85.7%, 99.4%)
08/11 05:51:37AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5010	Prec@(1,5) (85.6%, 99.4%)
08/11 05:51:37AM searchStage_trainer.py:249 [INFO] Valid: [ 43/49] Final Prec@1 85.6280%
08/11 05:51:37AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:51:38AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.8360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1043, 0.1182, 0.5761, 0.2014],
        [0.1069, 0.1166, 0.5584, 0.2181]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2749, 0.1402, 0.3470, 0.2378],
        [0.2488, 0.1328, 0.3809, 0.2376],
        [0.2039, 0.1425, 0.4143, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1124, 0.0963, 0.6057, 0.1856],
        [0.1484, 0.1128, 0.5212, 0.2176],
        [0.1878, 0.2138, 0.3004, 0.2980]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2345, 0.1885, 0.3676, 0.2094],
        [0.2042, 0.2317, 0.3178, 0.2462],
        [0.1903, 0.1838, 0.3415, 0.2844]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.1542, 0.4135, 0.1848],
        [0.3107, 0.1134, 0.3878, 0.1881],
        [0.1457, 0.1484, 0.2191, 0.4867]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2911, 0.1299, 0.4474, 0.1317],
        [0.2068, 0.1867, 0.2546, 0.3519],
        [0.1726, 0.1722, 0.2686, 0.3866]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1970, 0.2846, 0.2983, 0.2202],
        [0.1915, 0.2498, 0.2853, 0.2734]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1777, 0.2302, 0.4028, 0.1893],
        [0.1628, 0.2319, 0.4198, 0.1855],
        [0.1991, 0.2113, 0.2094, 0.3802]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1936, 0.2933, 0.3789, 0.1342],
        [0.1830, 0.2162, 0.2512, 0.3497],
        [0.1817, 0.2114, 0.2305, 0.3764]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2185, 0.2692, 0.3214, 0.1910],
        [0.2044, 0.2265, 0.3001, 0.2690],
        [0.2063, 0.2218, 0.2675, 0.3044]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1838, 0.2047, 0.4344, 0.1770],
        [0.1809, 0.2090, 0.3232, 0.2870],
        [0.2201, 0.2097, 0.2703, 0.2998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1614, 0.2616, 0.3846, 0.1923],
        [0.1767, 0.2398, 0.3567, 0.2269],
        [0.1643, 0.2485, 0.2760, 0.3112]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0784, 0.0845, 0.6686, 0.1685],
        [0.0894, 0.0922, 0.6246, 0.1937]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0643, 0.0715, 0.7506, 0.1136],
        [0.0671, 0.0712, 0.7287, 0.1330],
        [0.1525, 0.1357, 0.2236, 0.4882]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0372, 0.0392, 0.8613, 0.0624],
        [0.0771, 0.0845, 0.3348, 0.5036],
        [0.1131, 0.1305, 0.4453, 0.3111]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0847, 0.0961, 0.6294, 0.1898],
        [0.0799, 0.0907, 0.6635, 0.1659],
        [0.1078, 0.1221, 0.5506, 0.2195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0436, 0.0513, 0.7943, 0.1109],
        [0.0385, 0.0416, 0.7346, 0.1853],
        [0.0546, 0.0651, 0.6885, 0.1918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0496, 0.0512, 0.7868, 0.1124],
        [0.0510, 0.0566, 0.7719, 0.1205],
        [0.1672, 0.1284, 0.2717, 0.4327]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:52:30AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0865 (0.1041)	Prec@(1,5) (96.8%, 100.0%)	
08/11 05:53:22AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.1658 (0.1027)	Prec@(1,5) (96.7%, 99.9%)	
08/11 05:54:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.1236 (0.1070)	Prec@(1,5) (96.4%, 99.9%)	
08/11 05:55:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.1353 (0.1077)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:55:58AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0661 (0.1080)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:56:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.0903 (0.1082)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:57:41AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.2553 (0.1085)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:58:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0859 (0.1085)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:58:23AM searchStage_trainer.py:210 [INFO] Train: [ 44/49] Final Prec@1 96.3000%
08/11 05:58:32AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.4558	Prec@(1,5) (86.7%, 99.6%)
08/11 05:58:41AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.4797	Prec@(1,5) (86.1%, 99.5%)
08/11 05:58:49AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.4861	Prec@(1,5) (86.2%, 99.4%)
08/11 05:58:57AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.4882	Prec@(1,5) (86.1%, 99.4%)
08/11 05:59:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.4816	Prec@(1,5) (86.3%, 99.3%)
08/11 05:59:14AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.4937	Prec@(1,5) (86.1%, 99.3%)
08/11 05:59:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.4999	Prec@(1,5) (85.9%, 99.3%)
08/11 05:59:30AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5026	Prec@(1,5) (85.9%, 99.3%)
08/11 05:59:30AM searchStage_trainer.py:249 [INFO] Valid: [ 44/49] Final Prec@1 85.8680%
08/11 05:59:30AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:59:30AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.8680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1041, 0.1183, 0.5767, 0.2009],
        [0.1073, 0.1166, 0.5579, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2753, 0.1396, 0.3481, 0.2370],
        [0.2491, 0.1326, 0.3822, 0.2360],
        [0.2034, 0.1400, 0.4156, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1118, 0.0954, 0.6085, 0.1843],
        [0.1473, 0.1114, 0.5252, 0.2161],
        [0.1875, 0.2128, 0.2989, 0.3008]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2330, 0.1871, 0.3700, 0.2099],
        [0.2028, 0.2330, 0.3207, 0.2434],
        [0.1874, 0.1812, 0.3462, 0.2851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.1544, 0.4156, 0.1819],
        [0.3110, 0.1113, 0.3895, 0.1882],
        [0.1427, 0.1459, 0.2162, 0.4951]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2939, 0.1290, 0.4485, 0.1287],
        [0.2045, 0.1843, 0.2525, 0.3587],
        [0.1697, 0.1705, 0.2682, 0.3915]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1963, 0.2865, 0.2985, 0.2187],
        [0.1890, 0.2486, 0.2876, 0.2748]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1736, 0.2267, 0.4095, 0.1901],
        [0.1592, 0.2289, 0.4269, 0.1849],
        [0.1982, 0.2137, 0.2109, 0.3772]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1894, 0.2942, 0.3845, 0.1319],
        [0.1809, 0.2133, 0.2472, 0.3586],
        [0.1799, 0.2128, 0.2326, 0.3747]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2172, 0.2704, 0.3213, 0.1912],
        [0.2027, 0.2272, 0.3022, 0.2679],
        [0.2051, 0.2218, 0.2683, 0.3047]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1824, 0.2031, 0.4412, 0.1733],
        [0.1777, 0.2076, 0.3269, 0.2877],
        [0.2161, 0.2080, 0.2729, 0.3030]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1552, 0.2638, 0.3924, 0.1886],
        [0.1731, 0.2387, 0.3611, 0.2272],
        [0.1615, 0.2488, 0.2773, 0.3123]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0755, 0.0813, 0.6798, 0.1635],
        [0.0868, 0.0889, 0.6313, 0.1930]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0620, 0.0691, 0.7573, 0.1115],
        [0.0646, 0.0690, 0.7363, 0.1300],
        [0.1483, 0.1325, 0.2229, 0.4963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0351, 0.0370, 0.8675, 0.0604],
        [0.0743, 0.0815, 0.3312, 0.5129],
        [0.1111, 0.1281, 0.4488, 0.3120]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0799, 0.0907, 0.6462, 0.1832],
        [0.0768, 0.0874, 0.6750, 0.1607],
        [0.1046, 0.1194, 0.5539, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0416, 0.0486, 0.8062, 0.1036],
        [0.0369, 0.0400, 0.7389, 0.1841],
        [0.0526, 0.0629, 0.6881, 0.1965]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0467, 0.0481, 0.7976, 0.1076],
        [0.0477, 0.0529, 0.7851, 0.1144],
        [0.1631, 0.1238, 0.2680, 0.4452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:00:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0482 (0.0969)	Prec@(1,5) (97.0%, 99.9%)	
08/11 06:01:14AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0367 (0.0984)	Prec@(1,5) (96.7%, 100.0%)	
08/11 06:02:06AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0317 (0.0945)	Prec@(1,5) (96.8%, 100.0%)	
08/11 06:02:58AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.1168 (0.0949)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:03:50AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0829 (0.0922)	Prec@(1,5) (97.0%, 99.9%)	
08/11 06:04:42AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.1085 (0.0926)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:05:34AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.1463 (0.0940)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:06:15AM searchStage_trainer.py:200 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0455 (0.0939)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:06:16AM searchStage_trainer.py:210 [INFO] Train: [ 45/49] Final Prec@1 96.9000%
08/11 06:06:25AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5697	Prec@(1,5) (84.6%, 99.3%)
08/11 06:06:33AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5552	Prec@(1,5) (84.8%, 99.3%)
08/11 06:06:42AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5333	Prec@(1,5) (85.3%, 99.4%)
08/11 06:06:50AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5196	Prec@(1,5) (85.7%, 99.4%)
08/11 06:06:58AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5171	Prec@(1,5) (85.6%, 99.4%)
08/11 06:07:07AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5183	Prec@(1,5) (85.6%, 99.3%)
08/11 06:07:16AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5180	Prec@(1,5) (85.6%, 99.3%)
08/11 06:07:23AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5181	Prec@(1,5) (85.6%, 99.3%)
08/11 06:07:23AM searchStage_trainer.py:249 [INFO] Valid: [ 45/49] Final Prec@1 85.5800%
08/11 06:07:23AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:07:23AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.8680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1032, 0.1170, 0.5788, 0.2010],
        [0.1074, 0.1167, 0.5588, 0.2171]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2743, 0.1394, 0.3512, 0.2351],
        [0.2485, 0.1318, 0.3836, 0.2361],
        [0.2048, 0.1383, 0.4146, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1094, 0.0948, 0.6134, 0.1824],
        [0.1456, 0.1105, 0.5278, 0.2162],
        [0.1860, 0.2127, 0.2996, 0.3017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2312, 0.1863, 0.3724, 0.2100],
        [0.2054, 0.2324, 0.3199, 0.2422],
        [0.1859, 0.1803, 0.3480, 0.2858]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.1553, 0.4166, 0.1801],
        [0.3100, 0.1108, 0.3908, 0.1883],
        [0.1402, 0.1445, 0.2150, 0.5003]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2921, 0.1280, 0.4522, 0.1276],
        [0.2030, 0.1843, 0.2525, 0.3602],
        [0.1687, 0.1689, 0.2687, 0.3936]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1959, 0.2866, 0.2979, 0.2196],
        [0.1904, 0.2476, 0.2879, 0.2740]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1724, 0.2239, 0.4130, 0.1907],
        [0.1574, 0.2279, 0.4321, 0.1827],
        [0.1979, 0.2131, 0.2095, 0.3795]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1892, 0.2930, 0.3867, 0.1311],
        [0.1799, 0.2117, 0.2456, 0.3627],
        [0.1795, 0.2137, 0.2318, 0.3750]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2164, 0.2714, 0.3212, 0.1909],
        [0.2014, 0.2260, 0.3034, 0.2692],
        [0.2028, 0.2221, 0.2721, 0.3029]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1795, 0.1993, 0.4483, 0.1729],
        [0.1765, 0.2064, 0.3318, 0.2852],
        [0.2138, 0.2080, 0.2752, 0.3030]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1512, 0.2613, 0.3984, 0.1892],
        [0.1696, 0.2380, 0.3646, 0.2279],
        [0.1610, 0.2512, 0.2801, 0.3076]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0730, 0.0791, 0.6866, 0.1612],
        [0.0844, 0.0865, 0.6392, 0.1900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0594, 0.0664, 0.7650, 0.1093],
        [0.0621, 0.0665, 0.7442, 0.1273],
        [0.1448, 0.1299, 0.2228, 0.5025]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0331, 0.0350, 0.8738, 0.0580],
        [0.0720, 0.0791, 0.3280, 0.5209],
        [0.1071, 0.1249, 0.4523, 0.3157]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0759, 0.0862, 0.6648, 0.1730],
        [0.0731, 0.0832, 0.6854, 0.1583],
        [0.1010, 0.1156, 0.5575, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0393, 0.0458, 0.8192, 0.0957],
        [0.0353, 0.0384, 0.7423, 0.1841],
        [0.0510, 0.0604, 0.6849, 0.2037]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0436, 0.0452, 0.8099, 0.1012],
        [0.0449, 0.0496, 0.7947, 0.1109],
        [0.1579, 0.1194, 0.2644, 0.4584]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:08:16AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.1518 (0.0847)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:09:07AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.1284 (0.0819)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:09:59AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0372 (0.0854)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:10:51AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0340 (0.0843)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:11:43AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0560 (0.0833)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:12:35AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0488 (0.0854)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:13:27AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0988 (0.0869)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:14:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0806 (0.0878)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:14:09AM searchStage_trainer.py:210 [INFO] Train: [ 46/49] Final Prec@1 97.0840%
08/11 06:14:18AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5114	Prec@(1,5) (86.2%, 99.4%)
08/11 06:14:26AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5364	Prec@(1,5) (85.4%, 99.3%)
08/11 06:14:35AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5193	Prec@(1,5) (85.6%, 99.4%)
08/11 06:14:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5208	Prec@(1,5) (85.6%, 99.4%)
08/11 06:14:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5123	Prec@(1,5) (86.0%, 99.4%)
08/11 06:15:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5101	Prec@(1,5) (85.9%, 99.4%)
08/11 06:15:08AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5056	Prec@(1,5) (86.1%, 99.4%)
08/11 06:15:15AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5078	Prec@(1,5) (86.0%, 99.4%)
08/11 06:15:15AM searchStage_trainer.py:249 [INFO] Valid: [ 46/49] Final Prec@1 85.9800%
08/11 06:15:15AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:15:16AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.9800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1033, 0.1176, 0.5777, 0.2014],
        [0.1080, 0.1171, 0.5583, 0.2166]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2743, 0.1402, 0.3509, 0.2347],
        [0.2482, 0.1321, 0.3861, 0.2337],
        [0.2033, 0.1360, 0.4161, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1088, 0.0951, 0.6126, 0.1836],
        [0.1448, 0.1099, 0.5330, 0.2122],
        [0.1849, 0.2136, 0.2984, 0.3031]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2312, 0.1854, 0.3745, 0.2090],
        [0.2060, 0.2325, 0.3190, 0.2425],
        [0.1870, 0.1798, 0.3458, 0.2874]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.1548, 0.4192, 0.1790],
        [0.3119, 0.1102, 0.3902, 0.1877],
        [0.1378, 0.1436, 0.2130, 0.5056]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2915, 0.1279, 0.4561, 0.1245],
        [0.2009, 0.1815, 0.2515, 0.3661],
        [0.1668, 0.1655, 0.2677, 0.4000]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1936, 0.2852, 0.3012, 0.2201],
        [0.1883, 0.2476, 0.2915, 0.2726]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1674, 0.2200, 0.4226, 0.1900],
        [0.1543, 0.2259, 0.4399, 0.1799],
        [0.1961, 0.2129, 0.2086, 0.3824]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1874, 0.2941, 0.3896, 0.1289],
        [0.1775, 0.2088, 0.2433, 0.3704],
        [0.1777, 0.2146, 0.2323, 0.3753]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2138, 0.2716, 0.3246, 0.1900],
        [0.2001, 0.2256, 0.3055, 0.2688],
        [0.2023, 0.2211, 0.2730, 0.3037]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1752, 0.1977, 0.4571, 0.1700],
        [0.1736, 0.2067, 0.3382, 0.2815],
        [0.2106, 0.2056, 0.2754, 0.3084]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1475, 0.2595, 0.4050, 0.1880],
        [0.1656, 0.2371, 0.3722, 0.2251],
        [0.1584, 0.2514, 0.2817, 0.3085]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0704, 0.0761, 0.6957, 0.1578],
        [0.0815, 0.0832, 0.6481, 0.1871]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0562, 0.0629, 0.7743, 0.1066],
        [0.0593, 0.0632, 0.7530, 0.1245],
        [0.1413, 0.1274, 0.2237, 0.5076]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0316, 0.0333, 0.8785, 0.0566],
        [0.0701, 0.0770, 0.3272, 0.5256],
        [0.1048, 0.1218, 0.4563, 0.3171]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0716, 0.0809, 0.6816, 0.1659],
        [0.0696, 0.0791, 0.6977, 0.1536],
        [0.0976, 0.1119, 0.5628, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0369, 0.0428, 0.8309, 0.0895],
        [0.0338, 0.0367, 0.7480, 0.1816],
        [0.0490, 0.0579, 0.6849, 0.2082]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0412, 0.0427, 0.8196, 0.0965],
        [0.0421, 0.0463, 0.8049, 0.1066],
        [0.1535, 0.1152, 0.2613, 0.4700]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:16:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0340 (0.0836)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:17:00AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.1769 (0.0813)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:17:52AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0734 (0.0822)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:18:44AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0864 (0.0820)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:19:36AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0749 (0.0833)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:20:28AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0897 (0.0835)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:21:20AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0674 (0.0844)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:22:01AM searchStage_trainer.py:200 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.1167 (0.0839)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:22:02AM searchStage_trainer.py:210 [INFO] Train: [ 47/49] Final Prec@1 97.1880%
08/11 06:22:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.5407	Prec@(1,5) (85.4%, 99.0%)
08/11 06:22:19AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5185	Prec@(1,5) (85.9%, 99.3%)
08/11 06:22:28AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5074	Prec@(1,5) (86.1%, 99.3%)
08/11 06:22:34AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5135	Prec@(1,5) (86.0%, 99.3%)
08/11 06:22:40AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5203	Prec@(1,5) (85.9%, 99.3%)
08/11 06:22:45AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5240	Prec@(1,5) (85.8%, 99.3%)
08/11 06:22:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5185	Prec@(1,5) (85.9%, 99.3%)
08/11 06:22:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5148	Prec@(1,5) (86.0%, 99.3%)
08/11 06:22:56AM searchStage_trainer.py:249 [INFO] Valid: [ 47/49] Final Prec@1 86.0280%
08/11 06:22:56AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:22:56AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 86.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1013, 0.1161, 0.5854, 0.1971],
        [0.1071, 0.1157, 0.5579, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2762, 0.1384, 0.3491, 0.2363],
        [0.2509, 0.1305, 0.3840, 0.2345],
        [0.2036, 0.1343, 0.4200, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1081, 0.0945, 0.6145, 0.1828],
        [0.1432, 0.1084, 0.5373, 0.2110],
        [0.1851, 0.2127, 0.2979, 0.3043]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2312, 0.1852, 0.3738, 0.2098],
        [0.2086, 0.2338, 0.3175, 0.2401],
        [0.1880, 0.1785, 0.3435, 0.2900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.1549, 0.4191, 0.1784],
        [0.3093, 0.1092, 0.3937, 0.1878],
        [0.1362, 0.1438, 0.2134, 0.5066]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2923, 0.1262, 0.4580, 0.1235],
        [0.1988, 0.1804, 0.2509, 0.3699],
        [0.1664, 0.1651, 0.2677, 0.4008]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1958, 0.2835, 0.3011, 0.2196],
        [0.1893, 0.2468, 0.2900, 0.2739]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1664, 0.2170, 0.4259, 0.1908],
        [0.1529, 0.2242, 0.4456, 0.1772],
        [0.1941, 0.2127, 0.2084, 0.3848]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1880, 0.2922, 0.3918, 0.1280],
        [0.1758, 0.2069, 0.2429, 0.3744],
        [0.1763, 0.2150, 0.2329, 0.3757]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2121, 0.2720, 0.3269, 0.1890],
        [0.1994, 0.2230, 0.3069, 0.2707],
        [0.2024, 0.2205, 0.2748, 0.3024]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1723, 0.1948, 0.4655, 0.1675],
        [0.1714, 0.2057, 0.3439, 0.2790],
        [0.2080, 0.2037, 0.2760, 0.3123]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1433, 0.2580, 0.4125, 0.1862],
        [0.1636, 0.2350, 0.3758, 0.2257],
        [0.1565, 0.2528, 0.2836, 0.3071]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0671, 0.0727, 0.7068, 0.1534],
        [0.0782, 0.0797, 0.6577, 0.1844]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0538, 0.0606, 0.7816, 0.1039],
        [0.0569, 0.0609, 0.7601, 0.1221],
        [0.1376, 0.1243, 0.2230, 0.5150]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0296, 0.0312, 0.8847, 0.0545],
        [0.0675, 0.0738, 0.3219, 0.5368],
        [0.1019, 0.1191, 0.4624, 0.3166]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0675, 0.0757, 0.6983, 0.1585],
        [0.0652, 0.0748, 0.7133, 0.1467],
        [0.0942, 0.1077, 0.5660, 0.2321]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0347, 0.0404, 0.8427, 0.0822],
        [0.0323, 0.0351, 0.7516, 0.1811],
        [0.0472, 0.0555, 0.6792, 0.2181]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0384, 0.0399, 0.8308, 0.0909],
        [0.0391, 0.0429, 0.8159, 0.1021],
        [0.1480, 0.1110, 0.2567, 0.4844]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:23:31AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.1850 (0.0772)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:24:04AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0375 (0.0820)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:24:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0668 (0.0828)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:25:13AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.1245 (0.0832)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:25:48AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0499 (0.0824)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:26:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0486 (0.0799)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:26:57AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0891 (0.0806)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:27:25AM searchStage_trainer.py:200 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.1325 (0.0813)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:27:25AM searchStage_trainer.py:210 [INFO] Train: [ 48/49] Final Prec@1 97.2160%
08/11 06:27:31AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5224	Prec@(1,5) (85.7%, 99.3%)
08/11 06:27:37AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5426	Prec@(1,5) (85.7%, 99.3%)
08/11 06:27:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5278	Prec@(1,5) (86.1%, 99.3%)
08/11 06:27:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5249	Prec@(1,5) (86.0%, 99.2%)
08/11 06:27:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5123	Prec@(1,5) (86.0%, 99.3%)
08/11 06:28:00AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5159	Prec@(1,5) (85.9%, 99.3%)
08/11 06:28:06AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5155	Prec@(1,5) (85.9%, 99.3%)
08/11 06:28:10AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5159	Prec@(1,5) (85.9%, 99.3%)
08/11 06:28:10AM searchStage_trainer.py:249 [INFO] Valid: [ 48/49] Final Prec@1 85.9520%
08/11 06:28:10AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:28:11AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 86.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.1009, 0.1158, 0.5868, 0.1964],
        [0.1070, 0.1157, 0.5579, 0.2194]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2729, 0.1379, 0.3530, 0.2362],
        [0.2489, 0.1295, 0.3870, 0.2345],
        [0.2018, 0.1335, 0.4241, 0.2407]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1064, 0.0935, 0.6204, 0.1797],
        [0.1410, 0.1066, 0.5420, 0.2104],
        [0.1848, 0.2110, 0.2957, 0.3085]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2319, 0.1857, 0.3734, 0.2089],
        [0.2086, 0.2328, 0.3192, 0.2394],
        [0.1863, 0.1777, 0.3442, 0.2917]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.1545, 0.4198, 0.1783],
        [0.3090, 0.1082, 0.3949, 0.1878],
        [0.1352, 0.1440, 0.2141, 0.5067]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2905, 0.1254, 0.4616, 0.1224],
        [0.1987, 0.1810, 0.2511, 0.3692],
        [0.1642, 0.1627, 0.2670, 0.4060]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1931, 0.2855, 0.3017, 0.2197],
        [0.1876, 0.2492, 0.2900, 0.2732]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1630, 0.2142, 0.4331, 0.1897],
        [0.1489, 0.2249, 0.4531, 0.1731],
        [0.1911, 0.2113, 0.2061, 0.3916]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1859, 0.2920, 0.3978, 0.1243],
        [0.1733, 0.2034, 0.2405, 0.3828],
        [0.1743, 0.2133, 0.2316, 0.3808]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2122, 0.2715, 0.3266, 0.1897],
        [0.2001, 0.2232, 0.3096, 0.2671],
        [0.2011, 0.2188, 0.2748, 0.3053]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1695, 0.1915, 0.4738, 0.1653],
        [0.1690, 0.2044, 0.3484, 0.2782],
        [0.2050, 0.2031, 0.2780, 0.3139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1404, 0.2570, 0.4191, 0.1835],
        [0.1603, 0.2339, 0.3806, 0.2252],
        [0.1542, 0.2521, 0.2852, 0.3085]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0645, 0.0701, 0.7163, 0.1491],
        [0.0757, 0.0773, 0.6638, 0.1832]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0518, 0.0584, 0.7882, 0.1016],
        [0.0550, 0.0591, 0.7665, 0.1194],
        [0.1339, 0.1208, 0.2208, 0.5244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0280, 0.0296, 0.8903, 0.0521],
        [0.0649, 0.0707, 0.3135, 0.5509],
        [0.0997, 0.1159, 0.4636, 0.3208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0637, 0.0712, 0.7128, 0.1523],
        [0.0622, 0.0716, 0.7253, 0.1408],
        [0.0914, 0.1038, 0.5687, 0.2361]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0330, 0.0382, 0.8528, 0.0760],
        [0.0311, 0.0339, 0.7546, 0.1804],
        [0.0458, 0.0535, 0.6709, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.0363, 0.0378, 0.8396, 0.0863],
        [0.0365, 0.0403, 0.8253, 0.0979],
        [0.1428, 0.1072, 0.2516, 0.4985]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:28:45AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0464 (0.0662)	Prec@(1,5) (97.8%, 100.0%)	
08/11 06:29:08AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0663 (0.0680)	Prec@(1,5) (97.9%, 100.0%)	
08/11 06:29:23AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.1423 (0.0723)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:29:39AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0315 (0.0743)	Prec@(1,5) (97.6%, 100.0%)	
08/11 06:29:55AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0149 (0.0743)	Prec@(1,5) (97.6%, 100.0%)	
08/11 06:30:11AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0586 (0.0767)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:30:27AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0973 (0.0773)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:30:40AM searchStage_trainer.py:200 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0931 (0.0778)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:30:40AM searchStage_trainer.py:210 [INFO] Train: [ 49/49] Final Prec@1 97.4400%
08/11 06:30:43AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.5030	Prec@(1,5) (86.1%, 99.5%)
08/11 06:30:46AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5170	Prec@(1,5) (86.0%, 99.3%)
08/11 06:30:48AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.5278	Prec@(1,5) (85.8%, 99.3%)
08/11 06:30:51AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5211	Prec@(1,5) (85.8%, 99.3%)
08/11 06:30:54AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.5244	Prec@(1,5) (85.9%, 99.3%)
08/11 06:30:56AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5182	Prec@(1,5) (86.0%, 99.4%)
08/11 06:30:59AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5168	Prec@(1,5) (86.0%, 99.3%)
08/11 06:31:01AM searchStage_trainer.py:238 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5166	Prec@(1,5) (86.0%, 99.3%)
08/11 06:31:01AM searchStage_trainer.py:249 [INFO] Valid: [ 49/49] Final Prec@1 86.0160%
08/11 06:31:01AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:31:01AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 86.0280%
08/11 06:31:01AM searchStage_main.py:75 [INFO] Final best Prec@1 = 86.0280%
08/11 06:31:01AM searchStage_main.py:76 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
