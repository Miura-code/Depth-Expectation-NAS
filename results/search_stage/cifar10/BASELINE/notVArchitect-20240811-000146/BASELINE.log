08/11 12:01:46AM parser.py:28 [INFO] 
08/11 12:01:46AM parser.py:29 [INFO] Parameters:
08/11 12:01:46AM parser.py:31 [INFO] DAG_PATH=results/search_stage/cifar10/BASELINE/notVArchitect-20240811-000146/DAG
08/11 12:01:46AM parser.py:31 [INFO] ADVANCED=False
08/11 12:01:46AM parser.py:31 [INFO] ALPHA_LR=0.0003
08/11 12:01:46AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
08/11 12:01:46AM parser.py:31 [INFO] BATCH_SIZE=64
08/11 12:01:46AM parser.py:31 [INFO] CUTOUT_LENGTH=0
08/11 12:01:46AM parser.py:31 [INFO] DATA_PATH=../data/
08/11 12:01:46AM parser.py:31 [INFO] DATASET=cifar10
08/11 12:01:46AM parser.py:31 [INFO] EPOCHS=50
08/11 12:01:46AM parser.py:31 [INFO] EXP_NAME=notVArchitect-20240811-000146
08/11 12:01:46AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
08/11 12:01:46AM parser.py:31 [INFO] GPUS=[0]
08/11 12:01:46AM parser.py:31 [INFO] INIT_CHANNELS=16
08/11 12:01:46AM parser.py:31 [INFO] LAYERS=20
08/11 12:01:46AM parser.py:31 [INFO] LOCAL_RANK=0
08/11 12:01:46AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
08/11 12:01:46AM parser.py:31 [INFO] NAME=BASELINE
08/11 12:01:46AM parser.py:31 [INFO] PATH=results/search_stage/cifar10/BASELINE/notVArchitect-20240811-000146
08/11 12:01:46AM parser.py:31 [INFO] PLOT_PATH=results/search_stage/cifar10/BASELINE/notVArchitect-20240811-000146/plots
08/11 12:01:46AM parser.py:31 [INFO] PRINT_FREQ=50
08/11 12:01:46AM parser.py:31 [INFO] RESUME_PATH=None
08/11 12:01:46AM parser.py:31 [INFO] SAVE=notVArchitect
08/11 12:01:46AM parser.py:31 [INFO] SEED=0
08/11 12:01:46AM parser.py:31 [INFO] SPEC_CELL=True
08/11 12:01:46AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
08/11 12:01:46AM parser.py:31 [INFO] W_LR=0.025
08/11 12:01:46AM parser.py:31 [INFO] W_LR_MIN=0.001
08/11 12:01:46AM parser.py:31 [INFO] W_MOMENTUM=0.9
08/11 12:01:46AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
08/11 12:01:46AM parser.py:31 [INFO] WORKERS=4
08/11 12:01:46AM parser.py:32 [INFO] 
08/11 12:01:50AM searchStage_trainer.py:99 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:02:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.3583 (2.3289)	Prec@(1,5) (16.7%, 66.7%)	
08/11 12:03:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 1.8737 (2.2254)	Prec@(1,5) (18.7%, 71.7%)	
08/11 12:03:37AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.8420 (2.1239)	Prec@(1,5) (21.2%, 75.8%)	
08/11 12:04:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.8878 (2.0556)	Prec@(1,5) (23.0%, 78.1%)	
08/11 12:04:46AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.7666 (2.0034)	Prec@(1,5) (24.9%, 79.6%)	
08/11 12:05:21AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.7048 (1.9630)	Prec@(1,5) (26.4%, 80.7%)	
08/11 12:05:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.7079 (1.9214)	Prec@(1,5) (28.0%, 81.7%)	
08/11 12:06:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.8784 (1.8926)	Prec@(1,5) (28.9%, 82.4%)	
08/11 12:06:36AM searchStage_trainer.py:208 [INFO] Train: [  0/49] Final Prec@1 28.8760%
08/11 12:06:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.7974	Prec@(1,5) (35.2%, 84.5%)
08/11 12:06:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.7990	Prec@(1,5) (34.5%, 85.0%)
08/11 12:07:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.8039	Prec@(1,5) (34.3%, 85.2%)
08/11 12:07:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.8069	Prec@(1,5) (34.3%, 85.1%)
08/11 12:07:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.8043	Prec@(1,5) (34.4%, 85.1%)
08/11 12:07:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.8017	Prec@(1,5) (34.4%, 85.1%)
08/11 12:07:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.8038	Prec@(1,5) (34.3%, 85.2%)
08/11 12:07:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.8039	Prec@(1,5) (34.4%, 85.1%)
08/11 12:07:43AM searchStage_trainer.py:247 [INFO] Valid: [  0/49] Final Prec@1 34.3800%
08/11 12:07:43AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 4), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 2), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
08/11 12:07:43AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 34.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2508, 0.2504, 0.2483],
        [0.2495, 0.2508, 0.2511, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2500, 0.2507, 0.2493],
        [0.2500, 0.2506, 0.2510, 0.2484],
        [0.2517, 0.2527, 0.2481, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2507, 0.2510, 0.2468],
        [0.2514, 0.2518, 0.2488, 0.2481],
        [0.2501, 0.2515, 0.2485, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2510, 0.2497, 0.2484],
        [0.2513, 0.2506, 0.2495, 0.2486],
        [0.2511, 0.2507, 0.2491, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2500, 0.2493],
        [0.2506, 0.2504, 0.2495, 0.2495],
        [0.2509, 0.2507, 0.2492, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2503, 0.2499, 0.2494],
        [0.2497, 0.2500, 0.2497, 0.2506],
        [0.2501, 0.2502, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2503, 0.2502, 0.2492],
        [0.2494, 0.2501, 0.2504, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2500, 0.2497],
        [0.2499, 0.2502, 0.2500, 0.2499],
        [0.2496, 0.2504, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2501, 0.2498, 0.2501],
        [0.2500, 0.2501, 0.2499, 0.2500],
        [0.2500, 0.2502, 0.2500, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2500, 0.2495],
        [0.2495, 0.2506, 0.2499, 0.2500],
        [0.2501, 0.2500, 0.2498, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2504, 0.2497],
        [0.2498, 0.2502, 0.2502, 0.2499],
        [0.2496, 0.2504, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2500, 0.2497],
        [0.2501, 0.2500, 0.2499, 0.2500],
        [0.2496, 0.2502, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2500, 0.2501],
        [0.2501, 0.2503, 0.2502, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2500, 0.2503, 0.2498],
        [0.2498, 0.2495, 0.2508, 0.2499],
        [0.2499, 0.2497, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2509, 0.2493],
        [0.2495, 0.2503, 0.2502, 0.2500],
        [0.2500, 0.2496, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2503, 0.2500],
        [0.2498, 0.2499, 0.2501, 0.2503],
        [0.2499, 0.2503, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2501, 0.2507, 0.2498],
        [0.2495, 0.2500, 0.2508, 0.2497],
        [0.2494, 0.2505, 0.2499, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2503, 0.2497],
        [0.2499, 0.2495, 0.2510, 0.2496],
        [0.2498, 0.2499, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:08:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.5012 (1.6365)	Prec@(1,5) (38.0%, 89.0%)	
08/11 12:09:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.5763 (1.6275)	Prec@(1,5) (38.5%, 89.1%)	
08/11 12:10:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.4967 (1.6151)	Prec@(1,5) (39.2%, 89.4%)	
08/11 12:11:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.6576 (1.5970)	Prec@(1,5) (39.7%, 89.9%)	
08/11 12:12:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.5363 (1.5729)	Prec@(1,5) (40.6%, 90.3%)	
08/11 12:12:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4862 (1.5602)	Prec@(1,5) (41.1%, 90.6%)	
08/11 12:13:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.5484 (1.5473)	Prec@(1,5) (41.8%, 90.7%)	
08/11 12:14:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.4250 (1.5388)	Prec@(1,5) (42.3%, 90.9%)	
08/11 12:14:26AM searchStage_trainer.py:208 [INFO] Train: [  1/49] Final Prec@1 42.2880%
08/11 12:14:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.4987	Prec@(1,5) (44.4%, 90.8%)
08/11 12:14:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5022	Prec@(1,5) (44.3%, 91.3%)
08/11 12:14:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.4945	Prec@(1,5) (44.9%, 91.4%)
08/11 12:15:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.4848	Prec@(1,5) (45.1%, 91.9%)
08/11 12:15:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.4821	Prec@(1,5) (45.1%, 91.9%)
08/11 12:15:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.4809	Prec@(1,5) (45.3%, 91.8%)
08/11 12:15:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.4810	Prec@(1,5) (45.1%, 91.9%)
08/11 12:15:32AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.4827	Prec@(1,5) (45.0%, 91.9%)
08/11 12:15:32AM searchStage_trainer.py:247 [INFO] Valid: [  1/49] Final Prec@1 45.0600%
08/11 12:15:33AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 2), ('avg_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)]], DAG3_concat=range(6, 8))
08/11 12:15:33AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 45.0600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2508, 0.2504, 0.2508, 0.2480],
        [0.2499, 0.2504, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2492, 0.2513, 0.2488],
        [0.2506, 0.2498, 0.2515, 0.2480],
        [0.2520, 0.2528, 0.2478, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2504, 0.2515, 0.2456],
        [0.2517, 0.2517, 0.2488, 0.2478],
        [0.2504, 0.2512, 0.2481, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2510, 0.2498, 0.2482],
        [0.2517, 0.2505, 0.2493, 0.2485],
        [0.2513, 0.2508, 0.2491, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2503, 0.2500, 0.2486],
        [0.2511, 0.2500, 0.2493, 0.2496],
        [0.2512, 0.2506, 0.2490, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2501, 0.2490],
        [0.2498, 0.2500, 0.2497, 0.2505],
        [0.2501, 0.2501, 0.2495, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2503, 0.2492],
        [0.2495, 0.2500, 0.2504, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2501, 0.2496],
        [0.2500, 0.2501, 0.2501, 0.2499],
        [0.2497, 0.2503, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2498, 0.2499],
        [0.2500, 0.2500, 0.2500, 0.2500],
        [0.2500, 0.2502, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2501, 0.2494],
        [0.2494, 0.2506, 0.2499, 0.2500],
        [0.2501, 0.2500, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2503, 0.2506, 0.2495],
        [0.2497, 0.2501, 0.2501, 0.2500],
        [0.2496, 0.2503, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2501, 0.2496],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2501, 0.2502, 0.2501],
        [0.2500, 0.2501, 0.2504, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2505, 0.2497],
        [0.2497, 0.2494, 0.2510, 0.2499],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2511, 0.2491],
        [0.2494, 0.2501, 0.2504, 0.2501],
        [0.2499, 0.2495, 0.2505, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2496, 0.2504, 0.2500],
        [0.2497, 0.2498, 0.2501, 0.2503],
        [0.2499, 0.2502, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2510, 0.2495],
        [0.2495, 0.2499, 0.2508, 0.2498],
        [0.2493, 0.2504, 0.2499, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2506, 0.2496],
        [0.2499, 0.2494, 0.2511, 0.2496],
        [0.2497, 0.2498, 0.2503, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:16:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.4843 (1.4348)	Prec@(1,5) (46.7%, 92.5%)	
08/11 12:17:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.3003 (1.4136)	Prec@(1,5) (46.9%, 92.7%)	
08/11 12:18:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.3717 (1.4048)	Prec@(1,5) (47.3%, 92.8%)	
08/11 12:19:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.4849 (1.4025)	Prec@(1,5) (47.7%, 92.7%)	
08/11 12:19:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.2251 (1.3961)	Prec@(1,5) (48.2%, 92.9%)	
08/11 12:20:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.3063 (1.3808)	Prec@(1,5) (48.9%, 93.0%)	
08/11 12:21:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.3723 (1.3766)	Prec@(1,5) (49.0%, 93.1%)	
08/11 12:22:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.3720 (1.3643)	Prec@(1,5) (49.5%, 93.2%)	
08/11 12:22:17AM searchStage_trainer.py:208 [INFO] Train: [  2/49] Final Prec@1 49.4880%
08/11 12:22:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.4469	Prec@(1,5) (47.7%, 91.7%)
08/11 12:22:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.4355	Prec@(1,5) (48.0%, 91.9%)
08/11 12:22:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.4256	Prec@(1,5) (48.4%, 91.8%)
08/11 12:22:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.4291	Prec@(1,5) (48.4%, 91.9%)
08/11 12:23:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.4245	Prec@(1,5) (48.6%, 91.8%)
08/11 12:23:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.4253	Prec@(1,5) (48.6%, 91.7%)
08/11 12:23:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.4302	Prec@(1,5) (48.5%, 91.7%)
08/11 12:23:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.4336	Prec@(1,5) (48.4%, 91.6%)
08/11 12:23:24AM searchStage_trainer.py:247 [INFO] Valid: [  2/49] Final Prec@1 48.3480%
08/11 12:23:24AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)]], DAG3_concat=range(6, 8))
08/11 12:23:24AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 48.3480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2511, 0.2496, 0.2514, 0.2479],
        [0.2503, 0.2496, 0.2519, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2479, 0.2518, 0.2490],
        [0.2513, 0.2487, 0.2522, 0.2479],
        [0.2523, 0.2526, 0.2479, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2498, 0.2524, 0.2446],
        [0.2516, 0.2514, 0.2490, 0.2480],
        [0.2504, 0.2508, 0.2481, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2509, 0.2500, 0.2481],
        [0.2518, 0.2504, 0.2494, 0.2485],
        [0.2513, 0.2508, 0.2491, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2501, 0.2502, 0.2482],
        [0.2512, 0.2498, 0.2496, 0.2493],
        [0.2513, 0.2504, 0.2488, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2503, 0.2504, 0.2486],
        [0.2499, 0.2499, 0.2496, 0.2507],
        [0.2502, 0.2501, 0.2495, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2504, 0.2492],
        [0.2495, 0.2498, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2502, 0.2496],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2497, 0.2503, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2499, 0.2497],
        [0.2501, 0.2500, 0.2501, 0.2499],
        [0.2500, 0.2501, 0.2498, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2501, 0.2494],
        [0.2494, 0.2506, 0.2500, 0.2500],
        [0.2501, 0.2500, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2506, 0.2496],
        [0.2498, 0.2501, 0.2503, 0.2498],
        [0.2496, 0.2503, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2502, 0.2495],
        [0.2501, 0.2500, 0.2500, 0.2498],
        [0.2496, 0.2502, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2500, 0.2504, 0.2501],
        [0.2500, 0.2500, 0.2505, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2506, 0.2497],
        [0.2497, 0.2493, 0.2512, 0.2499],
        [0.2499, 0.2496, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2500, 0.2512, 0.2491],
        [0.2494, 0.2500, 0.2505, 0.2501],
        [0.2499, 0.2494, 0.2505, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2495, 0.2504, 0.2500],
        [0.2498, 0.2497, 0.2501, 0.2504],
        [0.2500, 0.2501, 0.2502, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2512, 0.2494],
        [0.2495, 0.2497, 0.2509, 0.2498],
        [0.2494, 0.2502, 0.2499, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2508, 0.2495],
        [0.2500, 0.2492, 0.2513, 0.2495],
        [0.2497, 0.2495, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:24:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.2767 (1.2421)	Prec@(1,5) (55.5%, 94.7%)	
08/11 12:25:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.2226 (1.2529)	Prec@(1,5) (55.1%, 94.8%)	
08/11 12:26:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.1975 (1.2505)	Prec@(1,5) (55.0%, 94.8%)	
08/11 12:26:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 0.9768 (1.2468)	Prec@(1,5) (54.9%, 94.7%)	
08/11 12:27:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.1275 (1.2401)	Prec@(1,5) (54.8%, 94.8%)	
08/11 12:28:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.3015 (1.2334)	Prec@(1,5) (55.0%, 94.9%)	
08/11 12:29:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.1102 (1.2246)	Prec@(1,5) (55.4%, 94.9%)	
08/11 12:30:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.1736 (1.2168)	Prec@(1,5) (55.6%, 95.0%)	
08/11 12:30:09AM searchStage_trainer.py:208 [INFO] Train: [  3/49] Final Prec@1 55.5600%
08/11 12:30:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.1269	Prec@(1,5) (59.5%, 95.7%)
08/11 12:30:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.1431	Prec@(1,5) (59.0%, 95.0%)
08/11 12:30:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.1543	Prec@(1,5) (58.4%, 95.1%)
08/11 12:30:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.1588	Prec@(1,5) (58.3%, 95.0%)
08/11 12:30:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.1577	Prec@(1,5) (58.4%, 95.2%)
08/11 12:31:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.1608	Prec@(1,5) (58.2%, 95.1%)
08/11 12:31:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.1653	Prec@(1,5) (58.1%, 95.0%)
08/11 12:31:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.1651	Prec@(1,5) (58.2%, 95.1%)
08/11 12:31:16AM searchStage_trainer.py:247 [INFO] Valid: [  3/49] Final Prec@1 58.2080%
08/11 12:31:16AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:31:16AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 58.2080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2508, 0.2490, 0.2523, 0.2479],
        [0.2502, 0.2490, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2469, 0.2525, 0.2490],
        [0.2515, 0.2478, 0.2529, 0.2478],
        [0.2524, 0.2523, 0.2482, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2491, 0.2533, 0.2441],
        [0.2515, 0.2511, 0.2494, 0.2480],
        [0.2503, 0.2506, 0.2482, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2509, 0.2503, 0.2477],
        [0.2518, 0.2502, 0.2493, 0.2488],
        [0.2513, 0.2506, 0.2492, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2498, 0.2503, 0.2482],
        [0.2513, 0.2496, 0.2499, 0.2491],
        [0.2512, 0.2504, 0.2487, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2502, 0.2508, 0.2481],
        [0.2499, 0.2499, 0.2496, 0.2506],
        [0.2501, 0.2500, 0.2493, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2505, 0.2492],
        [0.2495, 0.2498, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2502, 0.2496],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2497, 0.2503, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2503, 0.2500, 0.2493],
        [0.2500, 0.2499, 0.2500, 0.2500],
        [0.2500, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2501, 0.2494],
        [0.2495, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2508, 0.2494],
        [0.2498, 0.2501, 0.2503, 0.2499],
        [0.2495, 0.2503, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2503, 0.2495],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2505, 0.2501],
        [0.2499, 0.2499, 0.2507, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2496, 0.2508, 0.2497],
        [0.2497, 0.2492, 0.2513, 0.2499],
        [0.2499, 0.2495, 0.2503, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2500, 0.2515, 0.2489],
        [0.2493, 0.2499, 0.2506, 0.2502],
        [0.2499, 0.2494, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2495, 0.2505, 0.2499],
        [0.2498, 0.2496, 0.2502, 0.2504],
        [0.2500, 0.2501, 0.2502, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2498, 0.2516, 0.2490],
        [0.2494, 0.2495, 0.2511, 0.2500],
        [0.2492, 0.2501, 0.2500, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2496, 0.2512, 0.2494],
        [0.2499, 0.2491, 0.2515, 0.2494],
        [0.2495, 0.2493, 0.2506, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:32:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.1382 (1.1701)	Prec@(1,5) (58.0%, 95.2%)	
08/11 12:33:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.2225 (1.1630)	Prec@(1,5) (58.1%, 95.1%)	
08/11 12:33:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.1604 (1.1671)	Prec@(1,5) (57.7%, 95.2%)	
08/11 12:34:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.1648 (1.1525)	Prec@(1,5) (58.2%, 95.4%)	
08/11 12:35:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.1955 (1.1425)	Prec@(1,5) (58.7%, 95.5%)	
08/11 12:36:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 0.7701 (1.1330)	Prec@(1,5) (59.0%, 95.7%)	
08/11 12:37:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 0.8992 (1.1275)	Prec@(1,5) (59.4%, 95.7%)	
08/11 12:38:00AM searchStage_trainer.py:198 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 1.1112 (1.1232)	Prec@(1,5) (59.6%, 95.7%)	
08/11 12:38:01AM searchStage_trainer.py:208 [INFO] Train: [  4/49] Final Prec@1 59.5840%
08/11 12:38:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.2253	Prec@(1,5) (57.3%, 93.9%)
08/11 12:38:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.2161	Prec@(1,5) (57.2%, 94.4%)
08/11 12:38:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.2067	Prec@(1,5) (57.5%, 94.7%)
08/11 12:38:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.2085	Prec@(1,5) (57.7%, 94.6%)
08/11 12:38:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.2046	Prec@(1,5) (57.8%, 94.6%)
08/11 12:38:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.2028	Prec@(1,5) (57.8%, 94.5%)
08/11 12:39:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.2009	Prec@(1,5) (57.9%, 94.5%)
08/11 12:39:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.1992	Prec@(1,5) (57.9%, 94.6%)
08/11 12:39:07AM searchStage_trainer.py:247 [INFO] Valid: [  4/49] Final Prec@1 57.9440%
08/11 12:39:08AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:39:08AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 58.2080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2505, 0.2484, 0.2530, 0.2480],
        [0.2500, 0.2485, 0.2534, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2460, 0.2533, 0.2491],
        [0.2515, 0.2469, 0.2538, 0.2478],
        [0.2523, 0.2521, 0.2484, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2484, 0.2545, 0.2438],
        [0.2513, 0.2509, 0.2498, 0.2479],
        [0.2503, 0.2504, 0.2483, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2508, 0.2504, 0.2476],
        [0.2518, 0.2500, 0.2495, 0.2487],
        [0.2512, 0.2505, 0.2492, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2496, 0.2506, 0.2480],
        [0.2513, 0.2495, 0.2504, 0.2489],
        [0.2511, 0.2502, 0.2487, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2502, 0.2513, 0.2475],
        [0.2499, 0.2497, 0.2495, 0.2509],
        [0.2500, 0.2499, 0.2493, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2495, 0.2497, 0.2506, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2502, 0.2495],
        [0.2500, 0.2499, 0.2503, 0.2498],
        [0.2497, 0.2503, 0.2498, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2503, 0.2500, 0.2491],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2503, 0.2502, 0.2493],
        [0.2495, 0.2506, 0.2500, 0.2499],
        [0.2501, 0.2499, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2510, 0.2492],
        [0.2498, 0.2501, 0.2501, 0.2500],
        [0.2496, 0.2503, 0.2499, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2504, 0.2493],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2495, 0.2502, 0.2495, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2498, 0.2506, 0.2501],
        [0.2499, 0.2499, 0.2507, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2508, 0.2497],
        [0.2497, 0.2491, 0.2514, 0.2499],
        [0.2499, 0.2495, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2516, 0.2487],
        [0.2494, 0.2499, 0.2506, 0.2502],
        [0.2500, 0.2493, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2495, 0.2505, 0.2499],
        [0.2498, 0.2496, 0.2502, 0.2504],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2517, 0.2489],
        [0.2494, 0.2494, 0.2511, 0.2501],
        [0.2493, 0.2500, 0.2500, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2495, 0.2514, 0.2493],
        [0.2499, 0.2491, 0.2516, 0.2494],
        [0.2494, 0.2492, 0.2507, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:40:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.8100 (1.0739)	Prec@(1,5) (61.1%, 96.5%)	
08/11 12:40:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.0608 (1.0478)	Prec@(1,5) (62.2%, 96.5%)	
08/11 12:41:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 1.1541 (1.0493)	Prec@(1,5) (62.2%, 96.5%)	
08/11 12:42:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 1.2202 (1.0543)	Prec@(1,5) (62.0%, 96.4%)	
08/11 12:43:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 0.7903 (1.0438)	Prec@(1,5) (62.5%, 96.4%)	
08/11 12:44:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 1.1148 (1.0418)	Prec@(1,5) (62.5%, 96.4%)	
08/11 12:45:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 1.3799 (1.0376)	Prec@(1,5) (62.8%, 96.4%)	
08/11 12:45:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.8519 (1.0330)	Prec@(1,5) (63.0%, 96.5%)	
08/11 12:45:53AM searchStage_trainer.py:208 [INFO] Train: [  5/49] Final Prec@1 62.9680%
08/11 12:46:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.0125	Prec@(1,5) (64.2%, 96.7%)
08/11 12:46:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.0246	Prec@(1,5) (63.5%, 96.6%)
08/11 12:46:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.0271	Prec@(1,5) (63.0%, 96.8%)
08/11 12:46:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.0351	Prec@(1,5) (62.8%, 96.7%)
08/11 12:46:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.0394	Prec@(1,5) (62.7%, 96.8%)
08/11 12:46:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.0366	Prec@(1,5) (62.7%, 96.8%)
08/11 12:46:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.0372	Prec@(1,5) (62.8%, 96.7%)
08/11 12:46:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.0366	Prec@(1,5) (62.8%, 96.7%)
08/11 12:46:59AM searchStage_trainer.py:247 [INFO] Valid: [  5/49] Final Prec@1 62.8000%
08/11 12:46:59AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:47:00AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 62.8000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2480, 0.2536, 0.2481],
        [0.2499, 0.2480, 0.2540, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2454, 0.2540, 0.2489],
        [0.2515, 0.2462, 0.2545, 0.2477],
        [0.2523, 0.2519, 0.2485, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2477, 0.2556, 0.2432],
        [0.2512, 0.2507, 0.2501, 0.2480],
        [0.2502, 0.2503, 0.2483, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2507, 0.2505, 0.2475],
        [0.2518, 0.2499, 0.2495, 0.2487],
        [0.2512, 0.2503, 0.2493, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2494, 0.2508, 0.2478],
        [0.2512, 0.2493, 0.2507, 0.2488],
        [0.2509, 0.2500, 0.2488, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2500, 0.2516, 0.2472],
        [0.2499, 0.2496, 0.2495, 0.2510],
        [0.2499, 0.2498, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2505, 0.2492],
        [0.2495, 0.2497, 0.2506, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2502, 0.2495],
        [0.2500, 0.2499, 0.2503, 0.2498],
        [0.2497, 0.2503, 0.2498, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2503, 0.2501, 0.2491],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2503, 0.2502, 0.2493],
        [0.2495, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2503, 0.2511, 0.2491],
        [0.2498, 0.2501, 0.2502, 0.2500],
        [0.2495, 0.2503, 0.2499, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2504, 0.2493],
        [0.2502, 0.2500, 0.2502, 0.2497],
        [0.2495, 0.2502, 0.2495, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2498, 0.2507, 0.2501],
        [0.2499, 0.2498, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2509, 0.2497],
        [0.2497, 0.2490, 0.2514, 0.2498],
        [0.2499, 0.2495, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2500, 0.2518, 0.2485],
        [0.2493, 0.2498, 0.2506, 0.2502],
        [0.2500, 0.2493, 0.2505, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2506, 0.2498],
        [0.2498, 0.2496, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2519, 0.2487],
        [0.2494, 0.2493, 0.2511, 0.2502],
        [0.2493, 0.2499, 0.2500, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2494, 0.2516, 0.2492],
        [0.2500, 0.2490, 0.2516, 0.2494],
        [0.2494, 0.2490, 0.2507, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:47:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.8025 (0.9493)	Prec@(1,5) (66.9%, 96.9%)	
08/11 12:48:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 1.0135 (0.9612)	Prec@(1,5) (66.0%, 97.0%)	
08/11 12:49:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.8663 (0.9687)	Prec@(1,5) (65.7%, 96.9%)	
08/11 12:50:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.8725 (0.9692)	Prec@(1,5) (65.7%, 96.8%)	
08/11 12:51:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 1.0122 (0.9611)	Prec@(1,5) (66.0%, 96.9%)	
08/11 12:52:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 1.0553 (0.9639)	Prec@(1,5) (66.0%, 96.9%)	
08/11 12:53:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.8378 (0.9560)	Prec@(1,5) (66.3%, 96.9%)	
08/11 12:53:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 1.0083 (0.9551)	Prec@(1,5) (66.4%, 97.0%)	
08/11 12:53:44AM searchStage_trainer.py:208 [INFO] Train: [  6/49] Final Prec@1 66.4040%
08/11 12:53:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 1.0500	Prec@(1,5) (64.3%, 95.8%)
08/11 12:54:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 1.0584	Prec@(1,5) (63.7%, 95.9%)
08/11 12:54:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.0690	Prec@(1,5) (62.8%, 95.7%)
08/11 12:54:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 1.0643	Prec@(1,5) (62.8%, 95.9%)
08/11 12:54:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 1.0681	Prec@(1,5) (62.6%, 95.9%)
08/11 12:54:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 1.0704	Prec@(1,5) (62.4%, 95.8%)
08/11 12:54:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 1.0678	Prec@(1,5) (62.6%, 95.8%)
08/11 12:54:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 1.0670	Prec@(1,5) (62.7%, 95.9%)
08/11 12:54:51AM searchStage_trainer.py:247 [INFO] Valid: [  6/49] Final Prec@1 62.6440%
08/11 12:54:51AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 12:54:51AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 62.8000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2503, 0.2476, 0.2540, 0.2481],
        [0.2498, 0.2476, 0.2545, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2449, 0.2546, 0.2488],
        [0.2515, 0.2457, 0.2551, 0.2478],
        [0.2522, 0.2518, 0.2486, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2471, 0.2567, 0.2428],
        [0.2511, 0.2505, 0.2503, 0.2482],
        [0.2501, 0.2502, 0.2484, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2507, 0.2507, 0.2475],
        [0.2517, 0.2499, 0.2496, 0.2488],
        [0.2511, 0.2503, 0.2494, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2493, 0.2511, 0.2477],
        [0.2511, 0.2492, 0.2509, 0.2488],
        [0.2509, 0.2499, 0.2488, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2500, 0.2520, 0.2468],
        [0.2498, 0.2495, 0.2496, 0.2512],
        [0.2498, 0.2497, 0.2494, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2495, 0.2497, 0.2506, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2503, 0.2495],
        [0.2500, 0.2499, 0.2503, 0.2498],
        [0.2497, 0.2503, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2504, 0.2501, 0.2489],
        [0.2500, 0.2499, 0.2501, 0.2501],
        [0.2500, 0.2500, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2503, 0.2502, 0.2493],
        [0.2495, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2495, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2503, 0.2512, 0.2491],
        [0.2498, 0.2500, 0.2502, 0.2500],
        [0.2495, 0.2503, 0.2499, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2505, 0.2492],
        [0.2501, 0.2500, 0.2502, 0.2497],
        [0.2495, 0.2502, 0.2494, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2507, 0.2501],
        [0.2499, 0.2498, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2494, 0.2510, 0.2497],
        [0.2497, 0.2489, 0.2515, 0.2498],
        [0.2499, 0.2495, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2520, 0.2484],
        [0.2493, 0.2498, 0.2506, 0.2503],
        [0.2500, 0.2492, 0.2505, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2506, 0.2498],
        [0.2498, 0.2495, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2496, 0.2520, 0.2486],
        [0.2494, 0.2492, 0.2512, 0.2502],
        [0.2493, 0.2499, 0.2501, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2494, 0.2517, 0.2491],
        [0.2500, 0.2490, 0.2517, 0.2494],
        [0.2494, 0.2490, 0.2507, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 12:55:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.8653 (0.8793)	Prec@(1,5) (69.2%, 97.4%)	
08/11 12:56:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.7352 (0.9017)	Prec@(1,5) (68.3%, 97.2%)	
08/11 12:57:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 1.3707 (0.9009)	Prec@(1,5) (68.5%, 97.2%)	
08/11 12:58:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.9393 (0.8981)	Prec@(1,5) (68.6%, 97.3%)	
08/11 12:59:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.9161 (0.8936)	Prec@(1,5) (68.7%, 97.3%)	
08/11 01:00:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 1.1008 (0.8946)	Prec@(1,5) (68.6%, 97.4%)	
08/11 01:00:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.9054 (0.8979)	Prec@(1,5) (68.4%, 97.3%)	
08/11 01:01:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.7910 (0.8925)	Prec@(1,5) (68.6%, 97.4%)	
08/11 01:01:36AM searchStage_trainer.py:208 [INFO] Train: [  7/49] Final Prec@1 68.6160%
08/11 01:01:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.8881	Prec@(1,5) (67.7%, 97.6%)
08/11 01:01:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.8893	Prec@(1,5) (67.9%, 97.4%)
08/11 01:02:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.8979	Prec@(1,5) (67.9%, 97.5%)
08/11 01:02:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.8963	Prec@(1,5) (68.0%, 97.5%)
08/11 01:02:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.8976	Prec@(1,5) (68.1%, 97.4%)
08/11 01:02:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.8994	Prec@(1,5) (68.1%, 97.4%)
08/11 01:02:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.8965	Prec@(1,5) (68.2%, 97.4%)
08/11 01:02:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.9009	Prec@(1,5) (68.1%, 97.4%)
08/11 01:02:43AM searchStage_trainer.py:247 [INFO] Valid: [  7/49] Final Prec@1 68.1160%
08/11 01:02:43AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:02:43AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 68.1160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2473, 0.2545, 0.2481],
        [0.2496, 0.2473, 0.2549, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2443, 0.2553, 0.2488],
        [0.2514, 0.2451, 0.2558, 0.2477],
        [0.2522, 0.2516, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2466, 0.2577, 0.2424],
        [0.2510, 0.2503, 0.2504, 0.2483],
        [0.2500, 0.2501, 0.2484, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2508, 0.2475],
        [0.2517, 0.2498, 0.2497, 0.2488],
        [0.2511, 0.2502, 0.2495, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2492, 0.2513, 0.2476],
        [0.2512, 0.2491, 0.2511, 0.2487],
        [0.2508, 0.2498, 0.2488, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2499, 0.2523, 0.2466],
        [0.2498, 0.2494, 0.2496, 0.2512],
        [0.2497, 0.2496, 0.2494, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2497, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2503, 0.2494],
        [0.2499, 0.2499, 0.2504, 0.2498],
        [0.2496, 0.2503, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2502, 0.2488],
        [0.2500, 0.2499, 0.2501, 0.2501],
        [0.2500, 0.2500, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2503, 0.2492],
        [0.2494, 0.2506, 0.2501, 0.2498],
        [0.2501, 0.2499, 0.2495, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2503, 0.2513, 0.2490],
        [0.2497, 0.2500, 0.2502, 0.2500],
        [0.2495, 0.2503, 0.2498, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2506, 0.2491],
        [0.2501, 0.2500, 0.2502, 0.2498],
        [0.2495, 0.2502, 0.2494, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2508, 0.2501],
        [0.2498, 0.2498, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2494, 0.2510, 0.2497],
        [0.2497, 0.2489, 0.2515, 0.2498],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2522, 0.2482],
        [0.2493, 0.2497, 0.2506, 0.2503],
        [0.2499, 0.2492, 0.2505, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2506, 0.2498],
        [0.2498, 0.2495, 0.2502, 0.2505],
        [0.2500, 0.2499, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2496, 0.2522, 0.2484],
        [0.2493, 0.2491, 0.2513, 0.2503],
        [0.2492, 0.2498, 0.2501, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2493, 0.2519, 0.2491],
        [0.2499, 0.2489, 0.2518, 0.2494],
        [0.2494, 0.2489, 0.2508, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:03:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.7241 (0.8553)	Prec@(1,5) (70.7%, 97.4%)	
08/11 01:04:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.7445 (0.8325)	Prec@(1,5) (70.8%, 97.4%)	
08/11 01:05:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.7529 (0.8367)	Prec@(1,5) (70.6%, 97.4%)	
08/11 01:06:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.6944 (0.8334)	Prec@(1,5) (70.9%, 97.6%)	
08/11 01:07:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.9637 (0.8340)	Prec@(1,5) (70.8%, 97.7%)	
08/11 01:07:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.7762 (0.8320)	Prec@(1,5) (70.9%, 97.7%)	
08/11 01:08:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.6143 (0.8335)	Prec@(1,5) (71.0%, 97.7%)	
08/11 01:09:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.8740 (0.8344)	Prec@(1,5) (71.0%, 97.7%)	
08/11 01:09:28AM searchStage_trainer.py:208 [INFO] Train: [  8/49] Final Prec@1 71.0400%
08/11 01:09:37AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.9357	Prec@(1,5) (67.9%, 97.5%)
08/11 01:09:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.9535	Prec@(1,5) (67.5%, 97.5%)
08/11 01:09:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.9523	Prec@(1,5) (67.4%, 97.3%)
08/11 01:10:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.9620	Prec@(1,5) (66.9%, 97.3%)
08/11 01:10:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.9628	Prec@(1,5) (66.7%, 97.2%)
08/11 01:10:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.9626	Prec@(1,5) (67.0%, 97.2%)
08/11 01:10:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.9623	Prec@(1,5) (67.1%, 97.1%)
08/11 01:10:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.9598	Prec@(1,5) (67.2%, 97.1%)
08/11 01:10:35AM searchStage_trainer.py:247 [INFO] Valid: [  8/49] Final Prec@1 67.2040%
08/11 01:10:35AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:10:35AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 68.1160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2471, 0.2548, 0.2481],
        [0.2494, 0.2471, 0.2553, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2439, 0.2558, 0.2487],
        [0.2512, 0.2447, 0.2564, 0.2476],
        [0.2521, 0.2515, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2460, 0.2587, 0.2420],
        [0.2509, 0.2502, 0.2505, 0.2484],
        [0.2500, 0.2500, 0.2485, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2508, 0.2474],
        [0.2516, 0.2498, 0.2498, 0.2488],
        [0.2510, 0.2501, 0.2495, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2491, 0.2514, 0.2474],
        [0.2511, 0.2490, 0.2511, 0.2487],
        [0.2507, 0.2498, 0.2489, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2499, 0.2526, 0.2463],
        [0.2497, 0.2494, 0.2496, 0.2513],
        [0.2497, 0.2496, 0.2494, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2503, 0.2494],
        [0.2500, 0.2499, 0.2504, 0.2497],
        [0.2496, 0.2502, 0.2498, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2502, 0.2487],
        [0.2499, 0.2499, 0.2501, 0.2501],
        [0.2500, 0.2500, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2504, 0.2491],
        [0.2494, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2503, 0.2514, 0.2488],
        [0.2497, 0.2500, 0.2502, 0.2500],
        [0.2495, 0.2503, 0.2498, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2507, 0.2490],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2495, 0.2502, 0.2494, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2508, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2493, 0.2511, 0.2497],
        [0.2497, 0.2489, 0.2516, 0.2498],
        [0.2499, 0.2494, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2523, 0.2480],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2492, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2507, 0.2497],
        [0.2497, 0.2495, 0.2502, 0.2506],
        [0.2500, 0.2499, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2496, 0.2523, 0.2483],
        [0.2493, 0.2491, 0.2514, 0.2503],
        [0.2492, 0.2498, 0.2501, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2521, 0.2490],
        [0.2499, 0.2489, 0.2519, 0.2493],
        [0.2494, 0.2488, 0.2508, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:11:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.6004 (0.7931)	Prec@(1,5) (72.1%, 97.8%)	
08/11 01:12:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 1.0674 (0.8039)	Prec@(1,5) (72.0%, 97.9%)	
08/11 01:13:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.6607 (0.8009)	Prec@(1,5) (72.2%, 97.9%)	
08/11 01:14:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.6399 (0.7983)	Prec@(1,5) (72.3%, 97.9%)	
08/11 01:14:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.7041 (0.7919)	Prec@(1,5) (72.4%, 98.0%)	
08/11 01:15:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.9838 (0.7922)	Prec@(1,5) (72.5%, 98.0%)	
08/11 01:16:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.9301 (0.7950)	Prec@(1,5) (72.6%, 98.0%)	
08/11 01:17:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.8147 (0.7973)	Prec@(1,5) (72.4%, 98.0%)	
08/11 01:17:20AM searchStage_trainer.py:208 [INFO] Train: [  9/49] Final Prec@1 72.4320%
08/11 01:17:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.9703	Prec@(1,5) (65.5%, 96.8%)
08/11 01:17:37AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.9201	Prec@(1,5) (67.9%, 97.1%)
08/11 01:17:46AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.9219	Prec@(1,5) (68.2%, 97.0%)
08/11 01:17:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.9217	Prec@(1,5) (68.2%, 97.0%)
08/11 01:18:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.9226	Prec@(1,5) (68.2%, 97.0%)
08/11 01:18:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.9194	Prec@(1,5) (68.4%, 97.0%)
08/11 01:18:20AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.9160	Prec@(1,5) (68.4%, 97.1%)
08/11 01:18:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.9203	Prec@(1,5) (68.2%, 97.0%)
08/11 01:18:27AM searchStage_trainer.py:247 [INFO] Valid: [  9/49] Final Prec@1 68.2080%
08/11 01:18:27AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:18:27AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 68.2080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2469, 0.2551, 0.2482],
        [0.2493, 0.2469, 0.2555, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2436, 0.2564, 0.2487],
        [0.2511, 0.2444, 0.2569, 0.2476],
        [0.2520, 0.2515, 0.2489, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2455, 0.2594, 0.2420],
        [0.2508, 0.2501, 0.2507, 0.2484],
        [0.2499, 0.2500, 0.2485, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2505, 0.2509, 0.2474],
        [0.2516, 0.2497, 0.2498, 0.2489],
        [0.2509, 0.2501, 0.2495, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2490, 0.2514, 0.2474],
        [0.2511, 0.2489, 0.2513, 0.2486],
        [0.2507, 0.2497, 0.2489, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2498, 0.2527, 0.2462],
        [0.2497, 0.2493, 0.2496, 0.2514],
        [0.2496, 0.2496, 0.2494, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2504, 0.2494],
        [0.2500, 0.2499, 0.2505, 0.2497],
        [0.2496, 0.2502, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2506, 0.2503, 0.2485],
        [0.2499, 0.2499, 0.2501, 0.2501],
        [0.2499, 0.2500, 0.2495, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2504, 0.2490],
        [0.2494, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2503, 0.2515, 0.2487],
        [0.2497, 0.2500, 0.2502, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2508, 0.2489],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2495, 0.2502, 0.2494, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2508, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2493, 0.2511, 0.2497],
        [0.2497, 0.2488, 0.2516, 0.2499],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2499, 0.2525, 0.2481],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2492, 0.2505, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2506, 0.2498],
        [0.2498, 0.2495, 0.2502, 0.2506],
        [0.2500, 0.2499, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2495, 0.2524, 0.2483],
        [0.2492, 0.2490, 0.2514, 0.2503],
        [0.2492, 0.2498, 0.2502, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2522, 0.2489],
        [0.2499, 0.2488, 0.2520, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:19:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.8534 (0.7327)	Prec@(1,5) (74.1%, 98.5%)	
08/11 01:20:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 1.1513 (0.7381)	Prec@(1,5) (74.3%, 98.3%)	
08/11 01:21:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.5233 (0.7423)	Prec@(1,5) (73.9%, 98.4%)	
08/11 01:21:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.7396 (0.7464)	Prec@(1,5) (73.9%, 98.3%)	
08/11 01:22:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.6605 (0.7422)	Prec@(1,5) (74.1%, 98.3%)	
08/11 01:23:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.5839 (0.7453)	Prec@(1,5) (74.2%, 98.3%)	
08/11 01:24:31AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.6017 (0.7422)	Prec@(1,5) (74.2%, 98.3%)	
08/11 01:25:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.7283 (0.7445)	Prec@(1,5) (74.1%, 98.3%)	
08/11 01:25:12AM searchStage_trainer.py:208 [INFO] Train: [ 10/49] Final Prec@1 74.0880%
08/11 01:25:20AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.8329	Prec@(1,5) (71.7%, 97.0%)
08/11 01:25:29AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.8448	Prec@(1,5) (71.2%, 97.4%)
08/11 01:25:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.8544	Prec@(1,5) (70.6%, 97.4%)
08/11 01:25:46AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.8477	Prec@(1,5) (70.7%, 97.5%)
08/11 01:25:55AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.8513	Prec@(1,5) (70.7%, 97.5%)
08/11 01:26:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.8519	Prec@(1,5) (70.6%, 97.5%)
08/11 01:26:12AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.8539	Prec@(1,5) (70.5%, 97.6%)
08/11 01:26:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.8542	Prec@(1,5) (70.5%, 97.5%)
08/11 01:26:19AM searchStage_trainer.py:247 [INFO] Valid: [ 10/49] Final Prec@1 70.4880%
08/11 01:26:19AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:26:19AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 70.4880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2468, 0.2554, 0.2482],
        [0.2492, 0.2468, 0.2557, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2433, 0.2568, 0.2486],
        [0.2510, 0.2441, 0.2574, 0.2476],
        [0.2520, 0.2515, 0.2489, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2451, 0.2602, 0.2418],
        [0.2508, 0.2500, 0.2507, 0.2484],
        [0.2499, 0.2499, 0.2485, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2510, 0.2473],
        [0.2516, 0.2497, 0.2498, 0.2489],
        [0.2509, 0.2501, 0.2496, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2489, 0.2515, 0.2474],
        [0.2511, 0.2489, 0.2514, 0.2486],
        [0.2506, 0.2497, 0.2489, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2529, 0.2460],
        [0.2496, 0.2493, 0.2496, 0.2515],
        [0.2496, 0.2495, 0.2494, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2496],
        [0.2496, 0.2502, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2506, 0.2504, 0.2484],
        [0.2499, 0.2499, 0.2501, 0.2501],
        [0.2499, 0.2500, 0.2495, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2498, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2516, 0.2486],
        [0.2497, 0.2500, 0.2502, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2508, 0.2489],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2495, 0.2502, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2508, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2493, 0.2512, 0.2497],
        [0.2496, 0.2488, 0.2517, 0.2499],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2527, 0.2480],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2492, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2502, 0.2506],
        [0.2500, 0.2499, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2495, 0.2525, 0.2483],
        [0.2492, 0.2490, 0.2515, 0.2503],
        [0.2492, 0.2498, 0.2502, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2491, 0.2524, 0.2489],
        [0.2498, 0.2488, 0.2521, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:27:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.8388 (0.7258)	Prec@(1,5) (74.3%, 98.4%)	
08/11 01:28:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.7189 (0.7229)	Prec@(1,5) (74.5%, 98.5%)	
08/11 01:28:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.7316 (0.7222)	Prec@(1,5) (74.6%, 98.3%)	
08/11 01:29:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.5895 (0.7191)	Prec@(1,5) (74.6%, 98.3%)	
08/11 01:30:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.6554 (0.7102)	Prec@(1,5) (75.0%, 98.4%)	
08/11 01:31:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.5263 (0.7067)	Prec@(1,5) (75.2%, 98.4%)	
08/11 01:32:23AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.7126 (0.7119)	Prec@(1,5) (75.1%, 98.4%)	
08/11 01:33:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 1.0067 (0.7158)	Prec@(1,5) (75.0%, 98.4%)	
08/11 01:33:03AM searchStage_trainer.py:208 [INFO] Train: [ 11/49] Final Prec@1 74.9800%
08/11 01:33:12AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.8531	Prec@(1,5) (71.7%, 97.4%)
08/11 01:33:21AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.8472	Prec@(1,5) (71.3%, 97.3%)
08/11 01:33:29AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.8441	Prec@(1,5) (71.1%, 97.3%)
08/11 01:33:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.8325	Prec@(1,5) (71.3%, 97.5%)
08/11 01:33:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.8298	Prec@(1,5) (71.4%, 97.5%)
08/11 01:33:55AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.8296	Prec@(1,5) (71.4%, 97.4%)
08/11 01:34:04AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.8250	Prec@(1,5) (71.7%, 97.5%)
08/11 01:34:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.8237	Prec@(1,5) (71.9%, 97.5%)
08/11 01:34:11AM searchStage_trainer.py:247 [INFO] Valid: [ 11/49] Final Prec@1 71.8600%
08/11 01:34:11AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:34:11AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 71.8600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2466, 0.2555, 0.2482],
        [0.2491, 0.2467, 0.2559, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2430, 0.2571, 0.2487],
        [0.2509, 0.2438, 0.2577, 0.2475],
        [0.2520, 0.2514, 0.2489, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2447, 0.2610, 0.2417],
        [0.2508, 0.2500, 0.2508, 0.2484],
        [0.2498, 0.2499, 0.2486, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2510, 0.2472],
        [0.2515, 0.2497, 0.2498, 0.2490],
        [0.2509, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2489, 0.2516, 0.2473],
        [0.2511, 0.2489, 0.2514, 0.2486],
        [0.2505, 0.2497, 0.2489, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2530, 0.2459],
        [0.2496, 0.2493, 0.2496, 0.2516],
        [0.2496, 0.2495, 0.2494, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2496],
        [0.2496, 0.2502, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2507, 0.2504, 0.2483],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2499, 0.2500, 0.2495, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2498, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2517, 0.2485],
        [0.2497, 0.2500, 0.2502, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2509, 0.2488],
        [0.2501, 0.2500, 0.2501, 0.2498],
        [0.2495, 0.2502, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2508, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2512, 0.2497],
        [0.2496, 0.2487, 0.2517, 0.2499],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2528, 0.2478],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2495, 0.2526, 0.2483],
        [0.2492, 0.2490, 0.2516, 0.2503],
        [0.2492, 0.2497, 0.2502, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2491, 0.2525, 0.2489],
        [0.2498, 0.2488, 0.2521, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:35:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.7184 (0.6985)	Prec@(1,5) (75.2%, 98.3%)	
08/11 01:35:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.7685 (0.6932)	Prec@(1,5) (75.5%, 98.6%)	
08/11 01:36:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.7941 (0.6786)	Prec@(1,5) (76.0%, 98.7%)	
08/11 01:37:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.7825 (0.6784)	Prec@(1,5) (76.3%, 98.6%)	
08/11 01:38:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.4765 (0.6755)	Prec@(1,5) (76.4%, 98.5%)	
08/11 01:39:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.7887 (0.6724)	Prec@(1,5) (76.7%, 98.6%)	
08/11 01:40:15AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.8554 (0.6740)	Prec@(1,5) (76.6%, 98.6%)	
08/11 01:40:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.5699 (0.6758)	Prec@(1,5) (76.6%, 98.5%)	
08/11 01:40:55AM searchStage_trainer.py:208 [INFO] Train: [ 12/49] Final Prec@1 76.5680%
08/11 01:41:04AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.7274	Prec@(1,5) (74.8%, 97.9%)
08/11 01:41:13AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7551	Prec@(1,5) (73.9%, 98.0%)
08/11 01:41:21AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.7628	Prec@(1,5) (73.6%, 98.1%)
08/11 01:41:30AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.7636	Prec@(1,5) (73.6%, 98.2%)
08/11 01:41:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.7712	Prec@(1,5) (73.3%, 98.2%)
08/11 01:41:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.7712	Prec@(1,5) (73.1%, 98.1%)
08/11 01:41:56AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.7681	Prec@(1,5) (73.1%, 98.1%)
08/11 01:42:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.7676	Prec@(1,5) (73.1%, 98.1%)
08/11 01:42:03AM searchStage_trainer.py:247 [INFO] Valid: [ 12/49] Final Prec@1 73.1280%
08/11 01:42:03AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:42:03AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 73.1280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2465, 0.2557, 0.2482],
        [0.2490, 0.2466, 0.2561, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2428, 0.2574, 0.2487],
        [0.2509, 0.2436, 0.2580, 0.2475],
        [0.2520, 0.2514, 0.2489, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2444, 0.2617, 0.2414],
        [0.2507, 0.2499, 0.2508, 0.2485],
        [0.2498, 0.2499, 0.2486, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2510, 0.2472],
        [0.2515, 0.2497, 0.2498, 0.2490],
        [0.2509, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2489, 0.2516, 0.2472],
        [0.2511, 0.2488, 0.2515, 0.2486],
        [0.2505, 0.2497, 0.2489, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2530, 0.2458],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2495, 0.2495, 0.2494, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2496],
        [0.2496, 0.2502, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2507, 0.2505, 0.2482],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2501, 0.2499],
        [0.2501, 0.2498, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2518, 0.2484],
        [0.2497, 0.2500, 0.2502, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2509, 0.2488],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2502, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2508, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2513, 0.2497],
        [0.2496, 0.2487, 0.2518, 0.2499],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2499, 0.2530, 0.2477],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2494, 0.2526, 0.2483],
        [0.2491, 0.2489, 0.2516, 0.2503],
        [0.2492, 0.2497, 0.2502, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2490, 0.2526, 0.2489],
        [0.2498, 0.2488, 0.2522, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:42:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.6555 (0.6620)	Prec@(1,5) (76.6%, 98.6%)	
08/11 01:43:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.7116 (0.6462)	Prec@(1,5) (77.4%, 98.8%)	
08/11 01:44:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.7508 (0.6469)	Prec@(1,5) (77.2%, 98.8%)	
08/11 01:45:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.9113 (0.6541)	Prec@(1,5) (77.2%, 98.7%)	
08/11 01:46:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.3858 (0.6502)	Prec@(1,5) (77.3%, 98.7%)	
08/11 01:47:15AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.8125 (0.6492)	Prec@(1,5) (77.4%, 98.7%)	
08/11 01:48:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.6256 (0.6531)	Prec@(1,5) (77.3%, 98.7%)	
08/11 01:48:46AM searchStage_trainer.py:198 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.5359 (0.6521)	Prec@(1,5) (77.3%, 98.7%)	
08/11 01:48:47AM searchStage_trainer.py:208 [INFO] Train: [ 13/49] Final Prec@1 77.3240%
08/11 01:48:56AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.7309	Prec@(1,5) (75.4%, 97.8%)
08/11 01:49:05AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.7500	Prec@(1,5) (74.9%, 97.8%)
08/11 01:49:13AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.7487	Prec@(1,5) (74.6%, 97.9%)
08/11 01:49:22AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.7396	Prec@(1,5) (74.9%, 98.1%)
08/11 01:49:30AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.7362	Prec@(1,5) (74.9%, 98.1%)
08/11 01:49:39AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.7412	Prec@(1,5) (74.6%, 98.1%)
08/11 01:49:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.7443	Prec@(1,5) (74.5%, 98.0%)
08/11 01:49:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.7459	Prec@(1,5) (74.4%, 98.0%)
08/11 01:49:54AM searchStage_trainer.py:247 [INFO] Valid: [ 13/49] Final Prec@1 74.3920%
08/11 01:49:55AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:49:55AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 74.3920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2464, 0.2558, 0.2482],
        [0.2490, 0.2465, 0.2562, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2427, 0.2577, 0.2486],
        [0.2508, 0.2434, 0.2583, 0.2475],
        [0.2519, 0.2514, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2441, 0.2623, 0.2412],
        [0.2507, 0.2499, 0.2509, 0.2485],
        [0.2498, 0.2499, 0.2486, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2510, 0.2471],
        [0.2515, 0.2497, 0.2498, 0.2490],
        [0.2508, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2489, 0.2517, 0.2471],
        [0.2511, 0.2488, 0.2515, 0.2486],
        [0.2505, 0.2497, 0.2489, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2531, 0.2458],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2495, 0.2495, 0.2494, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2496],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2507, 0.2505, 0.2481],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2501, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2518, 0.2484],
        [0.2497, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2509, 0.2487],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2502, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2513, 0.2497],
        [0.2496, 0.2487, 0.2518, 0.2499],
        [0.2499, 0.2494, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2499, 0.2531, 0.2476],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2494, 0.2527, 0.2483],
        [0.2491, 0.2489, 0.2516, 0.2503],
        [0.2492, 0.2497, 0.2503, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2489, 0.2527, 0.2489],
        [0.2497, 0.2487, 0.2522, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:50:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.7480 (0.6106)	Prec@(1,5) (78.9%, 98.8%)	
08/11 01:51:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.4190 (0.6146)	Prec@(1,5) (78.6%, 98.7%)	
08/11 01:52:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.4878 (0.6089)	Prec@(1,5) (79.0%, 98.8%)	
08/11 01:53:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.6667 (0.6122)	Prec@(1,5) (78.8%, 98.9%)	
08/11 01:54:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.6869 (0.6178)	Prec@(1,5) (78.7%, 98.8%)	
08/11 01:55:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.4921 (0.6207)	Prec@(1,5) (78.6%, 98.8%)	
08/11 01:55:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.7334 (0.6192)	Prec@(1,5) (78.6%, 98.8%)	
08/11 01:56:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.5191 (0.6218)	Prec@(1,5) (78.5%, 98.8%)	
08/11 01:56:39AM searchStage_trainer.py:208 [INFO] Train: [ 14/49] Final Prec@1 78.5200%
08/11 01:56:48AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.6913	Prec@(1,5) (76.4%, 98.4%)
08/11 01:56:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.7070	Prec@(1,5) (75.8%, 98.2%)
08/11 01:57:05AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.7191	Prec@(1,5) (75.5%, 98.1%)
08/11 01:57:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.7195	Prec@(1,5) (75.5%, 98.1%)
08/11 01:57:22AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.7199	Prec@(1,5) (75.3%, 98.1%)
08/11 01:57:31AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.7197	Prec@(1,5) (75.3%, 98.2%)
08/11 01:57:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.7222	Prec@(1,5) (75.3%, 98.2%)
08/11 01:57:46AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.7211	Prec@(1,5) (75.3%, 98.2%)
08/11 01:57:47AM searchStage_trainer.py:247 [INFO] Valid: [ 14/49] Final Prec@1 75.3080%
08/11 01:57:47AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 01:57:47AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 75.3080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2464, 0.2559, 0.2482],
        [0.2490, 0.2464, 0.2563, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2425, 0.2579, 0.2486],
        [0.2507, 0.2432, 0.2586, 0.2474],
        [0.2519, 0.2514, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2438, 0.2629, 0.2411],
        [0.2507, 0.2498, 0.2509, 0.2486],
        [0.2498, 0.2499, 0.2486, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2510, 0.2471],
        [0.2515, 0.2497, 0.2498, 0.2490],
        [0.2508, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2488, 0.2517, 0.2471],
        [0.2511, 0.2488, 0.2515, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2532, 0.2457],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2495, 0.2495, 0.2494, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2508, 0.2505, 0.2480],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2501, 0.2498],
        [0.2501, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2518, 0.2483],
        [0.2497, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2510, 0.2487],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2514, 0.2497],
        [0.2496, 0.2487, 0.2518, 0.2499],
        [0.2499, 0.2494, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2499, 0.2533, 0.2475],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2494, 0.2528, 0.2483],
        [0.2491, 0.2489, 0.2517, 0.2503],
        [0.2492, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2489, 0.2527, 0.2489],
        [0.2497, 0.2487, 0.2523, 0.2493],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 01:58:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.5183 (0.6192)	Prec@(1,5) (79.3%, 98.7%)	
08/11 01:59:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.5864 (0.5995)	Prec@(1,5) (79.7%, 98.8%)	
08/11 02:00:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.6103 (0.5954)	Prec@(1,5) (79.7%, 98.9%)	
08/11 02:01:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.5626 (0.5995)	Prec@(1,5) (79.6%, 98.9%)	
08/11 02:02:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.8814 (0.6057)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:02:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.7071 (0.6053)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:03:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.4336 (0.6045)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:04:30AM searchStage_trainer.py:198 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.6213 (0.6041)	Prec@(1,5) (79.3%, 98.8%)	
08/11 02:04:31AM searchStage_trainer.py:208 [INFO] Train: [ 15/49] Final Prec@1 79.2880%
08/11 02:04:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.7149	Prec@(1,5) (74.8%, 98.7%)
08/11 02:04:49AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.7218	Prec@(1,5) (75.2%, 98.5%)
08/11 02:04:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.7058	Prec@(1,5) (75.8%, 98.6%)
08/11 02:05:06AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.6975	Prec@(1,5) (76.0%, 98.7%)
08/11 02:05:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.6949	Prec@(1,5) (76.0%, 98.7%)
08/11 02:05:23AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.6928	Prec@(1,5) (76.1%, 98.7%)
08/11 02:05:31AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.6931	Prec@(1,5) (76.1%, 98.7%)
08/11 02:05:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.6924	Prec@(1,5) (76.1%, 98.7%)
08/11 02:05:39AM searchStage_trainer.py:247 [INFO] Valid: [ 15/49] Final Prec@1 76.1200%
08/11 02:05:39AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:05:39AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 76.1200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2463, 0.2559, 0.2482],
        [0.2489, 0.2464, 0.2564, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2423, 0.2582, 0.2486],
        [0.2507, 0.2431, 0.2588, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2435, 0.2636, 0.2409],
        [0.2506, 0.2498, 0.2510, 0.2486],
        [0.2497, 0.2498, 0.2486, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2506, 0.2511, 0.2471],
        [0.2515, 0.2497, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2488, 0.2517, 0.2470],
        [0.2511, 0.2488, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2532, 0.2456],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2495, 0.2495, 0.2494, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2504, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2509, 0.2506, 0.2479],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2501, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2519, 0.2482],
        [0.2497, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2510, 0.2487],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2509, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2514, 0.2496],
        [0.2495, 0.2487, 0.2518, 0.2499],
        [0.2499, 0.2494, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2499, 0.2534, 0.2475],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2494, 0.2528, 0.2483],
        [0.2491, 0.2489, 0.2517, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2489, 0.2528, 0.2489],
        [0.2497, 0.2487, 0.2523, 0.2493],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:06:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.6992 (0.5998)	Prec@(1,5) (79.5%, 98.9%)	
08/11 02:07:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.3622 (0.5855)	Prec@(1,5) (79.8%, 98.9%)	
08/11 02:08:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.6028 (0.5864)	Prec@(1,5) (79.8%, 99.0%)	
08/11 02:09:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.6439 (0.5776)	Prec@(1,5) (80.1%, 99.0%)	
08/11 02:09:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.3953 (0.5738)	Prec@(1,5) (80.2%, 99.0%)	
08/11 02:10:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.5500 (0.5732)	Prec@(1,5) (80.2%, 99.0%)	
08/11 02:11:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.4039 (0.5739)	Prec@(1,5) (80.1%, 99.0%)	
08/11 02:12:22AM searchStage_trainer.py:198 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.6336 (0.5724)	Prec@(1,5) (80.2%, 99.0%)	
08/11 02:12:23AM searchStage_trainer.py:208 [INFO] Train: [ 16/49] Final Prec@1 80.2000%
08/11 02:12:32AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.6123	Prec@(1,5) (79.6%, 98.6%)
08/11 02:12:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6155	Prec@(1,5) (79.4%, 98.5%)
08/11 02:12:49AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.6189	Prec@(1,5) (79.1%, 98.5%)
08/11 02:12:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.6174	Prec@(1,5) (78.9%, 98.6%)
08/11 02:13:06AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6171	Prec@(1,5) (79.0%, 98.5%)
08/11 02:13:15AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6171	Prec@(1,5) (79.0%, 98.6%)
08/11 02:13:23AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6205	Prec@(1,5) (78.9%, 98.5%)
08/11 02:13:30AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6191	Prec@(1,5) (79.0%, 98.6%)
08/11 02:13:30AM searchStage_trainer.py:247 [INFO] Valid: [ 16/49] Final Prec@1 78.9640%
08/11 02:13:30AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:13:30AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 78.9640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2463, 0.2560, 0.2483],
        [0.2489, 0.2464, 0.2564, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2422, 0.2584, 0.2485],
        [0.2506, 0.2429, 0.2591, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2433, 0.2642, 0.2407],
        [0.2506, 0.2498, 0.2510, 0.2486],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2470],
        [0.2515, 0.2497, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2488, 0.2517, 0.2470],
        [0.2510, 0.2488, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2533, 0.2456],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2494, 0.2495, 0.2494, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2506, 0.2478],
        [0.2499, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2519, 0.2482],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2510, 0.2486],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2514, 0.2496],
        [0.2495, 0.2487, 0.2519, 0.2499],
        [0.2499, 0.2494, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2499, 0.2535, 0.2474],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2499, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2494, 0.2529, 0.2483],
        [0.2490, 0.2489, 0.2518, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2488, 0.2529, 0.2489],
        [0.2497, 0.2487, 0.2524, 0.2493],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:14:23AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.4599 (0.5529)	Prec@(1,5) (81.4%, 99.1%)	
08/11 02:15:15AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.4743 (0.5372)	Prec@(1,5) (81.6%, 99.1%)	
08/11 02:16:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.6236 (0.5317)	Prec@(1,5) (81.8%, 99.1%)	
08/11 02:16:58AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.5113 (0.5286)	Prec@(1,5) (81.7%, 99.2%)	
08/11 02:17:50AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.7057 (0.5345)	Prec@(1,5) (81.4%, 99.2%)	
08/11 02:18:41AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.5214 (0.5381)	Prec@(1,5) (81.3%, 99.2%)	
08/11 02:19:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.7440 (0.5427)	Prec@(1,5) (81.2%, 99.2%)	
08/11 02:20:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.4514 (0.5451)	Prec@(1,5) (81.2%, 99.2%)	
08/11 02:20:12AM searchStage_trainer.py:208 [INFO] Train: [ 17/49] Final Prec@1 81.1520%
08/11 02:20:21AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6279	Prec@(1,5) (79.5%, 98.5%)
08/11 02:20:30AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6295	Prec@(1,5) (79.0%, 98.6%)
08/11 02:20:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6311	Prec@(1,5) (78.4%, 98.8%)
08/11 02:20:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6312	Prec@(1,5) (78.3%, 98.8%)
08/11 02:20:55AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6331	Prec@(1,5) (78.4%, 98.7%)
08/11 02:21:04AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6321	Prec@(1,5) (78.3%, 98.7%)
08/11 02:21:12AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6315	Prec@(1,5) (78.3%, 98.7%)
08/11 02:21:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6308	Prec@(1,5) (78.4%, 98.7%)
08/11 02:21:19AM searchStage_trainer.py:247 [INFO] Valid: [ 17/49] Final Prec@1 78.3840%
08/11 02:21:19AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:21:20AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 78.9640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2463, 0.2561, 0.2483],
        [0.2489, 0.2463, 0.2565, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2421, 0.2585, 0.2485],
        [0.2505, 0.2428, 0.2592, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2430, 0.2647, 0.2406],
        [0.2506, 0.2498, 0.2510, 0.2486],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2470],
        [0.2514, 0.2497, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2488, 0.2518, 0.2470],
        [0.2510, 0.2487, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2533, 0.2455],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2494, 0.2495, 0.2494, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2505, 0.2493],
        [0.2500, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2506, 0.2477],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2505, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2520, 0.2481],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2511, 0.2486],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2514, 0.2496],
        [0.2495, 0.2487, 0.2519, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2499, 0.2536, 0.2474],
        [0.2493, 0.2497, 0.2507, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2493, 0.2529, 0.2483],
        [0.2490, 0.2488, 0.2518, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2488, 0.2529, 0.2489],
        [0.2497, 0.2487, 0.2524, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:22:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.5342 (0.4963)	Prec@(1,5) (82.7%, 98.9%)	
08/11 02:23:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.6071 (0.5125)	Prec@(1,5) (82.4%, 99.1%)	
08/11 02:23:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.3777 (0.5282)	Prec@(1,5) (81.8%, 99.1%)	
08/11 02:24:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.5338 (0.5362)	Prec@(1,5) (81.3%, 99.1%)	
08/11 02:25:38AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.5546 (0.5307)	Prec@(1,5) (81.6%, 99.1%)	
08/11 02:26:30AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.5872 (0.5356)	Prec@(1,5) (81.5%, 99.0%)	
08/11 02:27:21AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.6065 (0.5379)	Prec@(1,5) (81.3%, 99.0%)	
08/11 02:28:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.3708 (0.5321)	Prec@(1,5) (81.5%, 99.1%)	
08/11 02:28:01AM searchStage_trainer.py:208 [INFO] Train: [ 18/49] Final Prec@1 81.5240%
08/11 02:28:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6173	Prec@(1,5) (78.9%, 98.7%)
08/11 02:28:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.5995	Prec@(1,5) (79.7%, 98.5%)
08/11 02:28:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.5962	Prec@(1,5) (79.8%, 98.6%)
08/11 02:28:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.5936	Prec@(1,5) (79.9%, 98.6%)
08/11 02:28:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.5966	Prec@(1,5) (79.7%, 98.6%)
08/11 02:28:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.5958	Prec@(1,5) (79.7%, 98.6%)
08/11 02:29:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.5993	Prec@(1,5) (79.4%, 98.6%)
08/11 02:29:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.5969	Prec@(1,5) (79.5%, 98.6%)
08/11 02:29:09AM searchStage_trainer.py:247 [INFO] Valid: [ 18/49] Final Prec@1 79.4880%
08/11 02:29:09AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:29:09AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 79.4880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2462, 0.2561, 0.2483],
        [0.2488, 0.2463, 0.2565, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2420, 0.2587, 0.2485],
        [0.2505, 0.2427, 0.2594, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2428, 0.2651, 0.2405],
        [0.2506, 0.2498, 0.2510, 0.2486],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2470],
        [0.2514, 0.2497, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2487, 0.2518, 0.2470],
        [0.2510, 0.2487, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2533, 0.2455],
        [0.2495, 0.2493, 0.2496, 0.2516],
        [0.2494, 0.2495, 0.2494, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2505, 0.2493],
        [0.2501, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2510, 0.2507, 0.2475],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2506, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2520, 0.2481],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2511, 0.2486],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2515, 0.2496],
        [0.2495, 0.2487, 0.2519, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2499, 0.2538, 0.2473],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2493, 0.2530, 0.2483],
        [0.2490, 0.2488, 0.2518, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2488, 0.2530, 0.2489],
        [0.2497, 0.2487, 0.2524, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:30:02AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.5100 (0.5107)	Prec@(1,5) (81.6%, 99.3%)	
08/11 02:30:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.4726 (0.5153)	Prec@(1,5) (81.9%, 99.1%)	
08/11 02:31:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.3905 (0.5134)	Prec@(1,5) (81.9%, 99.2%)	
08/11 02:32:37AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.5549 (0.5105)	Prec@(1,5) (81.9%, 99.2%)	
08/11 02:33:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.4312 (0.5136)	Prec@(1,5) (81.9%, 99.2%)	
08/11 02:34:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.5224 (0.5143)	Prec@(1,5) (82.0%, 99.2%)	
08/11 02:35:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.3754 (0.5172)	Prec@(1,5) (82.0%, 99.2%)	
08/11 02:35:50AM searchStage_trainer.py:198 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.5722 (0.5149)	Prec@(1,5) (82.0%, 99.3%)	
08/11 02:35:51AM searchStage_trainer.py:208 [INFO] Train: [ 19/49] Final Prec@1 82.0200%
08/11 02:36:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6519	Prec@(1,5) (78.0%, 99.1%)
08/11 02:36:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6693	Prec@(1,5) (78.0%, 98.7%)
08/11 02:36:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6638	Prec@(1,5) (77.8%, 98.6%)
08/11 02:36:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6687	Prec@(1,5) (77.6%, 98.6%)
08/11 02:36:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6631	Prec@(1,5) (77.8%, 98.7%)
08/11 02:36:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6595	Prec@(1,5) (78.0%, 98.7%)
08/11 02:36:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6598	Prec@(1,5) (77.9%, 98.7%)
08/11 02:36:58AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6580	Prec@(1,5) (77.9%, 98.7%)
08/11 02:36:58AM searchStage_trainer.py:247 [INFO] Valid: [ 19/49] Final Prec@1 77.9240%
08/11 02:36:58AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:36:58AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 79.4880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2462, 0.2561, 0.2483],
        [0.2488, 0.2463, 0.2565, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2419, 0.2589, 0.2485],
        [0.2504, 0.2426, 0.2596, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2425, 0.2656, 0.2404],
        [0.2505, 0.2498, 0.2510, 0.2486],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2470],
        [0.2514, 0.2497, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2487, 0.2518, 0.2469],
        [0.2510, 0.2487, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2533, 0.2455],
        [0.2495, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2505, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2510, 0.2507, 0.2474],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2506, 0.2490],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2520, 0.2480],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2511, 0.2486],
        [0.2500, 0.2499, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2515, 0.2496],
        [0.2495, 0.2486, 0.2519, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2499, 0.2539, 0.2473],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2493, 0.2530, 0.2483],
        [0.2489, 0.2488, 0.2518, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2487, 0.2530, 0.2489],
        [0.2496, 0.2487, 0.2524, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:37:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.5985 (0.4622)	Prec@(1,5) (84.0%, 99.4%)	
08/11 02:38:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.6553 (0.4639)	Prec@(1,5) (84.1%, 99.3%)	
08/11 02:39:34AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4953 (0.4646)	Prec@(1,5) (84.0%, 99.3%)	
08/11 02:40:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.5069 (0.4748)	Prec@(1,5) (83.6%, 99.3%)	
08/11 02:41:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4171 (0.4823)	Prec@(1,5) (83.3%, 99.3%)	
08/11 02:42:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.4193 (0.4855)	Prec@(1,5) (83.3%, 99.2%)	
08/11 02:43:00AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.6098 (0.4870)	Prec@(1,5) (83.3%, 99.2%)	
08/11 02:43:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.4327 (0.4882)	Prec@(1,5) (83.3%, 99.2%)	
08/11 02:43:40AM searchStage_trainer.py:208 [INFO] Train: [ 20/49] Final Prec@1 83.2680%
08/11 02:43:49AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5830	Prec@(1,5) (80.0%, 98.7%)
08/11 02:43:58AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5786	Prec@(1,5) (80.2%, 98.8%)
08/11 02:44:06AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5771	Prec@(1,5) (80.2%, 98.7%)
08/11 02:44:15AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.5799	Prec@(1,5) (80.1%, 98.7%)
08/11 02:44:23AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.5794	Prec@(1,5) (80.1%, 98.8%)
08/11 02:44:32AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.5856	Prec@(1,5) (80.0%, 98.8%)
08/11 02:44:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.5890	Prec@(1,5) (80.1%, 98.7%)
08/11 02:44:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.5880	Prec@(1,5) (80.1%, 98.7%)
08/11 02:44:47AM searchStage_trainer.py:247 [INFO] Valid: [ 20/49] Final Prec@1 80.0480%
08/11 02:44:48AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:44:48AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 80.0480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2562, 0.2483],
        [0.2488, 0.2462, 0.2566, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2419, 0.2590, 0.2485],
        [0.2504, 0.2425, 0.2597, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2423, 0.2662, 0.2403],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2470],
        [0.2514, 0.2496, 0.2498, 0.2491],
        [0.2508, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2487, 0.2518, 0.2469],
        [0.2510, 0.2487, 0.2516, 0.2486],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2455],
        [0.2495, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2511, 0.2507, 0.2474],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2520, 0.2480],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2511, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2515, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2499, 0.2540, 0.2472],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2493, 0.2531, 0.2483],
        [0.2489, 0.2488, 0.2519, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2487, 0.2531, 0.2489],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:45:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.3848 (0.4785)	Prec@(1,5) (82.9%, 99.4%)	
08/11 02:46:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.4778 (0.4659)	Prec@(1,5) (83.5%, 99.4%)	
08/11 02:47:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.5428 (0.4721)	Prec@(1,5) (83.2%, 99.4%)	
08/11 02:48:15AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.6026 (0.4770)	Prec@(1,5) (83.0%, 99.4%)	
08/11 02:49:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.4039 (0.4769)	Prec@(1,5) (83.0%, 99.3%)	
08/11 02:49:58AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.4564 (0.4718)	Prec@(1,5) (83.3%, 99.4%)	
08/11 02:50:49AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.7153 (0.4728)	Prec@(1,5) (83.3%, 99.3%)	
08/11 02:51:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.5841 (0.4760)	Prec@(1,5) (83.2%, 99.3%)	
08/11 02:51:29AM searchStage_trainer.py:208 [INFO] Train: [ 21/49] Final Prec@1 83.2120%
08/11 02:51:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.5298	Prec@(1,5) (81.9%, 99.2%)
08/11 02:51:46AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.5371	Prec@(1,5) (81.6%, 98.9%)
08/11 02:51:55AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.5328	Prec@(1,5) (81.7%, 99.0%)
08/11 02:52:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.5335	Prec@(1,5) (81.8%, 99.0%)
08/11 02:52:12AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.5422	Prec@(1,5) (81.5%, 99.0%)
08/11 02:52:21AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.5470	Prec@(1,5) (81.4%, 98.9%)
08/11 02:52:29AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.5479	Prec@(1,5) (81.3%, 98.9%)
08/11 02:52:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5506	Prec@(1,5) (81.2%, 98.9%)
08/11 02:52:36AM searchStage_trainer.py:247 [INFO] Valid: [ 21/49] Final Prec@1 81.2240%
08/11 02:52:36AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 02:52:36AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.2240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2562, 0.2483],
        [0.2488, 0.2462, 0.2566, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2418, 0.2591, 0.2485],
        [0.2503, 0.2424, 0.2599, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2422, 0.2665, 0.2403],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2497, 0.2498, 0.2486, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2496, 0.2498, 0.2491],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2487, 0.2518, 0.2468],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2454],
        [0.2495, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2511, 0.2508, 0.2473],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2505, 0.2520, 0.2480],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2511, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2499],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2515, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2498, 0.2541, 0.2472],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2493, 0.2531, 0.2483],
        [0.2489, 0.2488, 0.2519, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2531, 0.2490],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 02:53:29AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.4548 (0.4432)	Prec@(1,5) (84.8%, 99.4%)	
08/11 02:54:21AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.4027 (0.4420)	Prec@(1,5) (84.7%, 99.4%)	
08/11 02:55:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.3079 (0.4548)	Prec@(1,5) (84.2%, 99.3%)	
08/11 02:56:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.6028 (0.4562)	Prec@(1,5) (84.3%, 99.3%)	
08/11 02:56:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.5396 (0.4573)	Prec@(1,5) (84.2%, 99.3%)	
08/11 02:57:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.2928 (0.4588)	Prec@(1,5) (84.2%, 99.3%)	
08/11 02:58:38AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.3662 (0.4541)	Prec@(1,5) (84.3%, 99.3%)	
08/11 02:59:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.3677 (0.4523)	Prec@(1,5) (84.3%, 99.3%)	
08/11 02:59:18AM searchStage_trainer.py:208 [INFO] Train: [ 22/49] Final Prec@1 84.3400%
08/11 02:59:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.6177	Prec@(1,5) (80.2%, 98.7%)
08/11 02:59:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.5953	Prec@(1,5) (80.5%, 98.7%)
08/11 02:59:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.6003	Prec@(1,5) (80.3%, 98.6%)
08/11 02:59:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.6051	Prec@(1,5) (80.2%, 98.7%)
08/11 03:00:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.5976	Prec@(1,5) (80.5%, 98.8%)
08/11 03:00:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.6035	Prec@(1,5) (80.3%, 98.7%)
08/11 03:00:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.6056	Prec@(1,5) (80.3%, 98.7%)
08/11 03:00:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.6052	Prec@(1,5) (80.3%, 98.7%)
08/11 03:00:26AM searchStage_trainer.py:247 [INFO] Valid: [ 22/49] Final Prec@1 80.2680%
08/11 03:00:26AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:00:26AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.2240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2562, 0.2483],
        [0.2488, 0.2462, 0.2566, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2417, 0.2592, 0.2485],
        [0.2503, 0.2423, 0.2600, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2420, 0.2669, 0.2402],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2497, 0.2498, 0.2486, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2496, 0.2498, 0.2491],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2487, 0.2518, 0.2468],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2454],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2511, 0.2508, 0.2473],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2515, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2498, 0.2542, 0.2472],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2493, 0.2532, 0.2484],
        [0.2489, 0.2488, 0.2519, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2532, 0.2490],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2487, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:01:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.3136 (0.4400)	Prec@(1,5) (84.7%, 99.4%)	
08/11 03:02:10AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.3759 (0.4259)	Prec@(1,5) (85.2%, 99.5%)	
08/11 03:03:02AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.6028 (0.4294)	Prec@(1,5) (85.0%, 99.4%)	
08/11 03:03:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.3905 (0.4336)	Prec@(1,5) (84.9%, 99.4%)	
08/11 03:04:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4238 (0.4344)	Prec@(1,5) (84.9%, 99.4%)	
08/11 03:05:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.5162 (0.4310)	Prec@(1,5) (85.0%, 99.4%)	
08/11 03:06:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.4821 (0.4310)	Prec@(1,5) (85.1%, 99.4%)	
08/11 03:07:06AM searchStage_trainer.py:198 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.6262 (0.4359)	Prec@(1,5) (84.9%, 99.3%)	
08/11 03:07:07AM searchStage_trainer.py:208 [INFO] Train: [ 23/49] Final Prec@1 84.8800%
08/11 03:07:15AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5543	Prec@(1,5) (81.4%, 98.9%)
08/11 03:07:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5618	Prec@(1,5) (81.0%, 98.8%)
08/11 03:07:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5614	Prec@(1,5) (81.2%, 98.9%)
08/11 03:07:41AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5577	Prec@(1,5) (81.4%, 98.9%)
08/11 03:07:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5596	Prec@(1,5) (81.2%, 99.0%)
08/11 03:07:58AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5626	Prec@(1,5) (81.1%, 98.9%)
08/11 03:08:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5610	Prec@(1,5) (81.1%, 98.9%)
08/11 03:08:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5575	Prec@(1,5) (81.2%, 98.9%)
08/11 03:08:14AM searchStage_trainer.py:247 [INFO] Valid: [ 23/49] Final Prec@1 81.2000%
08/11 03:08:14AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:08:14AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.2240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2562, 0.2483],
        [0.2487, 0.2462, 0.2566, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2417, 0.2593, 0.2485],
        [0.2502, 0.2423, 0.2601, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2418, 0.2672, 0.2401],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2486, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2496, 0.2498, 0.2491],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2518, 0.2468],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2454],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2511, 0.2508, 0.2472],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2516, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2498, 0.2543, 0.2471],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2492, 0.2532, 0.2484],
        [0.2489, 0.2488, 0.2519, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2532, 0.2490],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:09:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.4442 (0.4035)	Prec@(1,5) (86.4%, 99.3%)	
08/11 03:09:58AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.3037 (0.4025)	Prec@(1,5) (86.0%, 99.4%)	
08/11 03:10:50AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.3779 (0.4091)	Prec@(1,5) (85.7%, 99.5%)	
08/11 03:11:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.5589 (0.4115)	Prec@(1,5) (85.7%, 99.5%)	
08/11 03:12:33AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.4685 (0.4239)	Prec@(1,5) (85.1%, 99.5%)	
08/11 03:13:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.4620 (0.4240)	Prec@(1,5) (85.1%, 99.5%)	
08/11 03:14:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.3947 (0.4195)	Prec@(1,5) (85.3%, 99.5%)	
08/11 03:14:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.3545 (0.4178)	Prec@(1,5) (85.5%, 99.5%)	
08/11 03:14:56AM searchStage_trainer.py:208 [INFO] Train: [ 24/49] Final Prec@1 85.4400%
08/11 03:15:05AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.6064	Prec@(1,5) (81.0%, 98.7%)
08/11 03:15:13AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5685	Prec@(1,5) (81.7%, 98.9%)
08/11 03:15:22AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.5621	Prec@(1,5) (81.9%, 98.9%)
08/11 03:15:30AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5589	Prec@(1,5) (81.9%, 98.9%)
08/11 03:15:39AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5693	Prec@(1,5) (81.6%, 98.9%)
08/11 03:15:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5739	Prec@(1,5) (81.5%, 98.9%)
08/11 03:15:56AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5722	Prec@(1,5) (81.5%, 98.9%)
08/11 03:16:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5716	Prec@(1,5) (81.4%, 98.9%)
08/11 03:16:03AM searchStage_trainer.py:247 [INFO] Valid: [ 24/49] Final Prec@1 81.3680%
08/11 03:16:03AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:16:03AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.3680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2416, 0.2595, 0.2485],
        [0.2502, 0.2422, 0.2602, 0.2474],
        [0.2519, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2417, 0.2675, 0.2400],
        [0.2505, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2486, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2519, 0.2468],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2496, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2454],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2511, 0.2508, 0.2472],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2516, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2498, 0.2544, 0.2471],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2493, 0.2507, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2492, 0.2532, 0.2484],
        [0.2489, 0.2488, 0.2519, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2486, 0.2532, 0.2490],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:16:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.3959 (0.3983)	Prec@(1,5) (86.5%, 99.4%)	
08/11 03:17:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.4218 (0.3935)	Prec@(1,5) (86.5%, 99.4%)	
08/11 03:18:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.4101 (0.3913)	Prec@(1,5) (86.4%, 99.4%)	
08/11 03:19:30AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.3450 (0.3799)	Prec@(1,5) (86.7%, 99.5%)	
08/11 03:20:22AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.1918 (0.3835)	Prec@(1,5) (86.7%, 99.5%)	
08/11 03:21:14AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.4741 (0.3883)	Prec@(1,5) (86.6%, 99.4%)	
08/11 03:22:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.5405 (0.3928)	Prec@(1,5) (86.3%, 99.5%)	
08/11 03:22:43AM searchStage_trainer.py:198 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.5443 (0.3960)	Prec@(1,5) (86.2%, 99.5%)	
08/11 03:22:44AM searchStage_trainer.py:208 [INFO] Train: [ 25/49] Final Prec@1 86.1600%
08/11 03:22:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5197	Prec@(1,5) (82.8%, 99.1%)
08/11 03:23:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5212	Prec@(1,5) (82.2%, 99.1%)
08/11 03:23:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5236	Prec@(1,5) (82.4%, 99.2%)
08/11 03:23:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5237	Prec@(1,5) (82.4%, 99.2%)
08/11 03:23:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5356	Prec@(1,5) (82.0%, 99.1%)
08/11 03:23:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5372	Prec@(1,5) (81.9%, 99.1%)
08/11 03:23:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5410	Prec@(1,5) (81.8%, 99.1%)
08/11 03:23:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5432	Prec@(1,5) (81.8%, 99.1%)
08/11 03:23:51AM searchStage_trainer.py:247 [INFO] Valid: [ 25/49] Final Prec@1 81.8000%
08/11 03:23:51AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:23:52AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 81.8000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2462, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2416, 0.2596, 0.2484],
        [0.2501, 0.2421, 0.2604, 0.2474],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2415, 0.2679, 0.2400],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2486, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2519, 0.2468],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2497, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2495],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2509, 0.2471],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2516, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2498, 0.2545, 0.2471],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2493, 0.2508, 0.2498],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2492, 0.2533, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2486, 0.2533, 0.2490],
        [0.2496, 0.2487, 0.2525, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:24:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.5101 (0.3641)	Prec@(1,5) (87.2%, 99.5%)	
08/11 03:25:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.2686 (0.3678)	Prec@(1,5) (87.1%, 99.6%)	
08/11 03:26:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.3028 (0.3614)	Prec@(1,5) (87.3%, 99.6%)	
08/11 03:27:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.3717 (0.3700)	Prec@(1,5) (87.0%, 99.6%)	
08/11 03:28:10AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.4128 (0.3727)	Prec@(1,5) (86.9%, 99.6%)	
08/11 03:29:02AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.4679 (0.3733)	Prec@(1,5) (87.0%, 99.6%)	
08/11 03:29:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.2977 (0.3758)	Prec@(1,5) (86.9%, 99.6%)	
08/11 03:30:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.3882 (0.3804)	Prec@(1,5) (86.7%, 99.6%)	
08/11 03:30:33AM searchStage_trainer.py:208 [INFO] Train: [ 26/49] Final Prec@1 86.7280%
08/11 03:30:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5342	Prec@(1,5) (82.1%, 99.1%)
08/11 03:30:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5430	Prec@(1,5) (82.0%, 99.0%)
08/11 03:30:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5515	Prec@(1,5) (82.1%, 98.8%)
08/11 03:31:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5443	Prec@(1,5) (82.1%, 98.9%)
08/11 03:31:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5459	Prec@(1,5) (82.1%, 98.9%)
08/11 03:31:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5435	Prec@(1,5) (82.0%, 99.0%)
08/11 03:31:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5403	Prec@(1,5) (82.1%, 99.0%)
08/11 03:31:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5387	Prec@(1,5) (82.1%, 99.0%)
08/11 03:31:40AM searchStage_trainer.py:247 [INFO] Valid: [ 26/49] Final Prec@1 82.1040%
08/11 03:31:40AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:31:41AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.1040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2415, 0.2597, 0.2484],
        [0.2501, 0.2421, 0.2605, 0.2474],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2415, 0.2681, 0.2399],
        [0.2505, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2504, 0.2497, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2509, 0.2470],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2495, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2485],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2516, 0.2496],
        [0.2494, 0.2486, 0.2520, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2498, 0.2546, 0.2470],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2492, 0.2533, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2486, 0.2533, 0.2490],
        [0.2496, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:32:33AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.3639 (0.3622)	Prec@(1,5) (87.8%, 99.5%)	
08/11 03:33:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.3173 (0.3626)	Prec@(1,5) (87.6%, 99.5%)	
08/11 03:34:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.5086 (0.3614)	Prec@(1,5) (87.6%, 99.5%)	
08/11 03:35:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.5861 (0.3657)	Prec@(1,5) (87.6%, 99.6%)	
08/11 03:35:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.3001 (0.3618)	Prec@(1,5) (87.6%, 99.6%)	
08/11 03:36:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.3745 (0.3668)	Prec@(1,5) (87.4%, 99.6%)	
08/11 03:37:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.4982 (0.3686)	Prec@(1,5) (87.4%, 99.5%)	
08/11 03:38:21AM searchStage_trainer.py:198 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.2501 (0.3694)	Prec@(1,5) (87.4%, 99.5%)	
08/11 03:38:22AM searchStage_trainer.py:208 [INFO] Train: [ 27/49] Final Prec@1 87.3880%
08/11 03:38:31AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5399	Prec@(1,5) (82.4%, 98.8%)
08/11 03:38:39AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5100	Prec@(1,5) (83.3%, 99.0%)
08/11 03:38:48AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5176	Prec@(1,5) (82.9%, 99.0%)
08/11 03:38:56AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5183	Prec@(1,5) (82.9%, 99.0%)
08/11 03:39:05AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5163	Prec@(1,5) (82.9%, 99.1%)
08/11 03:39:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5191	Prec@(1,5) (82.7%, 99.0%)
08/11 03:39:22AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5199	Prec@(1,5) (82.7%, 99.0%)
08/11 03:39:29AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5183	Prec@(1,5) (82.8%, 99.0%)
08/11 03:39:29AM searchStage_trainer.py:247 [INFO] Valid: [ 27/49] Final Prec@1 82.8080%
08/11 03:39:29AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:39:29AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2415, 0.2597, 0.2484],
        [0.2500, 0.2420, 0.2606, 0.2474],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2414, 0.2684, 0.2398],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2489, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2509, 0.2470],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2516, 0.2496],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2498, 0.2547, 0.2470],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2492, 0.2533, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2486, 0.2533, 0.2490],
        [0.2496, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:40:22AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.2636 (0.3154)	Prec@(1,5) (89.2%, 99.7%)	
08/11 03:41:14AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.1842 (0.3220)	Prec@(1,5) (88.6%, 99.7%)	
08/11 03:42:05AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3219 (0.3329)	Prec@(1,5) (88.2%, 99.7%)	
08/11 03:42:57AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.3563 (0.3373)	Prec@(1,5) (87.9%, 99.7%)	
08/11 03:43:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.3307 (0.3428)	Prec@(1,5) (87.7%, 99.7%)	
08/11 03:44:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.2367 (0.3422)	Prec@(1,5) (87.7%, 99.7%)	
08/11 03:45:30AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.4630 (0.3454)	Prec@(1,5) (87.7%, 99.7%)	
08/11 03:46:10AM searchStage_trainer.py:198 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.4749 (0.3486)	Prec@(1,5) (87.6%, 99.7%)	
08/11 03:46:10AM searchStage_trainer.py:208 [INFO] Train: [ 28/49] Final Prec@1 87.6080%
08/11 03:46:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.5494	Prec@(1,5) (81.9%, 99.1%)
08/11 03:46:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.5673	Prec@(1,5) (81.4%, 99.1%)
08/11 03:46:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5659	Prec@(1,5) (81.3%, 98.9%)
08/11 03:46:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5678	Prec@(1,5) (81.2%, 99.0%)
08/11 03:46:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5642	Prec@(1,5) (81.3%, 99.0%)
08/11 03:47:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5691	Prec@(1,5) (81.3%, 98.9%)
08/11 03:47:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5665	Prec@(1,5) (81.3%, 98.9%)
08/11 03:47:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5641	Prec@(1,5) (81.4%, 98.9%)
08/11 03:47:18AM searchStage_trainer.py:247 [INFO] Valid: [ 28/49] Final Prec@1 81.4080%
08/11 03:47:18AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:47:18AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2415, 0.2598, 0.2484],
        [0.2500, 0.2420, 0.2606, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2413, 0.2687, 0.2398],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2506, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2513, 0.2509, 0.2469],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2479],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2516, 0.2496],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2498, 0.2548, 0.2470],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2492, 0.2534, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2486, 0.2534, 0.2490],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:48:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.2790 (0.3398)	Prec@(1,5) (88.5%, 99.5%)	
08/11 03:49:02AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.5798 (0.3404)	Prec@(1,5) (88.0%, 99.7%)	
08/11 03:49:54AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.3075 (0.3419)	Prec@(1,5) (87.9%, 99.7%)	
08/11 03:50:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.5314 (0.3334)	Prec@(1,5) (88.4%, 99.7%)	
08/11 03:51:37AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.5853 (0.3314)	Prec@(1,5) (88.4%, 99.7%)	
08/11 03:52:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2433 (0.3351)	Prec@(1,5) (88.3%, 99.6%)	
08/11 03:53:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.3142 (0.3336)	Prec@(1,5) (88.3%, 99.7%)	
08/11 03:53:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.3461 (0.3358)	Prec@(1,5) (88.2%, 99.7%)	
08/11 03:53:59AM searchStage_trainer.py:208 [INFO] Train: [ 29/49] Final Prec@1 88.2200%
08/11 03:54:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5066	Prec@(1,5) (82.9%, 99.0%)
08/11 03:54:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5126	Prec@(1,5) (82.8%, 99.1%)
08/11 03:54:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5126	Prec@(1,5) (82.6%, 99.1%)
08/11 03:54:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5055	Prec@(1,5) (83.0%, 99.2%)
08/11 03:54:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5098	Prec@(1,5) (82.9%, 99.1%)
08/11 03:54:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5143	Prec@(1,5) (82.8%, 99.1%)
08/11 03:55:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5138	Prec@(1,5) (82.7%, 99.1%)
08/11 03:55:06AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5127	Prec@(1,5) (82.8%, 99.1%)
08/11 03:55:07AM searchStage_trainer.py:247 [INFO] Valid: [ 29/49] Final Prec@1 82.7760%
08/11 03:55:07AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 03:55:07AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2414, 0.2599, 0.2484],
        [0.2500, 0.2420, 0.2607, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2412, 0.2689, 0.2397],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2513, 0.2510, 0.2468],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2516, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2498, 0.2549, 0.2470],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2492, 0.2534, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2486, 0.2534, 0.2490],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 03:56:00AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.3684 (0.2934)	Prec@(1,5) (89.8%, 99.7%)	
08/11 03:56:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.4671 (0.2925)	Prec@(1,5) (90.0%, 99.8%)	
08/11 03:57:43AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.3514 (0.3062)	Prec@(1,5) (89.5%, 99.7%)	
08/11 03:58:34AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.1601 (0.3080)	Prec@(1,5) (89.4%, 99.7%)	
08/11 03:59:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.4014 (0.3080)	Prec@(1,5) (89.4%, 99.7%)	
08/11 04:00:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.3900 (0.3103)	Prec@(1,5) (89.3%, 99.7%)	
08/11 04:01:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2607 (0.3102)	Prec@(1,5) (89.2%, 99.7%)	
08/11 04:01:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.3940 (0.3131)	Prec@(1,5) (89.2%, 99.7%)	
08/11 04:01:48AM searchStage_trainer.py:208 [INFO] Train: [ 30/49] Final Prec@1 89.1720%
08/11 04:01:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5252	Prec@(1,5) (83.1%, 98.8%)
08/11 04:02:05AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5197	Prec@(1,5) (82.9%, 99.0%)
08/11 04:02:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5272	Prec@(1,5) (82.7%, 99.0%)
08/11 04:02:23AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5332	Prec@(1,5) (82.5%, 99.1%)
08/11 04:02:31AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5296	Prec@(1,5) (82.7%, 99.1%)
08/11 04:02:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5316	Prec@(1,5) (82.7%, 99.1%)
08/11 04:02:48AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5346	Prec@(1,5) (82.7%, 99.1%)
08/11 04:02:55AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5361	Prec@(1,5) (82.6%, 99.1%)
08/11 04:02:55AM searchStage_trainer.py:247 [INFO] Valid: [ 30/49] Final Prec@1 82.5760%
08/11 04:02:55AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:02:56AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 82.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2414, 0.2599, 0.2484],
        [0.2500, 0.2419, 0.2608, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2411, 0.2692, 0.2397],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2453],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2513, 0.2510, 0.2468],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2498, 0.2549, 0.2469],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2492, 0.2534, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2485, 0.2534, 0.2490],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:03:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.3292 (0.2729)	Prec@(1,5) (90.4%, 99.8%)	
08/11 04:04:40AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.2673 (0.2779)	Prec@(1,5) (90.2%, 99.8%)	
08/11 04:05:31AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2916 (0.2823)	Prec@(1,5) (89.9%, 99.8%)	
08/11 04:06:23AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2277 (0.2831)	Prec@(1,5) (90.0%, 99.7%)	
08/11 04:07:14AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.3316 (0.2889)	Prec@(1,5) (89.8%, 99.7%)	
08/11 04:08:06AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.2191 (0.2915)	Prec@(1,5) (89.8%, 99.7%)	
08/11 04:08:57AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.3868 (0.2971)	Prec@(1,5) (89.6%, 99.7%)	
08/11 04:09:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.1894 (0.2967)	Prec@(1,5) (89.6%, 99.7%)	
08/11 04:09:37AM searchStage_trainer.py:208 [INFO] Train: [ 31/49] Final Prec@1 89.5800%
08/11 04:09:46AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5451	Prec@(1,5) (81.9%, 98.8%)
08/11 04:09:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5425	Prec@(1,5) (82.4%, 99.0%)
08/11 04:10:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5289	Prec@(1,5) (82.7%, 99.1%)
08/11 04:10:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5284	Prec@(1,5) (82.9%, 99.1%)
08/11 04:10:20AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5246	Prec@(1,5) (83.1%, 99.1%)
08/11 04:10:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5288	Prec@(1,5) (83.0%, 99.1%)
08/11 04:10:37AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5235	Prec@(1,5) (83.2%, 99.2%)
08/11 04:10:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5255	Prec@(1,5) (83.1%, 99.2%)
08/11 04:10:44AM searchStage_trainer.py:247 [INFO] Valid: [ 31/49] Final Prec@1 83.0960%
08/11 04:10:44AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:10:44AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 83.0960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2462, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2414, 0.2600, 0.2484],
        [0.2499, 0.2419, 0.2608, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2410, 0.2694, 0.2396],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2519, 0.2467],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2513, 0.2510, 0.2467],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2497, 0.2550, 0.2469],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2492, 0.2535, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2485, 0.2534, 0.2490],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:11:37AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.2943 (0.2666)	Prec@(1,5) (90.7%, 99.8%)	
08/11 04:12:29AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.1841 (0.2739)	Prec@(1,5) (90.4%, 99.8%)	
08/11 04:13:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.2640 (0.2702)	Prec@(1,5) (90.4%, 99.8%)	
08/11 04:14:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.2947 (0.2760)	Prec@(1,5) (90.3%, 99.8%)	
08/11 04:15:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.2219 (0.2772)	Prec@(1,5) (90.2%, 99.8%)	
08/11 04:15:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.2838 (0.2807)	Prec@(1,5) (90.2%, 99.8%)	
08/11 04:16:46AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.2784 (0.2819)	Prec@(1,5) (90.1%, 99.8%)	
08/11 04:17:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.2701 (0.2834)	Prec@(1,5) (90.1%, 99.8%)	
08/11 04:17:26AM searchStage_trainer.py:208 [INFO] Train: [ 32/49] Final Prec@1 90.1040%
08/11 04:17:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.4681	Prec@(1,5) (84.5%, 99.4%)
08/11 04:17:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.4702	Prec@(1,5) (84.8%, 99.3%)
08/11 04:17:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.4757	Prec@(1,5) (84.9%, 99.3%)
08/11 04:18:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.4715	Prec@(1,5) (85.0%, 99.2%)
08/11 04:18:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.4728	Prec@(1,5) (85.0%, 99.2%)
08/11 04:18:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.4744	Prec@(1,5) (85.0%, 99.2%)
08/11 04:18:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.4747	Prec@(1,5) (84.9%, 99.2%)
08/11 04:18:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.4750	Prec@(1,5) (84.8%, 99.3%)
08/11 04:18:33AM searchStage_trainer.py:247 [INFO] Valid: [ 32/49] Final Prec@1 84.8080%
08/11 04:18:33AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:18:33AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2461, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2413, 0.2601, 0.2484],
        [0.2499, 0.2419, 0.2609, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2409, 0.2696, 0.2396],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2496, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2505, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2514, 0.2510, 0.2467],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2489],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2512, 0.2484],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2497, 0.2551, 0.2469],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2491, 0.2535, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2485, 0.2535, 0.2491],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:19:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.2523 (0.2362)	Prec@(1,5) (91.8%, 99.8%)	
08/11 04:20:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.1639 (0.2511)	Prec@(1,5) (91.4%, 99.8%)	
08/11 04:21:10AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.2850 (0.2484)	Prec@(1,5) (91.5%, 99.8%)	
08/11 04:22:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.1424 (0.2524)	Prec@(1,5) (91.4%, 99.7%)	
08/11 04:22:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.2072 (0.2537)	Prec@(1,5) (91.4%, 99.8%)	
08/11 04:23:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.2811 (0.2586)	Prec@(1,5) (91.1%, 99.8%)	
08/11 04:24:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.2066 (0.2561)	Prec@(1,5) (91.2%, 99.8%)	
08/11 04:25:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.2107 (0.2606)	Prec@(1,5) (91.1%, 99.8%)	
08/11 04:25:16AM searchStage_trainer.py:208 [INFO] Train: [ 33/49] Final Prec@1 91.0720%
08/11 04:25:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5053	Prec@(1,5) (83.5%, 99.1%)
08/11 04:25:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.4918	Prec@(1,5) (83.5%, 99.1%)
08/11 04:25:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.4905	Prec@(1,5) (83.8%, 99.1%)
08/11 04:25:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5061	Prec@(1,5) (83.5%, 99.1%)
08/11 04:26:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5048	Prec@(1,5) (83.5%, 99.1%)
08/11 04:26:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5039	Prec@(1,5) (83.5%, 99.1%)
08/11 04:26:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5043	Prec@(1,5) (83.5%, 99.1%)
08/11 04:26:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5026	Prec@(1,5) (83.5%, 99.1%)
08/11 04:26:24AM searchStage_trainer.py:247 [INFO] Valid: [ 33/49] Final Prec@1 83.4720%
08/11 04:26:24AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:26:24AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2461, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2413, 0.2601, 0.2484],
        [0.2499, 0.2418, 0.2610, 0.2473],
        [0.2518, 0.2513, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2408, 0.2698, 0.2395],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2501, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2492],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2514, 0.2510, 0.2466],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2513, 0.2483],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2498, 0.2552, 0.2468],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2491, 0.2535, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2485, 0.2535, 0.2491],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:27:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1625 (0.2278)	Prec@(1,5) (92.2%, 99.9%)	
08/11 04:28:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.3362 (0.2352)	Prec@(1,5) (91.7%, 99.8%)	
08/11 04:29:00AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.2651 (0.2375)	Prec@(1,5) (91.6%, 99.8%)	
08/11 04:29:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.1675 (0.2390)	Prec@(1,5) (91.5%, 99.8%)	
08/11 04:30:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.2178 (0.2456)	Prec@(1,5) (91.3%, 99.8%)	
08/11 04:31:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.1756 (0.2445)	Prec@(1,5) (91.3%, 99.8%)	
08/11 04:32:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.3017 (0.2436)	Prec@(1,5) (91.3%, 99.8%)	
08/11 04:33:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.1791 (0.2451)	Prec@(1,5) (91.2%, 99.8%)	
08/11 04:33:07AM searchStage_trainer.py:208 [INFO] Train: [ 34/49] Final Prec@1 91.2120%
08/11 04:33:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.5380	Prec@(1,5) (82.9%, 99.2%)
08/11 04:33:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5329	Prec@(1,5) (83.1%, 99.1%)
08/11 04:33:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5136	Prec@(1,5) (84.0%, 99.2%)
08/11 04:33:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5258	Prec@(1,5) (83.6%, 99.1%)
08/11 04:33:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5233	Prec@(1,5) (83.7%, 99.1%)
08/11 04:33:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5234	Prec@(1,5) (83.7%, 99.1%)
08/11 04:34:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5305	Prec@(1,5) (83.5%, 99.1%)
08/11 04:34:14AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5322	Prec@(1,5) (83.5%, 99.1%)
08/11 04:34:15AM searchStage_trainer.py:247 [INFO] Valid: [ 34/49] Final Prec@1 83.5160%
08/11 04:34:15AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:34:15AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2563, 0.2483],
        [0.2487, 0.2461, 0.2567, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2413, 0.2602, 0.2484],
        [0.2499, 0.2418, 0.2610, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2408, 0.2699, 0.2395],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2506, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2514, 0.2510, 0.2466],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2498, 0.2553, 0.2468],
        [0.2493, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2499, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2491, 0.2536, 0.2484],
        [0.2488, 0.2488, 0.2520, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2485, 0.2535, 0.2491],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:35:08AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.2455 (0.2215)	Prec@(1,5) (92.2%, 99.9%)	
08/11 04:36:00AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.1897 (0.2206)	Prec@(1,5) (92.4%, 99.9%)	
08/11 04:36:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.4249 (0.2169)	Prec@(1,5) (92.5%, 99.8%)	
08/11 04:37:43AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.2372 (0.2261)	Prec@(1,5) (92.1%, 99.8%)	
08/11 04:38:35AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.2461 (0.2297)	Prec@(1,5) (92.0%, 99.8%)	
08/11 04:39:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.4120 (0.2317)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:40:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.2039 (0.2318)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:40:58AM searchStage_trainer.py:198 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.2335 (0.2310)	Prec@(1,5) (91.9%, 99.8%)	
08/11 04:40:59AM searchStage_trainer.py:208 [INFO] Train: [ 35/49] Final Prec@1 91.9480%
08/11 04:41:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.4743	Prec@(1,5) (84.9%, 99.3%)
08/11 04:41:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.4948	Prec@(1,5) (84.6%, 99.4%)
08/11 04:41:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.4956	Prec@(1,5) (84.5%, 99.3%)
08/11 04:41:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.4942	Prec@(1,5) (84.6%, 99.3%)
08/11 04:41:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.4951	Prec@(1,5) (84.7%, 99.3%)
08/11 04:41:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.4988	Prec@(1,5) (84.5%, 99.3%)
08/11 04:41:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.4963	Prec@(1,5) (84.4%, 99.3%)
08/11 04:42:06AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.4929	Prec@(1,5) (84.5%, 99.3%)
08/11 04:42:06AM searchStage_trainer.py:247 [INFO] Valid: [ 35/49] Final Prec@1 84.5440%
08/11 04:42:06AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:42:06AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2487, 0.2461, 0.2568, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2413, 0.2602, 0.2484],
        [0.2498, 0.2418, 0.2611, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2407, 0.2701, 0.2394],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2517],
        [0.2494, 0.2495, 0.2493, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2514, 0.2511, 0.2465],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2506, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2503, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2521, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2498, 0.2554, 0.2467],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2491, 0.2536, 0.2484],
        [0.2488, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2485, 0.2535, 0.2491],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:42:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.2628 (0.2051)	Prec@(1,5) (93.0%, 99.9%)	
08/11 04:43:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.1576 (0.2086)	Prec@(1,5) (93.0%, 99.9%)	
08/11 04:44:43AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1046 (0.2093)	Prec@(1,5) (92.9%, 99.9%)	
08/11 04:45:35AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.0916 (0.2071)	Prec@(1,5) (92.8%, 99.9%)	
08/11 04:46:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.1430 (0.2092)	Prec@(1,5) (92.7%, 99.9%)	
08/11 04:47:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.1698 (0.2101)	Prec@(1,5) (92.7%, 99.9%)	
08/11 04:48:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.2627 (0.2125)	Prec@(1,5) (92.6%, 99.9%)	
08/11 04:48:49AM searchStage_trainer.py:198 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.1347 (0.2156)	Prec@(1,5) (92.5%, 99.9%)	
08/11 04:48:50AM searchStage_trainer.py:208 [INFO] Train: [ 36/49] Final Prec@1 92.5040%
08/11 04:48:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.4977	Prec@(1,5) (84.4%, 99.3%)
08/11 04:49:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.4918	Prec@(1,5) (84.7%, 99.3%)
08/11 04:49:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.4942	Prec@(1,5) (84.9%, 99.2%)
08/11 04:49:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5039	Prec@(1,5) (84.5%, 99.2%)
08/11 04:49:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5004	Prec@(1,5) (84.6%, 99.2%)
08/11 04:49:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5039	Prec@(1,5) (84.5%, 99.2%)
08/11 04:49:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5068	Prec@(1,5) (84.4%, 99.2%)
08/11 04:49:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5073	Prec@(1,5) (84.4%, 99.2%)
08/11 04:49:57AM searchStage_trainer.py:247 [INFO] Valid: [ 36/49] Final Prec@1 84.4000%
08/11 04:49:57AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:49:58AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2487, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2602, 0.2484],
        [0.2498, 0.2417, 0.2611, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2407, 0.2703, 0.2394],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2495, 0.2493, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2515, 0.2511, 0.2465],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2498, 0.2555, 0.2467],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2497, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2491, 0.2536, 0.2484],
        [0.2488, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2485, 0.2536, 0.2491],
        [0.2495, 0.2487, 0.2526, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:50:50AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.1888 (0.1896)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:51:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.1040 (0.1963)	Prec@(1,5) (93.1%, 99.9%)	
08/11 04:52:34AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.3068 (0.1968)	Prec@(1,5) (93.1%, 99.9%)	
08/11 04:53:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.2610 (0.1950)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:54:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.1025 (0.1923)	Prec@(1,5) (93.3%, 99.9%)	
08/11 04:55:10AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.4006 (0.1940)	Prec@(1,5) (93.3%, 99.9%)	
08/11 04:56:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.1356 (0.1955)	Prec@(1,5) (93.2%, 99.9%)	
08/11 04:56:41AM searchStage_trainer.py:198 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.2132 (0.1986)	Prec@(1,5) (93.0%, 99.9%)	
08/11 04:56:41AM searchStage_trainer.py:208 [INFO] Train: [ 37/49] Final Prec@1 93.0320%
08/11 04:56:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.4851	Prec@(1,5) (84.5%, 99.2%)
08/11 04:56:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.4966	Prec@(1,5) (84.5%, 99.3%)
08/11 04:57:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.4983	Prec@(1,5) (84.5%, 99.3%)
08/11 04:57:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.4940	Prec@(1,5) (84.6%, 99.3%)
08/11 04:57:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.4955	Prec@(1,5) (84.6%, 99.2%)
08/11 04:57:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.4975	Prec@(1,5) (84.5%, 99.3%)
08/11 04:57:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.4995	Prec@(1,5) (84.6%, 99.2%)
08/11 04:57:49AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.4941	Prec@(1,5) (84.8%, 99.3%)
08/11 04:57:49AM searchStage_trainer.py:247 [INFO] Valid: [ 37/49] Final Prec@1 84.7560%
08/11 04:57:49AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 04:57:49AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 84.8080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2487, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2603, 0.2484],
        [0.2498, 0.2417, 0.2611, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2406, 0.2704, 0.2393],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2519, 0.2466],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2495, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2515, 0.2511, 0.2464],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2500, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2498, 0.2556, 0.2466],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2491, 0.2536, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2485, 0.2536, 0.2490],
        [0.2495, 0.2487, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 04:58:42AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.2167 (0.1778)	Prec@(1,5) (93.7%, 99.9%)	
08/11 04:59:34AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.2041 (0.1723)	Prec@(1,5) (93.8%, 99.9%)	
08/11 05:00:26AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.3210 (0.1735)	Prec@(1,5) (93.9%, 99.9%)	
08/11 05:01:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.2107 (0.1761)	Prec@(1,5) (93.9%, 99.9%)	
08/11 05:02:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.1791 (0.1793)	Prec@(1,5) (93.8%, 99.9%)	
08/11 05:03:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.2123 (0.1787)	Prec@(1,5) (93.8%, 99.9%)	
08/11 05:03:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.1774 (0.1829)	Prec@(1,5) (93.7%, 99.9%)	
08/11 05:04:32AM searchStage_trainer.py:198 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.3674 (0.1828)	Prec@(1,5) (93.6%, 99.9%)	
08/11 05:04:33AM searchStage_trainer.py:208 [INFO] Train: [ 38/49] Final Prec@1 93.6440%
08/11 05:04:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.4891	Prec@(1,5) (85.5%, 99.2%)
08/11 05:04:50AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.4946	Prec@(1,5) (85.5%, 99.3%)
08/11 05:04:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5038	Prec@(1,5) (85.1%, 99.3%)
08/11 05:05:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5047	Prec@(1,5) (85.1%, 99.3%)
08/11 05:05:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5057	Prec@(1,5) (85.2%, 99.3%)
08/11 05:05:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5053	Prec@(1,5) (85.3%, 99.2%)
08/11 05:05:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5019	Prec@(1,5) (85.3%, 99.2%)
08/11 05:05:40AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.4995	Prec@(1,5) (85.4%, 99.2%)
08/11 05:05:40AM searchStage_trainer.py:247 [INFO] Valid: [ 38/49] Final Prec@1 85.3480%
08/11 05:05:40AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:05:41AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.3480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2487, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2603, 0.2484],
        [0.2498, 0.2417, 0.2612, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2406, 0.2706, 0.2393],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2495, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2515, 0.2511, 0.2464],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2506, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2517, 0.2495],
        [0.2493, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2498, 0.2557, 0.2466],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2491, 0.2537, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2485, 0.2536, 0.2490],
        [0.2495, 0.2487, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:06:34AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.1613 (0.1692)	Prec@(1,5) (94.2%, 99.9%)	
08/11 05:07:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.1722 (0.1657)	Prec@(1,5) (94.3%, 99.9%)	
08/11 05:08:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.3098 (0.1625)	Prec@(1,5) (94.5%, 99.9%)	
08/11 05:09:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0887 (0.1579)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:10:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.2327 (0.1577)	Prec@(1,5) (94.5%, 99.9%)	
08/11 05:10:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.1955 (0.1599)	Prec@(1,5) (94.5%, 99.9%)	
08/11 05:11:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.1245 (0.1617)	Prec@(1,5) (94.5%, 99.9%)	
08/11 05:12:24AM searchStage_trainer.py:198 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.1046 (0.1624)	Prec@(1,5) (94.4%, 99.9%)	
08/11 05:12:25AM searchStage_trainer.py:208 [INFO] Train: [ 39/49] Final Prec@1 94.3760%
08/11 05:12:33AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.5065	Prec@(1,5) (85.2%, 99.3%)
08/11 05:12:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.5015	Prec@(1,5) (85.3%, 99.3%)
08/11 05:12:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5092	Prec@(1,5) (85.2%, 99.2%)
08/11 05:12:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5093	Prec@(1,5) (85.1%, 99.2%)
08/11 05:13:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5082	Prec@(1,5) (85.2%, 99.2%)
08/11 05:13:16AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5041	Prec@(1,5) (85.4%, 99.3%)
08/11 05:13:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5022	Prec@(1,5) (85.4%, 99.3%)
08/11 05:13:32AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.4947	Prec@(1,5) (85.5%, 99.3%)
08/11 05:13:32AM searchStage_trainer.py:247 [INFO] Valid: [ 39/49] Final Prec@1 85.4640%
08/11 05:13:32AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:13:32AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.4640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2604, 0.2483],
        [0.2498, 0.2417, 0.2612, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2405, 0.2708, 0.2393],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2495, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2515, 0.2512, 0.2464],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2493, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2498, 0.2558, 0.2465],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2491, 0.2537, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2484, 0.2536, 0.2490],
        [0.2495, 0.2487, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:14:25AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.2825 (0.1551)	Prec@(1,5) (94.3%, 99.9%)	
08/11 05:15:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.2071 (0.1537)	Prec@(1,5) (94.5%, 100.0%)	
08/11 05:16:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.1425 (0.1517)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:17:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.1192 (0.1497)	Prec@(1,5) (94.7%, 99.9%)	
08/11 05:17:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.1239 (0.1521)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:18:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.1480 (0.1541)	Prec@(1,5) (94.5%, 99.9%)	
08/11 05:19:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.1031 (0.1536)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:20:16AM searchStage_trainer.py:198 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0841 (0.1537)	Prec@(1,5) (94.6%, 99.9%)	
08/11 05:20:16AM searchStage_trainer.py:208 [INFO] Train: [ 40/49] Final Prec@1 94.6120%
08/11 05:20:25AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.4651	Prec@(1,5) (86.5%, 99.2%)
08/11 05:20:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.4710	Prec@(1,5) (86.5%, 99.2%)
08/11 05:20:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.4752	Prec@(1,5) (86.4%, 99.2%)
08/11 05:20:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.4952	Prec@(1,5) (85.9%, 99.2%)
08/11 05:21:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.4930	Prec@(1,5) (85.8%, 99.2%)
08/11 05:21:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.4941	Prec@(1,5) (85.7%, 99.2%)
08/11 05:21:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.4950	Prec@(1,5) (85.7%, 99.2%)
08/11 05:21:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.4980	Prec@(1,5) (85.6%, 99.2%)
08/11 05:21:24AM searchStage_trainer.py:247 [INFO] Valid: [ 40/49] Final Prec@1 85.6160%
08/11 05:21:24AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:21:24AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.6160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2604, 0.2483],
        [0.2497, 0.2417, 0.2613, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2404, 0.2710, 0.2392],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2498, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2535, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2495, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2512, 0.2463],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2483],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2493, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2559, 0.2464],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2508, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2491, 0.2537, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2484, 0.2537, 0.2490],
        [0.2495, 0.2487, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:22:17AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.1996 (0.1253)	Prec@(1,5) (95.8%, 100.0%)	
08/11 05:23:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.1229 (0.1291)	Prec@(1,5) (95.6%, 100.0%)	
08/11 05:24:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.1872 (0.1308)	Prec@(1,5) (95.5%, 100.0%)	
08/11 05:24:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.1562 (0.1360)	Prec@(1,5) (95.2%, 100.0%)	
08/11 05:25:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0622 (0.1393)	Prec@(1,5) (95.1%, 100.0%)	
08/11 05:26:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.1861 (0.1390)	Prec@(1,5) (95.1%, 100.0%)	
08/11 05:27:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.1364 (0.1370)	Prec@(1,5) (95.2%, 100.0%)	
08/11 05:28:07AM searchStage_trainer.py:198 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.1056 (0.1387)	Prec@(1,5) (95.1%, 99.9%)	
08/11 05:28:08AM searchStage_trainer.py:208 [INFO] Train: [ 41/49] Final Prec@1 95.1280%
08/11 05:28:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.4942	Prec@(1,5) (85.8%, 99.4%)
08/11 05:28:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5009	Prec@(1,5) (85.6%, 99.4%)
08/11 05:28:34AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5120	Prec@(1,5) (85.3%, 99.4%)
08/11 05:28:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5185	Prec@(1,5) (85.2%, 99.3%)
08/11 05:28:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5088	Prec@(1,5) (85.4%, 99.3%)
08/11 05:29:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5184	Prec@(1,5) (85.3%, 99.3%)
08/11 05:29:08AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5153	Prec@(1,5) (85.4%, 99.3%)
08/11 05:29:15AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5193	Prec@(1,5) (85.3%, 99.3%)
08/11 05:29:15AM searchStage_trainer.py:247 [INFO] Valid: [ 41/49] Final Prec@1 85.3000%
08/11 05:29:16AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:29:16AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.6160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2412, 0.2604, 0.2483],
        [0.2497, 0.2416, 0.2613, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2404, 0.2711, 0.2392],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2535, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2494, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2512, 0.2463],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2522, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2483],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2560, 0.2464],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2491, 0.2538, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2537, 0.2490],
        [0.2495, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:30:09AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.0648 (0.1146)	Prec@(1,5) (96.1%, 99.9%)	
08/11 05:31:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.1740 (0.1136)	Prec@(1,5) (96.1%, 100.0%)	
08/11 05:31:53AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.1768 (0.1128)	Prec@(1,5) (96.1%, 99.9%)	
08/11 05:32:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0672 (0.1162)	Prec@(1,5) (95.9%, 99.9%)	
08/11 05:33:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.1412 (0.1204)	Prec@(1,5) (95.8%, 99.9%)	
08/11 05:34:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.2462 (0.1249)	Prec@(1,5) (95.7%, 99.9%)	
08/11 05:35:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.1709 (0.1245)	Prec@(1,5) (95.7%, 99.9%)	
08/11 05:35:59AM searchStage_trainer.py:198 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.3369 (0.1269)	Prec@(1,5) (95.6%, 99.9%)	
08/11 05:36:00AM searchStage_trainer.py:208 [INFO] Train: [ 42/49] Final Prec@1 95.5800%
08/11 05:36:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5434	Prec@(1,5) (84.9%, 99.1%)
08/11 05:36:17AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5391	Prec@(1,5) (85.2%, 99.2%)
08/11 05:36:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5535	Prec@(1,5) (85.0%, 99.2%)
08/11 05:36:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5384	Prec@(1,5) (85.3%, 99.2%)
08/11 05:36:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5451	Prec@(1,5) (85.3%, 99.2%)
08/11 05:36:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5420	Prec@(1,5) (85.3%, 99.2%)
08/11 05:37:00AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5329	Prec@(1,5) (85.4%, 99.2%)
08/11 05:37:07AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5350	Prec@(1,5) (85.4%, 99.2%)
08/11 05:37:07AM searchStage_trainer.py:247 [INFO] Valid: [ 42/49] Final Prec@1 85.3840%
08/11 05:37:07AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:37:08AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.6160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2412, 0.2605, 0.2483],
        [0.2497, 0.2416, 0.2613, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2403, 0.2712, 0.2391],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2499, 0.2535, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2512, 0.2462],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2478],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2499, 0.2561, 0.2463],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2491, 0.2538, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2537, 0.2490],
        [0.2495, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:38:01AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0502 (0.1080)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:38:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.1611 (0.1084)	Prec@(1,5) (96.3%, 99.9%)	
08/11 05:39:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.1433 (0.1043)	Prec@(1,5) (96.4%, 99.9%)	
08/11 05:40:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0781 (0.1104)	Prec@(1,5) (96.2%, 100.0%)	
08/11 05:41:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0868 (0.1086)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:42:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.1288 (0.1107)	Prec@(1,5) (96.3%, 99.9%)	
08/11 05:43:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0394 (0.1118)	Prec@(1,5) (96.2%, 99.9%)	
08/11 05:43:51AM searchStage_trainer.py:198 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0984 (0.1117)	Prec@(1,5) (96.2%, 100.0%)	
08/11 05:43:52AM searchStage_trainer.py:208 [INFO] Train: [ 43/49] Final Prec@1 96.1760%
08/11 05:44:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5072	Prec@(1,5) (86.1%, 99.5%)
08/11 05:44:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5330	Prec@(1,5) (85.7%, 99.3%)
08/11 05:44:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5308	Prec@(1,5) (85.6%, 99.3%)
08/11 05:44:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.5195	Prec@(1,5) (85.9%, 99.3%)
08/11 05:44:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5176	Prec@(1,5) (85.9%, 99.3%)
08/11 05:44:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5194	Prec@(1,5) (86.0%, 99.3%)
08/11 05:44:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5135	Prec@(1,5) (86.0%, 99.3%)
08/11 05:44:59AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5220	Prec@(1,5) (85.8%, 99.3%)
08/11 05:44:59AM searchStage_trainer.py:247 [INFO] Valid: [ 43/49] Final Prec@1 85.7640%
08/11 05:44:59AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:44:59AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.7640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2412, 0.2605, 0.2483],
        [0.2497, 0.2416, 0.2614, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2403, 0.2714, 0.2390],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2498, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2499, 0.2534, 0.2452],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2497, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2513, 0.2462],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2487],
        [0.2494, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2499, 0.2562, 0.2463],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2498],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2491, 0.2538, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2537, 0.2491],
        [0.2495, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:45:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0841 (0.0979)	Prec@(1,5) (96.6%, 100.0%)	
08/11 05:46:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.1633 (0.0983)	Prec@(1,5) (96.6%, 100.0%)	
08/11 05:47:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.1117 (0.1039)	Prec@(1,5) (96.4%, 99.9%)	
08/11 05:48:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.2305 (0.1059)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:49:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.1291 (0.1038)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:50:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.1213 (0.1035)	Prec@(1,5) (96.4%, 100.0%)	
08/11 05:51:03AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.1358 (0.1040)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:51:43AM searchStage_trainer.py:198 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.1407 (0.1048)	Prec@(1,5) (96.3%, 100.0%)	
08/11 05:51:44AM searchStage_trainer.py:208 [INFO] Train: [ 44/49] Final Prec@1 96.2880%
08/11 05:51:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5239	Prec@(1,5) (86.0%, 99.6%)
08/11 05:52:01AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5383	Prec@(1,5) (85.8%, 99.4%)
08/11 05:52:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5280	Prec@(1,5) (85.8%, 99.4%)
08/11 05:52:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5307	Prec@(1,5) (85.8%, 99.4%)
08/11 05:52:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5333	Prec@(1,5) (85.7%, 99.4%)
08/11 05:52:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5413	Prec@(1,5) (85.6%, 99.4%)
08/11 05:52:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5429	Prec@(1,5) (85.5%, 99.3%)
08/11 05:52:51AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5443	Prec@(1,5) (85.4%, 99.3%)
08/11 05:52:51AM searchStage_trainer.py:247 [INFO] Valid: [ 44/49] Final Prec@1 85.3800%
08/11 05:52:51AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 05:52:51AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.7640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2411, 0.2605, 0.2483],
        [0.2497, 0.2416, 0.2614, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2402, 0.2715, 0.2390],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2499, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2499, 0.2535, 0.2451],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2499, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2513, 0.2461],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2487],
        [0.2493, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2499, 0.2562, 0.2462],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2498],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2491, 0.2538, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2537, 0.2490],
        [0.2495, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 05:53:44AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0436 (0.0910)	Prec@(1,5) (96.8%, 100.0%)	
08/11 05:54:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0541 (0.0903)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:55:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0663 (0.0911)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:56:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.1423 (0.0927)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:57:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.1169 (0.0928)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:58:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0473 (0.0965)	Prec@(1,5) (96.8%, 100.0%)	
08/11 05:58:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0959 (0.0942)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:59:35AM searchStage_trainer.py:198 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0647 (0.0930)	Prec@(1,5) (96.9%, 100.0%)	
08/11 05:59:36AM searchStage_trainer.py:208 [INFO] Train: [ 45/49] Final Prec@1 96.8960%
08/11 05:59:44AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5880	Prec@(1,5) (85.4%, 99.3%)
08/11 05:59:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5447	Prec@(1,5) (85.9%, 99.2%)
08/11 06:00:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5380	Prec@(1,5) (86.1%, 99.3%)
08/11 06:00:10AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5339	Prec@(1,5) (86.2%, 99.3%)
08/11 06:00:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5392	Prec@(1,5) (86.0%, 99.3%)
08/11 06:00:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5349	Prec@(1,5) (86.1%, 99.3%)
08/11 06:00:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5317	Prec@(1,5) (86.1%, 99.3%)
08/11 06:00:43AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5327	Prec@(1,5) (86.0%, 99.3%)
08/11 06:00:43AM searchStage_trainer.py:247 [INFO] Valid: [ 45/49] Final Prec@1 85.9520%
08/11 06:00:43AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:00:43AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.9520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2411, 0.2606, 0.2483],
        [0.2497, 0.2416, 0.2614, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2402, 0.2716, 0.2390],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2499, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2486, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2499, 0.2535, 0.2451],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2506, 0.2491],
        [0.2500, 0.2500, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2513, 0.2461],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2487],
        [0.2493, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2495],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2499, 0.2563, 0.2462],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2499],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2491, 0.2539, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2538, 0.2490],
        [0.2494, 0.2487, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:01:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.1574 (0.0857)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:02:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.1234 (0.0852)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:03:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.1483 (0.0894)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:04:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0709 (0.0884)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:05:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0911 (0.0883)	Prec@(1,5) (97.0%, 100.0%)	
08/11 06:05:55AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0904 (0.0899)	Prec@(1,5) (97.0%, 100.0%)	
08/11 06:06:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.1983 (0.0911)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:07:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.1012 (0.0904)	Prec@(1,5) (96.9%, 100.0%)	
08/11 06:07:27AM searchStage_trainer.py:208 [INFO] Train: [ 46/49] Final Prec@1 96.9160%
08/11 06:07:36AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5272	Prec@(1,5) (86.2%, 99.1%)
08/11 06:07:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5416	Prec@(1,5) (86.0%, 99.2%)
08/11 06:07:53AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5361	Prec@(1,5) (85.9%, 99.2%)
08/11 06:08:02AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5374	Prec@(1,5) (85.9%, 99.2%)
08/11 06:08:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5315	Prec@(1,5) (86.1%, 99.2%)
08/11 06:08:19AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5406	Prec@(1,5) (85.9%, 99.3%)
08/11 06:08:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5402	Prec@(1,5) (86.0%, 99.3%)
08/11 06:08:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5446	Prec@(1,5) (85.8%, 99.3%)
08/11 06:08:35AM searchStage_trainer.py:247 [INFO] Valid: [ 46/49] Final Prec@1 85.8160%
08/11 06:08:35AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:08:35AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.9520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2411, 0.2606, 0.2483],
        [0.2496, 0.2416, 0.2615, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2401, 0.2717, 0.2389],
        [0.2504, 0.2497, 0.2511, 0.2487],
        [0.2496, 0.2499, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2486, 0.2518, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2535, 0.2451],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2506, 0.2491],
        [0.2500, 0.2500, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2513, 0.2461],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2493, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2494],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2499, 0.2564, 0.2461],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2498],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2491, 0.2539, 0.2484],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2538, 0.2490],
        [0.2494, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:09:28AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0540 (0.0817)	Prec@(1,5) (97.3%, 100.0%)	
08/11 06:10:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.1527 (0.0843)	Prec@(1,5) (97.0%, 100.0%)	
08/11 06:11:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0614 (0.0810)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:12:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0981 (0.0812)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:12:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0933 (0.0834)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:13:47AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0855 (0.0836)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:14:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0833 (0.0837)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:15:19AM searchStage_trainer.py:198 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0682 (0.0832)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:15:19AM searchStage_trainer.py:208 [INFO] Train: [ 47/49] Final Prec@1 97.0720%
08/11 06:15:28AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.6059	Prec@(1,5) (85.1%, 99.2%)
08/11 06:15:37AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5572	Prec@(1,5) (86.0%, 99.3%)
08/11 06:15:45AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5441	Prec@(1,5) (86.1%, 99.2%)
08/11 06:15:54AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5515	Prec@(1,5) (86.0%, 99.2%)
08/11 06:16:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5594	Prec@(1,5) (85.8%, 99.2%)
08/11 06:16:11AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5663	Prec@(1,5) (85.6%, 99.2%)
08/11 06:16:20AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5646	Prec@(1,5) (85.7%, 99.3%)
08/11 06:16:27AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5643	Prec@(1,5) (85.7%, 99.3%)
08/11 06:16:27AM searchStage_trainer.py:247 [INFO] Valid: [ 47/49] Final Prec@1 85.7320%
08/11 06:16:27AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:16:27AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 85.9520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2411, 0.2606, 0.2483],
        [0.2496, 0.2415, 0.2615, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2401, 0.2718, 0.2389],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2499, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2485, 0.2519, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2498, 0.2535, 0.2451],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2506, 0.2491],
        [0.2500, 0.2500, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2513, 0.2460],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2488],
        [0.2493, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2518, 0.2494],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2499, 0.2565, 0.2461],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2498],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2491, 0.2539, 0.2483],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2538, 0.2490],
        [0.2494, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:17:20AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.1179 (0.0791)	Prec@(1,5) (97.0%, 100.0%)	
08/11 06:18:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0395 (0.0796)	Prec@(1,5) (97.1%, 100.0%)	
08/11 06:19:04AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.1075 (0.0788)	Prec@(1,5) (97.2%, 100.0%)	
08/11 06:19:56AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.1047 (0.0763)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:20:48AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.1116 (0.0763)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:21:39AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0561 (0.0763)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:22:30AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0688 (0.0762)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:22:57AM searchStage_trainer.py:198 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0271 (0.0771)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:22:57AM searchStage_trainer.py:208 [INFO] Train: [ 48/49] Final Prec@1 97.4560%
08/11 06:23:03AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5554	Prec@(1,5) (85.9%, 99.5%)
08/11 06:23:09AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5648	Prec@(1,5) (86.1%, 99.3%)
08/11 06:23:15AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5486	Prec@(1,5) (86.4%, 99.2%)
08/11 06:23:20AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5532	Prec@(1,5) (86.2%, 99.2%)
08/11 06:23:26AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5430	Prec@(1,5) (86.3%, 99.3%)
08/11 06:23:32AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5494	Prec@(1,5) (86.1%, 99.3%)
08/11 06:23:38AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5523	Prec@(1,5) (86.1%, 99.3%)
08/11 06:23:42AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5568	Prec@(1,5) (86.0%, 99.2%)
08/11 06:23:42AM searchStage_trainer.py:247 [INFO] Valid: [ 48/49] Final Prec@1 86.0240%
08/11 06:23:42AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:23:42AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 86.0240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2461, 0.2564, 0.2483],
        [0.2486, 0.2461, 0.2568, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2411, 0.2606, 0.2483],
        [0.2496, 0.2415, 0.2615, 0.2473],
        [0.2518, 0.2514, 0.2489, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2400, 0.2719, 0.2389],
        [0.2504, 0.2498, 0.2511, 0.2487],
        [0.2496, 0.2499, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2511, 0.2469],
        [0.2514, 0.2497, 0.2497, 0.2492],
        [0.2507, 0.2502, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2485, 0.2518, 0.2465],
        [0.2510, 0.2487, 0.2516, 0.2487],
        [0.2503, 0.2497, 0.2488, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2499, 0.2535, 0.2451],
        [0.2494, 0.2493, 0.2495, 0.2518],
        [0.2493, 0.2496, 0.2492, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2507, 0.2492],
        [0.2494, 0.2496, 0.2507, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2506, 0.2491],
        [0.2500, 0.2500, 0.2507, 0.2494],
        [0.2496, 0.2502, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2514, 0.2460],
        [0.2498, 0.2498, 0.2500, 0.2503],
        [0.2499, 0.2500, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2507, 0.2487],
        [0.2493, 0.2506, 0.2502, 0.2498],
        [0.2500, 0.2498, 0.2494, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2507, 0.2523, 0.2477],
        [0.2496, 0.2500, 0.2504, 0.2501],
        [0.2494, 0.2502, 0.2497, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2513, 0.2482],
        [0.2499, 0.2499, 0.2501, 0.2500],
        [0.2495, 0.2501, 0.2493, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2509, 0.2501],
        [0.2498, 0.2497, 0.2510, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2492, 0.2518, 0.2494],
        [0.2492, 0.2486, 0.2522, 0.2500],
        [0.2499, 0.2495, 0.2502, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2499, 0.2566, 0.2460],
        [0.2492, 0.2497, 0.2508, 0.2503],
        [0.2498, 0.2491, 0.2504, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2509, 0.2498],
        [0.2496, 0.2494, 0.2503, 0.2506],
        [0.2500, 0.2500, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2491, 0.2539, 0.2483],
        [0.2487, 0.2488, 0.2521, 0.2504],
        [0.2491, 0.2498, 0.2504, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2484, 0.2538, 0.2490],
        [0.2494, 0.2486, 0.2527, 0.2492],
        [0.2493, 0.2488, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/11 06:24:18AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0688 (0.0651)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:24:52AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0325 (0.0685)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:25:27AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0711 (0.0690)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:26:02AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0443 (0.0685)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:26:36AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0645 (0.0681)	Prec@(1,5) (97.7%, 100.0%)	
08/11 06:27:11AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0950 (0.0722)	Prec@(1,5) (97.6%, 100.0%)	
08/11 06:27:45AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0828 (0.0742)	Prec@(1,5) (97.5%, 100.0%)	
08/11 06:28:12AM searchStage_trainer.py:198 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0746 (0.0751)	Prec@(1,5) (97.4%, 100.0%)	
08/11 06:28:12AM searchStage_trainer.py:208 [INFO] Train: [ 49/49] Final Prec@1 97.4480%
08/11 06:28:18AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.5607	Prec@(1,5) (85.8%, 99.3%)
08/11 06:28:24AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5392	Prec@(1,5) (86.0%, 99.4%)
08/11 06:28:29AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.5378	Prec@(1,5) (86.1%, 99.5%)
08/11 06:28:35AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5404	Prec@(1,5) (86.1%, 99.4%)
08/11 06:28:41AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.5476	Prec@(1,5) (86.1%, 99.3%)
08/11 06:28:47AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5399	Prec@(1,5) (86.2%, 99.3%)
08/11 06:28:52AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5380	Prec@(1,5) (86.2%, 99.3%)
08/11 06:28:57AM searchStage_trainer.py:236 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5447	Prec@(1,5) (86.1%, 99.3%)
08/11 06:28:57AM searchStage_trainer.py:247 [INFO] Valid: [ 49/49] Final Prec@1 86.0640%
08/11 06:28:57AM searchStage_main.py:47 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/11 06:28:57AM searchStage_main.py:73 [INFO] Until now, best Prec@1 = 86.0640%
08/11 06:28:57AM searchStage_main.py:75 [INFO] Final best Prec@1 = 86.0640%
08/11 06:28:57AM searchStage_main.py:76 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
