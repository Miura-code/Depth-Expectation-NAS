12/27 05:00:13AM parser.py:28 [INFO] 
12/27 05:00:13AM parser.py:29 [INFO] Parameters:
12/27 05:00:13AM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g-1/DAG
12/27 05:00:13AM parser.py:31 [INFO] T=10.0
12/27 05:00:13AM parser.py:31 [INFO] ADVANCED=1
12/27 05:00:13AM parser.py:31 [INFO] ALPHA_LR=0.0003
12/27 05:00:13AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
12/27 05:00:13AM parser.py:31 [INFO] ARCH_CRITERION=expected
12/27 05:00:13AM parser.py:31 [INFO] BATCH_SIZE=64
12/27 05:00:13AM parser.py:31 [INFO] CASCADE=0
12/27 05:00:13AM parser.py:31 [INFO] CHECKPOINT_RESET=False
12/27 05:00:13AM parser.py:31 [INFO] CURRICULUM_EPOCHS=[30, 30]
12/27 05:00:13AM parser.py:31 [INFO] CUTOUT_LENGTH=0
12/27 05:00:13AM parser.py:31 [INFO] DATA_PATH=../data/
12/27 05:00:13AM parser.py:31 [INFO] DATASET=cifar100
12/27 05:00:13AM parser.py:31 [INFO] DEPTH_COEF=-1.0
12/27 05:00:13AM parser.py:31 [INFO] DESCRIPTION=search_and_evaluation_curriculum_learning_with_-Beta-expected-Depth-ecpected-constraint_slidewindow-3
12/27 05:00:13AM parser.py:31 [INFO] DISCRETE=1
12/27 05:00:13AM parser.py:31 [INFO] EPOCHS=50
12/27 05:00:13AM parser.py:31 [INFO] EVAL_EPOCHS=100
12/27 05:00:13AM parser.py:31 [INFO] EXP_NAME=s0-depth-sw3-g-1
12/27 05:00:13AM parser.py:31 [INFO] FINAL_L=0.0
12/27 05:00:13AM parser.py:31 [INFO] G=0.0
12/27 05:00:13AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
12/27 05:00:13AM parser.py:31 [INFO] GPUS=[0]
12/27 05:00:13AM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
12/27 05:00:13AM parser.py:31 [INFO] INIT_CHANNELS=16
12/27 05:00:13AM parser.py:31 [INFO] L=0.0
12/27 05:00:13AM parser.py:31 [INFO] LAYERS=32
12/27 05:00:13AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
12/27 05:00:13AM parser.py:31 [INFO] NAME=Pruning
12/27 05:00:13AM parser.py:31 [INFO] NONKD=1
12/27 05:00:13AM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g-1
12/27 05:00:13AM parser.py:31 [INFO] PCDARTS=0
12/27 05:00:13AM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g-1/plots
12/27 05:00:13AM parser.py:31 [INFO] PRINT_FREQ=100
12/27 05:00:13AM parser.py:31 [INFO] RESET=0
12/27 05:00:13AM parser.py:31 [INFO] RESUME_PATH=None
12/27 05:00:13AM parser.py:31 [INFO] SAVE=s0-depth-sw3-g-1
12/27 05:00:13AM parser.py:31 [INFO] SEED=0
12/27 05:00:13AM parser.py:31 [INFO] SHARE_STAGE=0
12/27 05:00:13AM parser.py:31 [INFO] SLIDE_WINDOW=3
12/27 05:00:13AM parser.py:31 [INFO] SPEC_CELL=1
12/27 05:00:13AM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
12/27 05:00:13AM parser.py:31 [INFO] TEACHER_NAME=none
12/27 05:00:13AM parser.py:31 [INFO] TEACHER_PATH=none
12/27 05:00:13AM parser.py:31 [INFO] TRAIN_PORTION=0.5
12/27 05:00:13AM parser.py:31 [INFO] TYPE=Distribution
12/27 05:00:13AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
12/27 05:00:13AM parser.py:31 [INFO] W_LR=0.025
12/27 05:00:13AM parser.py:31 [INFO] W_LR_MIN=0.001
12/27 05:00:13AM parser.py:31 [INFO] W_MOMENTUM=0.9
12/27 05:00:13AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
12/27 05:00:13AM parser.py:31 [INFO] WORKERS=4
12/27 05:00:13AM parser.py:32 [INFO] 
12/27 05:00:15AM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
12/27 05:01:00AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.5889 (4.5531)	Arch Loss 4.6135 (4.5686)	Arch Hard Loss 4.6135 (4.5686)	Arch Beta Loss 2.8137 (2.8184)	Arch depth Loss 2.8137 (2.8184)	Prec@(1,5) (1.8%, 8.3%)	
12/27 05:01:43AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.1118 (4.4518)	Arch Loss 4.2947 (4.4588)	Arch Hard Loss 4.2947 (4.4588)	Arch Beta Loss 2.8035 (2.8134)	Arch depth Loss 2.8035 (2.8134)	Prec@(1,5) (2.6%, 11.5%)	
12/27 05:02:26AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.3940 (4.3763)	Arch Loss 4.0866 (4.3724)	Arch Hard Loss 4.0866 (4.3724)	Arch Beta Loss 2.7934 (2.8084)	Arch depth Loss 2.7934 (2.8084)	Prec@(1,5) (3.2%, 13.9%)	
12/27 05:03:06AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 4.1646 (4.3184)	Arch Loss 4.2077 (4.3150)	Arch Hard Loss 4.2077 (4.3150)	Arch Beta Loss 2.7843 (2.8039)	Arch depth Loss 2.7843 (2.8039)	Prec@(1,5) (3.9%, 15.9%)	
12/27 05:03:07AM searchDistribution_trainer.py:166 [INFO] Train: [  0/49] Final Prec@1 3.8760%
12/27 05:03:15AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 4.0694	Prec@(1,5) (6.6%, 24.3%)
12/27 05:03:22AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 4.0755	Prec@(1,5) (6.6%, 23.9%)
12/27 05:03:29AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 4.0779	Prec@(1,5) (6.7%, 23.9%)
12/27 05:03:35AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 4.0839	Prec@(1,5) (6.6%, 23.8%)
12/27 05:03:35AM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 6.5640%
12/27 05:03:35AM trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:03:35AM trainer_runner.py:108 [INFO] Until now, best Prec@1 = 6.5640%
12/27 05:04:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 4.0497 (4.0121)	Arch Loss 3.8014 (4.0405)	Arch Hard Loss 3.8014 (4.0405)	Arch Beta Loss 2.7748 (2.7796)	Arch depth Loss 2.7748 (2.7796)	Prec@(1,5) (7.8%, 26.2%)	
12/27 05:05:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.9848 (3.9651)	Arch Loss 3.6773 (3.9693)	Arch Hard Loss 3.6773 (3.9693)	Arch Beta Loss 2.7651 (2.7748)	Arch depth Loss 2.7651 (2.7748)	Prec@(1,5) (8.4%, 27.5%)	
12/27 05:05:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.8063 (3.9231)	Arch Loss 4.0355 (3.9290)	Arch Hard Loss 4.0355 (3.9290)	Arch Beta Loss 2.7559 (2.7701)	Arch depth Loss 2.7559 (2.7701)	Prec@(1,5) (9.1%, 29.0%)	
12/27 05:06:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.8831 (3.8943)	Arch Loss 3.7105 (3.8908)	Arch Hard Loss 3.7105 (3.8908)	Arch Beta Loss 2.7477 (2.7658)	Arch depth Loss 2.7477 (2.7658)	Prec@(1,5) (9.4%, 29.9%)	
12/27 05:06:30午前 searchDistribution_trainer.py:166 [INFO] Train: [  1/49] Final Prec@1 9.4040%
12/27 05:06:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.7157	Prec@(1,5) (12.1%, 34.4%)
12/27 05:06:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.7102	Prec@(1,5) (11.9%, 35.0%)
12/27 05:06:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.7112	Prec@(1,5) (12.0%, 35.0%)
12/27 05:06:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.7152	Prec@(1,5) (11.9%, 34.8%)
12/27 05:06:57午前 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 11.9120%
12/27 05:06:57午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:06:58午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 11.9120%
12/27 05:07:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.8085 (3.6850)	Arch Loss 3.9706 (3.7104)	Arch Hard Loss 3.9706 (3.7104)	Arch Beta Loss 2.7402 (2.7439)	Arch depth Loss 2.7402 (2.7439)	Prec@(1,5) (11.5%, 35.7%)	
12/27 05:08:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5446 (3.6651)	Arch Loss 3.6897 (3.6853)	Arch Hard Loss 3.6897 (3.6853)	Arch Beta Loss 2.7344 (2.7404)	Arch depth Loss 2.7344 (2.7404)	Prec@(1,5) (12.2%, 36.1%)	
12/27 05:09:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.6160 (3.6534)	Arch Loss 3.5610 (3.6474)	Arch Hard Loss 3.5610 (3.6474)	Arch Beta Loss 2.7290 (2.7374)	Arch depth Loss 2.7290 (2.7374)	Prec@(1,5) (12.3%, 36.5%)	
12/27 05:09:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.6222 (3.6315)	Arch Loss 3.5593 (3.6218)	Arch Hard Loss 3.5593 (3.6218)	Arch Beta Loss 2.7253 (2.7350)	Arch depth Loss 2.7253 (2.7350)	Prec@(1,5) (12.8%, 37.2%)	
12/27 05:09:53午前 searchDistribution_trainer.py:166 [INFO] Train: [  2/49] Final Prec@1 12.8480%
12/27 05:10:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.5549	Prec@(1,5) (14.6%, 39.8%)
12/27 05:10:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.5515	Prec@(1,5) (14.9%, 39.7%)
12/27 05:10:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.5545	Prec@(1,5) (14.8%, 39.8%)
12/27 05:10:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.5490	Prec@(1,5) (14.9%, 40.0%)
12/27 05:10:19午前 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 14.9240%
12/27 05:10:19午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:10:20午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 14.9240%
12/27 05:11:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.3565 (3.4675)	Arch Loss 3.3329 (3.4970)	Arch Hard Loss 3.3329 (3.4970)	Arch Beta Loss 2.7219 (2.7234)	Arch depth Loss 2.7219 (2.7234)	Prec@(1,5) (15.7%, 42.0%)	
12/27 05:11:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.3221 (3.4419)	Arch Loss 3.6941 (3.4697)	Arch Hard Loss 3.6941 (3.4697)	Arch Beta Loss 2.7195 (2.7220)	Arch depth Loss 2.7195 (2.7220)	Prec@(1,5) (16.4%, 43.1%)	
12/27 05:12:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.5916 (3.4428)	Arch Loss 3.5069 (3.4477)	Arch Hard Loss 3.5069 (3.4477)	Arch Beta Loss 2.7172 (2.7207)	Arch depth Loss 2.7172 (2.7207)	Prec@(1,5) (16.2%, 43.2%)	
12/27 05:13:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.1386 (3.4215)	Arch Loss 3.4973 (3.4381)	Arch Hard Loss 3.4973 (3.4381)	Arch Beta Loss 2.7165 (2.7198)	Arch depth Loss 2.7165 (2.7198)	Prec@(1,5) (16.7%, 44.0%)	
12/27 05:13:13午前 searchDistribution_trainer.py:166 [INFO] Train: [  3/49] Final Prec@1 16.7240%
12/27 05:13:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.4469	Prec@(1,5) (16.7%, 42.8%)
12/27 05:13:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.4528	Prec@(1,5) (16.6%, 42.8%)
12/27 05:13:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.4422	Prec@(1,5) (16.6%, 43.1%)
12/27 05:13:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.4434	Prec@(1,5) (16.7%, 43.0%)
12/27 05:13:38午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 16.6600%
12/27 05:13:38午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:13:39午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 16.6600%
12/27 05:14:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.1115 (3.3047)	Arch Loss 3.0783 (3.3402)	Arch Hard Loss 3.0783 (3.3402)	Arch Beta Loss 2.7151 (2.7158)	Arch depth Loss 2.7151 (2.7158)	Prec@(1,5) (18.8%, 47.2%)	
12/27 05:15:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0337 (3.2868)	Arch Loss 3.0403 (3.3037)	Arch Hard Loss 3.0403 (3.3037)	Arch Beta Loss 2.7135 (2.7152)	Arch depth Loss 2.7135 (2.7152)	Prec@(1,5) (18.6%, 47.9%)	
12/27 05:15:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 3.4520 (3.2793)	Arch Loss 3.1475 (3.2880)	Arch Hard Loss 3.1475 (3.2880)	Arch Beta Loss 2.7119 (2.7143)	Arch depth Loss 2.7119 (2.7143)	Prec@(1,5) (19.0%, 48.1%)	
12/27 05:16:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.1578 (3.2617)	Arch Loss 2.9787 (3.2685)	Arch Hard Loss 2.9787 (3.2685)	Arch Beta Loss 2.7105 (2.7136)	Arch depth Loss 2.7105 (2.7136)	Prec@(1,5) (19.3%, 48.3%)	
12/27 05:16:30午前 searchDistribution_trainer.py:166 [INFO] Train: [  4/49] Final Prec@1 19.3280%
12/27 05:16:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.3044	Prec@(1,5) (19.5%, 48.5%)
12/27 05:16:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.2571	Prec@(1,5) (20.1%, 49.7%)
12/27 05:16:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.2532	Prec@(1,5) (19.9%, 49.9%)
12/27 05:16:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.2450	Prec@(1,5) (20.0%, 50.1%)
12/27 05:16:57午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 20.0080%
12/27 05:16:57午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:16:58午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 20.0080%
12/27 05:17:42午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.0187 (3.1297)	Arch Loss 3.2114 (3.1781)	Arch Hard Loss 3.2114 (3.1781)	Arch Beta Loss 2.7086 (2.7095)	Arch depth Loss 2.7086 (2.7095)	Prec@(1,5) (22.1%, 51.3%)	
12/27 05:18:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 2.7868 (3.1273)	Arch Loss 3.4319 (3.1633)	Arch Hard Loss 3.4319 (3.1633)	Arch Beta Loss 2.7073 (2.7087)	Arch depth Loss 2.7073 (2.7087)	Prec@(1,5) (21.8%, 51.9%)	
12/27 05:19:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 3.3234 (3.1138)	Arch Loss 3.0589 (3.1528)	Arch Hard Loss 3.0589 (3.1528)	Arch Beta Loss 2.7057 (2.7079)	Arch depth Loss 2.7057 (2.7079)	Prec@(1,5) (22.0%, 52.2%)	
12/27 05:19:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 3.0939 (3.0998)	Arch Loss 3.0456 (3.1422)	Arch Hard Loss 3.0456 (3.1422)	Arch Beta Loss 2.7042 (2.7073)	Arch depth Loss 2.7042 (2.7073)	Prec@(1,5) (22.1%, 52.6%)	
12/27 05:19:49午前 searchDistribution_trainer.py:166 [INFO] Train: [  5/49] Final Prec@1 22.1440%
12/27 05:19:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 3.0313	Prec@(1,5) (24.1%, 54.4%)
12/27 05:20:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 3.0491	Prec@(1,5) (23.5%, 54.2%)
12/27 05:20:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 3.0593	Prec@(1,5) (23.7%, 54.0%)
12/27 05:20:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 3.0634	Prec@(1,5) (23.6%, 54.0%)
12/27 05:20:16午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 23.6400%
12/27 05:20:16午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:20:16午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 23.6400%
12/27 05:21:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 3.0167 (2.9621)	Arch Loss 2.8987 (3.0582)	Arch Hard Loss 2.8987 (3.0582)	Arch Beta Loss 2.7026 (2.7034)	Arch depth Loss 2.7026 (2.7034)	Prec@(1,5) (24.2%, 55.8%)	
12/27 05:21:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.7341 (2.9797)	Arch Loss 2.7650 (3.0281)	Arch Hard Loss 2.7650 (3.0281)	Arch Beta Loss 2.7007 (2.7025)	Arch depth Loss 2.7007 (2.7025)	Prec@(1,5) (24.9%, 55.5%)	
12/27 05:22:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.1749 (2.9707)	Arch Loss 2.9742 (3.0248)	Arch Hard Loss 2.9742 (3.0248)	Arch Beta Loss 2.6990 (2.7016)	Arch depth Loss 2.6990 (2.7016)	Prec@(1,5) (24.9%, 55.8%)	
12/27 05:23:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.9635 (2.9666)	Arch Loss 3.2841 (3.0177)	Arch Hard Loss 3.2841 (3.0177)	Arch Beta Loss 2.6972 (2.7008)	Arch depth Loss 2.6972 (2.7008)	Prec@(1,5) (25.1%, 56.2%)	
12/27 05:23:12午前 searchDistribution_trainer.py:166 [INFO] Train: [  6/49] Final Prec@1 25.0840%
12/27 05:23:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.9533	Prec@(1,5) (25.8%, 56.7%)
12/27 05:23:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.9531	Prec@(1,5) (25.7%, 56.6%)
12/27 05:23:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.9674	Prec@(1,5) (25.3%, 56.2%)
12/27 05:23:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.9615	Prec@(1,5) (25.3%, 56.1%)
12/27 05:23:41午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 25.3000%
12/27 05:23:41午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:23:42午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 25.3000%
12/27 05:24:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.7919 (2.8623)	Arch Loss 2.7608 (2.9378)	Arch Hard Loss 2.7608 (2.9378)	Arch Beta Loss 2.6948 (2.6959)	Arch depth Loss 2.6948 (2.6959)	Prec@(1,5) (26.8%, 58.9%)	
12/27 05:25:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.9211 (2.8475)	Arch Loss 2.9479 (2.9350)	Arch Hard Loss 2.9479 (2.9350)	Arch Beta Loss 2.6932 (2.6949)	Arch depth Loss 2.6932 (2.6949)	Prec@(1,5) (27.1%, 59.1%)	
12/27 05:25:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.7542 (2.8389)	Arch Loss 2.8944 (2.9206)	Arch Hard Loss 2.8944 (2.9206)	Arch Beta Loss 2.6913 (2.6940)	Arch depth Loss 2.6913 (2.6940)	Prec@(1,5) (27.4%, 59.3%)	
12/27 05:26:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.8110 (2.8338)	Arch Loss 2.4861 (2.9167)	Arch Hard Loss 2.4861 (2.9167)	Arch Beta Loss 2.6894 (2.6931)	Arch depth Loss 2.6894 (2.6931)	Prec@(1,5) (27.4%, 59.3%)	
12/27 05:26:35午前 searchDistribution_trainer.py:166 [INFO] Train: [  7/49] Final Prec@1 27.3760%
12/27 05:26:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.8256	Prec@(1,5) (28.9%, 60.3%)
12/27 05:26:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.8381	Prec@(1,5) (28.0%, 59.9%)
12/27 05:26:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.8439	Prec@(1,5) (28.2%, 59.5%)
12/27 05:27:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.8567	Prec@(1,5) (27.9%, 59.1%)
12/27 05:27:00午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 27.9000%
12/27 05:27:00午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:27:00午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 27.9000%
12/27 05:27:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.5274 (2.7267)	Arch Loss 2.8624 (2.8760)	Arch Hard Loss 2.8624 (2.8760)	Arch Beta Loss 2.6877 (2.6886)	Arch depth Loss 2.6877 (2.6886)	Prec@(1,5) (30.2%, 61.7%)	
12/27 05:28:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.4702 (2.7232)	Arch Loss 2.7404 (2.8554)	Arch Hard Loss 2.7404 (2.8554)	Arch Beta Loss 2.6861 (2.6877)	Arch depth Loss 2.6861 (2.6877)	Prec@(1,5) (29.9%, 62.1%)	
12/27 05:29:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.8197 (2.7270)	Arch Loss 2.8616 (2.8306)	Arch Hard Loss 2.8616 (2.8306)	Arch Beta Loss 2.6843 (2.6869)	Arch depth Loss 2.6843 (2.6869)	Prec@(1,5) (29.9%, 62.1%)	
12/27 05:29:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.4639 (2.7306)	Arch Loss 3.0196 (2.8204)	Arch Hard Loss 3.0196 (2.8204)	Arch Beta Loss 2.6826 (2.6860)	Arch depth Loss 2.6826 (2.6860)	Prec@(1,5) (29.7%, 61.9%)	
12/27 05:29:45午前 searchDistribution_trainer.py:166 [INFO] Train: [  8/49] Final Prec@1 29.7080%
12/27 05:29:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.7515	Prec@(1,5) (30.2%, 61.5%)
12/27 05:29:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.7484	Prec@(1,5) (29.9%, 61.5%)
12/27 05:30:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.7418	Prec@(1,5) (30.1%, 61.8%)
12/27 05:30:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.7400	Prec@(1,5) (30.1%, 61.8%)
12/27 05:30:09午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 30.1280%
12/27 05:30:09午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:30:09午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 30.1280%
12/27 05:30:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.8328 (2.6026)	Arch Loss 2.8532 (2.7492)	Arch Hard Loss 2.8532 (2.7492)	Arch Beta Loss 2.6810 (2.6818)	Arch depth Loss 2.6810 (2.6818)	Prec@(1,5) (32.4%, 64.7%)	
12/27 05:31:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.5014 (2.6111)	Arch Loss 3.1253 (2.7614)	Arch Hard Loss 3.1253 (2.7614)	Arch Beta Loss 2.6794 (2.6810)	Arch depth Loss 2.6794 (2.6810)	Prec@(1,5) (32.1%, 64.6%)	
12/27 05:32:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.4988 (2.6117)	Arch Loss 2.5641 (2.7506)	Arch Hard Loss 2.5641 (2.7506)	Arch Beta Loss 2.6779 (2.6802)	Arch depth Loss 2.6779 (2.6802)	Prec@(1,5) (32.2%, 64.9%)	
12/27 05:32:56午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.6241 (2.6111)	Arch Loss 2.8533 (2.7374)	Arch Hard Loss 2.8533 (2.7374)	Arch Beta Loss 2.6765 (2.6795)	Arch depth Loss 2.6765 (2.6795)	Prec@(1,5) (32.3%, 64.9%)	
12/27 05:32:57午前 searchDistribution_trainer.py:166 [INFO] Train: [  9/49] Final Prec@1 32.2600%
12/27 05:33:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.7129	Prec@(1,5) (30.5%, 62.9%)
12/27 05:33:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.6702	Prec@(1,5) (30.8%, 63.4%)
12/27 05:33:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.6713	Prec@(1,5) (31.0%, 63.4%)
12/27 05:33:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.6637	Prec@(1,5) (31.0%, 63.7%)
12/27 05:33:21午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 31.0440%
12/27 05:33:21午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:33:21午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 31.0440%
12/27 05:34:04午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.9685 (2.5242)	Arch Loss 2.8219 (2.7068)	Arch Hard Loss 2.8219 (2.7068)	Arch Beta Loss 2.6752 (2.6758)	Arch depth Loss 2.6752 (2.6758)	Prec@(1,5) (33.5%, 66.6%)	
12/27 05:34:46午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.5363 (2.5142)	Arch Loss 2.7855 (2.6633)	Arch Hard Loss 2.7855 (2.6633)	Arch Beta Loss 2.6737 (2.6750)	Arch depth Loss 2.6737 (2.6750)	Prec@(1,5) (34.0%, 67.3%)	
12/27 05:35:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.3204 (2.5269)	Arch Loss 2.8556 (2.6633)	Arch Hard Loss 2.8556 (2.6633)	Arch Beta Loss 2.6720 (2.6743)	Arch depth Loss 2.6720 (2.6743)	Prec@(1,5) (33.8%, 67.0%)	
12/27 05:36:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.6190 (2.5225)	Arch Loss 2.9407 (2.6496)	Arch Hard Loss 2.9407 (2.6496)	Arch Beta Loss 2.6707 (2.6736)	Arch depth Loss 2.6707 (2.6736)	Prec@(1,5) (34.0%, 67.0%)	
12/27 05:36:08午前 searchDistribution_trainer.py:166 [INFO] Train: [ 10/49] Final Prec@1 34.0160%
12/27 05:36:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.6725	Prec@(1,5) (31.7%, 64.4%)
12/27 05:36:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.6767	Prec@(1,5) (31.8%, 64.0%)
12/27 05:36:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.6846	Prec@(1,5) (31.5%, 63.7%)
12/27 05:36:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.6795	Prec@(1,5) (31.7%, 63.8%)
12/27 05:36:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 31.7120%
12/27 05:36:32午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:36:32午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 31.7120%
12/27 05:37:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.6401 (2.4381)	Arch Loss 2.3012 (2.6298)	Arch Hard Loss 2.3012 (2.6298)	Arch Beta Loss 2.6693 (2.6700)	Arch depth Loss 2.6693 (2.6700)	Prec@(1,5) (36.4%, 69.4%)	
12/27 05:37:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.2675 (2.4351)	Arch Loss 2.6076 (2.6028)	Arch Hard Loss 2.6076 (2.6028)	Arch Beta Loss 2.6676 (2.6692)	Arch depth Loss 2.6676 (2.6692)	Prec@(1,5) (36.2%, 68.8%)	
12/27 05:38:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.6793 (2.4293)	Arch Loss 2.3550 (2.5821)	Arch Hard Loss 2.3550 (2.5821)	Arch Beta Loss 2.6665 (2.6685)	Arch depth Loss 2.6665 (2.6685)	Prec@(1,5) (36.1%, 69.1%)	
12/27 05:39:18午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.6109 (2.4276)	Arch Loss 2.2779 (2.5802)	Arch Hard Loss 2.2779 (2.5802)	Arch Beta Loss 2.6652 (2.6679)	Arch depth Loss 2.6652 (2.6679)	Prec@(1,5) (36.1%, 69.1%)	
12/27 05:39:18午前 searchDistribution_trainer.py:166 [INFO] Train: [ 11/49] Final Prec@1 36.1160%
12/27 05:39:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.6203	Prec@(1,5) (33.1%, 65.0%)
12/27 05:39:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.6131	Prec@(1,5) (32.8%, 65.2%)
12/27 05:39:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.6153	Prec@(1,5) (32.6%, 65.3%)
12/27 05:39:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.6184	Prec@(1,5) (32.7%, 65.0%)
12/27 05:39:42午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 32.6640%
12/27 05:39:42午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:39:43午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 32.6640%
12/27 05:40:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.2433 (2.3200)	Arch Loss 2.6256 (2.5262)	Arch Hard Loss 2.6256 (2.5262)	Arch Beta Loss 2.6642 (2.6647)	Arch depth Loss 2.6642 (2.6647)	Prec@(1,5) (37.7%, 71.6%)	
12/27 05:41:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.2343 (2.3225)	Arch Loss 2.5613 (2.5278)	Arch Hard Loss 2.5613 (2.5278)	Arch Beta Loss 2.6630 (2.6641)	Arch depth Loss 2.6630 (2.6641)	Prec@(1,5) (38.1%, 71.5%)	
12/27 05:41:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.5990 (2.3408)	Arch Loss 2.4917 (2.5321)	Arch Hard Loss 2.4917 (2.5321)	Arch Beta Loss 2.6616 (2.6635)	Arch depth Loss 2.6616 (2.6635)	Prec@(1,5) (37.5%, 70.8%)	
12/27 05:42:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 2.4701 (2.3367)	Arch Loss 2.8513 (2.5167)	Arch Hard Loss 2.8513 (2.5167)	Arch Beta Loss 2.6600 (2.6629)	Arch depth Loss 2.6600 (2.6629)	Prec@(1,5) (37.6%, 71.0%)	
12/27 05:42:28午前 searchDistribution_trainer.py:166 [INFO] Train: [ 12/49] Final Prec@1 37.5800%
12/27 05:42:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.4980	Prec@(1,5) (35.1%, 67.1%)
12/27 05:42:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.5131	Prec@(1,5) (34.8%, 67.2%)
12/27 05:42:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.5109	Prec@(1,5) (34.7%, 67.3%)
12/27 05:42:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.5125	Prec@(1,5) (34.7%, 67.4%)
12/27 05:42:52午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 34.7000%
12/27 05:42:52午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:42:52午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 34.7000%
12/27 05:43:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1493 (2.2431)	Arch Loss 2.1128 (2.4693)	Arch Hard Loss 2.1128 (2.4693)	Arch Beta Loss 2.6583 (2.6591)	Arch depth Loss 2.6583 (2.6591)	Prec@(1,5) (39.9%, 72.9%)	
12/27 05:44:18午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.4985 (2.2525)	Arch Loss 2.4785 (2.4641)	Arch Hard Loss 2.4785 (2.4641)	Arch Beta Loss 2.6569 (2.6584)	Arch depth Loss 2.6569 (2.6584)	Prec@(1,5) (39.3%, 72.6%)	
12/27 05:45:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 2.3418 (2.2553)	Arch Loss 2.6916 (2.4598)	Arch Hard Loss 2.6916 (2.4598)	Arch Beta Loss 2.6554 (2.6576)	Arch depth Loss 2.6554 (2.6576)	Prec@(1,5) (39.3%, 72.7%)	
12/27 05:45:38午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.8964 (2.2555)	Arch Loss 2.2845 (2.4560)	Arch Hard Loss 2.2845 (2.4560)	Arch Beta Loss 2.6542 (2.6569)	Arch depth Loss 2.6542 (2.6569)	Prec@(1,5) (39.4%, 72.6%)	
12/27 05:45:39午前 searchDistribution_trainer.py:166 [INFO] Train: [ 13/49] Final Prec@1 39.3840%
12/27 05:45:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.3906	Prec@(1,5) (37.4%, 70.1%)
12/27 05:45:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.3866	Prec@(1,5) (37.6%, 70.0%)
12/27 05:45:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.4012	Prec@(1,5) (37.4%, 69.8%)
12/27 05:46:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.3984	Prec@(1,5) (37.6%, 69.8%)
12/27 05:46:03午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 37.5520%
12/27 05:46:03午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:46:04午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 37.5520%
12/27 05:46:46午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.2498 (2.1180)	Arch Loss 2.3992 (2.4074)	Arch Hard Loss 2.3992 (2.4074)	Arch Beta Loss 2.6528 (2.6534)	Arch depth Loss 2.6528 (2.6534)	Prec@(1,5) (42.5%, 74.6%)	
12/27 05:47:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.0354 (2.1542)	Arch Loss 2.5674 (2.3937)	Arch Hard Loss 2.5674 (2.3937)	Arch Beta Loss 2.6515 (2.6527)	Arch depth Loss 2.6515 (2.6527)	Prec@(1,5) (41.5%, 74.3%)	
12/27 05:48:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.9078 (2.1604)	Arch Loss 2.3997 (2.3939)	Arch Hard Loss 2.3997 (2.3939)	Arch Beta Loss 2.6502 (2.6521)	Arch depth Loss 2.6502 (2.6521)	Prec@(1,5) (41.3%, 74.4%)	
12/27 05:48:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.1050 (2.1626)	Arch Loss 2.2523 (2.3878)	Arch Hard Loss 2.2523 (2.3878)	Arch Beta Loss 2.6489 (2.6515)	Arch depth Loss 2.6489 (2.6515)	Prec@(1,5) (41.5%, 74.3%)	
12/27 05:48:48午前 searchDistribution_trainer.py:166 [INFO] Train: [ 14/49] Final Prec@1 41.4600%
12/27 05:48:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.6575	Prec@(1,5) (33.6%, 64.5%)
12/27 05:49:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.6533	Prec@(1,5) (33.5%, 64.4%)
12/27 05:49:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.6605	Prec@(1,5) (33.2%, 64.6%)
12/27 05:49:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.6514	Prec@(1,5) (33.1%, 64.8%)
12/27 05:49:13午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 33.1400%
12/27 05:49:13午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:49:13午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 37.5520%
12/27 05:49:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.1060 (2.0756)	Arch Loss 2.3399 (2.3433)	Arch Hard Loss 2.3399 (2.3433)	Arch Beta Loss 2.6477 (2.6482)	Arch depth Loss 2.6477 (2.6482)	Prec@(1,5) (43.4%, 76.6%)	
12/27 05:50:38午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.6797 (2.0911)	Arch Loss 2.4779 (2.3456)	Arch Hard Loss 2.4779 (2.3456)	Arch Beta Loss 2.6464 (2.6476)	Arch depth Loss 2.6464 (2.6476)	Prec@(1,5) (42.8%, 76.2%)	
12/27 05:51:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.8926 (2.0944)	Arch Loss 2.3778 (2.3433)	Arch Hard Loss 2.3778 (2.3433)	Arch Beta Loss 2.6453 (2.6470)	Arch depth Loss 2.6453 (2.6470)	Prec@(1,5) (42.8%, 76.0%)	
12/27 05:51:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 2.2882 (2.0958)	Arch Loss 2.3169 (2.3400)	Arch Hard Loss 2.3169 (2.3400)	Arch Beta Loss 2.6444 (2.6465)	Arch depth Loss 2.6444 (2.6465)	Prec@(1,5) (42.8%, 75.8%)	
12/27 05:51:57午前 searchDistribution_trainer.py:166 [INFO] Train: [ 15/49] Final Prec@1 42.8360%
12/27 05:52:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.3346	Prec@(1,5) (39.4%, 70.9%)
12/27 05:52:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.3271	Prec@(1,5) (39.7%, 71.2%)
12/27 05:52:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.3297	Prec@(1,5) (39.6%, 71.0%)
12/27 05:52:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.3396	Prec@(1,5) (39.3%, 70.9%)
12/27 05:52:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 39.3320%
12/27 05:52:22午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:52:22午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.3320%
12/27 05:53:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 2.0755 (1.9725)	Arch Loss 2.2525 (2.3267)	Arch Hard Loss 2.2525 (2.3267)	Arch Beta Loss 2.6435 (2.6439)	Arch depth Loss 2.6435 (2.6439)	Prec@(1,5) (46.4%, 77.9%)	
12/27 05:53:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.7506 (2.0030)	Arch Loss 2.5620 (2.3101)	Arch Hard Loss 2.5620 (2.3101)	Arch Beta Loss 2.6424 (2.6434)	Arch depth Loss 2.6424 (2.6434)	Prec@(1,5) (45.2%, 77.5%)	
12/27 05:54:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.9182 (2.0167)	Arch Loss 2.2780 (2.2955)	Arch Hard Loss 2.2780 (2.2955)	Arch Beta Loss 2.6410 (2.6428)	Arch depth Loss 2.6410 (2.6428)	Prec@(1,5) (44.8%, 77.3%)	
12/27 05:55:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9434 (2.0163)	Arch Loss 2.6447 (2.3019)	Arch Hard Loss 2.6447 (2.3019)	Arch Beta Loss 2.6399 (2.6423)	Arch depth Loss 2.6399 (2.6423)	Prec@(1,5) (44.8%, 77.3%)	
12/27 05:55:07午前 searchDistribution_trainer.py:166 [INFO] Train: [ 16/49] Final Prec@1 44.7440%
12/27 05:55:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.3545	Prec@(1,5) (38.3%, 71.3%)
12/27 05:55:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.3563	Prec@(1,5) (38.0%, 71.4%)
12/27 05:55:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.3529	Prec@(1,5) (38.2%, 71.3%)
12/27 05:55:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.3552	Prec@(1,5) (38.1%, 71.1%)
12/27 05:55:33午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 38.0800%
12/27 05:55:33午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:55:33午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.3320%
12/27 05:56:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 2.1619 (1.8963)	Arch Loss 2.4723 (2.2458)	Arch Hard Loss 2.4723 (2.2458)	Arch Beta Loss 2.6384 (2.6392)	Arch depth Loss 2.6384 (2.6392)	Prec@(1,5) (47.8%, 80.0%)	
12/27 05:56:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.9758 (1.9272)	Arch Loss 2.1425 (2.2630)	Arch Hard Loss 2.1425 (2.2630)	Arch Beta Loss 2.6374 (2.6385)	Arch depth Loss 2.6374 (2.6385)	Prec@(1,5) (46.6%, 79.3%)	
12/27 05:57:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.9724 (1.9394)	Arch Loss 1.9110 (2.2417)	Arch Hard Loss 1.9110 (2.2417)	Arch Beta Loss 2.6363 (2.6380)	Arch depth Loss 2.6363 (2.6380)	Prec@(1,5) (46.5%, 79.0%)	
12/27 05:58:18午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 2.0710 (1.9386)	Arch Loss 2.1636 (2.2361)	Arch Hard Loss 2.1636 (2.2361)	Arch Beta Loss 2.6356 (2.6375)	Arch depth Loss 2.6356 (2.6375)	Prec@(1,5) (46.3%, 79.0%)	
12/27 05:58:19午前 searchDistribution_trainer.py:166 [INFO] Train: [ 17/49] Final Prec@1 46.3400%
12/27 05:58:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.2838	Prec@(1,5) (39.3%, 72.5%)
12/27 05:58:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.2786	Prec@(1,5) (39.9%, 72.4%)
12/27 05:58:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.2916	Prec@(1,5) (39.9%, 72.1%)
12/27 05:58:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.2849	Prec@(1,5) (40.0%, 72.3%)
12/27 05:58:42午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 39.9760%
12/27 05:58:42午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:58:43午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.9760%
12/27 05:59:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.9137 (1.8927)	Arch Loss 2.0042 (2.2116)	Arch Hard Loss 2.0042 (2.2116)	Arch Beta Loss 2.6346 (2.6351)	Arch depth Loss 2.6346 (2.6351)	Prec@(1,5) (46.8%, 79.9%)	
12/27 06:00:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.9614 (1.8622)	Arch Loss 1.7373 (2.2324)	Arch Hard Loss 1.7373 (2.2324)	Arch Beta Loss 2.6332 (2.6345)	Arch depth Loss 2.6332 (2.6345)	Prec@(1,5) (48.0%, 80.3%)	
12/27 06:00:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.6027 (1.8647)	Arch Loss 2.2004 (2.2313)	Arch Hard Loss 2.2004 (2.2313)	Arch Beta Loss 2.6318 (2.6338)	Arch depth Loss 2.6318 (2.6338)	Prec@(1,5) (48.1%, 80.3%)	
12/27 06:01:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 2.0983 (1.8717)	Arch Loss 2.0700 (2.2230)	Arch Hard Loss 2.0700 (2.2230)	Arch Beta Loss 2.6307 (2.6332)	Arch depth Loss 2.6307 (2.6332)	Prec@(1,5) (47.8%, 80.2%)	
12/27 06:01:29午前 searchDistribution_trainer.py:166 [INFO] Train: [ 18/49] Final Prec@1 47.8280%
12/27 06:01:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.1978	Prec@(1,5) (42.0%, 74.4%)
12/27 06:01:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.2102	Prec@(1,5) (41.8%, 73.7%)
12/27 06:01:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.2008	Prec@(1,5) (41.9%, 73.9%)
12/27 06:01:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.1995	Prec@(1,5) (41.9%, 74.0%)
12/27 06:01:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 41.9040%
12/27 06:01:53午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:01:54午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 41.9040%
12/27 06:02:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.7154 (1.7494)	Arch Loss 1.9154 (2.1956)	Arch Hard Loss 1.9154 (2.1956)	Arch Beta Loss 2.6296 (2.6300)	Arch depth Loss 2.6296 (2.6300)	Prec@(1,5) (51.5%, 82.4%)	
12/27 06:03:19午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.4856 (1.7740)	Arch Loss 2.1371 (2.1819)	Arch Hard Loss 2.1371 (2.1819)	Arch Beta Loss 2.6286 (2.6296)	Arch depth Loss 2.6286 (2.6296)	Prec@(1,5) (51.1%, 81.8%)	
12/27 06:04:01午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.0096 (1.7907)	Arch Loss 1.8171 (2.1803)	Arch Hard Loss 1.8171 (2.1803)	Arch Beta Loss 2.6276 (2.6292)	Arch depth Loss 2.6276 (2.6292)	Prec@(1,5) (50.4%, 81.5%)	
12/27 06:04:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.9149 (1.8012)	Arch Loss 2.3609 (2.1819)	Arch Hard Loss 2.3609 (2.1819)	Arch Beta Loss 2.6266 (2.6287)	Arch depth Loss 2.6266 (2.6287)	Prec@(1,5) (49.9%, 81.4%)	
12/27 06:04:39午前 searchDistribution_trainer.py:166 [INFO] Train: [ 19/49] Final Prec@1 49.8720%
12/27 06:04:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.3255	Prec@(1,5) (38.9%, 71.5%)
12/27 06:04:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.3204	Prec@(1,5) (39.5%, 71.9%)
12/27 06:04:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.3296	Prec@(1,5) (39.6%, 71.5%)
12/27 06:05:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.3383	Prec@(1,5) (39.5%, 71.3%)
12/27 06:05:03午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 39.5320%
12/27 06:05:03午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:05:03午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 41.9040%
12/27 06:05:46午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.4840 (1.6772)	Arch Loss 2.1521 (2.1544)	Arch Hard Loss 2.1521 (2.1544)	Arch Beta Loss 2.6256 (2.6261)	Arch depth Loss 2.6256 (2.6261)	Prec@(1,5) (53.0%, 83.8%)	
12/27 06:06:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.8615 (1.7181)	Arch Loss 1.7770 (2.1393)	Arch Hard Loss 1.7770 (2.1393)	Arch Beta Loss 2.6243 (2.6255)	Arch depth Loss 2.6243 (2.6255)	Prec@(1,5) (51.6%, 82.9%)	
12/27 06:07:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.8426 (1.7321)	Arch Loss 2.0447 (2.1492)	Arch Hard Loss 2.0447 (2.1492)	Arch Beta Loss 2.6235 (2.6250)	Arch depth Loss 2.6235 (2.6250)	Prec@(1,5) (51.3%, 82.7%)	
12/27 06:07:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.7504 (1.7425)	Arch Loss 1.9496 (2.1377)	Arch Hard Loss 1.9496 (2.1377)	Arch Beta Loss 2.6227 (2.6246)	Arch depth Loss 2.6227 (2.6246)	Prec@(1,5) (51.0%, 82.4%)	
12/27 06:07:49午前 searchDistribution_trainer.py:166 [INFO] Train: [ 20/49] Final Prec@1 51.0000%
12/27 06:07:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.1621	Prec@(1,5) (43.3%, 75.1%)
12/27 06:08:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.1932	Prec@(1,5) (42.4%, 74.3%)
12/27 06:08:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.2028	Prec@(1,5) (42.4%, 74.3%)
12/27 06:08:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.2014	Prec@(1,5) (42.2%, 74.3%)
12/27 06:08:13午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 42.1720%
12/27 06:08:13午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:08:13午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 42.1720%
12/27 06:08:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.7106 (1.6706)	Arch Loss 2.4080 (2.1306)	Arch Hard Loss 2.4080 (2.1306)	Arch Beta Loss 2.6220 (2.6223)	Arch depth Loss 2.6220 (2.6223)	Prec@(1,5) (53.2%, 83.5%)	
12/27 06:09:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 2.2321 (1.6606)	Arch Loss 2.2969 (2.1032)	Arch Hard Loss 2.2969 (2.1032)	Arch Beta Loss 2.6210 (2.6219)	Arch depth Loss 2.6210 (2.6219)	Prec@(1,5) (53.1%, 83.9%)	
12/27 06:10:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 2.0643 (1.6751)	Arch Loss 1.8708 (2.1106)	Arch Hard Loss 1.8708 (2.1106)	Arch Beta Loss 2.6200 (2.6215)	Arch depth Loss 2.6200 (2.6215)	Prec@(1,5) (52.7%, 83.7%)	
12/27 06:11:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.6952 (1.6865)	Arch Loss 2.3269 (2.1128)	Arch Hard Loss 2.3269 (2.1128)	Arch Beta Loss 2.6193 (2.6210)	Arch depth Loss 2.6193 (2.6210)	Prec@(1,5) (52.5%, 83.6%)	
12/27 06:11:00午前 searchDistribution_trainer.py:166 [INFO] Train: [ 21/49] Final Prec@1 52.4880%
12/27 06:11:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.2048	Prec@(1,5) (42.8%, 74.4%)
12/27 06:11:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.1955	Prec@(1,5) (42.8%, 74.7%)
12/27 06:11:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.2094	Prec@(1,5) (42.6%, 74.5%)
12/27 06:11:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.2143	Prec@(1,5) (42.5%, 74.4%)
12/27 06:11:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 42.4880%
12/27 06:11:24午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:11:25午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 42.4880%
12/27 06:12:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.8801 (1.5556)	Arch Loss 1.7946 (2.1052)	Arch Hard Loss 1.7946 (2.1052)	Arch Beta Loss 2.6187 (2.6191)	Arch depth Loss 2.6187 (2.6191)	Prec@(1,5) (55.5%, 85.6%)	
12/27 06:12:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.5560 (1.5862)	Arch Loss 1.8489 (2.1137)	Arch Hard Loss 1.8489 (2.1137)	Arch Beta Loss 2.6176 (2.6187)	Arch depth Loss 2.6176 (2.6187)	Prec@(1,5) (54.8%, 85.2%)	
12/27 06:13:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.8189 (1.6100)	Arch Loss 2.0810 (2.0921)	Arch Hard Loss 2.0810 (2.0921)	Arch Beta Loss 2.6163 (2.6181)	Arch depth Loss 2.6163 (2.6181)	Prec@(1,5) (54.3%, 84.8%)	
12/27 06:14:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.7152 (1.6249)	Arch Loss 2.0138 (2.0826)	Arch Hard Loss 2.0138 (2.0826)	Arch Beta Loss 2.6157 (2.6176)	Arch depth Loss 2.6157 (2.6176)	Prec@(1,5) (53.9%, 84.5%)	
12/27 06:14:10午前 searchDistribution_trainer.py:166 [INFO] Train: [ 22/49] Final Prec@1 53.9320%
12/27 06:14:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.2473	Prec@(1,5) (42.4%, 74.3%)
12/27 06:14:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.2572	Prec@(1,5) (42.2%, 73.6%)
12/27 06:14:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.2712	Prec@(1,5) (42.0%, 73.1%)
12/27 06:14:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.2684	Prec@(1,5) (41.9%, 73.1%)
12/27 06:14:35午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 41.9040%
12/27 06:14:35午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:14:35午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 42.4880%
12/27 06:15:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.5046 (1.5123)	Arch Loss 1.7829 (2.0547)	Arch Hard Loss 1.7829 (2.0547)	Arch Beta Loss 2.6151 (2.6154)	Arch depth Loss 2.6151 (2.6154)	Prec@(1,5) (56.4%, 86.5%)	
12/27 06:16:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.6879 (1.5385)	Arch Loss 2.2573 (2.0515)	Arch Hard Loss 2.2573 (2.0515)	Arch Beta Loss 2.6145 (2.6150)	Arch depth Loss 2.6145 (2.6150)	Prec@(1,5) (55.7%, 86.1%)	
12/27 06:16:42午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.9316 (1.5664)	Arch Loss 1.9022 (2.0609)	Arch Hard Loss 1.9022 (2.0609)	Arch Beta Loss 2.6138 (2.6147)	Arch depth Loss 2.6138 (2.6147)	Prec@(1,5) (54.9%, 85.7%)	
12/27 06:17:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.6327 (1.5812)	Arch Loss 2.2020 (2.0526)	Arch Hard Loss 2.2020 (2.0526)	Arch Beta Loss 2.6128 (2.6144)	Arch depth Loss 2.6128 (2.6144)	Prec@(1,5) (54.7%, 85.3%)	
12/27 06:17:21午前 searchDistribution_trainer.py:166 [INFO] Train: [ 23/49] Final Prec@1 54.7120%
12/27 06:17:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.2452	Prec@(1,5) (42.7%, 73.6%)
12/27 06:17:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.2490	Prec@(1,5) (42.0%, 73.6%)
12/27 06:17:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.2508	Prec@(1,5) (42.0%, 73.6%)
12/27 06:17:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.2433	Prec@(1,5) (42.1%, 73.8%)
12/27 06:17:45午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 42.0400%
12/27 06:17:45午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:17:45午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 42.4880%
12/27 06:18:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.1011 (1.4454)	Arch Loss 1.8816 (2.0257)	Arch Hard Loss 1.8816 (2.0257)	Arch Beta Loss 2.6120 (2.6124)	Arch depth Loss 2.6120 (2.6124)	Prec@(1,5) (57.6%, 88.1%)	
12/27 06:19:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.5572 (1.4675)	Arch Loss 1.9184 (2.0357)	Arch Hard Loss 1.9184 (2.0357)	Arch Beta Loss 2.6112 (2.6120)	Arch depth Loss 2.6112 (2.6120)	Prec@(1,5) (57.3%, 87.3%)	
12/27 06:19:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.8206 (1.4984)	Arch Loss 2.1851 (2.0498)	Arch Hard Loss 2.1851 (2.0498)	Arch Beta Loss 2.6107 (2.6116)	Arch depth Loss 2.6107 (2.6116)	Prec@(1,5) (56.8%, 86.7%)	
12/27 06:20:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.6342 (1.5122)	Arch Loss 2.1106 (2.0499)	Arch Hard Loss 2.1106 (2.0499)	Arch Beta Loss 2.6101 (2.6113)	Arch depth Loss 2.6101 (2.6113)	Prec@(1,5) (56.6%, 86.3%)	
12/27 06:20:32午前 searchDistribution_trainer.py:166 [INFO] Train: [ 24/49] Final Prec@1 56.5800%
12/27 06:20:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.2410	Prec@(1,5) (43.0%, 74.7%)
12/27 06:20:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.2380	Prec@(1,5) (42.5%, 74.6%)
12/27 06:20:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.2442	Prec@(1,5) (42.6%, 74.7%)
12/27 06:20:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.2429	Prec@(1,5) (42.6%, 74.6%)
12/27 06:20:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 42.5640%
12/27 06:20:56午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:20:57午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 42.5640%
12/27 06:21:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.3247 (1.4030)	Arch Loss 2.0008 (2.0317)	Arch Hard Loss 2.0008 (2.0317)	Arch Beta Loss 2.6094 (2.6096)	Arch depth Loss 2.6094 (2.6096)	Prec@(1,5) (59.4%, 88.4%)	
12/27 06:22:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.5532 (1.4313)	Arch Loss 1.7624 (2.0251)	Arch Hard Loss 1.7624 (2.0251)	Arch Beta Loss 2.6086 (2.6093)	Arch depth Loss 2.6086 (2.6093)	Prec@(1,5) (58.4%, 87.8%)	
12/27 06:23:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.3833 (1.4505)	Arch Loss 1.8604 (2.0130)	Arch Hard Loss 1.8604 (2.0130)	Arch Beta Loss 2.6079 (2.6089)	Arch depth Loss 2.6079 (2.6089)	Prec@(1,5) (58.1%, 87.4%)	
12/27 06:23:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.5770 (1.4631)	Arch Loss 2.1151 (2.0226)	Arch Hard Loss 2.1151 (2.0226)	Arch Beta Loss 2.6074 (2.6086)	Arch depth Loss 2.6074 (2.6086)	Prec@(1,5) (57.8%, 87.3%)	
12/27 06:23:42午前 searchDistribution_trainer.py:166 [INFO] Train: [ 25/49] Final Prec@1 57.8160%
12/27 06:23:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.1028	Prec@(1,5) (45.7%, 77.1%)
12/27 06:23:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.0826	Prec@(1,5) (45.6%, 77.5%)
12/27 06:24:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.0717	Prec@(1,5) (45.9%, 77.4%)
12/27 06:24:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.0632	Prec@(1,5) (46.0%, 77.3%)
12/27 06:24:06午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 46.0680%
12/27 06:24:06午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:24:06午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.0680%
12/27 06:24:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.6721 (1.3603)	Arch Loss 1.9262 (2.0253)	Arch Hard Loss 1.9262 (2.0253)	Arch Beta Loss 2.6069 (2.6071)	Arch depth Loss 2.6069 (2.6071)	Prec@(1,5) (60.3%, 89.0%)	
12/27 06:25:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.3665 (1.3697)	Arch Loss 2.1590 (2.0421)	Arch Hard Loss 2.1590 (2.0421)	Arch Beta Loss 2.6063 (2.6069)	Arch depth Loss 2.6063 (2.6069)	Prec@(1,5) (59.9%, 88.9%)	
12/27 06:26:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.2530 (1.3848)	Arch Loss 1.9889 (2.0171)	Arch Hard Loss 1.9889 (2.0171)	Arch Beta Loss 2.6056 (2.6066)	Arch depth Loss 2.6056 (2.6066)	Prec@(1,5) (59.6%, 88.5%)	
12/27 06:26:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.6598 (1.4050)	Arch Loss 2.0276 (2.0261)	Arch Hard Loss 2.0276 (2.0261)	Arch Beta Loss 2.6052 (2.6063)	Arch depth Loss 2.6052 (2.6063)	Prec@(1,5) (59.3%, 88.1%)	
12/27 06:26:52午前 searchDistribution_trainer.py:166 [INFO] Train: [ 26/49] Final Prec@1 59.3120%
12/27 06:26:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.3008	Prec@(1,5) (42.0%, 73.1%)
12/27 06:27:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.2810	Prec@(1,5) (42.3%, 74.0%)
12/27 06:27:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.2721	Prec@(1,5) (42.8%, 74.1%)
12/27 06:27:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.2755	Prec@(1,5) (42.8%, 74.0%)
12/27 06:27:16午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 42.8360%
12/27 06:27:16午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:27:16午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.0680%
12/27 06:27:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.3930 (1.2912)	Arch Loss 2.0812 (2.0042)	Arch Hard Loss 2.0812 (2.0042)	Arch Beta Loss 2.6048 (2.6051)	Arch depth Loss 2.6048 (2.6051)	Prec@(1,5) (61.2%, 90.0%)	
12/27 06:28:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.3709 (1.3294)	Arch Loss 2.1019 (2.0074)	Arch Hard Loss 2.1019 (2.0074)	Arch Beta Loss 2.6041 (2.6047)	Arch depth Loss 2.6041 (2.6047)	Prec@(1,5) (60.9%, 89.5%)	
12/27 06:29:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.7702 (1.3428)	Arch Loss 1.2750 (1.9864)	Arch Hard Loss 1.2750 (1.9864)	Arch Beta Loss 2.6034 (2.6044)	Arch depth Loss 2.6034 (2.6044)	Prec@(1,5) (60.7%, 89.2%)	
12/27 06:29:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.1679 (1.3610)	Arch Loss 2.3700 (1.9902)	Arch Hard Loss 2.3700 (1.9902)	Arch Beta Loss 2.6027 (2.6041)	Arch depth Loss 2.6027 (2.6041)	Prec@(1,5) (60.4%, 88.9%)	
12/27 06:30:00午前 searchDistribution_trainer.py:166 [INFO] Train: [ 27/49] Final Prec@1 60.3720%
12/27 06:30:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.0377	Prec@(1,5) (46.9%, 77.6%)
12/27 06:30:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.0702	Prec@(1,5) (46.5%, 76.9%)
12/27 06:30:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.0732	Prec@(1,5) (46.5%, 77.0%)
12/27 06:30:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.0773	Prec@(1,5) (46.4%, 76.9%)
12/27 06:30:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 46.3840%
12/27 06:30:24午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:30:25午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.3840%
12/27 06:31:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.3424 (1.2779)	Arch Loss 2.0960 (1.9801)	Arch Hard Loss 2.0960 (1.9801)	Arch Beta Loss 2.6023 (2.6025)	Arch depth Loss 2.6023 (2.6025)	Prec@(1,5) (62.7%, 90.4%)	
12/27 06:31:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.6938 (1.2908)	Arch Loss 1.9093 (1.9931)	Arch Hard Loss 1.9093 (1.9931)	Arch Beta Loss 2.6015 (2.6022)	Arch depth Loss 2.6015 (2.6022)	Prec@(1,5) (62.3%, 89.9%)	
12/27 06:32:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.2354 (1.2996)	Arch Loss 2.2064 (2.0029)	Arch Hard Loss 2.2064 (2.0029)	Arch Beta Loss 2.6012 (2.6019)	Arch depth Loss 2.6012 (2.6019)	Prec@(1,5) (62.3%, 89.9%)	
12/27 06:33:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.6684 (1.3102)	Arch Loss 1.6558 (1.9890)	Arch Hard Loss 1.6558 (1.9890)	Arch Beta Loss 2.6007 (2.6017)	Arch depth Loss 2.6007 (2.6017)	Prec@(1,5) (61.9%, 89.7%)	
12/27 06:33:10午前 searchDistribution_trainer.py:166 [INFO] Train: [ 28/49] Final Prec@1 61.9200%
12/27 06:33:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.6609	Prec@(1,5) (38.0%, 66.8%)
12/27 06:33:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.6295	Prec@(1,5) (38.1%, 67.7%)
12/27 06:33:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.6359	Prec@(1,5) (37.9%, 67.6%)
12/27 06:33:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.6371	Prec@(1,5) (37.9%, 67.6%)
12/27 06:33:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 37.8600%
12/27 06:33:34午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:33:34午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.3840%
12/27 06:34:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.1162 (1.1847)	Arch Loss 1.8235 (1.9780)	Arch Hard Loss 1.8235 (1.9780)	Arch Beta Loss 2.5999 (2.6003)	Arch depth Loss 2.5999 (2.6003)	Prec@(1,5) (65.9%, 91.3%)	
12/27 06:34:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.9988 (1.2073)	Arch Loss 1.6328 (1.9525)	Arch Hard Loss 1.6328 (1.9525)	Arch Beta Loss 2.5993 (2.6000)	Arch depth Loss 2.5993 (2.6000)	Prec@(1,5) (64.7%, 90.9%)	
12/27 06:35:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.3124 (1.2401)	Arch Loss 2.5788 (1.9609)	Arch Hard Loss 2.5788 (1.9609)	Arch Beta Loss 2.5989 (2.5997)	Arch depth Loss 2.5989 (2.5997)	Prec@(1,5) (63.7%, 90.4%)	
12/27 06:36:19午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.2917 (1.2494)	Arch Loss 1.8787 (1.9731)	Arch Hard Loss 1.8787 (1.9731)	Arch Beta Loss 2.5984 (2.5995)	Arch depth Loss 2.5984 (2.5995)	Prec@(1,5) (63.3%, 90.4%)	
12/27 06:36:19午前 searchDistribution_trainer.py:166 [INFO] Train: [ 29/49] Final Prec@1 63.2680%
12/27 06:36:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.7437	Prec@(1,5) (37.2%, 67.8%)
12/27 06:36:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.7990	Prec@(1,5) (36.2%, 67.1%)
12/27 06:36:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.8195	Prec@(1,5) (35.8%, 66.6%)
12/27 06:36:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.8151	Prec@(1,5) (36.0%, 66.6%)
12/27 06:36:43午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 35.9640%
12/27 06:36:43午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:36:43午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.3840%
12/27 06:37:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.2941 (1.1434)	Arch Loss 2.0790 (2.0059)	Arch Hard Loss 2.0790 (2.0059)	Arch Beta Loss 2.5980 (2.5983)	Arch depth Loss 2.5980 (2.5983)	Prec@(1,5) (66.2%, 91.7%)	
12/27 06:38:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.0804 (1.1754)	Arch Loss 2.1273 (1.9949)	Arch Hard Loss 2.1273 (1.9949)	Arch Beta Loss 2.5974 (2.5980)	Arch depth Loss 2.5974 (2.5980)	Prec@(1,5) (65.0%, 91.3%)	
12/27 06:38:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.0761 (1.1834)	Arch Loss 2.1703 (1.9884)	Arch Hard Loss 2.1703 (1.9884)	Arch Beta Loss 2.5969 (2.5977)	Arch depth Loss 2.5969 (2.5977)	Prec@(1,5) (65.0%, 91.2%)	
12/27 06:39:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.3441 (1.2065)	Arch Loss 1.8795 (1.9791)	Arch Hard Loss 1.8795 (1.9791)	Arch Beta Loss 2.5965 (2.5975)	Arch depth Loss 2.5965 (2.5975)	Prec@(1,5) (64.4%, 90.9%)	
12/27 06:39:29午前 searchDistribution_trainer.py:166 [INFO] Train: [ 30/49] Final Prec@1 64.3480%
12/27 06:39:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.1736	Prec@(1,5) (46.1%, 76.3%)
12/27 06:39:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.1874	Prec@(1,5) (45.4%, 76.3%)
12/27 06:39:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.1964	Prec@(1,5) (45.2%, 75.9%)
12/27 06:39:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.1933	Prec@(1,5) (45.2%, 75.9%)
12/27 06:39:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 45.1520%
12/27 06:39:53午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:39:53午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.3840%
12/27 06:40:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.5127 (1.0760)	Arch Loss 2.1438 (1.9184)	Arch Hard Loss 2.1438 (1.9184)	Arch Beta Loss 2.5961 (2.5963)	Arch depth Loss 2.5961 (2.5963)	Prec@(1,5) (68.4%, 92.7%)	
12/27 06:41:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.1600 (1.1073)	Arch Loss 1.7411 (1.9633)	Arch Hard Loss 1.7411 (1.9633)	Arch Beta Loss 2.5958 (2.5961)	Arch depth Loss 2.5958 (2.5961)	Prec@(1,5) (67.0%, 92.4%)	
12/27 06:41:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.9613 (1.1266)	Arch Loss 1.9015 (1.9764)	Arch Hard Loss 1.9015 (1.9764)	Arch Beta Loss 2.5954 (2.5959)	Arch depth Loss 2.5954 (2.5959)	Prec@(1,5) (66.5%, 92.2%)	
12/27 06:42:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.0221 (1.1470)	Arch Loss 2.1055 (1.9734)	Arch Hard Loss 2.1055 (1.9734)	Arch Beta Loss 2.5949 (2.5957)	Arch depth Loss 2.5949 (2.5957)	Prec@(1,5) (66.0%, 92.0%)	
12/27 06:42:38午前 searchDistribution_trainer.py:166 [INFO] Train: [ 31/49] Final Prec@1 66.0160%
12/27 06:42:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.3789	Prec@(1,5) (43.1%, 73.2%)
12/27 06:42:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.3914	Prec@(1,5) (42.3%, 72.9%)
12/27 06:42:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.3872	Prec@(1,5) (42.5%, 73.1%)
12/27 06:43:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.3887	Prec@(1,5) (42.5%, 73.1%)
12/27 06:43:02午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 42.4720%
12/27 06:43:02午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:43:02午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.3840%
12/27 06:43:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.0817 (1.0654)	Arch Loss 2.2684 (1.9411)	Arch Hard Loss 2.2684 (1.9411)	Arch Beta Loss 2.5947 (2.5948)	Arch depth Loss 2.5947 (2.5948)	Prec@(1,5) (68.5%, 92.4%)	
12/27 06:44:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.4576 (1.0773)	Arch Loss 1.8945 (1.9776)	Arch Hard Loss 1.8945 (1.9776)	Arch Beta Loss 2.5941 (2.5946)	Arch depth Loss 2.5941 (2.5946)	Prec@(1,5) (68.1%, 92.4%)	
12/27 06:45:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.9249 (1.0896)	Arch Loss 2.2082 (1.9749)	Arch Hard Loss 2.2082 (1.9749)	Arch Beta Loss 2.5934 (2.5943)	Arch depth Loss 2.5934 (2.5943)	Prec@(1,5) (67.6%, 92.3%)	
12/27 06:45:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.1081 (1.1037)	Arch Loss 2.1941 (1.9743)	Arch Hard Loss 2.1941 (1.9743)	Arch Beta Loss 2.5931 (2.5941)	Arch depth Loss 2.5931 (2.5941)	Prec@(1,5) (67.2%, 92.2%)	
12/27 06:45:45午前 searchDistribution_trainer.py:166 [INFO] Train: [ 32/49] Final Prec@1 67.1520%
12/27 06:45:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.1411	Prec@(1,5) (47.7%, 77.2%)
12/27 06:45:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.1341	Prec@(1,5) (47.4%, 77.4%)
12/27 06:46:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.1264	Prec@(1,5) (47.5%, 77.4%)
12/27 06:46:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.1099	Prec@(1,5) (47.9%, 77.6%)
12/27 06:46:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 47.8960%
12/27 06:46:09午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:46:10午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 06:46:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.2014 (0.9775)	Arch Loss 2.3968 (1.9532)	Arch Hard Loss 2.3968 (1.9532)	Arch Beta Loss 2.5926 (2.5927)	Arch depth Loss 2.5926 (2.5927)	Prec@(1,5) (70.6%, 94.0%)	
12/27 06:47:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.6997 (1.0116)	Arch Loss 1.9691 (1.9786)	Arch Hard Loss 1.9691 (1.9786)	Arch Beta Loss 2.5922 (2.5925)	Arch depth Loss 2.5922 (2.5925)	Prec@(1,5) (69.7%, 93.6%)	
12/27 06:48:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.9378 (1.0297)	Arch Loss 1.5933 (1.9780)	Arch Hard Loss 1.5933 (1.9780)	Arch Beta Loss 2.5919 (2.5924)	Arch depth Loss 2.5919 (2.5924)	Prec@(1,5) (69.1%, 93.3%)	
12/27 06:48:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.0511 (1.0402)	Arch Loss 2.1705 (1.9764)	Arch Hard Loss 2.1705 (1.9764)	Arch Beta Loss 2.5913 (2.5922)	Arch depth Loss 2.5913 (2.5922)	Prec@(1,5) (68.7%, 93.1%)	
12/27 06:48:54午前 searchDistribution_trainer.py:166 [INFO] Train: [ 33/49] Final Prec@1 68.6920%
12/27 06:49:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.2855	Prec@(1,5) (44.2%, 74.9%)
12/27 06:49:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.2887	Prec@(1,5) (44.5%, 75.0%)
12/27 06:49:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.2789	Prec@(1,5) (44.5%, 75.3%)
12/27 06:49:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.2856	Prec@(1,5) (44.6%, 75.3%)
12/27 06:49:18午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 44.6040%
12/27 06:49:18午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:49:19午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 06:50:01午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.9924 (0.9326)	Arch Loss 1.8760 (1.9649)	Arch Hard Loss 1.8760 (1.9649)	Arch Beta Loss 2.5909 (2.5910)	Arch depth Loss 2.5909 (2.5910)	Prec@(1,5) (72.2%, 94.6%)	
12/27 06:50:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 1.2108 (0.9499)	Arch Loss 1.9687 (1.9806)	Arch Hard Loss 1.9687 (1.9806)	Arch Beta Loss 2.5906 (2.5909)	Arch depth Loss 2.5906 (2.5909)	Prec@(1,5) (71.7%, 94.3%)	
12/27 06:51:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.9515 (0.9702)	Arch Loss 1.8642 (1.9697)	Arch Hard Loss 1.8642 (1.9697)	Arch Beta Loss 2.5905 (2.5908)	Arch depth Loss 2.5905 (2.5908)	Prec@(1,5) (71.0%, 93.9%)	
12/27 06:52:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.3703 (0.9858)	Arch Loss 2.0622 (1.9718)	Arch Hard Loss 2.0622 (1.9718)	Arch Beta Loss 2.5903 (2.5907)	Arch depth Loss 2.5903 (2.5907)	Prec@(1,5) (70.5%, 93.7%)	
12/27 06:52:03午前 searchDistribution_trainer.py:166 [INFO] Train: [ 34/49] Final Prec@1 70.5240%
12/27 06:52:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.7633	Prec@(1,5) (38.6%, 67.2%)
12/27 06:52:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.7464	Prec@(1,5) (38.2%, 67.9%)
12/27 06:52:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.7685	Prec@(1,5) (38.1%, 67.7%)
12/27 06:52:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.7570	Prec@(1,5) (38.2%, 67.7%)
12/27 06:52:27午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 38.2120%
12/27 06:52:27午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:52:28午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 06:53:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.7908 (0.8589)	Arch Loss 2.1309 (1.9725)	Arch Hard Loss 2.1309 (1.9725)	Arch Beta Loss 2.5902 (2.5902)	Arch depth Loss 2.5902 (2.5902)	Prec@(1,5) (73.6%, 95.2%)	
12/27 06:53:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.1439 (0.8949)	Arch Loss 1.7877 (1.9651)	Arch Hard Loss 1.7877 (1.9651)	Arch Beta Loss 2.5900 (2.5901)	Arch depth Loss 2.5900 (2.5901)	Prec@(1,5) (72.8%, 94.8%)	
12/27 06:54:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.0655 (0.9168)	Arch Loss 2.4027 (1.9796)	Arch Hard Loss 2.4027 (1.9796)	Arch Beta Loss 2.5896 (2.5900)	Arch depth Loss 2.5896 (2.5900)	Prec@(1,5) (72.2%, 94.4%)	
12/27 06:55:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.0106 (0.9270)	Arch Loss 1.6130 (1.9824)	Arch Hard Loss 1.6130 (1.9824)	Arch Beta Loss 2.5894 (2.5899)	Arch depth Loss 2.5894 (2.5899)	Prec@(1,5) (71.8%, 94.4%)	
12/27 06:55:13午前 searchDistribution_trainer.py:166 [INFO] Train: [ 35/49] Final Prec@1 71.8160%
12/27 06:55:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.8546	Prec@(1,5) (37.8%, 68.1%)
12/27 06:55:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.8184	Prec@(1,5) (38.4%, 68.6%)
12/27 06:55:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.8009	Prec@(1,5) (38.6%, 69.0%)
12/27 06:55:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.8196	Prec@(1,5) (38.3%, 68.6%)
12/27 06:55:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 38.3200%
12/27 06:55:38午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:55:38午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 06:56:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.7379 (0.8547)	Arch Loss 1.5570 (1.9766)	Arch Hard Loss 1.5570 (1.9766)	Arch Beta Loss 2.5892 (2.5894)	Arch depth Loss 2.5892 (2.5894)	Prec@(1,5) (74.0%, 95.4%)	
12/27 06:57:02午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 1.1639 (0.8693)	Arch Loss 1.9901 (1.9784)	Arch Hard Loss 1.9901 (1.9784)	Arch Beta Loss 2.5889 (2.5892)	Arch depth Loss 2.5889 (2.5892)	Prec@(1,5) (73.8%, 95.1%)	
12/27 06:57:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.7799 (0.8771)	Arch Loss 2.2727 (1.9589)	Arch Hard Loss 2.2727 (1.9589)	Arch Beta Loss 2.5887 (2.5891)	Arch depth Loss 2.5887 (2.5891)	Prec@(1,5) (73.6%, 95.0%)	
12/27 06:58:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.8594 (0.8806)	Arch Loss 2.1234 (1.9759)	Arch Hard Loss 2.1234 (1.9759)	Arch Beta Loss 2.5886 (2.5890)	Arch depth Loss 2.5886 (2.5890)	Prec@(1,5) (73.4%, 95.1%)	
12/27 06:58:23午前 searchDistribution_trainer.py:166 [INFO] Train: [ 36/49] Final Prec@1 73.3640%
12/27 06:58:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.5973	Prec@(1,5) (42.0%, 72.2%)
12/27 06:58:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.6063	Prec@(1,5) (41.7%, 71.8%)
12/27 06:58:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.6227	Prec@(1,5) (41.3%, 71.6%)
12/27 06:58:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.6283	Prec@(1,5) (41.2%, 71.5%)
12/27 06:58:47午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 41.1960%
12/27 06:58:47午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 06:58:48午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 06:59:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.9106 (0.7862)	Arch Loss 1.7205 (1.9698)	Arch Hard Loss 1.7205 (1.9698)	Arch Beta Loss 2.5883 (2.5885)	Arch depth Loss 2.5883 (2.5885)	Prec@(1,5) (76.8%, 95.7%)	
12/27 07:00:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.8849 (0.7986)	Arch Loss 2.0171 (1.9840)	Arch Hard Loss 2.0171 (1.9840)	Arch Beta Loss 2.5884 (2.5884)	Arch depth Loss 2.5884 (2.5884)	Prec@(1,5) (76.1%, 95.7%)	
12/27 07:00:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 1.0074 (0.8129)	Arch Loss 2.2075 (1.9826)	Arch Hard Loss 2.2075 (1.9826)	Arch Beta Loss 2.5882 (2.5884)	Arch depth Loss 2.5882 (2.5884)	Prec@(1,5) (75.6%, 95.6%)	
12/27 07:01:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.9724 (0.8236)	Arch Loss 1.8540 (1.9791)	Arch Hard Loss 1.8540 (1.9791)	Arch Beta Loss 2.5880 (2.5883)	Arch depth Loss 2.5880 (2.5883)	Prec@(1,5) (75.2%, 95.5%)	
12/27 07:01:32午前 searchDistribution_trainer.py:166 [INFO] Train: [ 37/49] Final Prec@1 75.2320%
12/27 07:01:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.2952	Prec@(1,5) (45.7%, 76.7%)
12/27 07:01:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.2573	Prec@(1,5) (46.7%, 77.2%)
12/27 07:01:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.2525	Prec@(1,5) (46.5%, 77.2%)
12/27 07:01:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.2469	Prec@(1,5) (46.6%, 77.3%)
12/27 07:01:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 46.6200%
12/27 07:01:56午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:01:56午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 07:02:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7200 (0.7491)	Arch Loss 1.6499 (1.9977)	Arch Hard Loss 1.6499 (1.9977)	Arch Beta Loss 2.5879 (2.5880)	Arch depth Loss 2.5879 (2.5880)	Prec@(1,5) (77.5%, 96.5%)	
12/27 07:03:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.9036 (0.7659)	Arch Loss 2.1421 (1.9827)	Arch Hard Loss 2.1421 (1.9827)	Arch Beta Loss 2.5876 (2.5878)	Arch depth Loss 2.5876 (2.5878)	Prec@(1,5) (76.9%, 96.1%)	
12/27 07:04:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.6110 (0.7740)	Arch Loss 2.4213 (1.9939)	Arch Hard Loss 2.4213 (1.9939)	Arch Beta Loss 2.5876 (2.5878)	Arch depth Loss 2.5876 (2.5878)	Prec@(1,5) (76.5%, 96.1%)	
12/27 07:04:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.9079 (0.7851)	Arch Loss 2.2460 (1.9900)	Arch Hard Loss 2.2460 (1.9900)	Arch Beta Loss 2.5877 (2.5878)	Arch depth Loss 2.5877 (2.5878)	Prec@(1,5) (76.2%, 96.0%)	
12/27 07:04:41午前 searchDistribution_trainer.py:166 [INFO] Train: [ 38/49] Final Prec@1 76.1960%
12/27 07:04:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.2183	Prec@(1,5) (47.6%, 77.0%)
12/27 07:04:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.2009	Prec@(1,5) (47.7%, 77.3%)
12/27 07:05:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.2131	Prec@(1,5) (47.3%, 77.3%)
12/27 07:05:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.2121	Prec@(1,5) (47.3%, 77.2%)
12/27 07:05:05午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 47.2360%
12/27 07:05:05午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:05:05午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 07:05:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.7599 (0.6976)	Arch Loss 1.6515 (2.0411)	Arch Hard Loss 1.6515 (2.0411)	Arch Beta Loss 2.5879 (2.5877)	Arch depth Loss 2.5879 (2.5877)	Prec@(1,5) (78.9%, 96.9%)	
12/27 07:06:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.5973 (0.7124)	Arch Loss 1.9995 (2.0113)	Arch Hard Loss 1.9995 (2.0113)	Arch Beta Loss 2.5875 (2.5877)	Arch depth Loss 2.5875 (2.5877)	Prec@(1,5) (78.4%, 96.7%)	
12/27 07:07:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 1.0030 (0.7235)	Arch Loss 1.7723 (1.9875)	Arch Hard Loss 1.7723 (1.9875)	Arch Beta Loss 2.5873 (2.5876)	Arch depth Loss 2.5873 (2.5876)	Prec@(1,5) (77.9%, 96.6%)	
12/27 07:07:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.5587 (0.7363)	Arch Loss 1.8245 (1.9822)	Arch Hard Loss 1.8245 (1.9822)	Arch Beta Loss 2.5872 (2.5875)	Arch depth Loss 2.5872 (2.5875)	Prec@(1,5) (77.6%, 96.5%)	
12/27 07:07:51午前 searchDistribution_trainer.py:166 [INFO] Train: [ 39/49] Final Prec@1 77.5960%
12/27 07:07:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.2888	Prec@(1,5) (46.0%, 76.5%)
12/27 07:08:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.3496	Prec@(1,5) (45.5%, 75.7%)
12/27 07:08:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.3477	Prec@(1,5) (45.6%, 75.9%)
12/27 07:08:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.3446	Prec@(1,5) (45.8%, 75.9%)
12/27 07:08:15午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 45.7640%
12/27 07:08:15午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:08:15午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 07:08:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.5781 (0.6411)	Arch Loss 2.3342 (1.9453)	Arch Hard Loss 2.3342 (1.9453)	Arch Beta Loss 2.5871 (2.5872)	Arch depth Loss 2.5871 (2.5872)	Prec@(1,5) (80.8%, 97.9%)	
12/27 07:09:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.4956 (0.6706)	Arch Loss 2.0513 (1.9637)	Arch Hard Loss 2.0513 (1.9637)	Arch Beta Loss 2.5871 (2.5872)	Arch depth Loss 2.5871 (2.5872)	Prec@(1,5) (79.8%, 97.3%)	
12/27 07:10:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.7427 (0.6861)	Arch Loss 1.8445 (1.9834)	Arch Hard Loss 1.8445 (1.9834)	Arch Beta Loss 2.5870 (2.5871)	Arch depth Loss 2.5870 (2.5871)	Prec@(1,5) (79.2%, 97.1%)	
12/27 07:10:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.6365 (0.6928)	Arch Loss 1.7880 (1.9780)	Arch Hard Loss 1.7880 (1.9780)	Arch Beta Loss 2.5869 (2.5871)	Arch depth Loss 2.5869 (2.5871)	Prec@(1,5) (79.0%, 97.0%)	
12/27 07:10:59午前 searchDistribution_trainer.py:166 [INFO] Train: [ 40/49] Final Prec@1 78.9920%
12/27 07:11:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 2.3529	Prec@(1,5) (45.7%, 75.7%)
12/27 07:11:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 2.3161	Prec@(1,5) (46.4%, 76.0%)
12/27 07:11:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 2.3163	Prec@(1,5) (46.6%, 76.1%)
12/27 07:11:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 2.3258	Prec@(1,5) (46.5%, 75.9%)
12/27 07:11:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 46.5200%
12/27 07:11:23午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:11:24午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.8960%
12/27 07:12:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.4973 (0.6204)	Arch Loss 1.4963 (1.9898)	Arch Hard Loss 1.4963 (1.9898)	Arch Beta Loss 2.5867 (2.5867)	Arch depth Loss 2.5867 (2.5867)	Prec@(1,5) (81.4%, 97.7%)	
12/27 07:12:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.6802 (0.6280)	Arch Loss 1.7427 (1.9719)	Arch Hard Loss 1.7427 (1.9719)	Arch Beta Loss 2.5866 (2.5867)	Arch depth Loss 2.5866 (2.5867)	Prec@(1,5) (81.3%, 97.6%)	
12/27 07:13:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.4518 (0.6393)	Arch Loss 1.8112 (1.9957)	Arch Hard Loss 1.8112 (1.9957)	Arch Beta Loss 2.5867 (2.5867)	Arch depth Loss 2.5867 (2.5867)	Prec@(1,5) (80.9%, 97.4%)	
12/27 07:14:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.5457 (0.6509)	Arch Loss 1.8539 (1.9851)	Arch Hard Loss 1.8539 (1.9851)	Arch Beta Loss 2.5867 (2.5867)	Arch depth Loss 2.5867 (2.5867)	Prec@(1,5) (80.4%, 97.4%)	
12/27 07:14:08午前 searchDistribution_trainer.py:166 [INFO] Train: [ 41/49] Final Prec@1 80.3680%
12/27 07:14:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.1930	Prec@(1,5) (48.5%, 78.6%)
12/27 07:14:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.1855	Prec@(1,5) (48.8%, 78.5%)
12/27 07:14:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.1748	Prec@(1,5) (49.1%, 78.4%)
12/27 07:14:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.1907	Prec@(1,5) (48.8%, 78.2%)
12/27 07:14:33午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 48.8440%
12/27 07:14:33午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:14:34午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.8440%
12/27 07:15:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.7219 (0.5968)	Arch Loss 1.9074 (1.9857)	Arch Hard Loss 1.9074 (1.9857)	Arch Beta Loss 2.5866 (2.5866)	Arch depth Loss 2.5866 (2.5866)	Prec@(1,5) (81.9%, 98.0%)	
12/27 07:15:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.6943 (0.5982)	Arch Loss 1.7762 (1.9969)	Arch Hard Loss 1.7762 (1.9969)	Arch Beta Loss 2.5865 (2.5866)	Arch depth Loss 2.5865 (2.5866)	Prec@(1,5) (82.0%, 98.0%)	
12/27 07:16:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.8964 (0.6155)	Arch Loss 1.6704 (1.9823)	Arch Hard Loss 1.6704 (1.9823)	Arch Beta Loss 2.5862 (2.5865)	Arch depth Loss 2.5862 (2.5865)	Prec@(1,5) (81.5%, 97.7%)	
12/27 07:17:18午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.7207 (0.6242)	Arch Loss 1.9352 (1.9845)	Arch Hard Loss 1.9352 (1.9845)	Arch Beta Loss 2.5865 (2.5864)	Arch depth Loss 2.5865 (2.5864)	Prec@(1,5) (81.3%, 97.6%)	
12/27 07:17:18午前 searchDistribution_trainer.py:166 [INFO] Train: [ 42/49] Final Prec@1 81.2360%
12/27 07:17:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.1106	Prec@(1,5) (50.8%, 79.6%)
12/27 07:17:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.0943	Prec@(1,5) (50.8%, 79.9%)
12/27 07:17:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.0530	Prec@(1,5) (51.3%, 80.3%)
12/27 07:17:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.0445	Prec@(1,5) (51.4%, 80.4%)
12/27 07:17:43午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 51.3880%
12/27 07:17:43午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:17:43午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.3880%
12/27 07:18:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.5242 (0.5755)	Arch Loss 1.8477 (1.9745)	Arch Hard Loss 1.8477 (1.9745)	Arch Beta Loss 2.5864 (2.5864)	Arch depth Loss 2.5864 (2.5864)	Prec@(1,5) (82.9%, 97.9%)	
12/27 07:19:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.5574 (0.5786)	Arch Loss 2.2638 (1.9807)	Arch Hard Loss 2.2638 (1.9807)	Arch Beta Loss 2.5864 (2.5864)	Arch depth Loss 2.5864 (2.5864)	Prec@(1,5) (83.1%, 97.9%)	
12/27 07:19:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.6537 (0.5869)	Arch Loss 2.1067 (1.9710)	Arch Hard Loss 2.1067 (1.9710)	Arch Beta Loss 2.5863 (2.5864)	Arch depth Loss 2.5863 (2.5864)	Prec@(1,5) (82.6%, 97.9%)	
12/27 07:20:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.4219 (0.5943)	Arch Loss 1.5388 (1.9807)	Arch Hard Loss 1.5388 (1.9807)	Arch Beta Loss 2.5861 (2.5864)	Arch depth Loss 2.5861 (2.5864)	Prec@(1,5) (82.4%, 97.8%)	
12/27 07:20:26午前 searchDistribution_trainer.py:166 [INFO] Train: [ 43/49] Final Prec@1 82.3520%
12/27 07:20:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 2.4198	Prec@(1,5) (44.3%, 75.4%)
12/27 07:20:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.4232	Prec@(1,5) (44.6%, 75.0%)
12/27 07:20:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.4682	Prec@(1,5) (44.1%, 74.6%)
12/27 07:20:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.4638	Prec@(1,5) (44.3%, 74.7%)
12/27 07:20:52午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 44.3200%
12/27 07:20:52午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:20:52午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.3880%
12/27 07:21:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.7666 (0.5246)	Arch Loss 1.4766 (2.0032)	Arch Hard Loss 1.4766 (2.0032)	Arch Beta Loss 2.5865 (2.5862)	Arch depth Loss 2.5865 (2.5862)	Prec@(1,5) (84.2%, 98.3%)	
12/27 07:22:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.4501 (0.5425)	Arch Loss 2.5593 (1.9883)	Arch Hard Loss 2.5593 (1.9883)	Arch Beta Loss 2.5865 (2.5864)	Arch depth Loss 2.5865 (2.5864)	Prec@(1,5) (83.8%, 98.2%)	
12/27 07:22:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.6464 (0.5568)	Arch Loss 1.9538 (1.9897)	Arch Hard Loss 1.9538 (1.9897)	Arch Beta Loss 2.5865 (2.5864)	Arch depth Loss 2.5865 (2.5864)	Prec@(1,5) (83.1%, 98.1%)	
12/27 07:23:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.6905 (0.5665)	Arch Loss 1.9135 (1.9836)	Arch Hard Loss 1.9135 (1.9836)	Arch Beta Loss 2.5864 (2.5864)	Arch depth Loss 2.5864 (2.5864)	Prec@(1,5) (83.0%, 98.0%)	
12/27 07:23:37午前 searchDistribution_trainer.py:166 [INFO] Train: [ 44/49] Final Prec@1 82.9920%
12/27 07:23:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.0388	Prec@(1,5) (52.0%, 80.1%)
12/27 07:23:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.0410	Prec@(1,5) (51.7%, 80.3%)
12/27 07:23:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.0494	Prec@(1,5) (51.4%, 80.3%)
12/27 07:24:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.0401	Prec@(1,5) (51.6%, 80.4%)
12/27 07:24:02午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 51.6280%
12/27 07:24:02午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:24:02午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.6280%
12/27 07:24:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.4630 (0.5202)	Arch Loss 2.1862 (1.9745)	Arch Hard Loss 2.1862 (1.9745)	Arch Beta Loss 2.5864 (2.5864)	Arch depth Loss 2.5864 (2.5864)	Prec@(1,5) (84.7%, 98.3%)	
12/27 07:25:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.5026 (0.5337)	Arch Loss 2.1568 (1.9734)	Arch Hard Loss 2.1568 (1.9734)	Arch Beta Loss 2.5865 (2.5865)	Arch depth Loss 2.5865 (2.5865)	Prec@(1,5) (84.3%, 98.3%)	
12/27 07:26:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.4512 (0.5365)	Arch Loss 1.6778 (1.9753)	Arch Hard Loss 1.6778 (1.9753)	Arch Beta Loss 2.5866 (2.5865)	Arch depth Loss 2.5866 (2.5865)	Prec@(1,5) (84.3%, 98.3%)	
12/27 07:26:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.5587 (0.5446)	Arch Loss 1.8336 (1.9760)	Arch Hard Loss 1.8336 (1.9760)	Arch Beta Loss 2.5866 (2.5865)	Arch depth Loss 2.5866 (2.5865)	Prec@(1,5) (84.0%, 98.3%)	
12/27 07:26:46午前 searchDistribution_trainer.py:166 [INFO] Train: [ 45/49] Final Prec@1 84.0160%
12/27 07:26:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 2.0376	Prec@(1,5) (52.1%, 80.2%)
12/27 07:26:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 2.0483	Prec@(1,5) (52.2%, 80.1%)
12/27 07:27:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 2.0707	Prec@(1,5) (51.7%, 80.0%)
12/27 07:27:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.0922	Prec@(1,5) (51.2%, 79.6%)
12/27 07:27:10午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 51.2520%
12/27 07:27:10午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:27:10午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.6280%
12/27 07:27:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.4545 (0.5182)	Arch Loss 1.8147 (1.9570)	Arch Hard Loss 1.8147 (1.9570)	Arch Beta Loss 2.5868 (2.5867)	Arch depth Loss 2.5868 (2.5867)	Prec@(1,5) (84.8%, 98.5%)	
12/27 07:28:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.5060 (0.5208)	Arch Loss 1.9051 (1.9524)	Arch Hard Loss 1.9051 (1.9524)	Arch Beta Loss 2.5869 (2.5868)	Arch depth Loss 2.5869 (2.5868)	Prec@(1,5) (85.0%, 98.4%)	
12/27 07:29:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.4110 (0.5242)	Arch Loss 2.5040 (1.9865)	Arch Hard Loss 2.5040 (1.9865)	Arch Beta Loss 2.5868 (2.5868)	Arch depth Loss 2.5868 (2.5868)	Prec@(1,5) (84.7%, 98.4%)	
12/27 07:29:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.4719 (0.5290)	Arch Loss 1.9386 (1.9849)	Arch Hard Loss 1.9386 (1.9849)	Arch Beta Loss 2.5870 (2.5869)	Arch depth Loss 2.5870 (2.5869)	Prec@(1,5) (84.6%, 98.4%)	
12/27 07:29:56午前 searchDistribution_trainer.py:166 [INFO] Train: [ 46/49] Final Prec@1 84.5960%
12/27 07:30:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 1.9768	Prec@(1,5) (52.3%, 81.3%)
12/27 07:30:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 1.9624	Prec@(1,5) (52.5%, 81.5%)
12/27 07:30:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 1.9564	Prec@(1,5) (52.9%, 81.6%)
12/27 07:30:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 1.9593	Prec@(1,5) (52.6%, 81.5%)
12/27 07:30:20午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 52.5360%
12/27 07:30:20午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:30:20午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 52.5360%
12/27 07:31:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.9991 (0.4921)	Arch Loss 1.8362 (1.9854)	Arch Hard Loss 1.8362 (1.9854)	Arch Beta Loss 2.5869 (2.5870)	Arch depth Loss 2.5869 (2.5870)	Prec@(1,5) (86.0%, 98.5%)	
12/27 07:31:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.8506 (0.5086)	Arch Loss 2.8881 (1.9878)	Arch Hard Loss 2.8881 (1.9878)	Arch Beta Loss 2.5870 (2.5870)	Arch depth Loss 2.5870 (2.5870)	Prec@(1,5) (85.6%, 98.4%)	
12/27 07:32:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.5629 (0.5147)	Arch Loss 1.9855 (1.9854)	Arch Hard Loss 1.9855 (1.9854)	Arch Beta Loss 2.5868 (2.5870)	Arch depth Loss 2.5868 (2.5870)	Prec@(1,5) (85.4%, 98.4%)	
12/27 07:33:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.4632 (0.5234)	Arch Loss 1.8878 (1.9802)	Arch Hard Loss 1.8878 (1.9802)	Arch Beta Loss 2.5867 (2.5869)	Arch depth Loss 2.5867 (2.5869)	Prec@(1,5) (84.9%, 98.3%)	
12/27 07:33:06午前 searchDistribution_trainer.py:166 [INFO] Train: [ 47/49] Final Prec@1 84.9280%
12/27 07:33:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.0600	Prec@(1,5) (51.8%, 79.7%)
12/27 07:33:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.0346	Prec@(1,5) (51.9%, 80.1%)
12/27 07:33:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.0151	Prec@(1,5) (52.0%, 80.6%)
12/27 07:33:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.0142	Prec@(1,5) (52.2%, 80.6%)
12/27 07:33:30午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 52.1600%
12/27 07:33:30午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:33:30午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 52.5360%
12/27 07:34:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.3779 (0.4987)	Arch Loss 1.9974 (1.9832)	Arch Hard Loss 1.9974 (1.9832)	Arch Beta Loss 2.5864 (2.5865)	Arch depth Loss 2.5864 (2.5865)	Prec@(1,5) (85.4%, 98.7%)	
12/27 07:34:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.5406 (0.5100)	Arch Loss 1.9478 (1.9914)	Arch Hard Loss 1.9478 (1.9914)	Arch Beta Loss 2.5863 (2.5864)	Arch depth Loss 2.5863 (2.5864)	Prec@(1,5) (85.0%, 98.6%)	
12/27 07:35:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.4241 (0.5141)	Arch Loss 2.1140 (1.9721)	Arch Hard Loss 2.1140 (1.9721)	Arch Beta Loss 2.5859 (2.5863)	Arch depth Loss 2.5859 (2.5863)	Prec@(1,5) (85.0%, 98.5%)	
12/27 07:36:14午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.7042 (0.5199)	Arch Loss 1.6854 (1.9715)	Arch Hard Loss 1.6854 (1.9715)	Arch Beta Loss 2.5860 (2.5862)	Arch depth Loss 2.5860 (2.5862)	Prec@(1,5) (84.8%, 98.4%)	
12/27 07:36:14午前 searchDistribution_trainer.py:166 [INFO] Train: [ 48/49] Final Prec@1 84.7920%
12/27 07:36:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.0416	Prec@(1,5) (51.3%, 80.6%)
12/27 07:36:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 1.9937	Prec@(1,5) (52.2%, 80.9%)
12/27 07:36:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 1.9834	Prec@(1,5) (52.6%, 81.1%)
12/27 07:36:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 1.9748	Prec@(1,5) (52.6%, 81.3%)
12/27 07:36:39午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 52.5960%
12/27 07:36:39午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:36:39午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 52.5960%
12/27 07:37:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.3170 (0.4973)	Arch Loss 2.2351 (1.9805)	Arch Hard Loss 2.2351 (1.9805)	Arch Beta Loss 2.5857 (2.5857)	Arch depth Loss 2.5857 (2.5857)	Prec@(1,5) (85.8%, 98.5%)	
12/27 07:38:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.4280 (0.4991)	Arch Loss 2.3392 (1.9934)	Arch Hard Loss 2.3392 (1.9934)	Arch Beta Loss 2.5857 (2.5857)	Arch depth Loss 2.5857 (2.5857)	Prec@(1,5) (85.8%, 98.5%)	
12/27 07:38:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.5788 (0.5059)	Arch Loss 1.8220 (1.9717)	Arch Hard Loss 1.8220 (1.9717)	Arch Beta Loss 2.5854 (2.5857)	Arch depth Loss 2.5854 (2.5857)	Prec@(1,5) (85.4%, 98.4%)	
12/27 07:39:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.5676 (0.5119)	Arch Loss 1.8070 (1.9742)	Arch Hard Loss 1.8070 (1.9742)	Arch Beta Loss 2.5853 (2.5856)	Arch depth Loss 2.5853 (2.5856)	Prec@(1,5) (85.1%, 98.4%)	
12/27 07:39:24午前 searchDistribution_trainer.py:166 [INFO] Train: [ 49/49] Final Prec@1 85.1320%
12/27 07:39:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0637	Prec@(1,5) (51.2%, 80.1%)
12/27 07:39:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.0501	Prec@(1,5) (51.4%, 80.4%)
12/27 07:39:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.0528	Prec@(1,5) (51.4%, 80.4%)
12/27 07:39:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.0445	Prec@(1,5) (51.6%, 80.5%)
12/27 07:39:48午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 51.6440%
12/27 07:39:48午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 07:39:48午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 52.5960%
12/27 07:39:48午前 trainer_runner.py:113 [INFO] Final best Prec@1 = 52.5960%
12/27 07:39:48午前 trainer_runner.py:114 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('avg_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
