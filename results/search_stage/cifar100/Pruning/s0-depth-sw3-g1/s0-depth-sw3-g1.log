12/27 03:56:50AM parser.py:28 [INFO] 
12/27 03:56:50AM parser.py:29 [INFO] Parameters:
12/27 03:56:50AM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g1/DAG
12/27 03:56:50AM parser.py:31 [INFO] T=10.0
12/27 03:56:50AM parser.py:31 [INFO] ADVANCED=1
12/27 03:56:50AM parser.py:31 [INFO] ALPHA_LR=0.0003
12/27 03:56:50AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
12/27 03:56:50AM parser.py:31 [INFO] ARCH_CRITERION=expected
12/27 03:56:50AM parser.py:31 [INFO] BATCH_SIZE=64
12/27 03:56:50AM parser.py:31 [INFO] CASCADE=0
12/27 03:56:50AM parser.py:31 [INFO] CHECKPOINT_RESET=False
12/27 03:56:50AM parser.py:31 [INFO] CURRICULUM_EPOCHS=[30, 30]
12/27 03:56:50AM parser.py:31 [INFO] CUTOUT_LENGTH=0
12/27 03:56:50AM parser.py:31 [INFO] DATA_PATH=../data/
12/27 03:56:50AM parser.py:31 [INFO] DATASET=cifar100
12/27 03:56:50AM parser.py:31 [INFO] DEPTH_COEF=1.0
12/27 03:56:50AM parser.py:31 [INFO] DESCRIPTION=search_and_evaluation_curriculum_learning_with_-Beta-expected-Depth-ecpected-constraint_slidewindow-3
12/27 03:56:50AM parser.py:31 [INFO] DISCRETE=1
12/27 03:56:50AM parser.py:31 [INFO] EPOCHS=50
12/27 03:56:50AM parser.py:31 [INFO] EVAL_EPOCHS=100
12/27 03:56:50AM parser.py:31 [INFO] EXP_NAME=s0-depth-sw3-g1
12/27 03:56:50AM parser.py:31 [INFO] FINAL_L=0.0
12/27 03:56:50AM parser.py:31 [INFO] G=0.0
12/27 03:56:50AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
12/27 03:56:50AM parser.py:31 [INFO] GPUS=[0]
12/27 03:56:50AM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
12/27 03:56:50AM parser.py:31 [INFO] INIT_CHANNELS=16
12/27 03:56:50AM parser.py:31 [INFO] L=0.0
12/27 03:56:50AM parser.py:31 [INFO] LAYERS=32
12/27 03:56:50AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
12/27 03:56:50AM parser.py:31 [INFO] NAME=Pruning
12/27 03:56:50AM parser.py:31 [INFO] NONKD=1
12/27 03:56:50AM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g1
12/27 03:56:50AM parser.py:31 [INFO] PCDARTS=0
12/27 03:56:50AM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g1/plots
12/27 03:56:50AM parser.py:31 [INFO] PRINT_FREQ=100
12/27 03:56:50AM parser.py:31 [INFO] RESET=0
12/27 03:56:50AM parser.py:31 [INFO] RESUME_PATH=None
12/27 03:56:50AM parser.py:31 [INFO] SAVE=s0-depth-sw3-g1
12/27 03:56:50AM parser.py:31 [INFO] SEED=0
12/27 03:56:50AM parser.py:31 [INFO] SHARE_STAGE=0
12/27 03:56:50AM parser.py:31 [INFO] SLIDE_WINDOW=3
12/27 03:56:50AM parser.py:31 [INFO] SPEC_CELL=1
12/27 03:56:50AM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
12/27 03:56:50AM parser.py:31 [INFO] TEACHER_NAME=none
12/27 03:56:50AM parser.py:31 [INFO] TEACHER_PATH=none
12/27 03:56:50AM parser.py:31 [INFO] TRAIN_PORTION=0.5
12/27 03:56:50AM parser.py:31 [INFO] TYPE=Distribution
12/27 03:56:50AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
12/27 03:56:50AM parser.py:31 [INFO] W_LR=0.025
12/27 03:56:50AM parser.py:31 [INFO] W_LR_MIN=0.001
12/27 03:56:50AM parser.py:31 [INFO] W_MOMENTUM=0.9
12/27 03:56:50AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
12/27 03:56:50AM parser.py:31 [INFO] WORKERS=4
12/27 03:56:50AM parser.py:32 [INFO] 
12/27 03:56:53AM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
12/27 03:57:36AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.3631 (4.5216)	Arch Loss 4.2489 (4.5150)	Arch Hard Loss 4.2489 (4.5150)	Arch Beta Loss 2.8399 (2.8315)	Arch depth Loss -2.8399 (-2.8315)	Prec@(1,5) (2.2%, 9.2%)	
12/27 03:58:16AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.1601 (4.4259)	Arch Loss 4.2658 (4.4267)	Arch Hard Loss 4.2658 (4.4267)	Arch Beta Loss 2.8583 (2.8403)	Arch depth Loss -2.8583 (-2.8403)	Prec@(1,5) (3.3%, 12.8%)	
12/27 03:58:57AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.2077 (4.3411)	Arch Loss 4.0801 (4.3335)	Arch Hard Loss 4.0801 (4.3335)	Arch Beta Loss 2.8777 (2.8495)	Arch depth Loss -2.8777 (-2.8495)	Prec@(1,5) (3.8%, 15.2%)	
12/27 03:59:33AM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.9360 (4.2709)	Arch Loss 4.1747 (4.2623)	Arch Hard Loss 4.1747 (4.2623)	Arch Beta Loss 2.8961 (2.8582)	Arch depth Loss -2.8961 (-2.8582)	Prec@(1,5) (4.6%, 17.4%)	
12/27 03:59:34AM searchDistribution_trainer.py:166 [INFO] Train: [  0/49] Final Prec@1 4.5720%
12/27 03:59:40AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 4.0086	Prec@(1,5) (7.1%, 25.2%)
12/27 03:59:46AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 4.0166	Prec@(1,5) (6.8%, 24.5%)
12/27 03:59:52AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 4.0113	Prec@(1,5) (6.9%, 24.7%)
12/27 03:59:58AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 4.0129	Prec@(1,5) (6.9%, 24.6%)
12/27 03:59:58AM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 6.8880%
12/27 03:59:58AM trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=range(10, 12))
12/27 03:59:58AM trainer_runner.py:108 [INFO] Until now, best Prec@1 = 6.8880%
12/27 04:00:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.9802 (3.9407)	Arch Loss 3.7792 (3.9640)	Arch Hard Loss 3.7792 (3.9640)	Arch Beta Loss 2.9174 (2.9070)	Arch depth Loss -2.9174 (-2.9070)	Prec@(1,5) (7.9%, 27.5%)	
12/27 04:01:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 4.0410 (3.9226)	Arch Loss 3.7789 (3.9235)	Arch Hard Loss 3.7789 (3.9235)	Arch Beta Loss 2.9390 (2.9177)	Arch depth Loss -2.9390 (-2.9177)	Prec@(1,5) (8.3%, 28.3%)	
12/27 04:02:04午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.9674 (3.8997)	Arch Loss 3.8501 (3.9012)	Arch Hard Loss 3.8501 (3.9012)	Arch Beta Loss 2.9614 (2.9285)	Arch depth Loss -2.9614 (-2.9285)	Prec@(1,5) (9.0%, 28.9%)	
12/27 04:02:42午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.7540 (3.8820)	Arch Loss 4.0868 (3.8691)	Arch Hard Loss 4.0868 (3.8691)	Arch Beta Loss 2.9817 (2.9385)	Arch depth Loss -2.9817 (-2.9385)	Prec@(1,5) (9.3%, 29.4%)	
12/27 04:02:42午前 searchDistribution_trainer.py:166 [INFO] Train: [  1/49] Final Prec@1 9.2600%
12/27 04:02:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.8359	Prec@(1,5) (9.3%, 31.1%)
12/27 04:02:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.8288	Prec@(1,5) (9.2%, 31.3%)
12/27 04:03:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.8274	Prec@(1,5) (9.3%, 31.2%)
12/27 04:03:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.8356	Prec@(1,5) (9.5%, 31.1%)
12/27 04:03:06午前 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 9.4640%
12/27 04:03:06午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 04:03:07午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 9.4640%
12/27 04:03:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.7381 (3.7387)	Arch Loss 4.0483 (3.7591)	Arch Hard Loss 4.0483 (3.7591)	Arch Beta Loss 3.0055 (2.9938)	Arch depth Loss -3.0055 (-2.9938)	Prec@(1,5) (11.1%, 34.1%)	
12/27 04:04:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5879 (3.7253)	Arch Loss 3.7627 (3.7359)	Arch Hard Loss 3.7627 (3.7359)	Arch Beta Loss 3.0293 (3.0056)	Arch depth Loss -3.0293 (-3.0056)	Prec@(1,5) (11.5%, 34.9%)	
12/27 04:05:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5824 (3.7074)	Arch Loss 3.6659 (3.6953)	Arch Hard Loss 3.6659 (3.6953)	Arch Beta Loss 3.0540 (3.0177)	Arch depth Loss -3.0540 (-3.0177)	Prec@(1,5) (11.7%, 35.3%)	
12/27 04:05:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.6046 (3.6864)	Arch Loss 3.5824 (3.6713)	Arch Hard Loss 3.5824 (3.6713)	Arch Beta Loss 3.0762 (3.0286)	Arch depth Loss -3.0762 (-3.0286)	Prec@(1,5) (12.0%, 35.8%)	
12/27 04:05:49午前 searchDistribution_trainer.py:166 [INFO] Train: [  2/49] Final Prec@1 12.0000%
12/27 04:05:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.6176	Prec@(1,5) (13.6%, 38.0%)
12/27 04:06:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.6103	Prec@(1,5) (13.7%, 38.8%)
12/27 04:06:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.6077	Prec@(1,5) (13.7%, 38.8%)
12/27 04:06:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.6031	Prec@(1,5) (13.8%, 38.9%)
12/27 04:06:13午前 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 13.8000%
12/27 04:06:13午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 04:06:14午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 13.8000%
12/27 04:06:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.3729 (3.5432)	Arch Loss 3.3440 (3.5616)	Arch Hard Loss 3.3440 (3.5616)	Arch Beta Loss 3.1014 (3.0890)	Arch depth Loss -3.1014 (-3.0890)	Prec@(1,5) (14.1%, 39.4%)	
12/27 04:07:36午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.4536 (3.5313)	Arch Loss 3.8505 (3.5404)	Arch Hard Loss 3.8505 (3.5404)	Arch Beta Loss 3.1266 (3.1015)	Arch depth Loss -3.1266 (-3.1015)	Prec@(1,5) (14.5%, 40.0%)	
12/27 04:08:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.6433 (3.5314)	Arch Loss 3.5532 (3.5183)	Arch Hard Loss 3.5532 (3.5183)	Arch Beta Loss 3.1514 (3.1140)	Arch depth Loss -3.1514 (-3.1140)	Prec@(1,5) (14.5%, 40.3%)	
12/27 04:08:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.2444 (3.5091)	Arch Loss 3.4372 (3.5078)	Arch Hard Loss 3.4372 (3.5078)	Arch Beta Loss 3.1742 (3.1253)	Arch depth Loss -3.1742 (-3.1253)	Prec@(1,5) (15.0%, 41.0%)	
12/27 04:08:53午前 searchDistribution_trainer.py:166 [INFO] Train: [  3/49] Final Prec@1 15.0400%
12/27 04:09:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.4886	Prec@(1,5) (15.7%, 42.4%)
12/27 04:09:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.4913	Prec@(1,5) (15.4%, 41.7%)
12/27 04:09:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.4910	Prec@(1,5) (15.2%, 41.7%)
12/27 04:09:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.4959	Prec@(1,5) (15.1%, 41.6%)
12/27 04:09:17午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 15.1600%
12/27 04:09:17午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:09:18午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 15.1600%
12/27 04:10:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.3338 (3.3994)	Arch Loss 3.2196 (3.4154)	Arch Hard Loss 3.2196 (3.4154)	Arch Beta Loss 3.1993 (3.1870)	Arch depth Loss -3.1993 (-3.1870)	Prec@(1,5) (17.1%, 44.7%)	
12/27 04:10:42午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0182 (3.3829)	Arch Loss 3.2088 (3.3875)	Arch Hard Loss 3.2088 (3.3875)	Arch Beta Loss 3.2238 (3.1994)	Arch depth Loss -3.2238 (-3.1994)	Prec@(1,5) (17.5%, 44.6%)	
12/27 04:11:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 3.4633 (3.3767)	Arch Loss 3.2724 (3.3815)	Arch Hard Loss 3.2724 (3.3815)	Arch Beta Loss 3.2478 (3.2116)	Arch depth Loss -3.2478 (-3.2116)	Prec@(1,5) (17.8%, 44.8%)	
12/27 04:12:01午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.3355 (3.3604)	Arch Loss 3.0733 (3.3625)	Arch Hard Loss 3.0733 (3.3625)	Arch Beta Loss 3.2691 (3.2224)	Arch depth Loss -3.2691 (-3.2224)	Prec@(1,5) (18.0%, 45.2%)	
12/27 04:12:02午前 searchDistribution_trainer.py:166 [INFO] Train: [  4/49] Final Prec@1 17.9960%
12/27 04:12:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.3475	Prec@(1,5) (18.6%, 46.9%)
12/27 04:12:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.2963	Prec@(1,5) (19.2%, 48.2%)
12/27 04:12:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.2933	Prec@(1,5) (19.0%, 48.2%)
12/27 04:12:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.2887	Prec@(1,5) (18.9%, 48.5%)
12/27 04:12:27午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 18.9720%
12/27 04:12:27午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:12:27午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 18.9720%
12/27 04:13:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1844 (3.2596)	Arch Loss 3.3044 (3.2841)	Arch Hard Loss 3.3044 (3.2841)	Arch Beta Loss 3.2924 (3.2810)	Arch depth Loss -3.2924 (-3.2810)	Prec@(1,5) (19.7%, 48.6%)	
12/27 04:13:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.0699 (3.2453)	Arch Loss 3.6258 (3.2713)	Arch Hard Loss 3.6258 (3.2713)	Arch Beta Loss 3.3151 (3.2924)	Arch depth Loss -3.3151 (-3.2924)	Prec@(1,5) (19.9%, 48.8%)	
12/27 04:14:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 3.4984 (3.2323)	Arch Loss 3.1123 (3.2593)	Arch Hard Loss 3.1123 (3.2593)	Arch Beta Loss 3.3373 (3.3037)	Arch depth Loss -3.3373 (-3.3037)	Prec@(1,5) (19.9%, 49.2%)	
12/27 04:15:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 3.0553 (3.2202)	Arch Loss 3.0300 (3.2501)	Arch Hard Loss 3.0300 (3.2501)	Arch Beta Loss 3.3565 (3.3137)	Arch depth Loss -3.3565 (-3.3137)	Prec@(1,5) (20.1%, 49.7%)	
12/27 04:15:12午前 searchDistribution_trainer.py:166 [INFO] Train: [  5/49] Final Prec@1 20.0760%
12/27 04:15:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 3.2394	Prec@(1,5) (20.4%, 49.1%)
12/27 04:15:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 3.2490	Prec@(1,5) (20.0%, 48.8%)
12/27 04:15:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 3.2469	Prec@(1,5) (20.2%, 48.7%)
12/27 04:15:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 3.2489	Prec@(1,5) (20.2%, 48.6%)
12/27 04:15:37午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 20.2440%
12/27 04:15:37午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 04:15:37午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 20.2440%
12/27 04:16:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 3.1462 (3.0890)	Arch Loss 3.0897 (3.1783)	Arch Hard Loss 3.0897 (3.1783)	Arch Beta Loss 3.3773 (3.3672)	Arch depth Loss -3.3773 (-3.3672)	Prec@(1,5) (22.5%, 52.8%)	
12/27 04:17:02午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.8875 (3.0944)	Arch Loss 2.9956 (3.1405)	Arch Hard Loss 2.9956 (3.1405)	Arch Beta Loss 3.3972 (3.3773)	Arch depth Loss -3.3972 (-3.3773)	Prec@(1,5) (22.5%, 52.5%)	
12/27 04:17:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.2248 (3.0902)	Arch Loss 2.9679 (3.1334)	Arch Hard Loss 2.9679 (3.1334)	Arch Beta Loss 3.4163 (3.3871)	Arch depth Loss -3.4163 (-3.3871)	Prec@(1,5) (22.5%, 52.8%)	
12/27 04:18:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 3.3019 (3.0887)	Arch Loss 3.3191 (3.1284)	Arch Hard Loss 3.3191 (3.1284)	Arch Beta Loss 3.4329 (3.3958)	Arch depth Loss -3.4329 (-3.3958)	Prec@(1,5) (22.5%, 52.9%)	
12/27 04:18:21午前 searchDistribution_trainer.py:166 [INFO] Train: [  6/49] Final Prec@1 22.5360%
12/27 04:18:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 3.1293	Prec@(1,5) (22.6%, 51.7%)
12/27 04:18:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 3.1170	Prec@(1,5) (22.4%, 52.4%)
12/27 04:18:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 3.1276	Prec@(1,5) (22.3%, 52.3%)
12/27 04:18:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 3.1232	Prec@(1,5) (22.4%, 52.4%)
12/27 04:18:46午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 22.3560%
12/27 04:18:46午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:18:47午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 22.3560%
12/27 04:19:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.9387 (2.9852)	Arch Loss 2.8298 (3.0739)	Arch Hard Loss 2.8298 (3.0739)	Arch Beta Loss 3.4508 (3.4420)	Arch depth Loss -3.4508 (-3.4420)	Prec@(1,5) (24.3%, 55.7%)	
12/27 04:20:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.9716 (2.9744)	Arch Loss 3.0049 (3.0481)	Arch Hard Loss 3.0049 (3.0481)	Arch Beta Loss 3.4681 (3.4508)	Arch depth Loss -3.4681 (-3.4508)	Prec@(1,5) (25.2%, 55.7%)	
12/27 04:20:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 3.0719 (2.9690)	Arch Loss 2.9323 (3.0305)	Arch Hard Loss 2.9323 (3.0305)	Arch Beta Loss 3.4847 (3.4594)	Arch depth Loss -3.4847 (-3.4594)	Prec@(1,5) (25.1%, 55.8%)	
12/27 04:21:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.9477 (2.9661)	Arch Loss 2.6837 (3.0228)	Arch Hard Loss 2.6837 (3.0228)	Arch Beta Loss 3.4990 (3.4669)	Arch depth Loss -3.4990 (-3.4669)	Prec@(1,5) (24.9%, 55.9%)	
12/27 04:21:27午前 searchDistribution_trainer.py:166 [INFO] Train: [  7/49] Final Prec@1 24.9400%
12/27 04:21:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 3.0213	Prec@(1,5) (24.4%, 56.0%)
12/27 04:21:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 3.0330	Prec@(1,5) (24.3%, 55.4%)
12/27 04:21:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 3.0266	Prec@(1,5) (24.7%, 55.5%)
12/27 04:21:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 3.0387	Prec@(1,5) (24.3%, 55.0%)
12/27 04:21:51午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 24.3280%
12/27 04:21:51午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:21:51午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 24.3280%
12/27 04:22:33午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.7292 (2.8532)	Arch Loss 3.0649 (2.9998)	Arch Hard Loss 3.0649 (2.9998)	Arch Beta Loss 3.5145 (3.5069)	Arch depth Loss -3.5145 (-3.5069)	Prec@(1,5) (27.6%, 58.7%)	
12/27 04:23:14午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.6661 (2.8616)	Arch Loss 2.8579 (2.9719)	Arch Hard Loss 2.8579 (2.9719)	Arch Beta Loss 3.5291 (3.5145)	Arch depth Loss -3.5291 (-3.5145)	Prec@(1,5) (27.7%, 58.6%)	
12/27 04:23:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.7663 (2.8626)	Arch Loss 2.9653 (2.9540)	Arch Hard Loss 2.9653 (2.9540)	Arch Beta Loss 3.5429 (3.5217)	Arch depth Loss -3.5429 (-3.5217)	Prec@(1,5) (27.5%, 58.7%)	
12/27 04:24:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.7799 (2.8628)	Arch Loss 3.1289 (2.9412)	Arch Hard Loss 3.1289 (2.9412)	Arch Beta Loss 3.5549 (3.5280)	Arch depth Loss -3.5549 (-3.5280)	Prec@(1,5) (27.4%, 58.5%)	
12/27 04:24:31午前 searchDistribution_trainer.py:166 [INFO] Train: [  8/49] Final Prec@1 27.4240%
12/27 04:24:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.8110	Prec@(1,5) (28.5%, 60.5%)
12/27 04:24:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.8157	Prec@(1,5) (28.4%, 60.2%)
12/27 04:24:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.8190	Prec@(1,5) (28.7%, 60.2%)
12/27 04:24:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.8252	Prec@(1,5) (28.5%, 60.0%)
12/27 04:24:55午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 28.4640%
12/27 04:24:55午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('avg_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:24:56午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 28.4640%
12/27 04:25:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.6414 (2.7708)	Arch Loss 2.9133 (2.8856)	Arch Hard Loss 2.9133 (2.8856)	Arch Beta Loss 3.5679 (3.5616)	Arch depth Loss -3.5679 (-3.5616)	Prec@(1,5) (29.0%, 60.9%)	
12/27 04:26:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.6059 (2.7721)	Arch Loss 3.3307 (2.8818)	Arch Hard Loss 3.3307 (2.8818)	Arch Beta Loss 3.5801 (3.5679)	Arch depth Loss -3.5801 (-3.5679)	Prec@(1,5) (28.8%, 61.1%)	
12/27 04:26:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.7522 (2.7678)	Arch Loss 2.6094 (2.8759)	Arch Hard Loss 2.6094 (2.8759)	Arch Beta Loss 3.5917 (3.5739)	Arch depth Loss -3.5917 (-3.5739)	Prec@(1,5) (28.8%, 61.3%)	
12/27 04:27:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.7260 (2.7642)	Arch Loss 2.9580 (2.8689)	Arch Hard Loss 2.9580 (2.8689)	Arch Beta Loss 3.6015 (3.5792)	Arch depth Loss -3.6015 (-3.5792)	Prec@(1,5) (28.9%, 61.3%)	
12/27 04:27:38午前 searchDistribution_trainer.py:166 [INFO] Train: [  9/49] Final Prec@1 28.8760%
12/27 04:27:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.8527	Prec@(1,5) (27.2%, 59.8%)
12/27 04:27:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.8451	Prec@(1,5) (27.9%, 59.7%)
12/27 04:27:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.8497	Prec@(1,5) (27.9%, 59.5%)
12/27 04:28:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.8398	Prec@(1,5) (28.0%, 59.7%)
12/27 04:28:02午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 28.0080%
12/27 04:28:02午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:28:02午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 28.4640%
12/27 04:28:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 3.0181 (2.6791)	Arch Loss 2.8950 (2.8367)	Arch Hard Loss 2.8950 (2.8367)	Arch Beta Loss 3.6120 (3.6068)	Arch depth Loss -3.6120 (-3.6068)	Prec@(1,5) (30.2%, 63.2%)	
12/27 04:29:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.7916 (2.6587)	Arch Loss 2.8929 (2.7931)	Arch Hard Loss 2.8929 (2.7931)	Arch Beta Loss 3.6219 (3.6120)	Arch depth Loss -3.6219 (-3.6120)	Prec@(1,5) (30.8%, 63.6%)	
12/27 04:30:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.3343 (2.6731)	Arch Loss 3.0749 (2.8032)	Arch Hard Loss 3.0749 (2.8032)	Arch Beta Loss 3.6316 (3.6169)	Arch depth Loss -3.6316 (-3.6169)	Prec@(1,5) (30.7%, 63.3%)	
12/27 04:30:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.7841 (2.6701)	Arch Loss 3.0169 (2.7934)	Arch Hard Loss 3.0169 (2.7934)	Arch Beta Loss 3.6399 (3.6213)	Arch depth Loss -3.6399 (-3.6213)	Prec@(1,5) (30.8%, 63.3%)	
12/27 04:30:44午前 searchDistribution_trainer.py:166 [INFO] Train: [ 10/49] Final Prec@1 30.7960%
12/27 04:30:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.7844	Prec@(1,5) (28.9%, 61.1%)
12/27 04:30:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.8023	Prec@(1,5) (28.8%, 60.5%)
12/27 04:31:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.8037	Prec@(1,5) (28.8%, 60.6%)
12/27 04:31:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.8023	Prec@(1,5) (28.6%, 60.6%)
12/27 04:31:07午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 28.6160%
12/27 04:31:07午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:31:08午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 28.6160%
12/27 04:31:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.7197 (2.5779)	Arch Loss 2.4109 (2.7611)	Arch Hard Loss 2.4109 (2.7611)	Arch Beta Loss 3.6487 (3.6443)	Arch depth Loss -3.6487 (-3.6443)	Prec@(1,5) (32.6%, 65.6%)	
12/27 04:32:33午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.4304 (2.5883)	Arch Loss 2.8872 (2.7410)	Arch Hard Loss 2.8872 (2.7410)	Arch Beta Loss 3.6567 (3.6486)	Arch depth Loss -3.6567 (-3.6486)	Prec@(1,5) (32.8%, 65.0%)	
12/27 04:33:14午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.6398 (2.5864)	Arch Loss 2.4727 (2.7314)	Arch Hard Loss 2.4727 (2.7314)	Arch Beta Loss 3.6645 (3.6526)	Arch depth Loss -3.6645 (-3.6526)	Prec@(1,5) (32.7%, 65.1%)	
12/27 04:33:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.6111 (2.5899)	Arch Loss 2.5481 (2.7237)	Arch Hard Loss 2.5481 (2.7237)	Arch Beta Loss 3.6713 (3.6561)	Arch depth Loss -3.6713 (-3.6561)	Prec@(1,5) (32.6%, 65.1%)	
12/27 04:33:52午前 searchDistribution_trainer.py:166 [INFO] Train: [ 11/49] Final Prec@1 32.5680%
12/27 04:33:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.7479	Prec@(1,5) (30.0%, 61.6%)
12/27 04:34:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.7414	Prec@(1,5) (30.1%, 61.6%)
12/27 04:34:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.7454	Prec@(1,5) (30.1%, 61.7%)
12/27 04:34:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.7504	Prec@(1,5) (30.0%, 61.6%)
12/27 04:34:16午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 30.0160%
12/27 04:34:16午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:34:17午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 30.0160%
12/27 04:34:59午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.3129 (2.4639)	Arch Loss 2.8056 (2.6700)	Arch Hard Loss 2.8056 (2.6700)	Arch Beta Loss 3.6785 (3.6749)	Arch depth Loss -3.6785 (-3.6749)	Prec@(1,5) (35.2%, 67.8%)	
12/27 04:35:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.3188 (2.4800)	Arch Loss 2.7047 (2.6831)	Arch Hard Loss 2.7047 (2.6831)	Arch Beta Loss 3.6851 (3.6784)	Arch depth Loss -3.6851 (-3.6784)	Prec@(1,5) (35.2%, 67.6%)	
12/27 04:36:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.8465 (2.5010)	Arch Loss 2.7743 (2.6896)	Arch Hard Loss 2.7743 (2.6896)	Arch Beta Loss 3.6914 (3.6817)	Arch depth Loss -3.6914 (-3.6817)	Prec@(1,5) (34.5%, 67.1%)	
12/27 04:37:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 2.9746 (2.5009)	Arch Loss 2.8822 (2.6725)	Arch Hard Loss 2.8822 (2.6725)	Arch Beta Loss 3.6969 (3.6846)	Arch depth Loss -3.6969 (-3.6846)	Prec@(1,5) (34.3%, 67.2%)	
12/27 04:37:00午前 searchDistribution_trainer.py:166 [INFO] Train: [ 12/49] Final Prec@1 34.2760%
12/27 04:37:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.6081	Prec@(1,5) (32.8%, 64.8%)
12/27 04:37:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.6304	Prec@(1,5) (32.4%, 64.5%)
12/27 04:37:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.6359	Prec@(1,5) (32.3%, 64.2%)
12/27 04:37:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.6404	Prec@(1,5) (32.1%, 64.1%)
12/27 04:37:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 32.1240%
12/27 04:37:24午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:37:25午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 32.1240%
12/27 04:38:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.3636 (2.4070)	Arch Loss 2.0913 (2.6281)	Arch Hard Loss 2.0913 (2.6281)	Arch Beta Loss 3.7027 (3.7000)	Arch depth Loss -3.7027 (-3.7000)	Prec@(1,5) (36.5%, 68.9%)	
12/27 04:38:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.6919 (2.4226)	Arch Loss 2.5921 (2.6186)	Arch Hard Loss 2.5921 (2.6186)	Arch Beta Loss 3.7082 (3.7027)	Arch depth Loss -3.7082 (-3.7027)	Prec@(1,5) (35.9%, 68.7%)	
12/27 04:39:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 2.4109 (2.4257)	Arch Loss 2.8656 (2.6181)	Arch Hard Loss 2.8656 (2.6181)	Arch Beta Loss 3.7134 (3.7054)	Arch depth Loss -3.7134 (-3.7054)	Prec@(1,5) (35.8%, 68.7%)	
12/27 04:40:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.0101 (2.4200)	Arch Loss 2.7600 (2.6109)	Arch Hard Loss 2.7600 (2.6109)	Arch Beta Loss 3.7178 (3.7078)	Arch depth Loss -3.7178 (-3.7078)	Prec@(1,5) (35.9%, 68.8%)	
12/27 04:40:10午前 searchDistribution_trainer.py:166 [INFO] Train: [ 13/49] Final Prec@1 35.9600%
12/27 04:40:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.6786	Prec@(1,5) (32.1%, 64.0%)
12/27 04:40:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.6596	Prec@(1,5) (32.4%, 64.3%)
12/27 04:40:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.6561	Prec@(1,5) (32.3%, 64.3%)
12/27 04:40:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.6558	Prec@(1,5) (32.3%, 64.2%)
12/27 04:40:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 32.2640%
12/27 04:40:34午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:40:34午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 32.2640%
12/27 04:41:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.3955 (2.3134)	Arch Loss 2.3082 (2.5727)	Arch Hard Loss 2.3082 (2.5727)	Arch Beta Loss 3.7225 (3.7202)	Arch depth Loss -3.7225 (-3.7202)	Prec@(1,5) (38.2%, 71.3%)	
12/27 04:41:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.2554 (2.3399)	Arch Loss 2.5744 (2.5642)	Arch Hard Loss 2.5744 (2.5642)	Arch Beta Loss 3.7268 (3.7225)	Arch depth Loss -3.7268 (-3.7225)	Prec@(1,5) (37.3%, 71.0%)	
12/27 04:42:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 2.1170 (2.3412)	Arch Loss 2.8537 (2.5656)	Arch Hard Loss 2.8537 (2.5656)	Arch Beta Loss 3.7311 (3.7246)	Arch depth Loss -3.7311 (-3.7246)	Prec@(1,5) (37.3%, 70.9%)	
12/27 04:43:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.3817 (2.3427)	Arch Loss 2.6759 (2.5608)	Arch Hard Loss 2.6759 (2.5608)	Arch Beta Loss 3.7348 (3.7266)	Arch depth Loss -3.7348 (-3.7266)	Prec@(1,5) (37.4%, 70.7%)	
12/27 04:43:18午前 searchDistribution_trainer.py:166 [INFO] Train: [ 14/49] Final Prec@1 37.4400%
12/27 04:43:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.5687	Prec@(1,5) (34.8%, 65.7%)
12/27 04:43:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.5597	Prec@(1,5) (34.8%, 66.0%)
12/27 04:43:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.5615	Prec@(1,5) (34.4%, 66.2%)
12/27 04:43:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.5575	Prec@(1,5) (34.2%, 66.3%)
12/27 04:43:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 34.2000%
12/27 04:43:41午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:43:42午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 34.2000%
12/27 04:44:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.2220 (2.2496)	Arch Loss 2.3957 (2.5118)	Arch Hard Loss 2.3957 (2.5118)	Arch Beta Loss 3.7385 (3.7367)	Arch depth Loss -3.7385 (-3.7367)	Prec@(1,5) (39.8%, 72.6%)	
12/27 04:45:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.8780 (2.2736)	Arch Loss 2.6163 (2.5014)	Arch Hard Loss 2.6163 (2.5014)	Arch Beta Loss 3.7421 (3.7386)	Arch depth Loss -3.7421 (-3.7386)	Prec@(1,5) (38.9%, 71.8%)	
12/27 04:45:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 2.2445 (2.2769)	Arch Loss 2.6060 (2.4968)	Arch Hard Loss 2.6060 (2.4968)	Arch Beta Loss 3.7454 (3.7403)	Arch depth Loss -3.7454 (-3.7403)	Prec@(1,5) (39.0%, 71.9%)	
12/27 04:46:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 2.4365 (2.2728)	Arch Loss 2.4771 (2.4930)	Arch Hard Loss 2.4771 (2.4930)	Arch Beta Loss 3.7483 (3.7418)	Arch depth Loss -3.7483 (-3.7418)	Prec@(1,5) (39.0%, 72.0%)	
12/27 04:46:26午前 searchDistribution_trainer.py:166 [INFO] Train: [ 15/49] Final Prec@1 39.0000%
12/27 04:46:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.4962	Prec@(1,5) (36.0%, 67.1%)
12/27 04:46:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.5025	Prec@(1,5) (35.9%, 67.1%)
12/27 04:46:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.5006	Prec@(1,5) (35.6%, 67.2%)
12/27 04:46:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.5078	Prec@(1,5) (35.3%, 67.0%)
12/27 04:46:50午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 35.2920%
12/27 04:46:50午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:46:51午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 35.2920%
12/27 04:47:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 2.0343 (2.1769)	Arch Loss 2.2885 (2.4936)	Arch Hard Loss 2.2885 (2.4936)	Arch Beta Loss 3.7513 (3.7499)	Arch depth Loss -3.7513 (-3.7499)	Prec@(1,5) (41.8%, 73.5%)	
12/27 04:48:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.2162 (2.1909)	Arch Loss 2.5808 (2.4915)	Arch Hard Loss 2.5808 (2.4915)	Arch Beta Loss 3.7540 (3.7513)	Arch depth Loss -3.7540 (-3.7513)	Prec@(1,5) (41.2%, 73.6%)	
12/27 04:48:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 2.0391 (2.2022)	Arch Loss 2.4978 (2.4720)	Arch Hard Loss 2.4978 (2.4720)	Arch Beta Loss 3.7567 (3.7527)	Arch depth Loss -3.7567 (-3.7527)	Prec@(1,5) (40.9%, 73.4%)	
12/27 04:49:36午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9699 (2.2006)	Arch Loss 2.5128 (2.4734)	Arch Hard Loss 2.5128 (2.4734)	Arch Beta Loss 3.7589 (3.7539)	Arch depth Loss -3.7589 (-3.7539)	Prec@(1,5) (40.7%, 73.6%)	
12/27 04:49:36午前 searchDistribution_trainer.py:166 [INFO] Train: [ 16/49] Final Prec@1 40.6800%
12/27 04:49:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.4486	Prec@(1,5) (36.0%, 68.9%)
12/27 04:49:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.4477	Prec@(1,5) (35.8%, 68.9%)
12/27 04:49:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.4386	Prec@(1,5) (36.0%, 69.0%)
12/27 04:50:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.4382	Prec@(1,5) (35.9%, 69.1%)
12/27 04:50:01午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 35.9280%
12/27 04:50:01午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:50:01午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 35.9280%
12/27 04:50:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 2.3156 (2.1037)	Arch Loss 2.6479 (2.4284)	Arch Hard Loss 2.6479 (2.4284)	Arch Beta Loss 3.7614 (3.7602)	Arch depth Loss -3.7614 (-3.7602)	Prec@(1,5) (42.5%, 75.9%)	
12/27 04:51:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 2.4009 (2.1097)	Arch Loss 2.5171 (2.4378)	Arch Hard Loss 2.5171 (2.4378)	Arch Beta Loss 3.7638 (3.7615)	Arch depth Loss -3.7638 (-3.7615)	Prec@(1,5) (42.1%, 75.9%)	
12/27 04:52:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 2.2971 (2.1266)	Arch Loss 2.3701 (2.4143)	Arch Hard Loss 2.3701 (2.4143)	Arch Beta Loss 3.7660 (3.7626)	Arch depth Loss -3.7660 (-3.7626)	Prec@(1,5) (41.8%, 75.5%)	
12/27 04:52:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 2.1331 (2.1274)	Arch Loss 2.5835 (2.4075)	Arch Hard Loss 2.5835 (2.4075)	Arch Beta Loss 3.7678 (3.7636)	Arch depth Loss -3.7678 (-3.7636)	Prec@(1,5) (41.9%, 75.3%)	
12/27 04:52:47午前 searchDistribution_trainer.py:166 [INFO] Train: [ 17/49] Final Prec@1 41.9160%
12/27 04:52:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.3546	Prec@(1,5) (38.2%, 70.8%)
12/27 04:53:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.3488	Prec@(1,5) (38.4%, 70.9%)
12/27 04:53:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.3528	Prec@(1,5) (38.3%, 70.9%)
12/27 04:53:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.3564	Prec@(1,5) (38.0%, 70.8%)
12/27 04:53:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 38.0080%
12/27 04:53:12午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 04:53:12午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 38.0080%
12/27 04:53:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 2.1709 (2.0639)	Arch Loss 2.3401 (2.3907)	Arch Hard Loss 2.3401 (2.3907)	Arch Beta Loss 3.7697 (3.7687)	Arch depth Loss -3.7697 (-3.7687)	Prec@(1,5) (44.1%, 76.2%)	
12/27 04:54:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 2.3551 (2.0439)	Arch Loss 2.1056 (2.4007)	Arch Hard Loss 2.1056 (2.4007)	Arch Beta Loss 3.7716 (3.7697)	Arch depth Loss -3.7716 (-3.7697)	Prec@(1,5) (44.4%, 76.6%)	
12/27 04:55:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.8802 (2.0499)	Arch Loss 2.3424 (2.4046)	Arch Hard Loss 2.3424 (2.4046)	Arch Beta Loss 3.7733 (3.7707)	Arch depth Loss -3.7733 (-3.7707)	Prec@(1,5) (44.1%, 76.5%)	
12/27 04:55:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.9777 (2.0532)	Arch Loss 2.4127 (2.3894)	Arch Hard Loss 2.4127 (2.3894)	Arch Beta Loss 3.7748 (3.7715)	Arch depth Loss -3.7748 (-3.7715)	Prec@(1,5) (44.0%, 76.5%)	
12/27 04:55:52午前 searchDistribution_trainer.py:166 [INFO] Train: [ 18/49] Final Prec@1 43.9960%
12/27 04:55:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.4621	Prec@(1,5) (36.2%, 69.1%)
12/27 04:56:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.4729	Prec@(1,5) (36.5%, 68.6%)
12/27 04:56:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.4607	Prec@(1,5) (36.4%, 68.8%)
12/27 04:56:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.4716	Prec@(1,5) (36.2%, 68.6%)
12/27 04:56:17午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 36.2000%
12/27 04:56:17午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 04:56:17午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 38.0080%
12/27 04:57:00午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.6960 (1.9375)	Arch Loss 2.2238 (2.3825)	Arch Hard Loss 2.2238 (2.3825)	Arch Beta Loss 3.7766 (3.7758)	Arch depth Loss -3.7766 (-3.7758)	Prec@(1,5) (46.7%, 79.0%)	
12/27 04:57:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.7140 (1.9578)	Arch Loss 2.3113 (2.3546)	Arch Hard Loss 2.3113 (2.3546)	Arch Beta Loss 3.7783 (3.7766)	Arch depth Loss -3.7783 (-3.7766)	Prec@(1,5) (46.0%, 78.7%)	
12/27 04:58:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.3310 (1.9819)	Arch Loss 2.0352 (2.3431)	Arch Hard Loss 2.0352 (2.3431)	Arch Beta Loss 3.7797 (3.7774)	Arch depth Loss -3.7797 (-3.7774)	Prec@(1,5) (45.6%, 78.0%)	
12/27 04:59:01午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 2.0608 (1.9938)	Arch Loss 2.7127 (2.3460)	Arch Hard Loss 2.7127 (2.3460)	Arch Beta Loss 3.7806 (3.7780)	Arch depth Loss -3.7806 (-3.7780)	Prec@(1,5) (45.4%, 77.8%)	
12/27 04:59:01午前 searchDistribution_trainer.py:166 [INFO] Train: [ 19/49] Final Prec@1 45.3440%
12/27 04:59:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.3413	Prec@(1,5) (38.6%, 70.5%)
12/27 04:59:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.3435	Prec@(1,5) (38.8%, 70.5%)
12/27 04:59:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.3467	Prec@(1,5) (38.9%, 70.3%)
12/27 04:59:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.3495	Prec@(1,5) (38.9%, 70.3%)
12/27 04:59:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 38.9000%
12/27 04:59:25午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 04:59:26午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 38.9000%
12/27 05:00:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.7386 (1.8670)	Arch Loss 2.2640 (2.2845)	Arch Hard Loss 2.2640 (2.2845)	Arch Beta Loss 3.7817 (3.7813)	Arch depth Loss -3.7817 (-3.7813)	Prec@(1,5) (48.3%, 80.7%)	
12/27 05:00:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 2.0291 (1.9021)	Arch Loss 1.9630 (2.2945)	Arch Hard Loss 1.9630 (2.2945)	Arch Beta Loss 3.7830 (3.7819)	Arch depth Loss -3.7830 (-3.7819)	Prec@(1,5) (47.6%, 79.7%)	
12/27 05:01:33午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 2.2104 (1.9125)	Arch Loss 1.9176 (2.3033)	Arch Hard Loss 1.9176 (2.3033)	Arch Beta Loss 3.7842 (3.7825)	Arch depth Loss -3.7842 (-3.7825)	Prec@(1,5) (47.4%, 79.5%)	
12/27 05:02:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 2.0252 (1.9259)	Arch Loss 2.1149 (2.3015)	Arch Hard Loss 2.1149 (2.3015)	Arch Beta Loss 3.7855 (3.7830)	Arch depth Loss -3.7855 (-3.7830)	Prec@(1,5) (47.1%, 79.1%)	
12/27 05:02:11午前 searchDistribution_trainer.py:166 [INFO] Train: [ 20/49] Final Prec@1 47.1080%
12/27 05:02:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.7217	Prec@(1,5) (32.7%, 64.2%)
12/27 05:02:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.7618	Prec@(1,5) (32.2%, 63.2%)
12/27 05:02:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.7660	Prec@(1,5) (32.1%, 63.1%)
12/27 05:02:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.7660	Prec@(1,5) (32.2%, 63.1%)
12/27 05:02:35午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 32.2080%
12/27 05:02:35午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:02:36午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 38.9000%
12/27 05:03:19午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.8233 (1.8505)	Arch Loss 2.4356 (2.3010)	Arch Hard Loss 2.4356 (2.3010)	Arch Beta Loss 3.7863 (3.7859)	Arch depth Loss -3.7863 (-3.7859)	Prec@(1,5) (48.0%, 80.4%)	
12/27 05:04:01午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 2.5335 (1.8486)	Arch Loss 2.6771 (2.2863)	Arch Hard Loss 2.6771 (2.2863)	Arch Beta Loss 3.7873 (3.7864)	Arch depth Loss -3.7873 (-3.7864)	Prec@(1,5) (48.4%, 80.5%)	
12/27 05:04:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.9913 (1.8648)	Arch Loss 2.2692 (2.2905)	Arch Hard Loss 2.2692 (2.2905)	Arch Beta Loss 3.7879 (3.7868)	Arch depth Loss -3.7879 (-3.7868)	Prec@(1,5) (48.0%, 80.3%)	
12/27 05:05:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.9178 (1.8746)	Arch Loss 2.3157 (2.2893)	Arch Hard Loss 2.3157 (2.2893)	Arch Beta Loss 3.7889 (3.7872)	Arch depth Loss -3.7889 (-3.7872)	Prec@(1,5) (47.8%, 80.2%)	
12/27 05:05:21午前 searchDistribution_trainer.py:166 [INFO] Train: [ 21/49] Final Prec@1 47.8360%
12/27 05:05:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.3257	Prec@(1,5) (39.7%, 72.2%)
12/27 05:05:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.3093	Prec@(1,5) (40.1%, 72.3%)
12/27 05:05:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.3179	Prec@(1,5) (40.0%, 72.2%)
12/27 05:05:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.3297	Prec@(1,5) (39.8%, 72.1%)
12/27 05:05:45午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 39.8040%
12/27 05:05:45午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:05:46午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.8040%
12/27 05:06:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.8069 (1.7552)	Arch Loss 2.0381 (2.2695)	Arch Hard Loss 2.0381 (2.2695)	Arch Beta Loss 3.7901 (3.7894)	Arch depth Loss -3.7901 (-3.7894)	Prec@(1,5) (51.2%, 82.6%)	
12/27 05:07:09午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.7504 (1.7801)	Arch Loss 2.0679 (2.2650)	Arch Hard Loss 2.0679 (2.2650)	Arch Beta Loss 3.7911 (3.7900)	Arch depth Loss -3.7911 (-3.7900)	Prec@(1,5) (50.0%, 81.9%)	
12/27 05:07:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.9591 (1.8001)	Arch Loss 2.5817 (2.2521)	Arch Hard Loss 2.5817 (2.2521)	Arch Beta Loss 3.7916 (3.7905)	Arch depth Loss -3.7916 (-3.7905)	Prec@(1,5) (49.8%, 81.5%)	
12/27 05:08:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.9587 (1.8134)	Arch Loss 2.0068 (2.2454)	Arch Hard Loss 2.0068 (2.2454)	Arch Beta Loss 3.7921 (3.7908)	Arch depth Loss -3.7921 (-3.7908)	Prec@(1,5) (49.4%, 81.2%)	
12/27 05:08:27午前 searchDistribution_trainer.py:166 [INFO] Train: [ 22/49] Final Prec@1 49.4360%
12/27 05:08:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.4436	Prec@(1,5) (38.1%, 70.2%)
12/27 05:08:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.4384	Prec@(1,5) (38.1%, 70.0%)
12/27 05:08:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.4425	Prec@(1,5) (38.1%, 69.8%)
12/27 05:08:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.4393	Prec@(1,5) (38.0%, 69.8%)
12/27 05:08:51午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 38.0480%
12/27 05:08:51午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:08:51午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.8040%
12/27 05:09:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.4321 (1.6733)	Arch Loss 2.0475 (2.2041)	Arch Hard Loss 2.0475 (2.2041)	Arch Beta Loss 3.7930 (3.7926)	Arch depth Loss -3.7930 (-3.7926)	Prec@(1,5) (53.0%, 84.0%)	
12/27 05:10:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.7599 (1.7213)	Arch Loss 2.4408 (2.2136)	Arch Hard Loss 2.4408 (2.2136)	Arch Beta Loss 3.7929 (3.7928)	Arch depth Loss -3.7929 (-3.7928)	Prec@(1,5) (51.7%, 82.9%)	
12/27 05:10:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 2.2224 (1.7369)	Arch Loss 2.0451 (2.2042)	Arch Hard Loss 2.0451 (2.2042)	Arch Beta Loss 3.7933 (3.7930)	Arch depth Loss -3.7933 (-3.7930)	Prec@(1,5) (51.4%, 82.6%)	
12/27 05:11:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.5876 (1.7534)	Arch Loss 2.2821 (2.2011)	Arch Hard Loss 2.2821 (2.2011)	Arch Beta Loss 3.7940 (3.7931)	Arch depth Loss -3.7940 (-3.7931)	Prec@(1,5) (51.1%, 82.3%)	
12/27 05:11:36午前 searchDistribution_trainer.py:166 [INFO] Train: [ 23/49] Final Prec@1 51.1200%
12/27 05:11:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.5850	Prec@(1,5) (34.9%, 66.2%)
12/27 05:11:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.5756	Prec@(1,5) (35.1%, 67.0%)
12/27 05:11:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.5615	Prec@(1,5) (35.2%, 67.3%)
12/27 05:12:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.5537	Prec@(1,5) (35.3%, 67.6%)
12/27 05:12:00午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 35.3120%
12/27 05:12:00午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:12:00午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.8040%
12/27 05:12:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.4175 (1.6319)	Arch Loss 2.1597 (2.2021)	Arch Hard Loss 2.1597 (2.2021)	Arch Beta Loss 3.7947 (3.7945)	Arch depth Loss -3.7947 (-3.7945)	Prec@(1,5) (53.9%, 84.4%)	
12/27 05:13:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.6142 (1.6513)	Arch Loss 2.0865 (2.2039)	Arch Hard Loss 2.0865 (2.2039)	Arch Beta Loss 3.7946 (3.7945)	Arch depth Loss -3.7946 (-3.7945)	Prec@(1,5) (53.5%, 84.1%)	
12/27 05:14:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.9720 (1.6786)	Arch Loss 2.3392 (2.1965)	Arch Hard Loss 2.3392 (2.1965)	Arch Beta Loss 3.7951 (3.7946)	Arch depth Loss -3.7951 (-3.7946)	Prec@(1,5) (52.8%, 83.8%)	
12/27 05:14:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.7247 (1.6906)	Arch Loss 2.4389 (2.1951)	Arch Hard Loss 2.4389 (2.1951)	Arch Beta Loss 3.7961 (3.7948)	Arch depth Loss -3.7961 (-3.7948)	Prec@(1,5) (52.6%, 83.5%)	
12/27 05:14:46午前 searchDistribution_trainer.py:166 [INFO] Train: [ 24/49] Final Prec@1 52.5680%
12/27 05:14:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.4544	Prec@(1,5) (38.8%, 70.0%)
12/27 05:14:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.4628	Prec@(1,5) (38.6%, 69.7%)
12/27 05:15:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.4630	Prec@(1,5) (38.6%, 69.8%)
12/27 05:15:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.4695	Prec@(1,5) (38.6%, 69.5%)
12/27 05:15:10午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 38.5480%
12/27 05:15:10午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:15:10午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 39.8040%
12/27 05:15:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.6010 (1.5921)	Arch Loss 1.9387 (2.1699)	Arch Hard Loss 1.9387 (2.1699)	Arch Beta Loss 3.7968 (3.7966)	Arch depth Loss -3.7968 (-3.7966)	Prec@(1,5) (54.9%, 84.9%)	
12/27 05:16:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.6048 (1.6187)	Arch Loss 1.8808 (2.1740)	Arch Hard Loss 1.8808 (2.1740)	Arch Beta Loss 3.7969 (3.7968)	Arch depth Loss -3.7969 (-3.7968)	Prec@(1,5) (54.2%, 84.8%)	
12/27 05:17:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.6347 (1.6310)	Arch Loss 1.8643 (2.1594)	Arch Hard Loss 1.8643 (2.1594)	Arch Beta Loss 3.7972 (3.7969)	Arch depth Loss -3.7972 (-3.7969)	Prec@(1,5) (53.9%, 84.5%)	
12/27 05:17:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.8638 (1.6394)	Arch Loss 2.2311 (2.1650)	Arch Hard Loss 2.2311 (2.1650)	Arch Beta Loss 3.7971 (3.7969)	Arch depth Loss -3.7971 (-3.7969)	Prec@(1,5) (53.8%, 84.4%)	
12/27 05:17:55午前 searchDistribution_trainer.py:166 [INFO] Train: [ 25/49] Final Prec@1 53.8280%
12/27 05:18:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.1899	Prec@(1,5) (43.0%, 74.1%)
12/27 05:18:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.1903	Prec@(1,5) (42.7%, 74.4%)
12/27 05:18:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.1829	Prec@(1,5) (42.8%, 74.4%)
12/27 05:18:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.1776	Prec@(1,5) (43.0%, 74.5%)
12/27 05:18:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 43.0360%
12/27 05:18:19午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:18:20午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.0360%
12/27 05:19:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.7347 (1.5458)	Arch Loss 1.9780 (2.1909)	Arch Hard Loss 1.9780 (2.1909)	Arch Beta Loss 3.7969 (3.7971)	Arch depth Loss -3.7969 (-3.7971)	Prec@(1,5) (56.1%, 85.9%)	
12/27 05:19:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.4265 (1.5458)	Arch Loss 2.0590 (2.1961)	Arch Hard Loss 2.0590 (2.1961)	Arch Beta Loss 3.7970 (3.7971)	Arch depth Loss -3.7970 (-3.7971)	Prec@(1,5) (55.5%, 86.0%)	
12/27 05:20:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.3901 (1.5595)	Arch Loss 1.8861 (2.1558)	Arch Hard Loss 1.8861 (2.1558)	Arch Beta Loss 3.7972 (3.7971)	Arch depth Loss -3.7972 (-3.7971)	Prec@(1,5) (55.2%, 85.8%)	
12/27 05:21:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.8975 (1.5800)	Arch Loss 2.1872 (2.1615)	Arch Hard Loss 2.1872 (2.1615)	Arch Beta Loss 3.7980 (3.7972)	Arch depth Loss -3.7980 (-3.7972)	Prec@(1,5) (54.9%, 85.3%)	
12/27 05:21:06午前 searchDistribution_trainer.py:166 [INFO] Train: [ 26/49] Final Prec@1 54.9520%
12/27 05:21:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.1985	Prec@(1,5) (42.3%, 75.0%)
12/27 05:21:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.1742	Prec@(1,5) (43.2%, 74.8%)
12/27 05:21:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.1800	Prec@(1,5) (42.9%, 74.4%)
12/27 05:21:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.1696	Prec@(1,5) (43.1%, 74.6%)
12/27 05:21:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 43.0920%
12/27 05:21:31午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:21:31午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.0920%
12/27 05:22:14午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.4043 (1.4366)	Arch Loss 2.0456 (2.1246)	Arch Hard Loss 2.0456 (2.1246)	Arch Beta Loss 3.7987 (3.7984)	Arch depth Loss -3.7987 (-3.7984)	Prec@(1,5) (58.1%, 87.6%)	
12/27 05:22:56午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.7986 (1.5280)	Arch Loss 2.1266 (2.1775)	Arch Hard Loss 2.1266 (2.1775)	Arch Beta Loss 3.7980 (3.7984)	Arch depth Loss -3.7980 (-3.7984)	Prec@(1,5) (56.4%, 86.0%)	
12/27 05:23:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.7549 (1.5594)	Arch Loss 1.3581 (2.1587)	Arch Hard Loss 1.3581 (2.1587)	Arch Beta Loss 3.7976 (3.7982)	Arch depth Loss -3.7976 (-3.7982)	Prec@(1,5) (55.7%, 85.4%)	
12/27 05:24:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.5985 (1.5700)	Arch Loss 2.2724 (2.1580)	Arch Hard Loss 2.2724 (2.1580)	Arch Beta Loss 3.7975 (3.7980)	Arch depth Loss -3.7975 (-3.7980)	Prec@(1,5) (55.5%, 85.3%)	
12/27 05:24:17午前 searchDistribution_trainer.py:166 [INFO] Train: [ 27/49] Final Prec@1 55.5200%
12/27 05:24:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.2075	Prec@(1,5) (42.8%, 74.1%)
12/27 05:24:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.2219	Prec@(1,5) (42.5%, 73.9%)
12/27 05:24:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.2383	Prec@(1,5) (42.2%, 73.6%)
12/27 05:24:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.2365	Prec@(1,5) (42.2%, 73.5%)
12/27 05:24:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 42.2360%
12/27 05:24:41午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:24:41午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.0920%
12/27 05:25:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.4589 (1.4509)	Arch Loss 2.2382 (2.1086)	Arch Hard Loss 2.2382 (2.1086)	Arch Beta Loss 3.7981 (3.7976)	Arch depth Loss -3.7981 (-3.7976)	Prec@(1,5) (58.1%, 87.5%)	
12/27 05:26:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.7280 (1.4692)	Arch Loss 2.0110 (2.1204)	Arch Hard Loss 2.0110 (2.1204)	Arch Beta Loss 3.7984 (3.7979)	Arch depth Loss -3.7984 (-3.7979)	Prec@(1,5) (57.7%, 86.9%)	
12/27 05:26:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.5766 (1.4762)	Arch Loss 2.1381 (2.1250)	Arch Hard Loss 2.1381 (2.1250)	Arch Beta Loss 3.7984 (3.7981)	Arch depth Loss -3.7984 (-3.7981)	Prec@(1,5) (57.8%, 86.8%)	
12/27 05:27:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.9022 (1.4810)	Arch Loss 1.8115 (2.1048)	Arch Hard Loss 1.8115 (2.1048)	Arch Beta Loss 3.7977 (3.7981)	Arch depth Loss -3.7977 (-3.7981)	Prec@(1,5) (57.6%, 86.8%)	
12/27 05:27:24午前 searchDistribution_trainer.py:166 [INFO] Train: [ 28/49] Final Prec@1 57.5800%
12/27 05:27:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.3909	Prec@(1,5) (41.3%, 71.6%)
12/27 05:27:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.3694	Prec@(1,5) (40.7%, 72.0%)
12/27 05:27:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.3660	Prec@(1,5) (40.7%, 72.2%)
12/27 05:27:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.3622	Prec@(1,5) (40.6%, 72.3%)
12/27 05:27:47午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 40.5840%
12/27 05:27:47午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:27:48午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.0920%
12/27 05:28:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.3480 (1.3572)	Arch Loss 1.9560 (2.1143)	Arch Hard Loss 1.9560 (2.1143)	Arch Beta Loss 3.7979 (3.7978)	Arch depth Loss -3.7979 (-3.7978)	Prec@(1,5) (61.2%, 88.7%)	
12/27 05:29:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.1108 (1.3709)	Arch Loss 1.7187 (2.1080)	Arch Hard Loss 1.7187 (2.1080)	Arch Beta Loss 3.7978 (3.7979)	Arch depth Loss -3.7978 (-3.7979)	Prec@(1,5) (60.7%, 88.4%)	
12/27 05:29:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.4501 (1.4082)	Arch Loss 2.5239 (2.0947)	Arch Hard Loss 2.5239 (2.0947)	Arch Beta Loss 3.7972 (3.7977)	Arch depth Loss -3.7972 (-3.7977)	Prec@(1,5) (59.5%, 87.9%)	
12/27 05:30:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.4215 (1.4192)	Arch Loss 1.8330 (2.1009)	Arch Hard Loss 1.8330 (2.1009)	Arch Beta Loss 3.7978 (3.7977)	Arch depth Loss -3.7978 (-3.7977)	Prec@(1,5) (59.0%, 87.7%)	
12/27 05:30:32午前 searchDistribution_trainer.py:166 [INFO] Train: [ 29/49] Final Prec@1 58.9960%
12/27 05:30:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.3498	Prec@(1,5) (41.9%, 72.6%)
12/27 05:30:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.3875	Prec@(1,5) (41.0%, 71.8%)
12/27 05:30:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.4143	Prec@(1,5) (40.5%, 71.3%)
12/27 05:30:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.4132	Prec@(1,5) (40.5%, 71.3%)
12/27 05:30:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 40.4600%
12/27 05:30:56午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:30:56午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.0920%
12/27 05:31:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.6517 (1.3052)	Arch Loss 2.4852 (2.1242)	Arch Hard Loss 2.4852 (2.1242)	Arch Beta Loss 3.7978 (3.7979)	Arch depth Loss -3.7978 (-3.7979)	Prec@(1,5) (62.1%, 89.7%)	
12/27 05:32:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.0652 (1.3364)	Arch Loss 2.7530 (2.1134)	Arch Hard Loss 2.7530 (2.1134)	Arch Beta Loss 3.7976 (3.7978)	Arch depth Loss -3.7976 (-3.7978)	Prec@(1,5) (61.6%, 88.9%)	
12/27 05:33:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.2109 (1.3425)	Arch Loss 2.1959 (2.1084)	Arch Hard Loss 2.1959 (2.1084)	Arch Beta Loss 3.7975 (3.7978)	Arch depth Loss -3.7975 (-3.7978)	Prec@(1,5) (61.1%, 88.9%)	
12/27 05:33:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.3265 (1.3659)	Arch Loss 2.0126 (2.0978)	Arch Hard Loss 2.0126 (2.0978)	Arch Beta Loss 3.7973 (3.7977)	Arch depth Loss -3.7973 (-3.7977)	Prec@(1,5) (60.5%, 88.6%)	
12/27 05:33:42午前 searchDistribution_trainer.py:166 [INFO] Train: [ 30/49] Final Prec@1 60.5440%
12/27 05:33:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.0021	Prec@(1,5) (47.8%, 78.0%)
12/27 05:33:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.0280	Prec@(1,5) (47.1%, 77.6%)
12/27 05:34:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.0453	Prec@(1,5) (46.7%, 77.2%)
12/27 05:34:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.0475	Prec@(1,5) (46.7%, 77.2%)
12/27 05:34:05午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 46.7000%
12/27 05:34:05午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:34:06午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:34:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.9222 (1.2182)	Arch Loss 1.9728 (2.0325)	Arch Hard Loss 1.9728 (2.0325)	Arch Beta Loss 3.7970 (3.7972)	Arch depth Loss -3.7970 (-3.7972)	Prec@(1,5) (64.2%, 90.9%)	
12/27 05:35:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.2720 (1.2557)	Arch Loss 1.9406 (2.0789)	Arch Hard Loss 1.9406 (2.0789)	Arch Beta Loss 3.7969 (3.7971)	Arch depth Loss -3.7969 (-3.7971)	Prec@(1,5) (62.9%, 90.3%)	
12/27 05:36:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.1168 (1.2823)	Arch Loss 1.9912 (2.0879)	Arch Hard Loss 1.9912 (2.0879)	Arch Beta Loss 3.7970 (3.7971)	Arch depth Loss -3.7970 (-3.7971)	Prec@(1,5) (62.4%, 89.8%)	
12/27 05:36:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.2485 (1.2964)	Arch Loss 2.4182 (2.0904)	Arch Hard Loss 2.4182 (2.0904)	Arch Beta Loss 3.7966 (3.7970)	Arch depth Loss -3.7966 (-3.7970)	Prec@(1,5) (62.1%, 89.6%)	
12/27 05:36:51午前 searchDistribution_trainer.py:166 [INFO] Train: [ 31/49] Final Prec@1 62.0760%
12/27 05:36:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.2506	Prec@(1,5) (43.5%, 74.7%)
12/27 05:37:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.2321	Prec@(1,5) (44.1%, 75.1%)
12/27 05:37:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.2213	Prec@(1,5) (44.2%, 75.4%)
12/27 05:37:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.2177	Prec@(1,5) (44.4%, 75.4%)
12/27 05:37:15午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 44.3760%
12/27 05:37:15午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:37:15午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:37:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.8923 (1.2115)	Arch Loss 2.4352 (2.0466)	Arch Hard Loss 2.4352 (2.0466)	Arch Beta Loss 3.7972 (3.7969)	Arch depth Loss -3.7972 (-3.7969)	Prec@(1,5) (65.2%, 90.2%)	
12/27 05:38:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.2864 (1.2190)	Arch Loss 1.9659 (2.0893)	Arch Hard Loss 1.9659 (2.0893)	Arch Beta Loss 3.7962 (3.7969)	Arch depth Loss -3.7962 (-3.7969)	Prec@(1,5) (64.7%, 90.5%)	
12/27 05:39:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.0084 (1.2367)	Arch Loss 2.2348 (2.0817)	Arch Hard Loss 2.2348 (2.0817)	Arch Beta Loss 3.7959 (3.7966)	Arch depth Loss -3.7959 (-3.7966)	Prec@(1,5) (64.0%, 90.4%)	
12/27 05:39:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.2510 (1.2555)	Arch Loss 2.1640 (2.0812)	Arch Hard Loss 2.1640 (2.0812)	Arch Beta Loss 3.7955 (3.7964)	Arch depth Loss -3.7955 (-3.7964)	Prec@(1,5) (63.4%, 90.2%)	
12/27 05:39:59午前 searchDistribution_trainer.py:166 [INFO] Train: [ 32/49] Final Prec@1 63.4120%
12/27 05:40:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.2046	Prec@(1,5) (44.8%, 75.9%)
12/27 05:40:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.1854	Prec@(1,5) (45.2%, 75.8%)
12/27 05:40:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.1896	Prec@(1,5) (44.9%, 75.6%)
12/27 05:40:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.1802	Prec@(1,5) (44.9%, 75.8%)
12/27 05:40:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 44.9240%
12/27 05:40:23午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:40:24午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:41:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.2490 (1.1304)	Arch Loss 2.4508 (2.0466)	Arch Hard Loss 2.4508 (2.0466)	Arch Beta Loss 3.7956 (3.7956)	Arch depth Loss -3.7956 (-3.7956)	Prec@(1,5) (66.8%, 92.4%)	
12/27 05:41:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.9916 (1.1550)	Arch Loss 1.8970 (2.0675)	Arch Hard Loss 1.8970 (2.0675)	Arch Beta Loss 3.7948 (3.7953)	Arch depth Loss -3.7948 (-3.7953)	Prec@(1,5) (66.2%, 91.8%)	
12/27 05:42:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.0412 (1.1715)	Arch Loss 1.5730 (2.0600)	Arch Hard Loss 1.5730 (2.0600)	Arch Beta Loss 3.7944 (3.7951)	Arch depth Loss -3.7944 (-3.7951)	Prec@(1,5) (65.8%, 91.5%)	
12/27 05:43:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.0914 (1.1879)	Arch Loss 2.0021 (2.0593)	Arch Hard Loss 2.0021 (2.0593)	Arch Beta Loss 3.7942 (3.7949)	Arch depth Loss -3.7942 (-3.7949)	Prec@(1,5) (65.3%, 91.1%)	
12/27 05:43:06午前 searchDistribution_trainer.py:166 [INFO] Train: [ 33/49] Final Prec@1 65.3000%
12/27 05:43:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.3642	Prec@(1,5) (41.9%, 73.0%)
12/27 05:43:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.3670	Prec@(1,5) (42.0%, 72.6%)
12/27 05:43:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.3652	Prec@(1,5) (42.2%, 72.7%)
12/27 05:43:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.3721	Prec@(1,5) (41.9%, 72.7%)
12/27 05:43:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 41.9000%
12/27 05:43:29午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:43:30午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:44:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 1.2468 (1.0752)	Arch Loss 1.9662 (2.0608)	Arch Hard Loss 1.9662 (2.0608)	Arch Beta Loss 3.7933 (3.7938)	Arch depth Loss -3.7933 (-3.7938)	Prec@(1,5) (68.0%, 92.7%)	
12/27 05:44:56午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 1.3026 (1.1046)	Arch Loss 1.9447 (2.0762)	Arch Hard Loss 1.9447 (2.0762)	Arch Beta Loss 3.7932 (3.7935)	Arch depth Loss -3.7932 (-3.7935)	Prec@(1,5) (67.4%, 92.3%)	
12/27 05:45:38午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.3146 (1.1160)	Arch Loss 1.9736 (2.0611)	Arch Hard Loss 1.9736 (2.0611)	Arch Beta Loss 3.7931 (3.7934)	Arch depth Loss -3.7931 (-3.7934)	Prec@(1,5) (67.2%, 92.2%)	
12/27 05:46:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.5420 (1.1301)	Arch Loss 2.4641 (2.0665)	Arch Hard Loss 2.4641 (2.0665)	Arch Beta Loss 3.7925 (3.7932)	Arch depth Loss -3.7925 (-3.7932)	Prec@(1,5) (66.7%, 92.0%)	
12/27 05:46:17午前 searchDistribution_trainer.py:166 [INFO] Train: [ 34/49] Final Prec@1 66.7120%
12/27 05:46:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.4031	Prec@(1,5) (41.6%, 73.1%)
12/27 05:46:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.3938	Prec@(1,5) (41.8%, 73.5%)
12/27 05:46:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.4069	Prec@(1,5) (42.3%, 73.3%)
12/27 05:46:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.3954	Prec@(1,5) (42.5%, 73.5%)
12/27 05:46:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 42.4680%
12/27 05:46:41午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:46:41午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:47:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 1.0415 (1.0137)	Arch Loss 2.4044 (2.0548)	Arch Hard Loss 2.4044 (2.0548)	Arch Beta Loss 3.7916 (3.7920)	Arch depth Loss -3.7916 (-3.7920)	Prec@(1,5) (70.0%, 93.3%)	
12/27 05:48:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.4238 (1.0475)	Arch Loss 1.5707 (2.0713)	Arch Hard Loss 1.5707 (2.0713)	Arch Beta Loss 3.7915 (3.7918)	Arch depth Loss -3.7915 (-3.7918)	Prec@(1,5) (69.0%, 93.0%)	
12/27 05:48:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.4270 (1.0654)	Arch Loss 2.5181 (2.0707)	Arch Hard Loss 2.5181 (2.0707)	Arch Beta Loss 3.7914 (3.7916)	Arch depth Loss -3.7914 (-3.7916)	Prec@(1,5) (68.3%, 92.8%)	
12/27 05:49:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.1471 (1.0742)	Arch Loss 2.0062 (2.0704)	Arch Hard Loss 2.0062 (2.0704)	Arch Beta Loss 3.7912 (3.7915)	Arch depth Loss -3.7912 (-3.7915)	Prec@(1,5) (68.2%, 92.7%)	
12/27 05:49:27午前 searchDistribution_trainer.py:166 [INFO] Train: [ 35/49] Final Prec@1 68.2080%
12/27 05:49:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.4530	Prec@(1,5) (41.9%, 72.5%)
12/27 05:49:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.4338	Prec@(1,5) (41.9%, 73.1%)
12/27 05:49:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.4244	Prec@(1,5) (42.1%, 73.1%)
12/27 05:49:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.4387	Prec@(1,5) (41.8%, 72.8%)
12/27 05:49:52午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 41.8120%
12/27 05:49:52午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:49:53午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:50:36午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.9444 (1.0075)	Arch Loss 1.6950 (2.1001)	Arch Hard Loss 1.6950 (2.1001)	Arch Beta Loss 3.7901 (3.7905)	Arch depth Loss -3.7901 (-3.7905)	Prec@(1,5) (69.8%, 93.5%)	
12/27 05:51:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 1.1664 (1.0105)	Arch Loss 2.1153 (2.0805)	Arch Hard Loss 2.1153 (2.0805)	Arch Beta Loss 3.7899 (3.7902)	Arch depth Loss -3.7899 (-3.7902)	Prec@(1,5) (70.1%, 93.4%)	
12/27 05:51:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.8878 (1.0185)	Arch Loss 2.1349 (2.0593)	Arch Hard Loss 2.1349 (2.0593)	Arch Beta Loss 3.7898 (3.7901)	Arch depth Loss -3.7898 (-3.7901)	Prec@(1,5) (69.9%, 93.3%)	
12/27 05:52:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 1.2892 (1.0226)	Arch Loss 1.7669 (2.0687)	Arch Hard Loss 1.7669 (2.0687)	Arch Beta Loss 3.7895 (3.7900)	Arch depth Loss -3.7895 (-3.7900)	Prec@(1,5) (69.6%, 93.3%)	
12/27 05:52:35午前 searchDistribution_trainer.py:166 [INFO] Train: [ 36/49] Final Prec@1 69.5840%
12/27 05:52:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.1913	Prec@(1,5) (46.6%, 76.1%)
12/27 05:52:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.2183	Prec@(1,5) (45.8%, 75.7%)
12/27 05:52:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.2226	Prec@(1,5) (45.6%, 75.9%)
12/27 05:52:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.2237	Prec@(1,5) (45.7%, 75.9%)
12/27 05:52:59午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 45.6720%
12/27 05:52:59午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 05:52:59午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 46.7000%
12/27 05:53:42午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 1.1952 (0.9399)	Arch Loss 1.6971 (2.0539)	Arch Hard Loss 1.6971 (2.0539)	Arch Beta Loss 3.7888 (3.7891)	Arch depth Loss -3.7888 (-3.7891)	Prec@(1,5) (72.0%, 94.3%)	
12/27 05:54:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 1.0003 (0.9508)	Arch Loss 2.3562 (2.0670)	Arch Hard Loss 2.3562 (2.0670)	Arch Beta Loss 3.7883 (3.7888)	Arch depth Loss -3.7883 (-3.7888)	Prec@(1,5) (71.7%, 94.2%)	
12/27 05:55:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 1.1643 (0.9623)	Arch Loss 1.9557 (2.0649)	Arch Hard Loss 1.9557 (2.0649)	Arch Beta Loss 3.7881 (3.7886)	Arch depth Loss -3.7881 (-3.7886)	Prec@(1,5) (71.3%, 94.0%)	
12/27 05:55:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 1.1549 (0.9708)	Arch Loss 2.2451 (2.0639)	Arch Hard Loss 2.2451 (2.0639)	Arch Beta Loss 3.7871 (3.7883)	Arch depth Loss -3.7871 (-3.7883)	Prec@(1,5) (70.9%, 93.9%)	
12/27 05:55:44午前 searchDistribution_trainer.py:166 [INFO] Train: [ 37/49] Final Prec@1 70.9400%
12/27 05:55:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.1820	Prec@(1,5) (46.5%, 77.4%)
12/27 05:55:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.1501	Prec@(1,5) (47.3%, 77.6%)
12/27 05:56:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.1554	Prec@(1,5) (47.4%, 77.3%)
12/27 05:56:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.1560	Prec@(1,5) (47.4%, 77.4%)
12/27 05:56:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 47.3840%
12/27 05:56:09午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:56:09午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 47.3840%
12/27 05:56:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7540 (0.8728)	Arch Loss 1.9434 (2.0830)	Arch Hard Loss 1.9434 (2.0830)	Arch Beta Loss 3.7868 (3.7869)	Arch depth Loss -3.7868 (-3.7869)	Prec@(1,5) (74.3%, 95.3%)	
12/27 05:57:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 1.1766 (0.9045)	Arch Loss 2.0379 (2.0652)	Arch Hard Loss 2.0379 (2.0652)	Arch Beta Loss 3.7865 (3.7869)	Arch depth Loss -3.7865 (-3.7869)	Prec@(1,5) (73.4%, 94.8%)	
12/27 05:58:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.9348 (0.9122)	Arch Loss 1.9006 (2.0731)	Arch Hard Loss 1.9006 (2.0731)	Arch Beta Loss 3.7857 (3.7866)	Arch depth Loss -3.7857 (-3.7866)	Prec@(1,5) (73.1%, 94.6%)	
12/27 05:58:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.9747 (0.9217)	Arch Loss 2.2524 (2.0630)	Arch Hard Loss 2.2524 (2.0630)	Arch Beta Loss 3.7852 (3.7863)	Arch depth Loss -3.7852 (-3.7863)	Prec@(1,5) (72.7%, 94.6%)	
12/27 05:58:53午前 searchDistribution_trainer.py:166 [INFO] Train: [ 38/49] Final Prec@1 72.7600%
12/27 05:59:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.1291	Prec@(1,5) (47.5%, 77.6%)
12/27 05:59:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.0819	Prec@(1,5) (48.4%, 78.0%)
12/27 05:59:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.0849	Prec@(1,5) (48.3%, 78.1%)
12/27 05:59:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.0898	Prec@(1,5) (48.3%, 78.1%)
12/27 05:59:18午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 48.3360%
12/27 05:59:18午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/27 05:59:18午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.3360%
12/27 06:00:02午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.9265 (0.8379)	Arch Loss 1.6568 (2.0641)	Arch Hard Loss 1.6568 (2.0641)	Arch Beta Loss 3.7849 (3.7851)	Arch depth Loss -3.7849 (-3.7851)	Prec@(1,5) (74.7%, 95.4%)	
12/27 06:00:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.7665 (0.8454)	Arch Loss 2.1824 (2.0580)	Arch Hard Loss 2.1824 (2.0580)	Arch Beta Loss 3.7842 (3.7848)	Arch depth Loss -3.7842 (-3.7848)	Prec@(1,5) (74.7%, 95.2%)	
12/27 06:01:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 1.1767 (0.8667)	Arch Loss 1.5330 (2.0454)	Arch Hard Loss 1.5330 (2.0454)	Arch Beta Loss 3.7835 (3.7844)	Arch depth Loss -3.7835 (-3.7844)	Prec@(1,5) (74.0%, 95.1%)	
12/27 06:02:04午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.7904 (0.8804)	Arch Loss 2.0051 (2.0531)	Arch Hard Loss 2.0051 (2.0531)	Arch Beta Loss 3.7828 (3.7841)	Arch depth Loss -3.7828 (-3.7841)	Prec@(1,5) (73.6%, 94.9%)	
12/27 06:02:05午前 searchDistribution_trainer.py:166 [INFO] Train: [ 39/49] Final Prec@1 73.6440%
12/27 06:02:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.0414	Prec@(1,5) (48.7%, 78.9%)
12/27 06:02:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.0736	Prec@(1,5) (48.5%, 78.6%)
12/27 06:02:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.0605	Prec@(1,5) (48.8%, 78.9%)
12/27 06:02:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.0617	Prec@(1,5) (48.9%, 78.9%)
12/27 06:02:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 48.8800%
12/27 06:02:29午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 06:02:30午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.8800%
12/27 06:03:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.5755 (0.7986)	Arch Loss 2.3546 (2.0376)	Arch Hard Loss 2.3546 (2.0376)	Arch Beta Loss 3.7826 (3.7827)	Arch depth Loss -3.7826 (-3.7827)	Prec@(1,5) (76.0%, 95.8%)	
12/27 06:03:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.7019 (0.8141)	Arch Loss 2.0894 (2.0545)	Arch Hard Loss 2.0894 (2.0545)	Arch Beta Loss 3.7823 (3.7827)	Arch depth Loss -3.7823 (-3.7827)	Prec@(1,5) (75.6%, 95.8%)	
12/27 06:04:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.8808 (0.8244)	Arch Loss 1.8389 (2.0706)	Arch Hard Loss 1.8389 (2.0706)	Arch Beta Loss 3.7814 (3.7823)	Arch depth Loss -3.7814 (-3.7823)	Prec@(1,5) (75.5%, 95.7%)	
12/27 06:05:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.8761 (0.8263)	Arch Loss 2.0407 (2.0665)	Arch Hard Loss 2.0407 (2.0665)	Arch Beta Loss 3.7806 (3.7820)	Arch depth Loss -3.7806 (-3.7820)	Prec@(1,5) (75.5%, 95.6%)	
12/27 06:05:16午前 searchDistribution_trainer.py:166 [INFO] Train: [ 40/49] Final Prec@1 75.4480%
12/27 06:05:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 2.0933	Prec@(1,5) (48.8%, 78.4%)
12/27 06:05:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 2.0610	Prec@(1,5) (49.4%, 78.8%)
12/27 06:05:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 2.0722	Prec@(1,5) (48.9%, 78.6%)
12/27 06:05:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 2.0868	Prec@(1,5) (48.8%, 78.4%)
12/27 06:05:40午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 48.8120%
12/27 06:05:40午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:05:40午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.8800%
12/27 06:06:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.5928 (0.7462)	Arch Loss 1.8268 (2.0791)	Arch Hard Loss 1.8268 (2.0791)	Arch Beta Loss 3.7803 (3.7805)	Arch depth Loss -3.7803 (-3.7805)	Prec@(1,5) (78.5%, 96.2%)	
12/27 06:07:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.8183 (0.7660)	Arch Loss 1.9484 (2.0742)	Arch Hard Loss 1.9484 (2.0742)	Arch Beta Loss 3.7801 (3.7803)	Arch depth Loss -3.7801 (-3.7803)	Prec@(1,5) (77.8%, 96.1%)	
12/27 06:07:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.7278 (0.7770)	Arch Loss 1.8983 (2.0794)	Arch Hard Loss 1.8983 (2.0794)	Arch Beta Loss 3.7792 (3.7801)	Arch depth Loss -3.7792 (-3.7801)	Prec@(1,5) (77.3%, 96.2%)	
12/27 06:08:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.8875 (0.7865)	Arch Loss 1.8576 (2.0724)	Arch Hard Loss 1.8576 (2.0724)	Arch Beta Loss 3.7783 (3.7798)	Arch depth Loss -3.7783 (-3.7798)	Prec@(1,5) (76.9%, 96.0%)	
12/27 06:08:25午前 searchDistribution_trainer.py:166 [INFO] Train: [ 41/49] Final Prec@1 76.9120%
12/27 06:08:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.2346	Prec@(1,5) (47.1%, 76.7%)
12/27 06:08:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.2151	Prec@(1,5) (46.9%, 76.9%)
12/27 06:08:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.2065	Prec@(1,5) (46.8%, 77.0%)
12/27 06:08:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.2357	Prec@(1,5) (46.4%, 76.6%)
12/27 06:08:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 46.4400%
12/27 06:08:49午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:08:49午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.8800%
12/27 06:09:33午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.8812 (0.7104)	Arch Loss 1.9579 (2.0576)	Arch Hard Loss 1.9579 (2.0576)	Arch Beta Loss 3.7777 (3.7779)	Arch depth Loss -3.7777 (-3.7779)	Prec@(1,5) (79.2%, 96.6%)	
12/27 06:10:14午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.8986 (0.7285)	Arch Loss 1.6994 (2.0776)	Arch Hard Loss 1.6994 (2.0776)	Arch Beta Loss 3.7774 (3.7777)	Arch depth Loss -3.7774 (-3.7777)	Prec@(1,5) (78.5%, 96.5%)	
12/27 06:10:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 1.0381 (0.7459)	Arch Loss 1.7843 (2.0709)	Arch Hard Loss 1.7843 (2.0709)	Arch Beta Loss 3.7766 (3.7775)	Arch depth Loss -3.7766 (-3.7775)	Prec@(1,5) (78.1%, 96.2%)	
12/27 06:11:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.8308 (0.7505)	Arch Loss 2.1654 (2.0686)	Arch Hard Loss 2.1654 (2.0686)	Arch Beta Loss 3.7764 (3.7773)	Arch depth Loss -3.7764 (-3.7773)	Prec@(1,5) (78.0%, 96.2%)	
12/27 06:11:32午前 searchDistribution_trainer.py:166 [INFO] Train: [ 42/49] Final Prec@1 77.9600%
12/27 06:11:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.2348	Prec@(1,5) (46.5%, 76.9%)
12/27 06:11:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.2129	Prec@(1,5) (47.2%, 77.2%)
12/27 06:11:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.1718	Prec@(1,5) (47.7%, 77.8%)
12/27 06:11:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.1644	Prec@(1,5) (48.0%, 77.9%)
12/27 06:11:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 47.9960%
12/27 06:11:56午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:11:56午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.8800%
12/27 06:12:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.8503 (0.7018)	Arch Loss 2.1162 (2.0552)	Arch Hard Loss 2.1162 (2.0552)	Arch Beta Loss 3.7761 (3.7763)	Arch depth Loss -3.7761 (-3.7763)	Prec@(1,5) (80.1%, 96.7%)	
12/27 06:13:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.7286 (0.7096)	Arch Loss 2.3901 (2.0735)	Arch Hard Loss 2.3901 (2.0735)	Arch Beta Loss 3.7754 (3.7760)	Arch depth Loss -3.7754 (-3.7760)	Prec@(1,5) (79.3%, 96.6%)	
12/27 06:14:02午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.8134 (0.7131)	Arch Loss 2.1693 (2.0665)	Arch Hard Loss 2.1693 (2.0665)	Arch Beta Loss 3.7750 (3.7756)	Arch depth Loss -3.7750 (-3.7756)	Prec@(1,5) (79.2%, 96.7%)	
12/27 06:14:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.4850 (0.7187)	Arch Loss 1.9250 (2.0735)	Arch Hard Loss 1.9250 (2.0735)	Arch Beta Loss 3.7740 (3.7754)	Arch depth Loss -3.7740 (-3.7754)	Prec@(1,5) (79.0%, 96.6%)	
12/27 06:14:40午前 searchDistribution_trainer.py:166 [INFO] Train: [ 43/49] Final Prec@1 78.9720%
12/27 06:14:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 2.0530	Prec@(1,5) (49.8%, 79.3%)
12/27 06:14:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.0617	Prec@(1,5) (49.7%, 79.0%)
12/27 06:14:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.0676	Prec@(1,5) (49.3%, 78.7%)
12/27 06:15:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.0645	Prec@(1,5) (49.5%, 78.8%)
12/27 06:15:04午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 49.4400%
12/27 06:15:04午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:15:04午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 49.4400%
12/27 06:15:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.8125 (0.6586)	Arch Loss 1.7149 (2.0833)	Arch Hard Loss 1.7149 (2.0833)	Arch Beta Loss 3.7735 (3.7736)	Arch depth Loss -3.7735 (-3.7736)	Prec@(1,5) (80.7%, 97.2%)	
12/27 06:16:29午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.5375 (0.6685)	Arch Loss 2.2336 (2.0776)	Arch Hard Loss 2.2336 (2.0776)	Arch Beta Loss 3.7734 (3.7736)	Arch depth Loss -3.7734 (-3.7736)	Prec@(1,5) (80.3%, 97.2%)	
12/27 06:17:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.8221 (0.6799)	Arch Loss 1.5733 (2.0622)	Arch Hard Loss 1.5733 (2.0622)	Arch Beta Loss 3.7728 (3.7734)	Arch depth Loss -3.7728 (-3.7734)	Prec@(1,5) (80.1%, 97.1%)	
12/27 06:17:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.8719 (0.6884)	Arch Loss 1.8735 (2.0657)	Arch Hard Loss 1.8735 (2.0657)	Arch Beta Loss 3.7725 (3.7732)	Arch depth Loss -3.7725 (-3.7732)	Prec@(1,5) (79.8%, 97.0%)	
12/27 06:17:48午前 searchDistribution_trainer.py:166 [INFO] Train: [ 44/49] Final Prec@1 79.7280%
12/27 06:17:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.0269	Prec@(1,5) (50.3%, 79.5%)
12/27 06:18:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.0278	Prec@(1,5) (50.2%, 79.6%)
12/27 06:18:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.0430	Prec@(1,5) (49.9%, 79.4%)
12/27 06:18:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.0390	Prec@(1,5) (50.1%, 79.4%)
12/27 06:18:11午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 50.0720%
12/27 06:18:11午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:18:12午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.0720%
12/27 06:18:55午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.5638 (0.6304)	Arch Loss 2.5629 (2.1150)	Arch Hard Loss 2.5629 (2.1150)	Arch Beta Loss 3.7719 (3.7721)	Arch depth Loss -3.7719 (-3.7721)	Prec@(1,5) (81.7%, 97.6%)	
12/27 06:19:37午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.5507 (0.6430)	Arch Loss 1.9215 (2.1053)	Arch Hard Loss 1.9215 (2.1053)	Arch Beta Loss 3.7709 (3.7717)	Arch depth Loss -3.7709 (-3.7717)	Prec@(1,5) (81.4%, 97.3%)	
12/27 06:20:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.6108 (0.6503)	Arch Loss 1.7775 (2.0830)	Arch Hard Loss 1.7775 (2.0830)	Arch Beta Loss 3.7705 (3.7714)	Arch depth Loss -3.7705 (-3.7714)	Prec@(1,5) (81.1%, 97.2%)	
12/27 06:20:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.6708 (0.6572)	Arch Loss 1.9141 (2.0756)	Arch Hard Loss 1.9141 (2.0756)	Arch Beta Loss 3.7699 (3.7712)	Arch depth Loss -3.7699 (-3.7712)	Prec@(1,5) (80.9%, 97.1%)	
12/27 06:20:58午前 searchDistribution_trainer.py:166 [INFO] Train: [ 45/49] Final Prec@1 80.9040%
12/27 06:21:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 1.9577	Prec@(1,5) (51.8%, 80.4%)
12/27 06:21:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 1.9885	Prec@(1,5) (51.4%, 80.1%)
12/27 06:21:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 1.9954	Prec@(1,5) (51.3%, 80.1%)
12/27 06:21:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.0257	Prec@(1,5) (50.8%, 79.9%)
12/27 06:21:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 50.8080%
12/27 06:21:23午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:21:23午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.8080%
12/27 06:22:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.5612 (0.6260)	Arch Loss 1.8682 (2.0571)	Arch Hard Loss 1.8682 (2.0571)	Arch Beta Loss 3.7699 (3.7699)	Arch depth Loss -3.7699 (-3.7699)	Prec@(1,5) (81.5%, 97.6%)	
12/27 06:22:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.8565 (0.6335)	Arch Loss 1.9762 (2.0616)	Arch Hard Loss 1.9762 (2.0616)	Arch Beta Loss 3.7701 (3.7699)	Arch depth Loss -3.7701 (-3.7699)	Prec@(1,5) (81.4%, 97.5%)	
12/27 06:23:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.6242 (0.6361)	Arch Loss 2.2739 (2.0779)	Arch Hard Loss 2.2739 (2.0779)	Arch Beta Loss 3.7687 (3.7697)	Arch depth Loss -3.7687 (-3.7697)	Prec@(1,5) (81.5%, 97.5%)	
12/27 06:24:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.6122 (0.6438)	Arch Loss 2.1538 (2.0703)	Arch Hard Loss 2.1538 (2.0703)	Arch Beta Loss 3.7683 (3.7694)	Arch depth Loss -3.7683 (-3.7694)	Prec@(1,5) (81.3%, 97.4%)	
12/27 06:24:08午前 searchDistribution_trainer.py:166 [INFO] Train: [ 46/49] Final Prec@1 81.2680%
12/27 06:24:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.0342	Prec@(1,5) (51.1%, 79.1%)
12/27 06:24:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.0253	Prec@(1,5) (51.3%, 79.5%)
12/27 06:24:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.0258	Prec@(1,5) (51.1%, 79.4%)
12/27 06:24:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.0262	Prec@(1,5) (50.8%, 79.7%)
12/27 06:24:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 50.7600%
12/27 06:24:32午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:24:32午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.8080%
12/27 06:25:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.7150 (0.5969)	Arch Loss 1.9092 (2.0879)	Arch Hard Loss 1.9092 (2.0879)	Arch Beta Loss 3.7679 (3.7681)	Arch depth Loss -3.7679 (-3.7681)	Prec@(1,5) (83.2%, 97.7%)	
12/27 06:25:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.8199 (0.6180)	Arch Loss 2.9963 (2.0681)	Arch Hard Loss 2.9963 (2.0681)	Arch Beta Loss 3.7681 (3.7680)	Arch depth Loss -3.7681 (-3.7680)	Prec@(1,5) (82.5%, 97.5%)	
12/27 06:26:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.6433 (0.6235)	Arch Loss 2.0660 (2.0725)	Arch Hard Loss 2.0660 (2.0725)	Arch Beta Loss 3.7673 (3.7679)	Arch depth Loss -3.7673 (-3.7679)	Prec@(1,5) (82.3%, 97.4%)	
12/27 06:27:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.7499 (0.6306)	Arch Loss 1.7491 (2.0784)	Arch Hard Loss 1.7491 (2.0784)	Arch Beta Loss 3.7666 (3.7676)	Arch depth Loss -3.7666 (-3.7676)	Prec@(1,5) (82.0%, 97.4%)	
12/27 06:27:18午前 searchDistribution_trainer.py:166 [INFO] Train: [ 47/49] Final Prec@1 81.9640%
12/27 06:27:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.0627	Prec@(1,5) (49.9%, 78.9%)
12/27 06:27:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.0583	Prec@(1,5) (50.2%, 79.2%)
12/27 06:27:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.0514	Prec@(1,5) (50.3%, 79.4%)
12/27 06:27:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.0496	Prec@(1,5) (50.3%, 79.5%)
12/27 06:27:42午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 50.2960%
12/27 06:27:42午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:27:42午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.8080%
12/27 06:28:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.4872 (0.5907)	Arch Loss 1.9419 (2.0670)	Arch Hard Loss 1.9419 (2.0670)	Arch Beta Loss 3.7665 (3.7666)	Arch depth Loss -3.7665 (-3.7666)	Prec@(1,5) (83.6%, 97.9%)	
12/27 06:29:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.7661 (0.6128)	Arch Loss 2.2367 (2.0860)	Arch Hard Loss 2.2367 (2.0860)	Arch Beta Loss 3.7658 (3.7664)	Arch depth Loss -3.7658 (-3.7664)	Prec@(1,5) (82.4%, 97.7%)	
12/27 06:29:46午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.5903 (0.6121)	Arch Loss 2.5044 (2.0816)	Arch Hard Loss 2.5044 (2.0816)	Arch Beta Loss 3.7652 (3.7661)	Arch depth Loss -3.7652 (-3.7661)	Prec@(1,5) (82.6%, 97.7%)	
12/27 06:30:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.7812 (0.6200)	Arch Loss 2.1554 (2.0780)	Arch Hard Loss 2.1554 (2.0780)	Arch Beta Loss 3.7651 (3.7659)	Arch depth Loss -3.7651 (-3.7659)	Prec@(1,5) (82.3%, 97.6%)	
12/27 06:30:23午前 searchDistribution_trainer.py:166 [INFO] Train: [ 48/49] Final Prec@1 82.3120%
12/27 06:30:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.0378	Prec@(1,5) (49.8%, 79.8%)
12/27 06:30:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 2.0477	Prec@(1,5) (50.3%, 79.8%)
12/27 06:30:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 2.0264	Prec@(1,5) (50.7%, 80.2%)
12/27 06:30:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 2.0192	Prec@(1,5) (50.9%, 80.3%)
12/27 06:30:47午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 50.8520%
12/27 06:30:47午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:30:47午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.8520%
12/27 06:31:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.4556 (0.5815)	Arch Loss 2.3613 (2.0463)	Arch Hard Loss 2.3613 (2.0463)	Arch Beta Loss 3.7647 (3.7649)	Arch depth Loss -3.7647 (-3.7649)	Prec@(1,5) (83.3%, 97.9%)	
12/27 06:32:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.4554 (0.5892)	Arch Loss 1.8200 (2.0744)	Arch Hard Loss 1.8200 (2.0744)	Arch Beta Loss 3.7649 (3.7649)	Arch depth Loss -3.7649 (-3.7649)	Prec@(1,5) (83.2%, 97.7%)	
12/27 06:32:53午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.6046 (0.5996)	Arch Loss 2.1548 (2.0586)	Arch Hard Loss 2.1548 (2.0586)	Arch Beta Loss 3.7644 (3.7648)	Arch depth Loss -3.7644 (-3.7648)	Prec@(1,5) (83.0%, 97.7%)	
12/27 06:33:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.6100 (0.6025)	Arch Loss 2.3333 (2.0657)	Arch Hard Loss 2.3333 (2.0657)	Arch Beta Loss 3.7642 (3.7647)	Arch depth Loss -3.7642 (-3.7647)	Prec@(1,5) (82.9%, 97.7%)	
12/27 06:33:31午前 searchDistribution_trainer.py:166 [INFO] Train: [ 49/49] Final Prec@1 82.9360%
12/27 06:33:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0687	Prec@(1,5) (49.8%, 79.7%)
12/27 06:33:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.0548	Prec@(1,5) (50.4%, 79.8%)
12/27 06:33:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.0536	Prec@(1,5) (50.6%, 80.0%)
12/27 06:33:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.0465	Prec@(1,5) (50.7%, 80.1%)
12/27 06:33:55午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 50.7240%
12/27 06:33:55午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 06:33:56午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.8520%
12/27 06:33:56午前 trainer_runner.py:113 [INFO] Final best Prec@1 = 50.8520%
12/27 06:33:56午前 trainer_runner.py:114 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
