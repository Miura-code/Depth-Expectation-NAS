08/10 05:37:35PM parser.py:28 [INFO] 
08/10 05:37:35PM parser.py:29 [INFO] Parameters:
08/10 05:37:35PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/ONLY_ARCH/s0-20240810-173735/DAG
08/10 05:37:35PM parser.py:31 [INFO] T=10.0
08/10 05:37:35PM parser.py:31 [INFO] ADVANCED=True
08/10 05:37:35PM parser.py:31 [INFO] ALPHA_LR=0.0003
08/10 05:37:35PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
08/10 05:37:35PM parser.py:31 [INFO] BATCH_SIZE=64
08/10 05:37:35PM parser.py:31 [INFO] CHECKPOINT_RESET=False
08/10 05:37:35PM parser.py:31 [INFO] CUTOUT_LENGTH=0
08/10 05:37:35PM parser.py:31 [INFO] DATA_PATH=../data/
08/10 05:37:35PM parser.py:31 [INFO] DATASET=cifar100
08/10 05:37:35PM parser.py:31 [INFO] DESCRIPTION=KD_for_stage_architecture_search_kd_for_architecture_parameters
08/10 05:37:35PM parser.py:31 [INFO] EPOCHS=50
08/10 05:37:35PM parser.py:31 [INFO] EXP_NAME=s0-20240810-173735
08/10 05:37:35PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
08/10 05:37:35PM parser.py:31 [INFO] GPUS=[0]
08/10 05:37:35PM parser.py:31 [INFO] INIT_CHANNELS=16
08/10 05:37:35PM parser.py:31 [INFO] L=0.5
08/10 05:37:35PM parser.py:31 [INFO] LAYERS=20
08/10 05:37:35PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
08/10 05:37:35PM parser.py:31 [INFO] NAME=ONLY_ARCH
08/10 05:37:35PM parser.py:31 [INFO] NONKD=False
08/10 05:37:35PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/ONLY_ARCH/s0-20240810-173735
08/10 05:37:35PM parser.py:31 [INFO] PCDARTS=False
08/10 05:37:35PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/ONLY_ARCH/s0-20240810-173735/plots
08/10 05:37:35PM parser.py:31 [INFO] PRINT_FREQ=50
08/10 05:37:35PM parser.py:31 [INFO] RESUME_PATH=None
08/10 05:37:35PM parser.py:31 [INFO] SAVE=s0
08/10 05:37:35PM parser.py:31 [INFO] SEED=0
08/10 05:37:35PM parser.py:31 [INFO] SHARE_STAGE=False
08/10 05:37:35PM parser.py:31 [INFO] SPEC_CELL=True
08/10 05:37:35PM parser.py:31 [INFO] TEACHER_NAME=efficientnet_v2_s
08/10 05:37:35PM parser.py:31 [INFO] TEACHER_PATH=/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar
08/10 05:37:35PM parser.py:31 [INFO] TRAIN_PORTION=0.5
08/10 05:37:35PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
08/10 05:37:35PM parser.py:31 [INFO] W_LR=0.025
08/10 05:37:35PM parser.py:31 [INFO] W_LR_MIN=0.001
08/10 05:37:35PM parser.py:31 [INFO] W_MOMENTUM=0.9
08/10 05:37:35PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
08/10 05:37:35PM parser.py:31 [INFO] WORKERS=4
08/10 05:37:35PM parser.py:32 [INFO] 
08/10 05:37:36PM searchStage_trainer.py:121 [INFO] --> Loaded teacher model 'efficientnet_v2_s' from '/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar' and Freezed parameters)
08/10 05:37:38PM eval_util.py:125 [INFO] Test: Step 000/390 Prec@(1,5) (100.0%, 100.0%)
08/10 05:38:08PM eval_util.py:125 [INFO] Test: Step 390/390 Prec@(1,5) (97.1%, 99.6%)
08/10 05:38:08PM eval_util.py:130 [INFO] Teacher=(efficientnet_v2_s <- (/home/miura/lab/KD-hdas/results/teacher/cifar100/efficientnet_v2_s/FINETUNE2/pretrained-20240716-002108/best.pth.tar)) achives Test Prec(@1, @5) = (97.1280%, 99.6360%)
08/10 05:38:33PM searchStage_trainer.py:128 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:38:54PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 4.6633 (4.6447)	Hard Loss 4.6633 (4.6447)	Soft Loss 4.6633 (4.6447)	Arch Loss 2.3347 (2.3634)	Arch Hard Loss 4.6180 (4.6748)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (1.5%, 6.9%)	
08/10 05:39:13PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.4865 (4.6046)	Hard Loss 4.4865 (4.6046)	Soft Loss 4.4865 (4.6046)	Arch Loss 2.2733 (2.3360)	Arch Hard Loss 4.4961 (4.6211)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (1.9%, 8.3%)	
08/10 05:39:32PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 4.3629 (4.5156)	Hard Loss 4.3629 (4.5156)	Soft Loss 4.3629 (4.5156)	Arch Loss 2.1113 (2.2925)	Arch Hard Loss 4.1778 (4.5349)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (2.4%, 10.9%)	
08/10 05:39:51PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.3620 (4.4459)	Hard Loss 4.3620 (4.4459)	Soft Loss 4.3620 (4.4459)	Arch Loss 2.2539 (2.2522)	Arch Hard Loss 4.4611 (4.4550)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (3.0%, 13.1%)	
08/10 05:40:10PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 4.2442 (4.3870)	Hard Loss 4.2442 (4.3870)	Soft Loss 4.2442 (4.3870)	Arch Loss 2.0648 (2.2185)	Arch Hard Loss 4.0798 (4.3881)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (3.3%, 14.6%)	
08/10 05:40:29PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0176 (4.3420)	Hard Loss 4.0176 (4.3420)	Soft Loss 4.0176 (4.3420)	Arch Loss 2.1490 (2.1948)	Arch Hard Loss 4.2521 (4.3410)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (3.6%, 15.6%)	
08/10 05:40:48PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 4.0445 (4.3030)	Hard Loss 4.0445 (4.3030)	Soft Loss 4.0445 (4.3030)	Arch Loss 1.9408 (2.1715)	Arch Hard Loss 3.8346 (4.2946)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (3.9%, 16.6%)	
08/10 05:41:03PM searchStage_trainer.py:259 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.7717 (4.2710)	Hard Loss 3.7717 (4.2710)	Soft Loss 3.7717 (4.2710)	Arch Loss 2.0602 (2.1564)	Arch Hard Loss 4.0720 (4.2646)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (4.3%, 17.5%)	
08/10 05:41:04PM searchStage_trainer.py:275 [INFO] Train: [  0/49] Final Prec@1 4.3360%
08/10 05:41:08PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 3.9821	Prec@(1,5) (7.5%, 26.5%)
08/10 05:41:11PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9903	Prec@(1,5) (7.6%, 26.0%)
08/10 05:41:14PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 4.0014	Prec@(1,5) (7.2%, 25.7%)
08/10 05:41:17PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 4.0025	Prec@(1,5) (7.2%, 25.6%)
08/10 05:41:19PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 4.0034	Prec@(1,5) (7.3%, 25.7%)
08/10 05:41:22PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 4.0033	Prec@(1,5) (7.2%, 25.5%)
08/10 05:41:25PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 4.0021	Prec@(1,5) (7.3%, 25.5%)
08/10 05:41:28PM searchStage_trainer.py:303 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 4.0016	Prec@(1,5) (7.3%, 25.4%)
08/10 05:41:28PM searchStage_trainer.py:314 [INFO] Valid: [  0/49] Final Prec@1 7.2560%
08/10 05:41:28PM searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
08/10 05:41:29PM searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 7.2560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:41:49午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 3.9483 (3.9581)	Hard Loss 3.9483 (3.9581)	Soft Loss 3.9483 (3.9581)	Arch Loss 1.9516 (2.0006)	Arch Hard Loss 3.8592 (3.9540)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (7.2%, 25.3%)	
08/10 05:42:08午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.9517 (3.9396)	Hard Loss 3.9517 (3.9396)	Soft Loss 3.9517 (3.9396)	Arch Loss 2.0361 (1.9993)	Arch Hard Loss 4.0262 (3.9516)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (7.8%, 26.3%)	
08/10 05:42:28午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 3.8508 (3.9309)	Hard Loss 3.8508 (3.9309)	Soft Loss 3.8508 (3.9309)	Arch Loss 2.0372 (1.9967)	Arch Hard Loss 4.0315 (3.9465)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (8.0%, 27.1%)	
08/10 05:42:47午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 4.1031 (3.9293)	Hard Loss 4.1031 (3.9293)	Soft Loss 4.1031 (3.9293)	Arch Loss 1.9248 (1.9844)	Arch Hard Loss 3.8027 (3.9220)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (8.1%, 27.4%)	
08/10 05:43:07午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 3.7439 (3.9165)	Hard Loss 3.7439 (3.9165)	Soft Loss 3.7439 (3.9165)	Arch Loss 1.9835 (1.9802)	Arch Hard Loss 3.9242 (3.9138)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (8.4%, 28.1%)	
08/10 05:43:26午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7956 (3.9082)	Hard Loss 3.7956 (3.9082)	Soft Loss 3.7956 (3.9082)	Arch Loss 1.9867 (1.9731)	Arch Hard Loss 3.9270 (3.8993)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (8.5%, 28.3%)	
08/10 05:43:46午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 3.8406 (3.8950)	Hard Loss 3.8406 (3.8950)	Soft Loss 3.8406 (3.8950)	Arch Loss 1.8357 (1.9676)	Arch Hard Loss 3.6264 (3.8882)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (8.7%, 28.9%)	
08/10 05:44:01午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.4888 (3.8792)	Hard Loss 3.4888 (3.8792)	Soft Loss 3.4888 (3.8792)	Arch Loss 1.7718 (1.9591)	Arch Hard Loss 3.4985 (3.8713)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (9.0%, 29.5%)	
08/10 05:44:02午後 searchStage_trainer.py:275 [INFO] Train: [  1/49] Final Prec@1 8.9920%
08/10 05:44:05午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 3.7908	Prec@(1,5) (9.9%, 32.4%)
08/10 05:44:08午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.7979	Prec@(1,5) (10.4%, 32.6%)
08/10 05:44:11午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 3.7899	Prec@(1,5) (10.5%, 32.8%)
08/10 05:44:14午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.7789	Prec@(1,5) (10.5%, 33.0%)
08/10 05:44:16午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 3.7664	Prec@(1,5) (10.7%, 33.2%)
08/10 05:44:19午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.7661	Prec@(1,5) (10.7%, 33.2%)
08/10 05:44:22午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 3.7664	Prec@(1,5) (10.8%, 33.2%)
08/10 05:44:25午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.7659	Prec@(1,5) (10.9%, 33.3%)
08/10 05:44:25午後 searchStage_trainer.py:314 [INFO] Valid: [  1/49] Final Prec@1 10.8840%
08/10 05:44:25午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('skip_connect', 4)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
08/10 05:44:25午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 10.8840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:44:46午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 4.0565 (3.7052)	Hard Loss 4.0565 (3.7052)	Soft Loss 4.0565 (3.7052)	Arch Loss 1.7726 (1.8786)	Arch Hard Loss 3.4992 (3.7102)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (11.9%, 35.2%)	
08/10 05:45:05午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.9237 (3.6908)	Hard Loss 3.9237 (3.6908)	Soft Loss 3.9237 (3.6908)	Arch Loss 1.9837 (1.8769)	Arch Hard Loss 3.9155 (3.7072)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (12.4%, 36.3%)	
08/10 05:45:25午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 3.6449 (3.6748)	Hard Loss 3.6449 (3.6748)	Soft Loss 3.6449 (3.6748)	Arch Loss 1.9203 (1.8612)	Arch Hard Loss 3.7918 (3.6758)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (12.8%, 36.7%)	
08/10 05:45:44午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.7402 (3.6539)	Hard Loss 3.7402 (3.6539)	Soft Loss 3.7402 (3.6539)	Arch Loss 1.8336 (1.8545)	Arch Hard Loss 3.6182 (3.6624)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (13.1%, 37.4%)	
08/10 05:46:03午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 3.5407 (3.6420)	Hard Loss 3.5407 (3.6420)	Soft Loss 3.5407 (3.6420)	Arch Loss 1.7155 (1.8469)	Arch Hard Loss 3.3872 (3.6471)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (13.3%, 37.4%)	
08/10 05:46:23午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5915 (3.6256)	Hard Loss 3.5915 (3.6256)	Soft Loss 3.5915 (3.6256)	Arch Loss 1.8763 (1.8431)	Arch Hard Loss 3.7057 (3.6395)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (13.6%, 37.8%)	
08/10 05:46:43午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 3.3847 (3.6182)	Hard Loss 3.3847 (3.6182)	Soft Loss 3.3847 (3.6182)	Arch Loss 1.8610 (1.8312)	Arch Hard Loss 3.6701 (3.6158)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (13.6%, 38.0%)	
08/10 05:46:59午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.3937 (3.6069)	Hard Loss 3.3937 (3.6069)	Soft Loss 3.3937 (3.6069)	Arch Loss 1.8540 (1.8257)	Arch Hard Loss 3.6549 (3.6049)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (13.8%, 38.3%)	
08/10 05:46:59午後 searchStage_trainer.py:275 [INFO] Train: [  2/49] Final Prec@1 13.8000%
08/10 05:47:02午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 3.4460	Prec@(1,5) (16.0%, 42.7%)
08/10 05:47:05午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4644	Prec@(1,5) (15.9%, 42.7%)
08/10 05:47:08午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 3.4680	Prec@(1,5) (16.2%, 42.6%)
08/10 05:47:11午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4679	Prec@(1,5) (16.3%, 42.7%)
08/10 05:47:13午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 3.4753	Prec@(1,5) (16.1%, 42.6%)
08/10 05:47:16午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4771	Prec@(1,5) (16.0%, 42.5%)
08/10 05:47:19午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 3.4724	Prec@(1,5) (16.0%, 42.6%)
08/10 05:47:21午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4735	Prec@(1,5) (15.9%, 42.6%)
08/10 05:47:21午後 searchStage_trainer.py:314 [INFO] Valid: [  2/49] Final Prec@1 15.9160%
08/10 05:47:21午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
08/10 05:47:22午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 15.9160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:47:42午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 3.3548 (3.4370)	Hard Loss 3.3548 (3.4370)	Soft Loss 3.3548 (3.4370)	Arch Loss 1.7480 (1.7638)	Arch Hard Loss 3.4509 (3.4809)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (16.9%, 43.8%)	
08/10 05:48:02午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.2894 (3.4295)	Hard Loss 3.2894 (3.4295)	Soft Loss 3.2894 (3.4295)	Arch Loss 1.6309 (1.7603)	Arch Hard Loss 3.2121 (3.4739)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (16.9%, 44.3%)	
08/10 05:48:22午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 3.2956 (3.4145)	Hard Loss 3.2956 (3.4145)	Soft Loss 3.2956 (3.4145)	Arch Loss 1.7753 (1.7504)	Arch Hard Loss 3.5002 (3.4540)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (17.1%, 44.7%)	
08/10 05:48:42午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.3135 (3.4098)	Hard Loss 3.3135 (3.4098)	Soft Loss 3.3135 (3.4098)	Arch Loss 1.5151 (1.7411)	Arch Hard Loss 2.9817 (3.4356)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (17.0%, 44.6%)	
08/10 05:49:02午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 3.2101 (3.4073)	Hard Loss 3.2101 (3.4073)	Soft Loss 3.2101 (3.4073)	Arch Loss 1.7958 (1.7320)	Arch Hard Loss 3.5440 (3.4173)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (16.9%, 44.7%)	
08/10 05:49:21午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.0303 (3.3902)	Hard Loss 3.0303 (3.3902)	Soft Loss 3.0303 (3.3902)	Arch Loss 1.7554 (1.7217)	Arch Hard Loss 3.4689 (3.3968)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (17.3%, 44.9%)	
08/10 05:49:41午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 3.4705 (3.3821)	Hard Loss 3.4705 (3.3821)	Soft Loss 3.4705 (3.3821)	Arch Loss 1.5915 (1.7150)	Arch Hard Loss 3.1361 (3.3834)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (17.4%, 44.9%)	
08/10 05:49:57午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.2472 (3.3711)	Hard Loss 3.2472 (3.3711)	Soft Loss 3.2472 (3.3711)	Arch Loss 1.7379 (1.7124)	Arch Hard Loss 3.4311 (3.3781)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (17.5%, 45.2%)	
08/10 05:49:57午後 searchStage_trainer.py:275 [INFO] Train: [  3/49] Final Prec@1 17.5560%
08/10 05:50:01午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 3.3236	Prec@(1,5) (18.3%, 47.2%)
08/10 05:50:03午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2972	Prec@(1,5) (19.1%, 47.6%)
08/10 05:50:06午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 3.2869	Prec@(1,5) (19.5%, 47.7%)
08/10 05:50:09午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2872	Prec@(1,5) (19.5%, 48.1%)
08/10 05:50:12午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 3.2947	Prec@(1,5) (19.2%, 48.0%)
08/10 05:50:14午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2933	Prec@(1,5) (19.2%, 48.0%)
08/10 05:50:17午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 3.2926	Prec@(1,5) (19.2%, 48.0%)
08/10 05:50:19午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2907	Prec@(1,5) (19.3%, 48.1%)
08/10 05:50:19午後 searchStage_trainer.py:314 [INFO] Valid: [  3/49] Final Prec@1 19.2560%
08/10 05:50:19午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('skip_connect', 4)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
08/10 05:50:20午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 19.2560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:50:41午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 3.3743 (3.1836)	Hard Loss 3.3743 (3.1836)	Soft Loss 3.3743 (3.1836)	Arch Loss 1.4898 (1.6495)	Arch Hard Loss 2.9343 (3.2520)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (21.3%, 50.5%)	
08/10 05:51:01午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.3092 (3.2184)	Hard Loss 3.3092 (3.2184)	Soft Loss 3.3092 (3.2184)	Arch Loss 1.5685 (1.6604)	Arch Hard Loss 3.0955 (3.2741)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (20.7%, 49.5%)	
08/10 05:51:21午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 2.9958 (3.1945)	Hard Loss 2.9958 (3.1945)	Soft Loss 2.9958 (3.1945)	Arch Loss 1.4951 (1.6545)	Arch Hard Loss 2.9421 (3.2622)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (20.9%, 50.2%)	
08/10 05:51:41午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 2.8514 (3.1961)	Hard Loss 2.8514 (3.1961)	Soft Loss 2.8514 (3.1961)	Arch Loss 1.6013 (1.6410)	Arch Hard Loss 3.1590 (3.2355)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (20.7%, 50.2%)	
08/10 05:52:00午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 3.2050 (3.1785)	Hard Loss 3.2050 (3.1785)	Soft Loss 3.2050 (3.1785)	Arch Loss 1.6422 (1.6338)	Arch Hard Loss 3.2325 (3.2211)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (21.3%, 50.8%)	
08/10 05:52:20午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.9682 (3.1651)	Hard Loss 2.9682 (3.1651)	Soft Loss 2.9682 (3.1651)	Arch Loss 1.8935 (1.6248)	Arch Hard Loss 3.7380 (3.2029)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (21.5%, 51.2%)	
08/10 05:52:39午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 3.0574 (3.1595)	Hard Loss 3.0574 (3.1595)	Soft Loss 3.0574 (3.1595)	Arch Loss 1.4670 (1.6167)	Arch Hard Loss 2.8845 (3.1868)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (21.6%, 51.3%)	
08/10 05:52:55午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.1629 (3.1511)	Hard Loss 3.1629 (3.1511)	Soft Loss 3.1629 (3.1511)	Arch Loss 1.6426 (1.6120)	Arch Hard Loss 3.2401 (3.1774)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (21.6%, 51.5%)	
08/10 05:52:55午後 searchStage_trainer.py:275 [INFO] Train: [  4/49] Final Prec@1 21.5680%
08/10 05:52:58午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 3.0467	Prec@(1,5) (22.9%, 52.8%)
08/10 05:53:01午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.0795	Prec@(1,5) (22.4%, 52.3%)
08/10 05:53:04午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 3.0743	Prec@(1,5) (22.5%, 53.0%)
08/10 05:53:07午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0697	Prec@(1,5) (22.7%, 53.3%)
08/10 05:53:10午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 3.0671	Prec@(1,5) (22.8%, 53.4%)
08/10 05:53:13午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0797	Prec@(1,5) (22.6%, 53.1%)
08/10 05:53:16午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 3.0784	Prec@(1,5) (22.8%, 53.1%)
08/10 05:53:18午後 searchStage_trainer.py:303 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0767	Prec@(1,5) (22.9%, 53.2%)
08/10 05:53:18午後 searchStage_trainer.py:314 [INFO] Valid: [  4/49] Final Prec@1 22.8760%
08/10 05:53:18午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('skip_connect', 4)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
08/10 05:53:19午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 22.8760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2501, 0.2504, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2501, 0.2500],
        [0.2504, 0.2499, 0.2499, 0.2497],
        [0.2501, 0.2505, 0.2498, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2504, 0.2501],
        [0.2496, 0.2497, 0.2502, 0.2505],
        [0.2500, 0.2500, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2502, 0.2500, 0.2495],
        [0.2496, 0.2502, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2498, 0.2500],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2499, 0.2503, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503],
        [0.2502, 0.2500, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2500, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499],
        [0.2496, 0.2501, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2497, 0.2501],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2496, 0.2502, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2504, 0.2496, 0.2499],
        [0.2503, 0.2503, 0.2498, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2500],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2503, 0.2498, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2503, 0.2498],
        [0.2498, 0.2504, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2500, 0.2499],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2499, 0.2499],
        [0.2502, 0.2496, 0.2504, 0.2499],
        [0.2501, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/10 05:53:39午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 2.9307 (3.0380)	Hard Loss 2.9307 (3.0380)	Soft Loss 2.9307 (3.0380)	Arch Loss 1.5477 (1.5377)	Arch Hard Loss 3.0495 (3.0291)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (23.3%, 54.3%)	
08/10 05:53:59午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 2.8541 (3.0057)	Hard Loss 2.8541 (3.0057)	Soft Loss 2.8541 (3.0057)	Arch Loss 1.3969 (1.5484)	Arch Hard Loss 2.7455 (3.0503)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (25.0%, 55.5%)	
08/10 05:54:18午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 3.2392 (2.9937)	Hard Loss 3.2392 (2.9937)	Soft Loss 3.2392 (2.9937)	Arch Loss 1.7551 (1.5549)	Arch Hard Loss 3.4649 (3.0629)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (24.8%, 55.6%)	
08/10 05:54:38午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 2.9631 (2.9850)	Hard Loss 2.9631 (2.9850)	Soft Loss 2.9631 (2.9850)	Arch Loss 1.4875 (1.5460)	Arch Hard Loss 2.9325 (3.0450)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (25.1%, 56.0%)	
08/10 05:54:57午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 2.7019 (2.9748)	Hard Loss 2.7019 (2.9748)	Soft Loss 2.7019 (2.9748)	Arch Loss 1.5664 (1.5365)	Arch Hard Loss 3.0868 (3.0261)	Arch Soft Loss 0.0005 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (25.3%, 56.1%)	
08/10 05:55:17午後 searchStage_trainer.py:259 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.9741 (2.9742)	Hard Loss 2.9741 (2.9742)	Soft Loss 2.9741 (2.9742)	Arch Loss 1.2615 (1.5281)	Arch Hard Loss 2.4808 (3.0095)	Arch Soft Loss 0.0004 (0.0005)	Arch depth Loss 0.0005 (0.0005)	Prec@(1,5) (25.3%, 56.2%)	
