09/26 08:19:53PM parser.py:28 [INFO] 
09/26 08:19:53PM parser.py:29 [INFO] Parameters:
09/26 08:19:53PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher/DAG
09/26 08:19:53PM parser.py:31 [INFO] T=10.0
09/26 08:19:53PM parser.py:31 [INFO] ADVANCED=True
09/26 08:19:53PM parser.py:31 [INFO] ALPHA_LR=0.0003
09/26 08:19:53PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
09/26 08:19:53PM parser.py:31 [INFO] BATCH_SIZE=64
09/26 08:19:53PM parser.py:31 [INFO] CASCADE=False
09/26 08:19:53PM parser.py:31 [INFO] CHECKPOINT_RESET=False
09/26 08:19:53PM parser.py:31 [INFO] CUTOUT_LENGTH=0
09/26 08:19:53PM parser.py:31 [INFO] DATA_PATH=../data/
09/26 08:19:53PM parser.py:31 [INFO] DATASET=cifar100
09/26 08:19:53PM parser.py:31 [INFO] DEPTH_COEF=0.0
09/26 08:19:53PM parser.py:31 [INFO] DESCRIPTION=ArchHint_KD_mimic_only_teacher_archtecture
09/26 08:19:53PM parser.py:31 [INFO] EPOCHS=3
09/26 08:19:53PM parser.py:31 [INFO] EXP_NAME=s0-h_das_teacher
09/26 08:19:53PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
09/26 08:19:53PM parser.py:31 [INFO] GPUS=[0]
09/26 08:19:53PM parser.py:31 [INFO] HINT_EPOCHS=[0, 1, 3]
09/26 08:19:53PM parser.py:31 [INFO] INIT_CHANNELS=16
09/26 08:19:53PM parser.py:31 [INFO] L=0.5
09/26 08:19:53PM parser.py:31 [INFO] LAYERS=20
09/26 08:19:53PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
09/26 08:19:53PM parser.py:31 [INFO] NAME=ARCHHINT-KD-TEST
09/26 08:19:53PM parser.py:31 [INFO] NONKD=False
09/26 08:19:53PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher
09/26 08:19:53PM parser.py:31 [INFO] PCDARTS=False
09/26 08:19:53PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher/plots
09/26 08:19:53PM parser.py:31 [INFO] PRINT_FREQ=50
09/26 08:19:53PM parser.py:31 [INFO] RESUME_PATH=None
09/26 08:19:53PM parser.py:31 [INFO] SAVE=s0-h_das_teacher
09/26 08:19:53PM parser.py:31 [INFO] SEED=0
09/26 08:19:53PM parser.py:31 [INFO] SHARE_STAGE=False
09/26 08:19:53PM parser.py:31 [INFO] SLIDE_WINDOW=8
09/26 08:19:53PM parser.py:31 [INFO] SPEC_CELL=True
09/26 08:19:53PM parser.py:31 [INFO] TEACHER_NAME=h_das_224baseline
09/26 08:19:53PM parser.py:31 [INFO] TEACHER_PATH=/home/miura/lab/KD-hdas/results/evaluate_stage_KD/cifar100/noDepthLoss/s0-BaselineBestCell/best.pth.tar
09/26 08:19:53PM parser.py:31 [INFO] TRAIN_PORTION=0.5
09/26 08:19:53PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
09/26 08:19:53PM parser.py:31 [INFO] W_LR=0.025
09/26 08:19:53PM parser.py:31 [INFO] W_LR_MIN=0.001
09/26 08:19:53PM parser.py:31 [INFO] W_MOMENTUM=0.9
09/26 08:19:53PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
09/26 08:19:53PM parser.py:31 [INFO] WORKERS=4
09/26 08:19:53PM parser.py:32 [INFO] 
09/26 08:19:55PM searchStage_ArchHint_trainer.py:110 [INFO] --> Loaded teacher model 'h_das_224baseline' from '/home/miura/lab/KD-hdas/results/evaluate_stage_KD/cifar100/noDepthLoss/s0-BaselineBestCell/best.pth.tar' and Freezed parameters)
09/26 08:20:19PM searchStage_Hint_trainer.py:138 [INFO] --> No loaded checkpoint!
09/26 08:20:20PM searchStage_ArchHint_main.py:63 [INFO] Step1: Start Hint learning until stage1: Epoch:[0 - 0][3]
09/26 08:20:20PM searchStage_ArchHint_main.py:69 [INFO] Step2: Start Hint learning until stage2: Epoch:[0 - 1][3]
09/26 08:20:20PM searchStage_ArchHint_main.py:69 [INFO] Step3: Start Hint learning until stage3: Epoch:[1 - 3][3]
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2498, 0.2501, 0.2501],
        [0.2499, 0.2498, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2497, 0.2500],
        [0.2499, 0.2500, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2499],
        [0.2499, 0.2501, 0.2499, 0.2501],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2498, 0.2500, 0.2501, 0.2501],
        [0.2499, 0.2506, 0.2495, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2499, 0.2497],
        [0.2503, 0.2496, 0.2500, 0.2501],
        [0.2502, 0.2501, 0.2497, 0.2500],
        [0.2499, 0.2504, 0.2499, 0.2498],
        [0.2498, 0.2501, 0.2500, 0.2501],
        [0.2503, 0.2501, 0.2500, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2500, 0.2504, 0.2496],
        [0.2503, 0.2500, 0.2502, 0.2495],
        [0.2500, 0.2498, 0.2500, 0.2501],
        [0.2502, 0.2497, 0.2501, 0.2500],
        [0.2501, 0.2498, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2499, 0.2500],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2499, 0.2498],
        [0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2502, 0.2498],
        [0.2501, 0.2495, 0.2499, 0.2505],
        [0.2500, 0.2497, 0.2503, 0.2500],
        [0.2501, 0.2503, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2505, 0.2498, 0.2499],
        [0.2500, 0.2500, 0.2499, 0.2501],
        [0.2501, 0.2499, 0.2500, 0.2500],
        [0.2505, 0.2498, 0.2498, 0.2499],
        [0.2499, 0.2503, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2501, 0.2497, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2501, 0.2498, 0.2501, 0.2500],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2503, 0.2498, 0.2502, 0.2498],
        [0.2499, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2501, 0.2499],
        [0.2498, 0.2500, 0.2505, 0.2497],
        [0.2503, 0.2499, 0.2498, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2497, 0.2503],
        [0.2507, 0.2499, 0.2496, 0.2497],
        [0.2502, 0.2498, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2499, 0.2499],
        [0.2497, 0.2503, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499],
        [0.2500, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2499, 0.2504],
        [0.2497, 0.2500, 0.2500, 0.2502],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2497, 0.2504, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2499, 0.2504],
        [0.2497, 0.2502, 0.2499, 0.2502],
        [0.2503, 0.2501, 0.2497, 0.2499],
        [0.2498, 0.2502, 0.2498, 0.2501],
        [0.2501, 0.2500, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2500, 0.2502],
        [0.2497, 0.2500, 0.2505, 0.2498],
        [0.2501, 0.2499, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2502, 0.2499],
        [0.2502, 0.2499, 0.2505, 0.2494],
        [0.2502, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2503, 0.2495],
        [0.2501, 0.2498, 0.2500, 0.2501],
        [0.2497, 0.2504, 0.2498, 0.2501],
        [0.2499, 0.2499, 0.2499, 0.2503],
        [0.2500, 0.2499, 0.2504, 0.2497],
        [0.2498, 0.2502, 0.2497, 0.2503],
        [0.2500, 0.2502, 0.2497, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
09/26 08:20:48PM parser.py:28 [INFO] 
09/26 08:20:48PM parser.py:29 [INFO] Parameters:
09/26 08:20:48PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher/DAG
09/26 08:20:48PM parser.py:31 [INFO] T=10.0
09/26 08:20:48PM parser.py:31 [INFO] ADVANCED=True
09/26 08:20:48PM parser.py:31 [INFO] ALPHA_LR=0.0003
09/26 08:20:48PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
09/26 08:20:48PM parser.py:31 [INFO] BATCH_SIZE=64
09/26 08:20:48PM parser.py:31 [INFO] CASCADE=False
09/26 08:20:48PM parser.py:31 [INFO] CHECKPOINT_RESET=False
09/26 08:20:48PM parser.py:31 [INFO] CUTOUT_LENGTH=0
09/26 08:20:48PM parser.py:31 [INFO] DATA_PATH=../data/
09/26 08:20:48PM parser.py:31 [INFO] DATASET=cifar100
09/26 08:20:48PM parser.py:31 [INFO] DEPTH_COEF=0.0
09/26 08:20:48PM parser.py:31 [INFO] DESCRIPTION=ArchHint_KD_mimic_only_teacher_archtecture
09/26 08:20:48PM parser.py:31 [INFO] EPOCHS=3
09/26 08:20:48PM parser.py:31 [INFO] EXP_NAME=s0-h_das_teacher
09/26 08:20:48PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
09/26 08:20:48PM parser.py:31 [INFO] GPUS=[0]
09/26 08:20:48PM parser.py:31 [INFO] HINT_EPOCHS=[0, 1, 3]
09/26 08:20:48PM parser.py:31 [INFO] INIT_CHANNELS=16
09/26 08:20:48PM parser.py:31 [INFO] L=0.5
09/26 08:20:48PM parser.py:31 [INFO] LAYERS=20
09/26 08:20:48PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
09/26 08:20:48PM parser.py:31 [INFO] NAME=ARCHHINT-KD-TEST
09/26 08:20:48PM parser.py:31 [INFO] NONKD=False
09/26 08:20:48PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher
09/26 08:20:48PM parser.py:31 [INFO] PCDARTS=False
09/26 08:20:48PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/ARCHHINT-KD-TEST/s0-h_das_teacher/plots
09/26 08:20:48PM parser.py:31 [INFO] PRINT_FREQ=50
09/26 08:20:48PM parser.py:31 [INFO] RESUME_PATH=None
09/26 08:20:48PM parser.py:31 [INFO] SAVE=s0-h_das_teacher
09/26 08:20:48PM parser.py:31 [INFO] SEED=0
09/26 08:20:48PM parser.py:31 [INFO] SHARE_STAGE=False
09/26 08:20:48PM parser.py:31 [INFO] SLIDE_WINDOW=8
09/26 08:20:48PM parser.py:31 [INFO] SPEC_CELL=True
09/26 08:20:48PM parser.py:31 [INFO] TEACHER_NAME=h_das_224baseline
09/26 08:20:48PM parser.py:31 [INFO] TEACHER_PATH=/home/miura/lab/KD-hdas/results/evaluate_stage_KD/cifar100/noDepthLoss/s0-BaselineBestCell/best.pth.tar
09/26 08:20:48PM parser.py:31 [INFO] TRAIN_PORTION=0.5
09/26 08:20:48PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
09/26 08:20:48PM parser.py:31 [INFO] W_LR=0.025
09/26 08:20:48PM parser.py:31 [INFO] W_LR_MIN=0.001
09/26 08:20:48PM parser.py:31 [INFO] W_MOMENTUM=0.9
09/26 08:20:48PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
09/26 08:20:48PM parser.py:31 [INFO] WORKERS=4
09/26 08:20:48PM parser.py:32 [INFO] 
09/26 08:20:50PM searchStage_ArchHint_trainer.py:110 [INFO] --> Loaded teacher model 'h_das_224baseline' from '/home/miura/lab/KD-hdas/results/evaluate_stage_KD/cifar100/noDepthLoss/s0-BaselineBestCell/best.pth.tar' and Freezed parameters)
09/26 08:21:14PM searchStage_Hint_trainer.py:138 [INFO] --> No loaded checkpoint!
09/26 08:21:15PM searchStage_ArchHint_main.py:63 [INFO] Step1: Start Hint learning until stage1: Epoch:[0 - 0][3]
09/26 08:21:15PM searchStage_ArchHint_main.py:69 [INFO] Step2: Start Hint learning until stage2: Epoch:[0 - 1][3]
09/26 08:21:15PM searchStage_ArchHint_main.py:69 [INFO] Step3: Start Hint learning until stage3: Epoch:[1 - 3][3]
####### ALPHA #######
# Alpha - DAG
tensor([[0.2504, 0.2499, 0.2495, 0.2502],
        [0.2498, 0.2497, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2498, 0.2501, 0.2501],
        [0.2499, 0.2498, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2497, 0.2500],
        [0.2499, 0.2500, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2499, 0.2499],
        [0.2499, 0.2501, 0.2499, 0.2501],
        [0.2500, 0.2502, 0.2498, 0.2500],
        [0.2498, 0.2500, 0.2501, 0.2501],
        [0.2499, 0.2506, 0.2495, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2499, 0.2497],
        [0.2503, 0.2496, 0.2500, 0.2501],
        [0.2502, 0.2501, 0.2497, 0.2500],
        [0.2499, 0.2504, 0.2499, 0.2498],
        [0.2498, 0.2501, 0.2500, 0.2501],
        [0.2503, 0.2501, 0.2500, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2500, 0.2504, 0.2496],
        [0.2503, 0.2500, 0.2502, 0.2495],
        [0.2500, 0.2498, 0.2500, 0.2501],
        [0.2502, 0.2497, 0.2501, 0.2500],
        [0.2501, 0.2498, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2499, 0.2500],
        [0.2501, 0.2499, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2497, 0.2503],
        [0.2501, 0.2500, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2499, 0.2498],
        [0.2502, 0.2505, 0.2497, 0.2496],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2502, 0.2498],
        [0.2501, 0.2495, 0.2499, 0.2505],
        [0.2500, 0.2497, 0.2503, 0.2500],
        [0.2501, 0.2503, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2505, 0.2498, 0.2499],
        [0.2500, 0.2500, 0.2499, 0.2501],
        [0.2501, 0.2499, 0.2500, 0.2500],
        [0.2505, 0.2498, 0.2498, 0.2499],
        [0.2499, 0.2503, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2501, 0.2497, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2501, 0.2498, 0.2501, 0.2500],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2503, 0.2498, 0.2502, 0.2498],
        [0.2499, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2501, 0.2499],
        [0.2498, 0.2500, 0.2505, 0.2497],
        [0.2503, 0.2499, 0.2498, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498],
        [0.2501, 0.2499, 0.2497, 0.2503],
        [0.2507, 0.2499, 0.2496, 0.2497],
        [0.2502, 0.2498, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2499, 0.2499],
        [0.2497, 0.2503, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2501, 0.2499],
        [0.2499, 0.2505, 0.2497, 0.2499],
        [0.2500, 0.2502, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2499, 0.2504],
        [0.2497, 0.2500, 0.2500, 0.2502],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2497, 0.2504, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2499, 0.2504],
        [0.2497, 0.2502, 0.2499, 0.2502],
        [0.2503, 0.2501, 0.2497, 0.2499],
        [0.2498, 0.2502, 0.2498, 0.2501],
        [0.2501, 0.2500, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2500, 0.2502],
        [0.2497, 0.2500, 0.2505, 0.2498],
        [0.2501, 0.2499, 0.2501, 0.2499],
        [0.2501, 0.2499, 0.2502, 0.2499],
        [0.2502, 0.2499, 0.2505, 0.2494],
        [0.2502, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2503, 0.2495],
        [0.2501, 0.2498, 0.2500, 0.2501],
        [0.2497, 0.2504, 0.2498, 0.2501],
        [0.2499, 0.2499, 0.2499, 0.2503],
        [0.2500, 0.2499, 0.2504, 0.2497],
        [0.2498, 0.2502, 0.2497, 0.2503],
        [0.2500, 0.2502, 0.2497, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
09/26 08:21:30PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][50/390]	Step 50	lr 0.025	HintLoss 37.6772 (37.9589)	Arch Loss 37.6888 (37.9704)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.0%, 5.2%)	
09/26 08:21:43PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][100/390]	Step 100	lr 0.025	HintLoss 37.0934 (37.6696)	Arch Loss 37.1052 (37.6811)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.0%, 5.2%)	
09/26 08:21:56PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][150/390]	Step 150	lr 0.025	HintLoss 36.5020 (37.3772)	Arch Loss 36.5139 (37.3888)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.0%, 5.0%)	
09/26 08:22:10PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][200/390]	Step 200	lr 0.025	HintLoss 35.9042 (37.0823)	Arch Loss 35.9163 (37.0940)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.0%, 5.0%)	
09/26 08:22:23PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][250/390]	Step 250	lr 0.025	HintLoss 35.3014 (36.7853)	Arch Loss 35.3135 (36.7970)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.0%, 5.1%)	
09/26 08:22:37PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][300/390]	Step 300	lr 0.025	HintLoss 34.6946 (36.4864)	Arch Loss 34.7068 (36.4983)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.1%, 5.1%)	
09/26 08:22:51PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][350/390]	Step 350	lr 0.025	HintLoss 34.0851 (36.1861)	Arch Loss 34.0974 (36.1980)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.1%, 5.2%)	
09/26 08:23:01PM searchStage_ArchHint_trainer.py:185 [INFO] Train: Epoch: [2][390/390]	Step 390	lr 0.025	HintLoss 33.5964 (35.9449)	Arch Loss 33.6086 (35.9568)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (1.1%, 5.2%)	
09/26 08:23:02PM searchStage_ArchHint_trainer.py:197 [INFO] Train: [  2/2] Final Prec@1 1.0920%
09/26 08:23:05PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][50/391]	Step 391	Loss 4.8305	Prec@(1,5) (0.7%, 5.5%)
09/26 08:23:08PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][100/391]	Step 391	Loss 4.8677	Prec@(1,5) (0.9%, 5.1%)
09/26 08:23:11PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][150/391]	Step 391	Loss 4.8539	Prec@(1,5) (1.0%, 5.0%)
09/26 08:23:14PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][200/391]	Step 391	Loss 4.8474	Prec@(1,5) (1.0%, 5.1%)
09/26 08:23:17PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][250/391]	Step 391	Loss 4.8521	Prec@(1,5) (1.0%, 5.3%)
09/26 08:23:20PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][300/391]	Step 391	Loss 4.8435	Prec@(1,5) (1.0%, 5.2%)
09/26 08:23:23PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][350/391]	Step 391	Loss 4.8558	Prec@(1,5) (1.0%, 5.2%)
09/26 08:23:25PM searchStage_ArchHint_trainer.py:315 [INFO] Valid: Epoch: [2][390/391]	Step 391	Loss 4.8567	Prec@(1,5) (1.0%, 5.1%)
09/26 08:23:25PM searchStage_ArchHint_trainer.py:326 [INFO] Valid: [  2/2] Final Prec@1 1.0560%
09/26 08:23:25PM searchStage_ArchHint_main.py:80 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
09/26 08:23:26午後 searchStage_ArchHint_main.py:105 [INFO] Step4: Start KD learning: Epoch:[2 - 3][3]
09/26 08:23:26午後 searchStage_ArchHint_main.py:151 [INFO] Final best Prec@1 = 0.0000%
09/26 08:23:26午後 searchStage_ArchHint_main.py:152 [INFO] Final Best Genotype = Genotype2(DAG1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)]], DAG1_concat=range(6, 8), DAG2=[[('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 2)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('avg_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 4)], [('avg_pool_3x3', 3), ('skip_connect', 6)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 3)], [('max_pool_3x3', 2), ('avg_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 2)]], DAG3_concat=range(6, 8))
