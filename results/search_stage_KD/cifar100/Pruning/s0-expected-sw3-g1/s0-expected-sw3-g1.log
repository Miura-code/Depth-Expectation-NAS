11/21 11:07:44PM parser.py:28 [INFO] 
11/21 11:07:44PM parser.py:29 [INFO] Parameters:
11/21 11:07:44PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/DAG
11/21 11:07:44PM parser.py:31 [INFO] T=10.0
11/21 11:07:44PM parser.py:31 [INFO] ADVANCED=1
11/21 11:07:44PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/21 11:07:44PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/21 11:07:44PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/21 11:07:44PM parser.py:31 [INFO] BATCH_SIZE=64
11/21 11:07:44PM parser.py:31 [INFO] CASCADE=0
11/21 11:07:44PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/21 11:07:44PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/21 11:07:44PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/21 11:07:44PM parser.py:31 [INFO] DATA_PATH=../data/
11/21 11:07:44PM parser.py:31 [INFO] DATASET=cifar100
11/21 11:07:44PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/21 11:07:44PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/21 11:07:44PM parser.py:31 [INFO] DISCRETE=0
11/21 11:07:44PM parser.py:31 [INFO] EPOCHS=50
11/21 11:07:44PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/21 11:07:44PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g1
11/21 11:07:44PM parser.py:31 [INFO] FINAL_L=0.0
11/21 11:07:44PM parser.py:31 [INFO] G=1.0
11/21 11:07:44PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/21 11:07:44PM parser.py:31 [INFO] GPUS=[0]
11/21 11:07:44PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/21 11:07:44PM parser.py:31 [INFO] INIT_CHANNELS=16
11/21 11:07:44PM parser.py:31 [INFO] L=0.0
11/21 11:07:44PM parser.py:31 [INFO] LAYERS=32
11/21 11:07:44PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/21 11:07:44PM parser.py:31 [INFO] NAME=Pruning
11/21 11:07:44PM parser.py:31 [INFO] NONKD=1
11/21 11:07:44PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1
11/21 11:07:44PM parser.py:31 [INFO] PCDARTS=0
11/21 11:07:44PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/plots
11/21 11:07:44PM parser.py:31 [INFO] PRINT_FREQ=100
11/21 11:07:44PM parser.py:31 [INFO] RESET=0
11/21 11:07:44PM parser.py:31 [INFO] RESUME_PATH=None
11/21 11:07:44PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g1
11/21 11:07:44PM parser.py:31 [INFO] SEED=0
11/21 11:07:44PM parser.py:31 [INFO] SHARE_STAGE=0
11/21 11:07:44PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/21 11:07:44PM parser.py:31 [INFO] SPEC_CELL=1
11/21 11:07:44PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/21 11:07:44PM parser.py:31 [INFO] TEACHER_NAME=none
11/21 11:07:44PM parser.py:31 [INFO] TEACHER_PATH=none
11/21 11:07:44PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/21 11:07:44PM parser.py:31 [INFO] TYPE=Pruning
11/21 11:07:44PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/21 11:07:44PM parser.py:31 [INFO] W_LR=0.025
11/21 11:07:44PM parser.py:31 [INFO] W_LR_MIN=0.001
11/21 11:07:44PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/21 11:07:44PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/21 11:07:44PM parser.py:31 [INFO] WORKERS=4
11/21 11:07:44PM parser.py:32 [INFO] 
11/21 11:07:45PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/21 11:08:17PM parser.py:28 [INFO] 
11/21 11:08:17PM parser.py:29 [INFO] Parameters:
11/21 11:08:17PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/DAG
11/21 11:08:17PM parser.py:31 [INFO] T=10.0
11/21 11:08:17PM parser.py:31 [INFO] ADVANCED=1
11/21 11:08:17PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/21 11:08:17PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/21 11:08:17PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/21 11:08:17PM parser.py:31 [INFO] BATCH_SIZE=64
11/21 11:08:17PM parser.py:31 [INFO] CASCADE=0
11/21 11:08:17PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/21 11:08:17PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/21 11:08:17PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/21 11:08:17PM parser.py:31 [INFO] DATA_PATH=../data/
11/21 11:08:17PM parser.py:31 [INFO] DATASET=cifar100
11/21 11:08:17PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/21 11:08:17PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/21 11:08:17PM parser.py:31 [INFO] DISCRETE=0
11/21 11:08:17PM parser.py:31 [INFO] EPOCHS=2
11/21 11:08:17PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/21 11:08:17PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g1
11/21 11:08:17PM parser.py:31 [INFO] FINAL_L=0.0
11/21 11:08:17PM parser.py:31 [INFO] G=1.0
11/21 11:08:17PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/21 11:08:17PM parser.py:31 [INFO] GPUS=[0]
11/21 11:08:17PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/21 11:08:17PM parser.py:31 [INFO] INIT_CHANNELS=16
11/21 11:08:17PM parser.py:31 [INFO] L=0.0
11/21 11:08:17PM parser.py:31 [INFO] LAYERS=32
11/21 11:08:17PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/21 11:08:17PM parser.py:31 [INFO] NAME=Pruning
11/21 11:08:17PM parser.py:31 [INFO] NONKD=1
11/21 11:08:17PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1
11/21 11:08:17PM parser.py:31 [INFO] PCDARTS=0
11/21 11:08:17PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/plots
11/21 11:08:17PM parser.py:31 [INFO] PRINT_FREQ=100
11/21 11:08:17PM parser.py:31 [INFO] RESET=0
11/21 11:08:17PM parser.py:31 [INFO] RESUME_PATH=None
11/21 11:08:17PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g1
11/21 11:08:17PM parser.py:31 [INFO] SEED=0
11/21 11:08:17PM parser.py:31 [INFO] SHARE_STAGE=0
11/21 11:08:17PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/21 11:08:17PM parser.py:31 [INFO] SPEC_CELL=1
11/21 11:08:17PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/21 11:08:17PM parser.py:31 [INFO] TEACHER_NAME=none
11/21 11:08:17PM parser.py:31 [INFO] TEACHER_PATH=none
11/21 11:08:17PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/21 11:08:17PM parser.py:31 [INFO] TYPE=Pruning
11/21 11:08:17PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/21 11:08:17PM parser.py:31 [INFO] W_LR=0.025
11/21 11:08:17PM parser.py:31 [INFO] W_LR_MIN=0.001
11/21 11:08:17PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/21 11:08:17PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/21 11:08:17PM parser.py:31 [INFO] WORKERS=4
11/21 11:08:17PM parser.py:32 [INFO] 
11/21 11:08:18PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/21 11:09:14PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.2479 (4.4973)	Arch Loss 355.9519 (360.4439)	Arch Hard Loss 4.1991 (4.4874)	Arch Beta Loss 351.7528 (355.9565)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.8%, 10.9%)	
11/21 11:10:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0702 (4.3479)	Arch Loss 347.6285 (356.0824)	Arch Hard Loss 4.2285 (4.3440)	Arch Beta Loss 343.3999 (351.7384)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.9%, 15.1%)	
11/21 11:11:03PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 3.9571 (4.2336)	Arch Loss 339.2635 (351.8107)	Arch Hard Loss 4.0388 (4.2330)	Arch Beta Loss 335.2247 (347.5777)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.0%, 18.6%)	
11/21 11:11:53PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.8243 (4.1666)	Arch Loss 331.8430 (348.0452)	Arch Hard Loss 3.8168 (4.1610)	Arch Beta Loss 328.0262 (343.8842)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.7%, 20.9%)	
11/21 11:11:55PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/1] Final Prec@1 5.7160%
11/21 11:12:03PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.8806	Prec@(1,5) (8.7%, 29.9%)
11/21 11:12:11PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.8727	Prec@(1,5) (9.2%, 30.1%)
11/21 11:12:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.8671	Prec@(1,5) (9.3%, 30.4%)
11/21 11:12:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.8676	Prec@(1,5) (9.3%, 30.2%)
11/21 11:12:26PM searchStage_trainer.py:323 [INFO] Valid: [  0/1] Final Prec@1 9.2520%
11/21 11:12:26PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 8), ('max_pool_3x3', 6)], [('skip_connect', 8), ('max_pool_3x3', 9)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[2, 4], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:12:26PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 9.2520%
11/21 11:13:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.013	Loss 3.7822 (3.7794)	Arch Loss 324.1159 (327.7544)	Arch Hard Loss 3.9820 (3.7693)	Arch Beta Loss 320.1339 (323.9851)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.7%, 31.7%)	
11/21 11:14:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.013	Loss 3.5918 (3.7449)	Arch Loss 316.4362 (323.8477)	Arch Hard Loss 3.9214 (3.7201)	Arch Beta Loss 312.5148 (320.1276)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (11.5%, 33.5%)	
11/21 11:15:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.013	Loss 3.5055 (3.7154)	Arch Loss 308.4150 (320.0308)	Arch Hard Loss 3.3258 (3.6961)	Arch Beta Loss 305.0892 (316.3347)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (12.1%, 34.5%)	
11/21 11:16:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.013	Loss 3.7461 (3.6820)	Arch Loss 302.4529 (316.6466)	Arch Hard Loss 3.8847 (3.6707)	Arch Beta Loss 298.5681 (312.9759)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (12.5%, 35.6%)	
11/21 11:16:12午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/1] Final Prec@1 12.4640%
11/21 11:16:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.5405	Prec@(1,5) (14.3%, 40.0%)
11/21 11:16:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.5314	Prec@(1,5) (14.6%, 40.4%)
11/21 11:16:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.5355	Prec@(1,5) (14.7%, 40.4%)
11/21 11:16:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.5390	Prec@(1,5) (14.5%, 40.4%)
11/21 11:16:43午後 searchStage_trainer.py:323 [INFO] Valid: [  1/1] Final Prec@1 14.5360%
11/21 11:16:43午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:16:44午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 14.5360%
11/21 11:16:44午後 trainer_runner.py:110 [INFO] Final best Prec@1 = 14.5360%
11/21 11:16:44午後 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:18:14PM parser.py:28 [INFO] 
11/21 11:18:14PM parser.py:29 [INFO] Parameters:
11/21 11:18:14PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/DAG
11/21 11:18:14PM parser.py:31 [INFO] T=10.0
11/21 11:18:14PM parser.py:31 [INFO] ADVANCED=1
11/21 11:18:14PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/21 11:18:14PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/21 11:18:14PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/21 11:18:14PM parser.py:31 [INFO] BATCH_SIZE=64
11/21 11:18:14PM parser.py:31 [INFO] CASCADE=0
11/21 11:18:14PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/21 11:18:14PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/21 11:18:14PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/21 11:18:14PM parser.py:31 [INFO] DATA_PATH=../data/
11/21 11:18:14PM parser.py:31 [INFO] DATASET=cifar100
11/21 11:18:14PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/21 11:18:14PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/21 11:18:14PM parser.py:31 [INFO] DISCRETE=0
11/21 11:18:14PM parser.py:31 [INFO] EPOCHS=50
11/21 11:18:14PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/21 11:18:14PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g1
11/21 11:18:14PM parser.py:31 [INFO] FINAL_L=0.0
11/21 11:18:14PM parser.py:31 [INFO] G=1.0
11/21 11:18:14PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/21 11:18:14PM parser.py:31 [INFO] GPUS=[0]
11/21 11:18:14PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/21 11:18:14PM parser.py:31 [INFO] INIT_CHANNELS=16
11/21 11:18:14PM parser.py:31 [INFO] L=0.0
11/21 11:18:14PM parser.py:31 [INFO] LAYERS=32
11/21 11:18:14PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/21 11:18:14PM parser.py:31 [INFO] NAME=Pruning
11/21 11:18:14PM parser.py:31 [INFO] NONKD=1
11/21 11:18:14PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1
11/21 11:18:14PM parser.py:31 [INFO] PCDARTS=0
11/21 11:18:14PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g1/plots
11/21 11:18:14PM parser.py:31 [INFO] PRINT_FREQ=100
11/21 11:18:14PM parser.py:31 [INFO] RESET=0
11/21 11:18:14PM parser.py:31 [INFO] RESUME_PATH=None
11/21 11:18:14PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g1
11/21 11:18:14PM parser.py:31 [INFO] SEED=0
11/21 11:18:14PM parser.py:31 [INFO] SHARE_STAGE=0
11/21 11:18:14PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/21 11:18:14PM parser.py:31 [INFO] SPEC_CELL=1
11/21 11:18:14PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/21 11:18:14PM parser.py:31 [INFO] TEACHER_NAME=none
11/21 11:18:14PM parser.py:31 [INFO] TEACHER_PATH=none
11/21 11:18:14PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/21 11:18:14PM parser.py:31 [INFO] TYPE=Pruning
11/21 11:18:14PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/21 11:18:14PM parser.py:31 [INFO] W_LR=0.025
11/21 11:18:14PM parser.py:31 [INFO] W_LR_MIN=0.001
11/21 11:18:14PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/21 11:18:14PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/21 11:18:14PM parser.py:31 [INFO] WORKERS=4
11/21 11:18:14PM parser.py:32 [INFO] 
11/21 11:18:16PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/21 11:19:14PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.2131 (4.4995)	Arch Loss 356.0804 (360.4505)	Arch Hard Loss 4.3280 (4.4942)	Arch Beta Loss 351.7524 (355.9563)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.3%, 11.0%)	
11/21 11:20:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0304 (4.3641)	Arch Loss 347.6018 (356.0876)	Arch Hard Loss 4.2021 (4.3495)	Arch Beta Loss 343.3997 (351.7381)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.5%, 14.5%)	
11/21 11:21:07PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0445 (4.2509)	Arch Loss 339.1358 (351.8233)	Arch Hard Loss 3.9114 (4.2459)	Arch Beta Loss 335.2244 (347.5774)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.6%, 17.9%)	
11/21 11:21:57PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.7986 (4.1845)	Arch Loss 331.8902 (348.0592)	Arch Hard Loss 3.8641 (4.1753)	Arch Beta Loss 328.0261 (343.8839)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.3%, 20.1%)	
11/21 11:21:59PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 5.3320%
11/21 11:22:07PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.8829	Prec@(1,5) (9.2%, 30.0%)
11/21 11:22:15PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.8723	Prec@(1,5) (9.3%, 29.8%)
11/21 11:22:23PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.8725	Prec@(1,5) (9.2%, 29.9%)
11/21 11:22:30PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.8705	Prec@(1,5) (9.3%, 30.0%)
11/21 11:22:30PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 9.3400%
11/21 11:22:30PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 4], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 4])
11/21 11:22:31PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 9.3400%
11/21 11:23:27午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.8655 (3.8682)	Arch Loss 324.1766 (327.8425)	Arch Hard Loss 4.0433 (3.8576)	Arch Beta Loss 320.1332 (323.9849)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.3%, 30.4%)	
11/21 11:24:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.6625 (3.8261)	Arch Loss 316.3293 (323.9372)	Arch Hard Loss 3.8152 (3.8100)	Arch Beta Loss 312.5140 (320.1272)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.8%, 31.8%)	
11/21 11:25:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7006 (3.7945)	Arch Loss 308.5886 (320.1095)	Arch Hard Loss 3.5006 (3.7755)	Arch Beta Loss 305.0879 (316.3340)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.5%, 32.8%)	
11/21 11:26:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.8773 (3.7532)	Arch Loss 302.5096 (316.7177)	Arch Hard Loss 3.9429 (3.7427)	Arch Beta Loss 298.5668 (312.9751)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (11.1%, 33.9%)	
11/21 11:26:10午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 11.1520%
11/21 11:26:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6248	Prec@(1,5) (12.9%, 38.0%)
11/21 11:26:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6125	Prec@(1,5) (13.4%, 38.2%)
11/21 11:26:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6184	Prec@(1,5) (13.4%, 38.2%)
11/21 11:26:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6193	Prec@(1,5) (13.3%, 38.2%)
11/21 11:26:42午後 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 13.2600%
11/21 11:26:42午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 4])
11/21 11:26:42午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 13.2600%
11/21 11:27:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.7400 (3.5757)	Arch Loss 295.0305 (298.4758)	Arch Hard Loss 3.6037 (3.5653)	Arch Beta Loss 291.4268 (294.9105)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.1%, 38.5%)	
11/21 11:28:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5538 (3.5434)	Arch Loss 288.0798 (294.9753)	Arch Hard Loss 3.5408 (3.5532)	Arch Beta Loss 284.5391 (291.4221)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.7%, 40.1%)	
11/21 11:29:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.3833 (3.5064)	Arch Loss 281.3971 (291.5182)	Arch Hard Loss 3.5684 (3.5249)	Arch Beta Loss 277.8288 (287.9933)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.3%, 41.1%)	
11/21 11:30:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.4119 (3.4837)	Arch Loss 275.2342 (288.4510)	Arch Hard Loss 3.2960 (3.4933)	Arch Beta Loss 271.9382 (284.9577)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.6%, 41.9%)	
11/21 11:30:26午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 15.6480%
11/21 11:30:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.3605	Prec@(1,5) (18.0%, 46.6%)
11/21 11:30:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.3720	Prec@(1,5) (17.9%, 46.0%)
11/21 11:30:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.3677	Prec@(1,5) (18.0%, 45.9%)
11/21 11:30:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.3659	Prec@(1,5) (18.1%, 45.8%)
11/21 11:30:57午後 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 18.0760%
11/21 11:30:57午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:30:58午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 18.0760%
11/21 11:31:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.6311 (3.3403)	Arch Loss 268.8362 (271.9702)	Arch Hard Loss 3.3454 (3.3339)	Arch Beta Loss 265.4908 (268.6363)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.2%, 46.3%)	
11/21 11:32:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.1886 (3.3013)	Arch Loss 262.7128 (268.7986)	Arch Hard Loss 3.4407 (3.3118)	Arch Beta Loss 259.2720 (265.4868)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (19.1%, 47.4%)	
11/21 11:33:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1402 (3.2694)	Arch Loss 256.6312 (265.6744)	Arch Hard Loss 3.4224 (3.2841)	Arch Beta Loss 253.2088 (262.3903)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (19.7%, 48.0%)	
11/21 11:34:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.0026 (3.2469)	Arch Loss 251.0027 (262.9104)	Arch Hard Loss 3.1242 (3.2631)	Arch Beta Loss 247.8785 (259.6474)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (20.3%, 48.6%)	
11/21 11:34:37午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 20.3240%
11/21 11:34:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.1449	Prec@(1,5) (22.6%, 50.9%)
11/21 11:34:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.1477	Prec@(1,5) (22.3%, 51.1%)
11/21 11:35:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.1562	Prec@(1,5) (22.0%, 51.1%)
11/21 11:35:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.1558	Prec@(1,5) (22.2%, 51.3%)
11/21 11:35:08午後 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 22.1520%
11/21 11:35:08午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:35:09午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 22.1520%
11/21 11:36:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.1358 (3.0861)	Arch Loss 245.0277 (248.0812)	Arch Hard Loss 2.9941 (3.1949)	Arch Beta Loss 242.0336 (244.8863)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (23.2%, 53.5%)	
11/21 11:37:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.1538 (3.0759)	Arch Loss 239.3684 (245.1745)	Arch Hard Loss 2.9871 (3.1473)	Arch Beta Loss 236.3813 (242.0273)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (23.3%, 53.1%)	
11/21 11:38:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.6949 (3.0663)	Arch Loss 234.0028 (242.3404)	Arch Hard Loss 3.1453 (3.1284)	Arch Beta Loss 230.8575 (239.2120)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (23.4%, 53.5%)	
11/21 11:38:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.0788 (3.0551)	Arch Loss 228.8099 (239.8144)	Arch Hard Loss 2.8176 (3.1000)	Arch Beta Loss 225.9924 (236.7145)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (23.6%, 53.9%)	
11/21 11:38:52午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 23.6400%
11/21 11:39:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.0480	Prec@(1,5) (23.3%, 54.7%)
11/21 11:39:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0263	Prec@(1,5) (23.6%, 55.3%)
11/21 11:39:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0210	Prec@(1,5) (23.9%, 55.6%)
11/21 11:39:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0208	Prec@(1,5) (24.2%, 55.6%)
11/21 11:39:24午後 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 24.2320%
11/21 11:39:24午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:39:24午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 24.2320%
11/21 11:40:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1361 (2.9002)	Arch Loss 223.6555 (226.2103)	Arch Hard Loss 3.0052 (2.9521)	Arch Beta Loss 220.6503 (223.2582)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (26.1%, 57.4%)	
11/21 11:41:16午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.2363 (2.8861)	Arch Loss 218.7976 (223.6131)	Arch Hard Loss 3.3172 (2.9693)	Arch Beta Loss 215.4804 (220.6439)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (26.7%, 57.8%)	
11/21 11:42:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.8265 (2.8829)	Arch Loss 212.9802 (221.0207)	Arch Hard Loss 2.5512 (2.9516)	Arch Beta Loss 210.4290 (218.0691)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (26.6%, 57.8%)	
11/21 11:43:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.5107 (2.8713)	Arch Loss 208.7266 (218.7200)	Arch Hard Loss 2.7415 (2.9342)	Arch Beta Loss 205.9851 (215.7857)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (26.8%, 58.3%)	
11/21 11:43:05午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 26.8320%
11/21 11:43:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.8680	Prec@(1,5) (26.7%, 59.6%)
11/21 11:43:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.8713	Prec@(1,5) (26.9%, 59.5%)
11/21 11:43:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.8758	Prec@(1,5) (26.7%, 59.2%)
11/21 11:43:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.8673	Prec@(1,5) (26.9%, 59.3%)
11/21 11:43:36午後 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 26.8920%
11/21 11:43:36午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/21 11:43:37午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 26.8920%
11/21 11:44:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.7371 (2.7324)	Arch Loss 203.9991 (206.3328)	Arch Hard Loss 2.8854 (2.8420)	Arch Beta Loss 201.1136 (203.4908)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.3%, 61.2%)	
11/21 11:45:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.9517 (2.7373)	Arch Loss 199.2928 (203.9374)	Arch Hard Loss 2.8806 (2.8274)	Arch Beta Loss 196.4122 (201.1101)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.8%, 61.4%)	
11/21 11:46:27午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.6997 (2.7317)	Arch Loss 194.3173 (201.5777)	Arch Hard Loss 2.4848 (2.8079)	Arch Beta Loss 191.8325 (198.7698)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.8%, 61.6%)	
11/21 11:47:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.8392 (2.7278)	Arch Loss 190.7139 (199.5045)	Arch Hard Loss 2.8988 (2.8062)	Arch Beta Loss 187.8151 (196.6983)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.0%, 61.9%)	
11/21 11:47:18午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 30.0000%
11/21 11:47:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.7460	Prec@(1,5) (29.9%, 61.4%)
11/21 11:47:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.7375	Prec@(1,5) (30.3%, 62.2%)
11/21 11:47:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.7414	Prec@(1,5) (30.0%, 62.1%)
11/21 11:47:49午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.7497	Prec@(1,5) (29.9%, 61.8%)
11/21 11:47:49午後 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 29.9040%
11/21 11:47:49午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/21 11:47:50午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 29.9040%
11/21 11:48:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.8266 (2.5764)	Arch Loss 186.1379 (188.2976)	Arch Hard Loss 2.7151 (2.7323)	Arch Beta Loss 183.4228 (185.5654)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.2%, 65.8%)	
11/21 11:49:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.6295 (2.5892)	Arch Loss 181.7810 (186.1370)	Arch Hard Loss 2.5875 (2.7157)	Arch Beta Loss 179.1935 (183.4212)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.9%, 65.4%)	
11/21 11:50:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.6628 (2.5831)	Arch Loss 178.2398 (184.0273)	Arch Hard Loss 3.1575 (2.7106)	Arch Beta Loss 175.0823 (181.3167)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.9%, 65.4%)	
11/21 11:51:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.5906 (2.5743)	Arch Loss 174.1373 (182.1454)	Arch Hard Loss 2.6571 (2.6894)	Arch Beta Loss 171.4802 (179.4560)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.0%, 65.6%)	
11/21 11:51:28午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 33.0200%
11/21 11:51:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6582	Prec@(1,5) (31.5%, 63.3%)
11/21 11:51:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.6331	Prec@(1,5) (32.0%, 64.1%)
11/21 11:51:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6333	Prec@(1,5) (32.1%, 64.4%)
11/21 11:52:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6430	Prec@(1,5) (31.8%, 64.2%)
11/21 11:52:00午後 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 31.8280%
11/21 11:52:00午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/21 11:52:01午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 31.8280%
11/21 11:53:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.4231 (2.4664)	Arch Loss 170.3186 (172.0903)	Arch Hard Loss 2.7730 (2.6260)	Arch Beta Loss 167.5456 (169.4643)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.2%, 68.2%)	
11/21 11:53:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.1909 (2.4587)	Arch Loss 166.7422 (170.1643)	Arch Hard Loss 2.9815 (2.6195)	Arch Beta Loss 163.7607 (167.5448)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.5%, 68.2%)	
11/21 11:54:54午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.4846 (2.4574)	Arch Loss 162.6122 (168.2726)	Arch Hard Loss 2.5300 (2.6113)	Arch Beta Loss 160.0821 (165.6613)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.5%, 68.3%)	
11/21 11:55:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.2407 (2.4569)	Arch Loss 159.7735 (166.5987)	Arch Hard Loss 2.9139 (2.6024)	Arch Beta Loss 156.8596 (163.9963)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.7%, 68.3%)	
11/21 11:55:43午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 35.6880%
11/21 11:55:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5463	Prec@(1,5) (33.1%, 66.9%)
11/21 11:55:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.5590	Prec@(1,5) (33.4%, 66.3%)
11/21 11:56:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.5760	Prec@(1,5) (33.2%, 65.9%)
11/21 11:56:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.5779	Prec@(1,5) (33.2%, 65.9%)
11/21 11:56:14午後 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 33.1760%
11/21 11:56:14午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/21 11:56:14午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.1760%
11/21 11:57:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.4637 (2.3540)	Arch Loss 156.1605 (157.5937)	Arch Hard Loss 2.8223 (2.5382)	Arch Beta Loss 153.3382 (155.0555)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.5%, 70.4%)	
11/21 11:58:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.3825 (2.3608)	Arch Loss 152.5552 (155.8517)	Arch Hard Loss 2.6072 (2.5147)	Arch Beta Loss 149.9481 (153.3370)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.5%, 70.9%)	
11/21 11:58:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.1188 (2.3526)	Arch Loss 148.7638 (154.1580)	Arch Hard Loss 2.1136 (2.5083)	Arch Beta Loss 146.6502 (151.6497)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.6%, 70.9%)	
11/21 11:59:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.1694 (2.3468)	Arch Loss 146.3107 (152.6608)	Arch Hard Loss 2.5531 (2.5036)	Arch Beta Loss 143.7576 (150.1571)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.9%, 71.0%)	
11/21 11:59:46午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 37.8640%
11/21 11:59:54午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.4780	Prec@(1,5) (35.1%, 68.0%)
11/22 12:00:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.4590	Prec@(1,5) (35.6%, 68.4%)
11/22 12:00:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.4675	Prec@(1,5) (35.5%, 68.2%)
11/22 12:00:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.4696	Prec@(1,5) (35.6%, 68.1%)
11/22 12:00:17午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 35.6280%
11/22 12:00:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/22 12:00:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 35.6280%
11/22 12:01:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.4448 (2.2277)	Arch Loss 143.0691 (144.6315)	Arch Hard Loss 2.4759 (2.4945)	Arch Beta Loss 140.5933 (142.1370)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.0%, 73.2%)	
11/22 12:02:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.2934 (2.2505)	Arch Loss 139.7897 (143.0617)	Arch Hard Loss 2.2484 (2.4705)	Arch Beta Loss 137.5413 (140.5912)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.9%, 72.8%)	
11/22 12:03:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.2739 (2.2560)	Arch Loss 136.8006 (141.5252)	Arch Hard Loss 2.2332 (2.4533)	Arch Beta Loss 134.5675 (139.0719)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.8%, 72.8%)	
11/22 12:03:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.2085 (2.2493)	Arch Loss 134.3161 (140.1689)	Arch Hard Loss 2.3617 (2.4425)	Arch Beta Loss 131.9544 (137.7264)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.1%, 72.8%)	
11/22 12:03:49午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 40.1200%
11/22 12:03:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.3639	Prec@(1,5) (37.7%, 70.1%)
11/22 12:04:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.3398	Prec@(1,5) (38.1%, 70.7%)
11/22 12:04:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.3522	Prec@(1,5) (38.3%, 70.5%)
11/22 12:04:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.3449	Prec@(1,5) (38.5%, 70.8%)
11/22 12:04:20午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 38.4560%
11/22 12:04:20午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 8), ('max_pool_3x3', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:04:20午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 38.4560%
11/22 12:05:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.1007 (2.1447)	Arch Loss 131.9733 (132.8583)	Arch Hard Loss 2.8824 (2.3701)	Arch Beta Loss 129.0908 (130.4882)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.5%, 75.2%)	
11/22 12:06:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.3572 (2.1335)	Arch Loss 128.8864 (131.4521)	Arch Hard Loss 2.5619 (2.3638)	Arch Beta Loss 126.3246 (129.0883)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.3%, 75.5%)	
11/22 12:07:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.4427 (2.1424)	Arch Loss 125.7066 (130.0737)	Arch Hard Loss 2.0834 (2.3631)	Arch Beta Loss 123.6232 (127.7106)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.2%, 75.1%)	
11/22 12:07:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 1.9485 (2.1522)	Arch Loss 123.4110 (128.8471)	Arch Hard Loss 2.1639 (2.3579)	Arch Beta Loss 121.2472 (126.4891)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.1%, 74.9%)	
11/22 12:07:53午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 42.0680%
11/22 12:08:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.3310	Prec@(1,5) (39.3%, 71.7%)
11/22 12:08:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.3026	Prec@(1,5) (39.7%, 72.0%)
11/22 12:08:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.2977	Prec@(1,5) (40.0%, 72.0%)
11/22 12:08:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.2990	Prec@(1,5) (39.8%, 72.0%)
11/22 12:08:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 39.7800%
11/22 12:08:24午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:08:24午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 39.7800%
11/22 12:09:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 1.9929 (2.0436)	Arch Loss 120.6264 (122.2460)	Arch Hard Loss 1.9875 (2.3338)	Arch Beta Loss 118.6389 (119.9122)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.5%, 76.5%)	
11/22 12:10:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.2132 (2.0639)	Arch Loss 118.2502 (120.9615)	Arch Hard Loss 2.1328 (2.3250)	Arch Beta Loss 116.1174 (118.6364)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.0%, 76.4%)	
11/22 12:11:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.0471 (2.0662)	Arch Loss 116.0296 (119.6898)	Arch Hard Loss 2.3771 (2.3095)	Arch Beta Loss 113.6525 (117.3803)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.1%, 76.3%)	
11/22 12:11:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 2.0051 (2.0725)	Arch Loss 113.8029 (118.5617)	Arch Hard Loss 2.3201 (2.2956)	Arch Beta Loss 111.4828 (116.2660)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.0%, 76.2%)	
11/22 12:11:57午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 43.9400%
11/22 12:12:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.2724	Prec@(1,5) (40.3%, 72.8%)
11/22 12:12:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.2596	Prec@(1,5) (40.2%, 72.9%)
11/22 12:12:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.2623	Prec@(1,5) (40.2%, 72.9%)
11/22 12:12:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.2686	Prec@(1,5) (40.2%, 72.5%)
11/22 12:12:27午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 40.1920%
11/22 12:12:27午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 8), ('max_pool_3x3', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:12:28午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.1920%
11/22 12:13:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.0903 (1.9443)	Arch Loss 111.5805 (112.5014)	Arch Hard Loss 2.4783 (2.2375)	Arch Beta Loss 109.1022 (110.2640)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.6%, 79.2%)	
11/22 12:14:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.8776 (1.9954)	Arch Loss 108.9058 (111.3357)	Arch Hard Loss 2.1054 (2.2361)	Arch Beta Loss 106.8004 (109.0996)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.5%, 78.1%)	
11/22 12:15:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8104 (1.9962)	Arch Loss 106.7076 (110.1871)	Arch Hard Loss 2.1547 (2.2338)	Arch Beta Loss 104.5529 (107.9533)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.7%, 78.0%)	
11/22 12:15:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.9844 (1.9987)	Arch Loss 104.5185 (109.1676)	Arch Hard Loss 1.9428 (2.2305)	Arch Beta Loss 102.5757 (106.9371)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.6%, 77.9%)	
11/22 12:16:00午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 45.5960%
11/22 12:16:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2030	Prec@(1,5) (42.5%, 73.0%)
11/22 12:16:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2302	Prec@(1,5) (41.4%, 72.9%)
11/22 12:16:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2151	Prec@(1,5) (41.8%, 73.0%)
11/22 12:16:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.2175	Prec@(1,5) (41.7%, 73.1%)
11/22 12:16:30午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 41.7200%
11/22 12:16:30午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:16:31午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.7200%
11/22 12:17:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 1.8816 (1.8961)	Arch Loss 102.6352 (103.6663)	Arch Hard Loss 2.2277 (2.2008)	Arch Beta Loss 100.4075 (101.4655)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.0%, 80.3%)	
11/22 12:18:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.0500 (1.9220)	Arch Loss 100.4204 (102.6030)	Arch Hard Loss 2.1067 (2.1974)	Arch Beta Loss 98.3136 (100.4056)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.6%, 79.1%)	
11/22 12:19:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.8725 (1.9237)	Arch Loss 98.5680 (101.5613)	Arch Hard Loss 2.2976 (2.1984)	Arch Beta Loss 96.2704 (99.3629)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.2%, 79.3%)	
11/22 12:20:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.9542 (1.9241)	Arch Loss 96.7138 (100.6285)	Arch Hard Loss 2.2390 (2.1895)	Arch Beta Loss 94.4748 (98.4390)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.2%, 79.3%)	
11/22 12:20:03午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 47.2240%
11/22 12:20:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.1221	Prec@(1,5) (43.7%, 76.2%)
11/22 12:20:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.1390	Prec@(1,5) (43.1%, 75.3%)
11/22 12:20:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.1429	Prec@(1,5) (43.0%, 75.1%)
11/22 12:20:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.1452	Prec@(1,5) (42.8%, 74.9%)
11/22 12:20:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 42.8240%
11/22 12:20:34午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:20:35午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.8240%
11/22 12:21:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.2331 (1.8232)	Arch Loss 94.6628 (95.6354)	Arch Hard Loss 2.1554 (2.1682)	Arch Beta Loss 92.5074 (93.4672)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.0%, 80.8%)	
11/22 12:22:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.5327 (1.8273)	Arch Loss 92.5368 (94.6636)	Arch Hard Loss 1.9284 (2.1577)	Arch Beta Loss 90.6083 (92.5058)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.1%, 80.7%)	
11/22 12:23:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.9826 (1.8395)	Arch Loss 90.8262 (93.7235)	Arch Hard Loss 2.0700 (2.1632)	Arch Beta Loss 88.7563 (91.5603)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.6%, 80.5%)	
11/22 12:24:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.5836 (1.8495)	Arch Loss 89.1375 (92.8786)	Arch Hard Loss 2.0087 (2.1560)	Arch Beta Loss 87.1287 (90.7226)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.2%, 80.4%)	
11/22 12:24:07午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 49.1880%
11/22 12:24:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.1332	Prec@(1,5) (43.1%, 75.7%)
11/22 12:24:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.1140	Prec@(1,5) (43.3%, 75.7%)
11/22 12:24:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.1244	Prec@(1,5) (43.1%, 75.4%)
11/22 12:24:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.1209	Prec@(1,5) (43.0%, 75.5%)
11/22 12:24:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 43.0360%
11/22 12:24:38午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:24:39午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.0360%
11/22 12:25:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.6506 (1.7453)	Arch Loss 87.1699 (88.3211)	Arch Hard Loss 1.8246 (2.1055)	Arch Beta Loss 85.3453 (86.2156)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (51.7%, 82.2%)	
11/22 12:26:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.7904 (1.7731)	Arch Loss 85.6274 (87.4422)	Arch Hard Loss 2.0040 (2.0983)	Arch Beta Loss 83.6234 (85.3439)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (51.0%, 81.7%)	
11/22 12:27:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.7792 (1.7871)	Arch Loss 84.4320 (86.6071)	Arch Hard Loss 2.4887 (2.1207)	Arch Beta Loss 81.9433 (84.4864)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.5%, 81.5%)	
11/22 12:28:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.7757 (1.7921)	Arch Loss 82.5690 (85.8352)	Arch Hard Loss 2.1022 (2.1085)	Arch Beta Loss 80.4668 (83.7266)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.4%, 81.2%)	
11/22 12:28:10午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 50.3520%
11/22 12:28:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1155	Prec@(1,5) (44.0%, 75.3%)
11/22 12:28:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.0913	Prec@(1,5) (44.1%, 75.9%)
11/22 12:28:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.0986	Prec@(1,5) (44.0%, 76.0%)
11/22 12:28:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.0890	Prec@(1,5) (44.1%, 76.1%)
11/22 12:28:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 44.1000%
11/22 12:28:41午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:28:42午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.1000%
11/22 12:29:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.7269 (1.6684)	Arch Loss 80.8629 (81.7017)	Arch Hard Loss 2.0155 (2.0639)	Arch Beta Loss 78.8474 (79.6378)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.2%, 84.2%)	
11/22 12:30:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.6704 (1.7045)	Arch Loss 79.3727 (80.9207)	Arch Hard Loss 2.0900 (2.0748)	Arch Beta Loss 77.2828 (78.8459)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.9%, 83.5%)	
11/22 12:31:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 2.0737 (1.7299)	Arch Loss 77.7238 (80.1534)	Arch Hard Loss 1.9694 (2.0869)	Arch Beta Loss 75.7544 (78.0666)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 82.9%)	
11/22 12:32:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.4608 (1.7407)	Arch Loss 76.3822 (79.4560)	Arch Hard Loss 1.9729 (2.0805)	Arch Beta Loss 74.4093 (77.3755)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.0%, 82.7%)	
11/22 12:32:14午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 51.0400%
11/22 12:32:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.0399	Prec@(1,5) (45.7%, 77.0%)
11/22 12:32:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.0346	Prec@(1,5) (46.3%, 77.0%)
11/22 12:32:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.0435	Prec@(1,5) (45.9%, 76.7%)
11/22 12:32:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.0481	Prec@(1,5) (45.6%, 76.5%)
11/22 12:32:44午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 45.5840%
11/22 12:32:44午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:32:45午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.5840%
11/22 12:33:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.5843 (1.6302)	Arch Loss 75.1047 (75.6718)	Arch Hard Loss 2.1717 (2.0181)	Arch Beta Loss 72.9330 (73.6537)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.2%, 84.4%)	
11/22 12:34:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.4805 (1.6656)	Arch Loss 73.3457 (74.9705)	Arch Hard Loss 1.8420 (2.0393)	Arch Beta Loss 71.5037 (72.9311)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.4%, 83.9%)	
11/22 12:35:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.6194 (1.6670)	Arch Loss 72.3159 (74.2674)	Arch Hard Loss 2.2086 (2.0480)	Arch Beta Loss 70.1073 (72.2194)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.5%, 83.6%)	
11/22 12:36:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.6628 (1.6805)	Arch Loss 70.4302 (73.6217)	Arch Hard Loss 1.5534 (2.0338)	Arch Beta Loss 68.8767 (71.5879)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.0%, 83.5%)	
11/22 12:36:17午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 53.0200%
11/22 12:36:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.0423	Prec@(1,5) (46.5%, 76.8%)
11/22 12:36:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.0262	Prec@(1,5) (46.4%, 77.3%)
11/22 12:36:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.0228	Prec@(1,5) (46.4%, 77.3%)
11/22 12:36:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.0262	Prec@(1,5) (46.1%, 77.5%)
11/22 12:36:47午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 46.0960%
11/22 12:36:47午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('skip_connect', 9), ('max_pool_3x3', 7)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:36:48午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.0960%
11/22 12:37:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.3343 (1.5821)	Arch Loss 69.5332 (70.2014)	Arch Hard Loss 2.0089 (2.0168)	Arch Beta Loss 67.5243 (68.1847)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.6%, 85.3%)	
11/22 12:38:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.7538 (1.6075)	Arch Loss 68.5032 (69.5519)	Arch Hard Loss 2.2889 (2.0293)	Arch Beta Loss 66.2142 (67.5225)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.2%, 84.8%)	
11/22 12:39:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 1.7975 (1.6201)	Arch Loss 66.7370 (68.8796)	Arch Hard Loss 1.8051 (2.0099)	Arch Beta Loss 64.9320 (66.8697)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.0%, 84.7%)	
11/22 12:40:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.6799 (1.6309)	Arch Loss 65.7535 (68.2904)	Arch Hard Loss 1.9511 (2.0001)	Arch Beta Loss 63.8024 (66.2903)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.8%, 84.4%)	
11/22 12:40:20午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 53.8400%
11/22 12:40:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.0265	Prec@(1,5) (46.2%, 77.0%)
11/22 12:40:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.0122	Prec@(1,5) (46.7%, 77.1%)
11/22 12:40:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 1.9914	Prec@(1,5) (47.0%, 77.5%)
11/22 12:40:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 1.9877	Prec@(1,5) (46.8%, 77.6%)
11/22 12:40:51午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 46.7960%
11/22 12:40:51午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:40:51午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.7960%
11/22 12:41:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.5943 (1.5352)	Arch Loss 64.4740 (65.1593)	Arch Hard Loss 1.9144 (1.9928)	Arch Beta Loss 62.5596 (63.1666)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.5%, 86.4%)	
11/22 12:42:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.4893 (1.5570)	Arch Loss 63.9459 (64.5470)	Arch Hard Loss 2.5892 (1.9887)	Arch Beta Loss 61.3568 (62.5583)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.2%, 85.9%)	
11/22 12:43:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.1906 (1.5633)	Arch Loss 61.8828 (63.9377)	Arch Hard Loss 1.7043 (1.9789)	Arch Beta Loss 60.1785 (61.9588)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.3%, 85.5%)	
11/22 12:44:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.8356 (1.5821)	Arch Loss 60.9519 (63.4056)	Arch Hard Loss 1.8118 (1.9792)	Arch Beta Loss 59.1400 (61.4263)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.1%, 85.2%)	
11/22 12:44:22午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 55.1320%
11/22 12:44:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 1.9711	Prec@(1,5) (47.2%, 79.0%)
11/22 12:44:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 1.9608	Prec@(1,5) (47.6%, 79.1%)
11/22 12:44:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 1.9561	Prec@(1,5) (47.6%, 79.2%)
11/22 12:44:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 1.9597	Prec@(1,5) (47.5%, 79.1%)
11/22 12:44:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 47.4520%
11/22 12:44:53午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 12:44:54午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4520%
11/22 12:45:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.7666 (1.4730)	Arch Loss 60.3178 (60.5377)	Arch Hard Loss 2.3182 (1.9812)	Arch Beta Loss 57.9996 (58.5565)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.2%, 87.3%)	
11/22 12:46:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.5776 (1.5067)	Arch Loss 59.0161 (59.9685)	Arch Hard Loss 2.1211 (1.9704)	Arch Beta Loss 56.8950 (57.9981)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.5%, 86.5%)	
11/22 12:47:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.6027 (1.5215)	Arch Loss 57.5455 (59.4047)	Arch Hard Loss 1.7292 (1.9566)	Arch Beta Loss 55.8164 (57.4481)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.2%, 86.3%)	
11/22 12:48:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.5748 (1.5290)	Arch Loss 56.6118 (58.9087)	Arch Hard Loss 1.7455 (1.9484)	Arch Beta Loss 54.8663 (56.9603)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.0%, 86.2%)	
11/22 12:48:27午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 56.9680%
11/22 12:48:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 1.8808	Prec@(1,5) (49.6%, 79.4%)
11/22 12:48:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 1.8881	Prec@(1,5) (49.2%, 79.7%)
11/22 12:48:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 1.8928	Prec@(1,5) (49.0%, 79.5%)
11/22 12:48:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 1.8849	Prec@(1,5) (49.0%, 79.7%)
11/22 12:48:58午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 49.0200%
11/22 12:48:58午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 12:48:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.0200%
11/22 12:49:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.4483 (1.4188)	Arch Loss 56.0591 (56.2994)	Arch Hard Loss 2.2367 (1.9676)	Arch Beta Loss 53.8223 (54.3319)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.9%, 88.3%)	
11/22 12:50:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.5226 (1.4473)	Arch Loss 54.7181 (55.7497)	Arch Hard Loss 1.9043 (1.9282)	Arch Beta Loss 52.8137 (53.8215)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.9%, 87.5%)	
11/22 12:51:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.4284 (1.4655)	Arch Loss 54.1359 (55.2436)	Arch Hard Loss 2.3080 (1.9247)	Arch Beta Loss 51.8279 (53.3190)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.2%, 87.1%)	
11/22 12:52:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.7032 (1.4838)	Arch Loss 52.9049 (54.8003)	Arch Hard Loss 1.9449 (1.9271)	Arch Beta Loss 50.9600 (52.8733)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.5%, 86.8%)	
11/22 12:52:31午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 57.4840%
11/22 12:52:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 1.9151	Prec@(1,5) (48.3%, 79.5%)
11/22 12:52:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 1.9186	Prec@(1,5) (47.8%, 79.4%)
11/22 12:52:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 1.9177	Prec@(1,5) (48.0%, 79.6%)
11/22 12:53:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 1.9226	Prec@(1,5) (47.8%, 79.5%)
11/22 12:53:02午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 47.8040%
11/22 12:53:02午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 12:53:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.0200%
11/22 12:53:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.3327 (1.3514)	Arch Loss 51.6875 (52.3835)	Arch Hard Loss 1.6795 (1.9110)	Arch Beta Loss 50.0080 (50.4725)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.2%, 89.0%)	
11/22 12:54:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.3630 (1.4000)	Arch Loss 51.1479 (51.9083)	Arch Hard Loss 2.0602 (1.9014)	Arch Beta Loss 49.0877 (50.0069)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.0%, 88.1%)	
11/22 12:55:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.4066 (1.4182)	Arch Loss 49.9909 (51.4580)	Arch Hard Loss 1.8031 (1.9097)	Arch Beta Loss 48.1878 (49.5483)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.3%, 88.1%)	
11/22 12:56:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.6243 (1.4307)	Arch Loss 49.3865 (51.0505)	Arch Hard Loss 1.9897 (1.9088)	Arch Beta Loss 47.3969 (49.1418)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.0%, 87.9%)	
11/22 12:56:35午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 59.0480%
11/22 12:56:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.8627	Prec@(1,5) (49.1%, 79.9%)
11/22 12:56:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.8627	Prec@(1,5) (49.0%, 79.9%)
11/22 12:56:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.8737	Prec@(1,5) (48.9%, 79.9%)
11/22 12:57:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.8752	Prec@(1,5) (49.0%, 80.0%)
11/22 12:57:06午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 48.9760%
11/22 12:57:06午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 12:57:06午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.0200%
11/22 12:58:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.1351 (1.3551)	Arch Loss 48.6839 (48.8408)	Arch Hard Loss 2.1558 (1.8888)	Arch Beta Loss 46.5281 (46.9520)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.0%, 88.7%)	
11/22 12:58:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.4002 (1.3771)	Arch Loss 47.6570 (48.4285)	Arch Hard Loss 1.9692 (1.9013)	Arch Beta Loss 45.6878 (46.5272)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.4%, 88.6%)	
11/22 12:59:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.6817 (1.3893)	Arch Loss 46.6774 (48.0060)	Arch Hard Loss 1.8117 (1.8975)	Arch Beta Loss 44.8657 (46.1085)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.3%)	
11/22 01:00:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.2262 (1.4001)	Arch Loss 45.9562 (47.6392)	Arch Hard Loss 1.8141 (1.9022)	Arch Beta Loss 44.1421 (45.7370)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.1%, 88.1%)	
11/22 01:00:38午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 60.0640%
11/22 01:00:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 1.9164	Prec@(1,5) (48.4%, 80.1%)
11/22 01:00:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 1.9016	Prec@(1,5) (49.2%, 80.2%)
11/22 01:01:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 1.9137	Prec@(1,5) (48.9%, 79.7%)
11/22 01:01:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 1.9034	Prec@(1,5) (49.0%, 79.8%)
11/22 01:01:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 48.9920%
11/22 01:01:09午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 01:01:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.0200%
11/22 01:02:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.6342 (1.3150)	Arch Loss 45.2974 (45.5912)	Arch Hard Loss 1.9505 (1.8562)	Arch Beta Loss 43.3469 (43.7350)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.8%, 89.2%)	
11/22 01:02:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.4026 (1.3374)	Arch Loss 44.8729 (45.2115)	Arch Hard Loss 2.2960 (1.8657)	Arch Beta Loss 42.5769 (43.3459)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.8%, 89.0%)	
11/22 01:03:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.1879 (1.3450)	Arch Loss 43.2782 (44.8329)	Arch Hard Loss 1.4550 (1.8707)	Arch Beta Loss 41.8231 (42.9622)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.4%, 88.8%)	
11/22 01:04:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.2775 (1.3583)	Arch Loss 42.7946 (44.4968)	Arch Hard Loss 1.6351 (1.8751)	Arch Beta Loss 41.1595 (42.6217)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.9%, 88.7%)	
11/22 01:04:41午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 60.9080%
11/22 01:04:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 1.8578	Prec@(1,5) (50.2%, 80.0%)
11/22 01:04:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 1.8658	Prec@(1,5) (50.0%, 80.2%)
11/22 01:05:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 1.8544	Prec@(1,5) (50.2%, 80.4%)
11/22 01:05:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 1.8575	Prec@(1,5) (50.0%, 80.3%)
11/22 01:05:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 50.0480%
11/22 01:05:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 01:05:12午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.0480%
11/22 01:06:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.6455 (1.2763)	Arch Loss 42.0230 (42.5813)	Arch Hard Loss 1.5939 (1.7954)	Arch Beta Loss 40.4291 (40.7859)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.1%, 90.1%)	
11/22 01:07:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.9619 (1.2974)	Arch Loss 41.6878 (42.2539)	Arch Hard Loss 1.9669 (1.8257)	Arch Beta Loss 39.7209 (40.4281)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.6%, 89.7%)	
11/22 01:07:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.1889 (1.2945)	Arch Loss 40.6187 (41.9170)	Arch Hard Loss 1.5915 (1.8419)	Arch Beta Loss 39.0271 (40.0751)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.5%, 89.8%)	
11/22 01:08:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.1140 (1.3171)	Arch Loss 40.6738 (41.6179)	Arch Hard Loss 2.2591 (1.8563)	Arch Beta Loss 38.4147 (39.7616)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.9%, 89.4%)	
11/22 01:08:44午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 61.9320%
11/22 01:08:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.8307	Prec@(1,5) (51.1%, 81.0%)
11/22 01:08:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.8160	Prec@(1,5) (51.0%, 80.9%)
11/22 01:09:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.8248	Prec@(1,5) (51.0%, 80.8%)
11/22 01:09:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.8286	Prec@(1,5) (50.8%, 80.6%)
11/22 01:09:14午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 50.8120%
11/22 01:09:14午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 01:09:15午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.8120%
11/22 01:10:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.0014 (1.2159)	Arch Loss 39.9005 (39.9354)	Arch Hard Loss 2.1604 (1.8659)	Arch Beta Loss 37.7401 (38.0696)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.0%, 90.7%)	
11/22 01:11:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.6043 (1.2547)	Arch Loss 38.6967 (39.5938)	Arch Hard Loss 1.6106 (1.8547)	Arch Beta Loss 37.0861 (37.7391)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.0%, 90.2%)	
11/22 01:11:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.0971 (1.2629)	Arch Loss 38.5958 (39.2640)	Arch Hard Loss 2.1513 (1.8509)	Arch Beta Loss 36.4445 (37.4131)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.8%, 90.1%)	
11/22 01:12:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.2581 (1.2811)	Arch Loss 37.4650 (38.9671)	Arch Hard Loss 1.5871 (1.8438)	Arch Beta Loss 35.8778 (37.1233)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.4%, 89.8%)	
11/22 01:12:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 63.3720%
11/22 01:12:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.8250	Prec@(1,5) (51.1%, 80.8%)
11/22 01:13:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.8101	Prec@(1,5) (51.1%, 81.0%)
11/22 01:13:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.8146	Prec@(1,5) (50.9%, 81.1%)
11/22 01:13:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.8058	Prec@(1,5) (51.1%, 81.2%)
11/22 01:13:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 51.1080%
11/22 01:13:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 01:13:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.1080%
11/22 01:14:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.4869 (1.1870)	Arch Loss 37.2332 (37.3616)	Arch Hard Loss 1.9795 (1.8030)	Arch Beta Loss 35.2537 (35.5586)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.5%, 91.2%)	
11/22 01:15:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.9813 (1.2091)	Arch Loss 36.0469 (37.0593)	Arch Hard Loss 1.3988 (1.8065)	Arch Beta Loss 34.6481 (35.2528)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.0%, 91.0%)	
11/22 01:16:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.3159 (1.2186)	Arch Loss 35.7241 (36.7674)	Arch Hard Loss 1.6684 (1.8163)	Arch Beta Loss 34.0557 (34.9512)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.8%, 90.9%)	
11/22 01:16:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.0955 (1.2398)	Arch Loss 35.0553 (36.5151)	Arch Hard Loss 1.5226 (1.8318)	Arch Beta Loss 33.5328 (34.6833)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.5%)	
11/22 01:16:51午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 64.1680%
11/22 01:16:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.8153	Prec@(1,5) (50.9%, 81.6%)
11/22 01:17:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 1.8322	Prec@(1,5) (50.4%, 81.1%)
11/22 01:17:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.8122	Prec@(1,5) (51.3%, 81.4%)
11/22 01:17:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.8056	Prec@(1,5) (51.5%, 81.4%)
11/22 01:17:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 51.5200%
11/22 01:17:22午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 01:17:22午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.5200%
11/22 01:18:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.3341 (1.1622)	Arch Loss 34.6612 (35.0693)	Arch Hard Loss 1.7038 (1.8311)	Arch Beta Loss 32.9574 (33.2382)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.1%, 91.4%)	
11/22 01:19:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.1178 (1.1829)	Arch Loss 33.9168 (34.7841)	Arch Hard Loss 1.5168 (1.8274)	Arch Beta Loss 32.4001 (32.9567)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.5%, 91.2%)	
11/22 01:20:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.1942 (1.1997)	Arch Loss 33.8303 (34.5040)	Arch Hard Loss 1.9755 (1.8250)	Arch Beta Loss 31.8548 (32.6790)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.1%, 91.0%)	
11/22 01:20:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.0311 (1.2129)	Arch Loss 33.2391 (34.2573)	Arch Hard Loss 1.8646 (1.8247)	Arch Beta Loss 31.3745 (32.4326)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.7%, 90.8%)	
11/22 01:20:55午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 64.6600%
11/22 01:21:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 1.8020	Prec@(1,5) (52.3%, 81.7%)
11/22 01:21:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 1.7913	Prec@(1,5) (52.4%, 81.6%)
11/22 01:21:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 1.8001	Prec@(1,5) (51.9%, 81.4%)
11/22 01:21:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 1.7994	Prec@(1,5) (51.9%, 81.5%)
11/22 01:21:26午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 51.8960%
11/22 01:21:26午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 01:21:26午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.8960%
11/22 01:22:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.2211 (1.1676)	Arch Loss 32.3111 (32.8928)	Arch Hard Loss 1.4646 (1.7887)	Arch Beta Loss 30.8465 (31.1041)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.2%, 91.6%)	
11/22 01:23:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.3162 (1.1537)	Arch Loss 32.2250 (32.6563)	Arch Hard Loss 1.8883 (1.8102)	Arch Beta Loss 30.3367 (30.8461)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.5%, 91.8%)	
11/22 01:24:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.2955 (1.1711)	Arch Loss 31.4145 (32.3982)	Arch Hard Loss 1.5773 (1.8063)	Arch Beta Loss 29.8372 (30.5919)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.0%, 91.5%)	
11/22 01:24:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.3715 (1.1871)	Arch Loss 31.4785 (32.1725)	Arch Hard Loss 2.0811 (1.8062)	Arch Beta Loss 29.3973 (30.3663)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.7%, 91.3%)	
11/22 01:24:59午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 65.7000%
11/22 01:25:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 1.8073	Prec@(1,5) (51.2%, 81.9%)
11/22 01:25:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 1.7918	Prec@(1,5) (51.7%, 81.8%)
11/22 01:25:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 1.8006	Prec@(1,5) (51.7%, 81.7%)
11/22 01:25:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 1.7959	Prec@(1,5) (51.8%, 81.7%)
11/22 01:25:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 51.8320%
11/22 01:25:29午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 01:25:30午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.8960%
11/22 01:26:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.1968 (1.1214)	Arch Loss 30.9254 (30.9239)	Arch Hard Loss 2.0110 (1.7738)	Arch Beta Loss 28.9144 (29.1501)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.0%, 91.9%)	
11/22 01:27:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.0002 (1.1279)	Arch Loss 30.1566 (30.7129)	Arch Hard Loss 1.7089 (1.7990)	Arch Beta Loss 28.4476 (28.9140)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.4%, 92.1%)	
11/22 01:28:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.2755 (1.1492)	Arch Loss 30.0666 (30.4806)	Arch Hard Loss 2.0751 (1.7992)	Arch Beta Loss 27.9915 (28.6814)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.8%, 91.9%)	
11/22 01:29:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.4808 (1.1628)	Arch Loss 29.4624 (30.2689)	Arch Hard Loss 1.8724 (1.7937)	Arch Beta Loss 27.5899 (28.4752)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 91.6%)	
11/22 01:29:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 66.3160%
11/22 01:29:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 1.7328	Prec@(1,5) (53.4%, 82.7%)
11/22 01:29:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 1.7319	Prec@(1,5) (53.5%, 82.5%)
11/22 01:29:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 1.7480	Prec@(1,5) (53.0%, 82.2%)
11/22 01:29:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 1.7554	Prec@(1,5) (52.8%, 82.1%)
11/22 01:29:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 52.7960%
11/22 01:29:32午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 01:29:32午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.7960%
11/22 01:30:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.3537 (1.0887)	Arch Loss 28.7385 (29.1311)	Arch Hard Loss 1.5897 (1.7670)	Arch Beta Loss 27.1488 (27.3641)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.0%, 92.6%)	
11/22 01:31:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.9801 (1.1071)	Arch Loss 28.6835 (28.9235)	Arch Hard Loss 1.9625 (1.7755)	Arch Beta Loss 26.7210 (27.1481)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.9%, 92.5%)	
11/22 01:32:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.0431 (1.1105)	Arch Loss 28.2800 (28.7250)	Arch Hard Loss 1.9771 (1.7899)	Arch Beta Loss 26.3029 (26.9350)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.8%, 92.4%)	
11/22 01:33:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.1324 (1.1222)	Arch Loss 27.6646 (28.5308)	Arch Hard Loss 1.7309 (1.7848)	Arch Beta Loss 25.9337 (26.7460)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.3%, 92.2%)	
11/22 01:33:03午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 67.2760%
11/22 01:33:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 1.7693	Prec@(1,5) (52.8%, 82.4%)
11/22 01:33:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 1.7606	Prec@(1,5) (53.0%, 82.6%)
11/22 01:33:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 1.7657	Prec@(1,5) (53.0%, 82.4%)
11/22 01:33:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 1.7636	Prec@(1,5) (53.0%, 82.3%)
11/22 01:33:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 53.0440%
11/22 01:33:34午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 01:33:34午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.0440%
11/22 01:34:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.0324 (1.0266)	Arch Loss 27.2529 (27.4930)	Arch Hard Loss 1.7248 (1.7671)	Arch Beta Loss 25.5281 (25.7259)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.6%, 93.5%)	
11/22 01:35:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 1.0255 (1.0592)	Arch Loss 26.7024 (27.2788)	Arch Hard Loss 1.5674 (1.7514)	Arch Beta Loss 25.1350 (25.5275)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.2%, 93.1%)	
11/22 01:36:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.1225 (1.0784)	Arch Loss 27.1085 (27.1073)	Arch Hard Loss 2.3588 (1.7758)	Arch Beta Loss 24.7497 (25.3315)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.6%, 92.9%)	
11/22 01:37:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.2550 (1.0932)	Arch Loss 26.0993 (26.9326)	Arch Hard Loss 1.6897 (1.7752)	Arch Beta Loss 24.4097 (25.1574)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.1%, 92.7%)	
11/22 01:37:06午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 68.1040%
11/22 01:37:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 1.7529	Prec@(1,5) (53.5%, 82.5%)
11/22 01:37:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 1.7318	Prec@(1,5) (53.9%, 82.6%)
11/22 01:37:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 1.7351	Prec@(1,5) (53.7%, 82.7%)
11/22 01:37:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 1.7364	Prec@(1,5) (53.8%, 82.5%)
11/22 01:37:37午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 53.7480%
11/22 01:37:37午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 01:37:38午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.7480%
11/22 01:38:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.8303 (1.0106)	Arch Loss 25.7730 (25.9568)	Arch Hard Loss 1.7385 (1.7391)	Arch Beta Loss 24.0345 (24.2177)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.1%, 93.5%)	
11/22 01:39:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.9051 (1.0401)	Arch Loss 25.6467 (25.7884)	Arch Hard Loss 1.9759 (1.7544)	Arch Beta Loss 23.6708 (24.0340)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.1%)	
11/22 01:40:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.2986 (1.0492)	Arch Loss 25.5999 (25.6217)	Arch Hard Loss 2.2861 (1.7691)	Arch Beta Loss 23.3139 (23.8525)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.5%, 93.0%)	
11/22 01:41:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.1256 (1.0650)	Arch Loss 24.6612 (25.4678)	Arch Hard Loss 1.6627 (1.7764)	Arch Beta Loss 22.9986 (23.6914)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.1%, 92.9%)	
11/22 01:41:09午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 69.1160%
11/22 01:41:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 1.7797	Prec@(1,5) (52.9%, 82.0%)
11/22 01:41:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 1.7675	Prec@(1,5) (52.8%, 82.3%)
11/22 01:41:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 1.7509	Prec@(1,5) (53.2%, 82.6%)
11/22 01:41:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 1.7493	Prec@(1,5) (53.2%, 82.6%)
11/22 01:41:40午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 53.1840%
11/22 01:41:40午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 01:41:40午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.7480%
11/22 01:42:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 1.2722 (1.0013)	Arch Loss 24.4136 (24.5824)	Arch Hard Loss 1.7623 (1.7615)	Arch Beta Loss 22.6513 (22.8209)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.2%, 93.4%)	
11/22 01:43:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.0557 (1.0199)	Arch Loss 23.8574 (24.4194)	Arch Hard Loss 1.5447 (1.7691)	Arch Beta Loss 22.3127 (22.6503)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.5%, 93.4%)	
11/22 01:44:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.0968 (1.0357)	Arch Loss 23.8898 (24.2410)	Arch Hard Loss 1.9091 (1.7594)	Arch Beta Loss 21.9807 (22.4817)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.0%, 93.1%)	
11/22 01:45:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.4645 (1.0409)	Arch Loss 23.2702 (24.1009)	Arch Hard Loss 1.5826 (1.7692)	Arch Beta Loss 21.6876 (22.3318)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.1%)	
11/22 01:45:12午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 69.8000%
11/22 01:45:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 1.7069	Prec@(1,5) (54.3%, 82.9%)
11/22 01:45:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 1.7305	Prec@(1,5) (54.1%, 82.7%)
11/22 01:45:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 1.7363	Prec@(1,5) (53.9%, 82.5%)
11/22 01:45:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 1.7348	Prec@(1,5) (53.8%, 82.6%)
11/22 01:45:43午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 53.8520%
11/22 01:45:43午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 01:45:43午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.8520%
11/22 01:46:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.9809 (0.9789)	Arch Loss 23.3458 (23.3110)	Arch Hard Loss 1.9818 (1.7890)	Arch Beta Loss 21.3640 (21.5220)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.6%, 94.2%)	
11/22 01:47:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 1.0679 (1.0000)	Arch Loss 22.5196 (23.1340)	Arch Hard Loss 1.4695 (1.7706)	Arch Beta Loss 21.0501 (21.3634)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.7%, 93.8%)	
11/22 01:48:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 1.0311 (1.0099)	Arch Loss 22.5040 (22.9726)	Arch Hard Loss 1.7647 (1.7661)	Arch Beta Loss 20.7393 (21.2065)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.6%, 93.5%)	
11/22 01:49:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.8505 (1.0183)	Arch Loss 22.1023 (22.8311)	Arch Hard Loss 1.6375 (1.7645)	Arch Beta Loss 20.4648 (21.0666)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.2%, 93.4%)	
11/22 01:49:16午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 70.1880%
11/22 01:49:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 1.7240	Prec@(1,5) (53.9%, 82.8%)
11/22 01:49:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 1.7214	Prec@(1,5) (54.0%, 83.1%)
11/22 01:49:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 1.7358	Prec@(1,5) (53.6%, 82.6%)
11/22 01:49:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 1.7397	Prec@(1,5) (53.5%, 82.6%)
11/22 01:49:47午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 53.5560%
11/22 01:49:47午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 01:49:47午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.8520%
11/22 01:50:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.9361 (0.9400)	Arch Loss 22.1101 (22.0804)	Arch Hard Loss 1.9483 (1.7704)	Arch Beta Loss 20.1618 (20.3100)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.4%)	
11/22 01:51:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 1.0513 (0.9610)	Arch Loss 21.7663 (21.9091)	Arch Hard Loss 1.8997 (1.7479)	Arch Beta Loss 19.8666 (20.1612)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.6%, 94.2%)	
11/22 01:52:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.9408 (0.9784)	Arch Loss 21.1492 (21.7669)	Arch Hard Loss 1.5720 (1.7528)	Arch Beta Loss 19.5772 (20.0141)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.1%, 93.9%)	
11/22 01:53:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.8261 (0.9868)	Arch Loss 20.9155 (21.6415)	Arch Hard Loss 1.5933 (1.7581)	Arch Beta Loss 19.3222 (19.8835)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.0%, 93.7%)	
11/22 01:53:18午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 70.9920%
11/22 01:53:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 1.7219	Prec@(1,5) (53.9%, 82.7%)
11/22 01:53:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 1.7210	Prec@(1,5) (54.3%, 83.0%)
11/22 01:53:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 1.7116	Prec@(1,5) (54.4%, 83.0%)
11/22 01:53:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 1.7193	Prec@(1,5) (54.2%, 82.9%)
11/22 01:53:50午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 54.1480%
11/22 01:53:50午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/22 01:53:50午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.1480%
11/22 01:54:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.8469 (0.9462)	Arch Loss 20.6474 (20.9309)	Arch Hard Loss 1.6060 (1.7523)	Arch Beta Loss 19.0415 (19.1786)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.3%, 94.2%)	
11/22 01:55:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.6897 (0.9615)	Arch Loss 20.5129 (20.7683)	Arch Hard Loss 1.7444 (1.7274)	Arch Beta Loss 18.7685 (19.0409)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.9%, 94.0%)	
11/22 01:56:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 1.1120 (0.9632)	Arch Loss 20.4386 (20.6456)	Arch Hard Loss 1.9396 (1.7411)	Arch Beta Loss 18.4990 (18.9046)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.9%, 94.0%)	
11/22 01:57:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.8110 (0.9665)	Arch Loss 20.2633 (20.5397)	Arch Hard Loss 2.0009 (1.7564)	Arch Beta Loss 18.2624 (18.7833)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.8%, 94.0%)	
11/22 01:57:21午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 71.7960%
11/22 01:57:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 1.7212	Prec@(1,5) (54.6%, 82.9%)
11/22 01:57:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 1.7166	Prec@(1,5) (54.2%, 83.2%)
11/22 01:57:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 1.7164	Prec@(1,5) (54.1%, 83.2%)
11/22 01:57:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 1.7131	Prec@(1,5) (54.1%, 83.3%)
11/22 01:57:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 54.1240%
11/22 01:57:53午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 01:57:53午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.1480%
11/22 01:58:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.9815 (0.9197)	Arch Loss 19.4772 (19.8851)	Arch Hard Loss 1.4746 (1.7561)	Arch Beta Loss 18.0025 (18.1291)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.0%, 95.0%)	
11/22 01:59:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.6974 (0.9319)	Arch Loss 19.4466 (19.7432)	Arch Hard Loss 1.6959 (1.7412)	Arch Beta Loss 17.7507 (18.0019)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.0%, 94.6%)	
11/22 02:00:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.9844 (0.9468)	Arch Loss 19.2444 (19.6263)	Arch Hard Loss 1.7396 (1.7497)	Arch Beta Loss 17.5048 (17.8766)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.5%)	
11/22 02:01:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 1.0219 (0.9524)	Arch Loss 19.0436 (19.5156)	Arch Hard Loss 1.7544 (1.7501)	Arch Beta Loss 17.2892 (17.7655)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.4%)	
11/22 02:01:24午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 72.4160%
11/22 02:01:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 1.7245	Prec@(1,5) (54.6%, 82.9%)
11/22 02:01:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 1.7133	Prec@(1,5) (54.3%, 83.2%)
11/22 02:01:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 1.7140	Prec@(1,5) (54.4%, 83.1%)
11/22 02:01:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 1.7175	Prec@(1,5) (54.3%, 83.1%)
11/22 02:01:55午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 54.3160%
11/22 02:01:55午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('avg_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 02:01:56午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.3160%
11/22 02:02:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.8292 (0.9080)	Arch Loss 19.2226 (18.9325)	Arch Hard Loss 2.1702 (1.7645)	Arch Beta Loss 17.0524 (17.1680)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.0%, 94.7%)	
11/22 02:03:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.9950 (0.9164)	Arch Loss 18.8080 (18.8201)	Arch Hard Loss 1.9842 (1.7678)	Arch Beta Loss 16.8238 (17.0523)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.4%, 94.7%)	
11/22 02:04:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.6954 (0.9365)	Arch Loss 18.2837 (18.7003)	Arch Hard Loss 1.6825 (1.7617)	Arch Beta Loss 16.6012 (16.9386)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.7%, 94.5%)	
11/22 02:05:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 1.1951 (0.9391)	Arch Loss 18.0145 (18.5876)	Arch Hard Loss 1.6091 (1.7498)	Arch Beta Loss 16.4054 (16.8377)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.5%)	
11/22 02:05:27午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 72.6440%
11/22 02:05:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 1.6838	Prec@(1,5) (54.5%, 83.8%)
11/22 02:05:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 1.6963	Prec@(1,5) (54.2%, 83.4%)
11/22 02:05:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 1.6996	Prec@(1,5) (54.1%, 83.4%)
11/22 02:05:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 1.7025	Prec@(1,5) (54.2%, 83.5%)
11/22 02:05:58午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 54.1800%
11/22 02:05:58午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('avg_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 02:05:58午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.3160%
11/22 02:06:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.8356 (0.8948)	Arch Loss 18.0973 (18.0519)	Arch Hard Loss 1.9065 (1.7565)	Arch Beta Loss 16.1908 (16.2954)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.1%, 95.3%)	
11/22 02:07:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.8209 (0.9014)	Arch Loss 17.6011 (17.9385)	Arch Hard Loss 1.6172 (1.7478)	Arch Beta Loss 15.9840 (16.1907)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.1%, 95.0%)	
11/22 02:08:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.6943 (0.9028)	Arch Loss 17.4105 (17.8372)	Arch Hard Loss 1.6276 (1.7494)	Arch Beta Loss 15.7829 (16.0878)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 94.9%)	
11/22 02:09:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 1.0996 (0.9124)	Arch Loss 17.1894 (17.7438)	Arch Hard Loss 1.5833 (1.7470)	Arch Beta Loss 15.6061 (15.9967)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.6%, 94.8%)	
11/22 02:09:30午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 73.6480%
11/22 02:09:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 1.7261	Prec@(1,5) (54.0%, 83.2%)
11/22 02:09:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 1.7203	Prec@(1,5) (54.0%, 83.2%)
11/22 02:09:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 1.7075	Prec@(1,5) (54.5%, 83.4%)
11/22 02:10:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 1.7078	Prec@(1,5) (54.6%, 83.3%)
11/22 02:10:01午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 54.6280%
11/22 02:10:01午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 02:10:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.6280%
11/22 02:10:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.8517 (0.8720)	Arch Loss 17.0467 (17.2772)	Arch Hard Loss 1.6344 (1.7702)	Arch Beta Loss 15.4123 (15.5070)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.4%, 95.3%)	
11/22 02:11:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.9700 (0.8894)	Arch Loss 17.1174 (17.1451)	Arch Hard Loss 1.8919 (1.7328)	Arch Beta Loss 15.2255 (15.4123)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 94.9%)	
11/22 02:12:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.9429 (0.8988)	Arch Loss 17.1885 (17.0520)	Arch Hard Loss 2.1467 (1.7330)	Arch Beta Loss 15.0418 (15.3190)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 94.9%)	
11/22 02:13:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.8058 (0.9026)	Arch Loss 16.9401 (16.9748)	Arch Hard Loss 2.0585 (1.7386)	Arch Beta Loss 14.8816 (15.2363)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.8%, 94.9%)	
11/22 02:13:34午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 73.8040%
11/22 02:13:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 1.6342	Prec@(1,5) (56.0%, 84.5%)
11/22 02:13:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 1.6779	Prec@(1,5) (54.9%, 83.6%)
11/22 02:13:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 1.6775	Prec@(1,5) (55.2%, 83.6%)
11/22 02:14:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 1.6843	Prec@(1,5) (55.0%, 83.4%)
11/22 02:14:05午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 54.9960%
11/22 02:14:05午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 02:14:05午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.9960%
11/22 02:15:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.8680 (0.8617)	Arch Loss 16.3971 (16.5244)	Arch Hard Loss 1.6919 (1.7331)	Arch Beta Loss 14.7052 (14.7913)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.5%, 95.1%)	
11/22 02:15:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.9424 (0.8878)	Arch Loss 16.2427 (16.4372)	Arch Hard Loss 1.7091 (1.7325)	Arch Beta Loss 14.5336 (14.7047)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.0%)	
11/22 02:16:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.9637 (0.8887)	Arch Loss 16.5604 (16.3538)	Arch Hard Loss 2.1945 (1.7345)	Arch Beta Loss 14.3659 (14.6193)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.0%)	
11/22 02:17:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.7106 (0.8936)	Arch Loss 15.8667 (16.2787)	Arch Hard Loss 1.6486 (1.7351)	Arch Beta Loss 14.2181 (14.5436)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.2%, 94.9%)	
11/22 02:17:37午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 74.1360%
11/22 02:17:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 1.6631	Prec@(1,5) (55.6%, 83.8%)
11/22 02:17:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 1.6653	Prec@(1,5) (55.6%, 83.9%)
11/22 02:18:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 1.6644	Prec@(1,5) (55.4%, 83.9%)
11/22 02:18:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 1.6756	Prec@(1,5) (55.4%, 83.6%)
11/22 02:18:08午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 55.4480%
11/22 02:18:08午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 02:18:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:19:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 1.0387 (0.8735)	Arch Loss 15.7463 (15.8796)	Arch Hard Loss 1.6914 (1.7449)	Arch Beta Loss 14.0549 (14.1347)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.1%)	
11/22 02:19:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 1.0815 (0.8755)	Arch Loss 16.1082 (15.7948)	Arch Hard Loss 2.2114 (1.7401)	Arch Beta Loss 13.8967 (14.0547)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.1%)	
11/22 02:20:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.6873 (0.8853)	Arch Loss 15.1832 (15.7208)	Arch Hard Loss 1.4416 (1.7449)	Arch Beta Loss 13.7415 (13.9759)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.3%, 95.0%)	
11/22 02:21:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.7707 (0.8888)	Arch Loss 15.3462 (15.6452)	Arch Hard Loss 1.7418 (1.7394)	Arch Beta Loss 13.6044 (13.9058)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.2%, 95.0%)	
11/22 02:21:43午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 74.2120%
11/22 02:21:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 1.6597	Prec@(1,5) (55.4%, 83.9%)
11/22 02:21:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 1.6764	Prec@(1,5) (55.3%, 83.7%)
11/22 02:22:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 1.6809	Prec@(1,5) (55.2%, 83.6%)
11/22 02:22:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 1.6863	Prec@(1,5) (55.0%, 83.5%)
11/22 02:22:14午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 55.0480%
11/22 02:22:14午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 02:22:14午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:23:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.6616 (0.8751)	Arch Loss 14.8011 (15.2516)	Arch Hard Loss 1.3486 (1.7248)	Arch Beta Loss 13.4525 (13.5269)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.2%, 95.6%)	
11/22 02:24:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 1.1975 (0.8673)	Arch Loss 15.3527 (15.1860)	Arch Hard Loss 2.0484 (1.7337)	Arch Beta Loss 13.3043 (13.4523)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.7%, 95.6%)	
11/22 02:24:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.9152 (0.8729)	Arch Loss 15.3795 (15.1116)	Arch Hard Loss 2.2207 (1.7333)	Arch Beta Loss 13.1588 (13.3784)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.5%)	
11/22 02:25:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.9220 (0.8784)	Arch Loss 14.8183 (15.0478)	Arch Hard Loss 1.7887 (1.7352)	Arch Beta Loss 13.0296 (13.3126)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.4%)	
11/22 02:25:46午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 74.5920%
11/22 02:25:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 1.6718	Prec@(1,5) (56.1%, 83.1%)
11/22 02:26:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 1.6866	Prec@(1,5) (55.5%, 83.4%)
11/22 02:26:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 1.6930	Prec@(1,5) (55.1%, 83.3%)
11/22 02:26:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 1.6880	Prec@(1,5) (55.2%, 83.3%)
11/22 02:26:17午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 55.2240%
11/22 02:26:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 02:26:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:27:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 1.0363 (0.8410)	Arch Loss 14.3942 (14.6919)	Arch Hard Loss 1.5079 (1.7356)	Arch Beta Loss 12.8863 (12.9563)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.8%, 95.7%)	
11/22 02:28:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.7401 (0.8693)	Arch Loss 14.5006 (14.6231)	Arch Hard Loss 1.7534 (1.7370)	Arch Beta Loss 12.7472 (12.8861)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.4%)	
11/22 02:28:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.8272 (0.8681)	Arch Loss 14.2461 (14.5516)	Arch Hard Loss 1.6362 (1.7349)	Arch Beta Loss 12.6099 (12.8167)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.5%)	
11/22 02:29:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.7665 (0.8787)	Arch Loss 14.5288 (14.4929)	Arch Hard Loss 2.0411 (1.7382)	Arch Beta Loss 12.4876 (12.7547)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.4%)	
11/22 02:29:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 74.6520%
11/22 02:29:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 1.6961	Prec@(1,5) (55.1%, 82.9%)
11/22 02:30:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 1.6996	Prec@(1,5) (55.2%, 83.0%)
11/22 02:30:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 1.6887	Prec@(1,5) (55.1%, 83.4%)
11/22 02:30:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 1.6884	Prec@(1,5) (55.1%, 83.4%)
11/22 02:30:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 55.0480%
11/22 02:30:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('avg_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 02:30:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:31:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.9355 (0.8689)	Arch Loss 13.6265 (14.1275)	Arch Hard Loss 1.2731 (1.7088)	Arch Beta Loss 12.3535 (12.4187)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.5%, 95.4%)	
11/22 02:32:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.7994 (0.8740)	Arch Loss 13.8903 (14.0952)	Arch Hard Loss 1.6687 (1.7424)	Arch Beta Loss 12.2216 (12.3527)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.3%)	
11/22 02:33:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.7565 (0.8863)	Arch Loss 13.7933 (14.0198)	Arch Hard Loss 1.7011 (1.7326)	Arch Beta Loss 12.0922 (12.2872)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.5%, 95.0%)	
11/22 02:33:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.9848 (0.8921)	Arch Loss 13.7411 (13.9630)	Arch Hard Loss 1.7645 (1.7343)	Arch Beta Loss 11.9766 (12.2287)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.3%, 95.1%)	
11/22 02:33:52午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 74.3320%
11/22 02:34:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 1.6947	Prec@(1,5) (55.0%, 82.7%)
11/22 02:34:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 1.6811	Prec@(1,5) (55.1%, 83.1%)
11/22 02:34:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 1.6838	Prec@(1,5) (55.0%, 83.3%)
11/22 02:34:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 1.6877	Prec@(1,5) (55.0%, 83.3%)
11/22 02:34:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 55.0000%
11/22 02:34:23午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 02:34:23午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:35:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.9508 (0.8866)	Arch Loss 13.4185 (13.6272)	Arch Hard Loss 1.5702 (1.7163)	Arch Beta Loss 11.8483 (11.9110)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.7%, 95.0%)	
11/22 02:36:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 1.0160 (0.8882)	Arch Loss 13.8010 (13.5939)	Arch Hard Loss 2.0767 (1.7458)	Arch Beta Loss 11.7243 (11.8481)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.9%, 95.3%)	
11/22 02:37:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 1.0643 (0.8977)	Arch Loss 13.4427 (13.5308)	Arch Hard Loss 1.8403 (1.7445)	Arch Beta Loss 11.6025 (11.7863)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.1%)	
11/22 02:37:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.8778 (0.9048)	Arch Loss 13.2659 (13.4759)	Arch Hard Loss 1.7723 (1.7449)	Arch Beta Loss 11.4936 (11.7311)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.2%, 95.0%)	
11/22 02:37:54午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 74.2080%
11/22 02:38:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 1.7097	Prec@(1,5) (54.7%, 83.2%)
11/22 02:38:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 1.6903	Prec@(1,5) (54.8%, 83.2%)
11/22 02:38:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 1.6910	Prec@(1,5) (54.7%, 83.3%)
11/22 02:38:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 1.6945	Prec@(1,5) (54.6%, 83.2%)
11/22 02:38:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 54.5960%
11/22 02:38:25午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 02:38:25午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:39:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.8215 (0.8899)	Arch Loss 12.9486 (13.1650)	Arch Hard Loss 1.5759 (1.7331)	Arch Beta Loss 11.3728 (11.4319)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.5%, 95.1%)	
11/22 02:40:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.7228 (0.8938)	Arch Loss 12.9523 (13.1153)	Arch Hard Loss 1.6972 (1.7428)	Arch Beta Loss 11.2550 (11.3725)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.0%)	
11/22 02:41:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.6736 (0.9013)	Arch Loss 12.9575 (13.0583)	Arch Hard Loss 1.8196 (1.7447)	Arch Beta Loss 11.1379 (11.3136)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.9%, 94.9%)	
11/22 02:41:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 1.0057 (0.9113)	Arch Loss 12.3230 (13.0067)	Arch Hard Loss 1.2892 (1.7458)	Arch Beta Loss 11.0338 (11.2609)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 94.8%)	
11/22 02:41:57午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 74.3960%
11/22 02:42:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 1.6569	Prec@(1,5) (54.9%, 83.5%)
11/22 02:42:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 1.6889	Prec@(1,5) (54.7%, 83.3%)
11/22 02:42:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 1.7037	Prec@(1,5) (54.5%, 82.9%)
11/22 02:42:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 1.6916	Prec@(1,5) (54.7%, 83.1%)
11/22 02:42:28午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 54.7400%
11/22 02:42:28午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 02:42:28午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.4480%
11/22 02:42:29午前 trainer_runner.py:110 [INFO] Final best Prec@1 = 55.4480%
11/22 02:42:29午前 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
