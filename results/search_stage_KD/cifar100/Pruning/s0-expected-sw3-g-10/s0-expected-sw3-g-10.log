11/22 08:34:24PM parser.py:28 [INFO] 
11/22 08:34:24PM parser.py:29 [INFO] Parameters:
11/22 08:34:24PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-10/DAG
11/22 08:34:24PM parser.py:31 [INFO] T=10.0
11/22 08:34:24PM parser.py:31 [INFO] ADVANCED=1
11/22 08:34:24PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/22 08:34:24PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/22 08:34:24PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/22 08:34:24PM parser.py:31 [INFO] BATCH_SIZE=64
11/22 08:34:24PM parser.py:31 [INFO] CASCADE=0
11/22 08:34:24PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/22 08:34:24PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/22 08:34:24PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/22 08:34:24PM parser.py:31 [INFO] DATA_PATH=../data/
11/22 08:34:24PM parser.py:31 [INFO] DATASET=cifar100
11/22 08:34:24PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/22 08:34:24PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/22 08:34:24PM parser.py:31 [INFO] DISCRETE=0
11/22 08:34:24PM parser.py:31 [INFO] EPOCHS=50
11/22 08:34:24PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/22 08:34:24PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g-10
11/22 08:34:24PM parser.py:31 [INFO] FINAL_L=0.0
11/22 08:34:24PM parser.py:31 [INFO] G=-10.0
11/22 08:34:24PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/22 08:34:24PM parser.py:31 [INFO] GPUS=[0]
11/22 08:34:24PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/22 08:34:24PM parser.py:31 [INFO] INIT_CHANNELS=16
11/22 08:34:24PM parser.py:31 [INFO] L=0.0
11/22 08:34:24PM parser.py:31 [INFO] LAYERS=32
11/22 08:34:24PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/22 08:34:24PM parser.py:31 [INFO] NAME=Pruning
11/22 08:34:24PM parser.py:31 [INFO] NONKD=1
11/22 08:34:24PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-10
11/22 08:34:24PM parser.py:31 [INFO] PCDARTS=0
11/22 08:34:24PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-10/plots
11/22 08:34:24PM parser.py:31 [INFO] PRINT_FREQ=100
11/22 08:34:24PM parser.py:31 [INFO] RESET=0
11/22 08:34:24PM parser.py:31 [INFO] RESUME_PATH=None
11/22 08:34:24PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g-10
11/22 08:34:24PM parser.py:31 [INFO] SEED=0
11/22 08:34:24PM parser.py:31 [INFO] SHARE_STAGE=0
11/22 08:34:24PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/22 08:34:24PM parser.py:31 [INFO] SPEC_CELL=1
11/22 08:34:24PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/22 08:34:24PM parser.py:31 [INFO] TEACHER_NAME=none
11/22 08:34:24PM parser.py:31 [INFO] TEACHER_PATH=none
11/22 08:34:24PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/22 08:34:24PM parser.py:31 [INFO] TYPE=Pruning
11/22 08:34:24PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/22 08:34:24PM parser.py:31 [INFO] W_LR=0.025
11/22 08:34:24PM parser.py:31 [INFO] W_LR_MIN=0.001
11/22 08:34:24PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/22 08:34:24PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/22 08:34:24PM parser.py:31 [INFO] WORKERS=4
11/22 08:34:24PM parser.py:32 [INFO] 
11/22 08:34:26PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/22 08:35:21PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.5470 (4.5541)	Arch Loss -3683.0029 (-3640.0929)	Arch Hard Loss 4.6823 (4.5617)	Arch Beta Loss 368.7685 (364.4655)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.0%, 8.9%)	
11/22 08:36:14PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0718 (4.4213)	Arch Loss -3771.5701 (-3683.9359)	Arch Hard Loss 4.2257 (4.4239)	Arch Beta Loss 377.5796 (368.8360)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.9%, 12.9%)	
11/22 08:37:07PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0717 (4.3181)	Arch Loss -3861.2397 (-3728.2112)	Arch Hard Loss 4.0740 (4.3228)	Arch Beta Loss 386.5314 (373.2534)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.0%, 16.0%)	
11/22 08:37:55PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.9118 (4.2511)	Arch Loss -3943.3052 (-3768.4489)	Arch Hard Loss 3.7924 (4.2466)	Arch Beta Loss 394.7098 (377.2695)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.9%, 18.2%)	
11/22 08:37:57PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 4.8680%
11/22 08:38:05PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9333	Prec@(1,5) (9.1%, 28.1%)
11/22 08:38:13PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9269	Prec@(1,5) (9.2%, 28.7%)
11/22 08:38:21PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9264	Prec@(1,5) (9.0%, 28.6%)
11/22 08:38:28PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9283	Prec@(1,5) (9.0%, 28.5%)
11/22 08:38:28PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 9.0080%
11/22 08:38:28PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[5, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG3_concat=[4, 11])
11/22 08:38:28PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 9.0080%
11/22 08:39:23午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 4.0470 (3.9219)	Arch Loss -4036.0684 (-3990.5093)	Arch Hard Loss 4.0266 (3.9090)	Arch Beta Loss 404.0095 (399.4418)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.9%, 29.1%)	
11/22 08:40:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.8939 (3.8808)	Arch Loss -4129.2603 (-4036.8630)	Arch Hard Loss 3.9508 (3.8672)	Arch Beta Loss 413.3211 (404.0730)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.1%, 30.4%)	
11/22 08:41:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.6688 (3.8447)	Arch Loss -4223.6191 (-4083.5213)	Arch Hard Loss 3.5262 (3.8280)	Arch Beta Loss 422.7146 (408.7349)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.8%, 31.3%)	
11/22 08:41:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.6889 (3.8012)	Arch Loss -4308.2627 (-4125.7372)	Arch Hard Loss 3.9787 (3.7916)	Arch Beta Loss 431.2241 (412.9529)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.4%, 32.7%)	
11/22 08:41:59午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 10.4560%
11/22 08:42:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6199	Prec@(1,5) (13.3%, 38.2%)
11/22 08:42:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6111	Prec@(1,5) (13.3%, 38.1%)
11/22 08:42:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6175	Prec@(1,5) (13.1%, 38.0%)
11/22 08:42:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6143	Prec@(1,5) (13.1%, 38.0%)
11/22 08:42:30午後 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 13.0800%
11/22 08:42:30午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 08:42:31午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 13.0800%
11/22 08:43:25午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.6843 (3.6199)	Arch Loss -4404.6484 (-4357.5463)	Arch Hard Loss 3.5905 (3.6038)	Arch Beta Loss 440.8239 (436.1150)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.2%, 37.5%)	
11/22 08:44:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.6235 (3.5747)	Arch Loss -4500.1450 (-4405.2073)	Arch Hard Loss 3.5924 (3.5828)	Arch Beta Loss 450.3737 (440.8790)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.1%, 39.3%)	
11/22 08:45:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.4221 (3.5417)	Arch Loss -4596.1162 (-4453.0262)	Arch Hard Loss 3.5591 (3.5498)	Arch Beta Loss 459.9675 (445.6576)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.7%, 40.2%)	
11/22 08:46:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.3719 (3.5178)	Arch Loss -4683.1865 (-4496.1910)	Arch Hard Loss 3.2631 (3.5227)	Arch Beta Loss 468.6450 (449.9714)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.2%, 41.0%)	
11/22 08:46:02午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 15.2040%
11/22 08:46:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.3928	Prec@(1,5) (17.4%, 45.3%)
11/22 08:46:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4053	Prec@(1,5) (17.2%, 44.5%)
11/22 08:46:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4013	Prec@(1,5) (17.3%, 44.7%)
11/22 08:46:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4046	Prec@(1,5) (17.4%, 44.5%)
11/22 08:46:33午後 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 17.3520%
11/22 08:46:33午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/22 08:46:33午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 17.3520%
11/22 08:47:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.6804 (3.3829)	Arch Loss -4781.1182 (-4732.9760)	Arch Hard Loss 3.3213 (3.3776)	Arch Beta Loss 478.4439 (473.6354)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.4%, 44.9%)	
11/22 08:48:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.0767 (3.3552)	Arch Loss -4878.7808 (-4781.6972)	Arch Hard Loss 3.4776 (3.3618)	Arch Beta Loss 488.2258 (478.5059)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.9%, 45.9%)	
11/22 08:49:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1261 (3.3201)	Arch Loss -4977.6777 (-4830.7212)	Arch Hard Loss 3.3488 (3.3355)	Arch Beta Loss 498.1027 (483.4057)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.5%, 47.0%)	
11/22 08:50:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 2.9835 (3.2987)	Arch Loss -5067.5430 (-4875.1119)	Arch Hard Loss 3.2632 (3.3187)	Arch Beta Loss 507.0806 (487.8431)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (19.0%, 47.5%)	
11/22 08:50:04午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 19.0120%
11/22 08:50:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2625	Prec@(1,5) (20.1%, 48.7%)
11/22 08:50:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2591	Prec@(1,5) (20.1%, 48.8%)
11/22 08:50:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2672	Prec@(1,5) (19.8%, 48.5%)
11/22 08:50:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2727	Prec@(1,5) (19.6%, 48.4%)
11/22 08:50:35午後 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 19.6400%
11/22 08:50:35午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/22 08:50:36午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 19.6400%
11/22 08:51:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 2.9708 (3.1229)	Arch Loss -5169.6289 (-5119.3779)	Arch Hard Loss 2.9348 (3.2268)	Arch Beta Loss 517.2563 (512.2605)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.5%, 52.1%)	
11/22 08:52:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0718 (3.1140)	Arch Loss -5271.3052 (-5170.0427)	Arch Hard Loss 2.9721 (3.1868)	Arch Beta Loss 527.4277 (517.3230)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.5%, 52.2%)	
11/22 08:53:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.7584 (3.1028)	Arch Loss -5373.5752 (-5220.9874)	Arch Hard Loss 3.2248 (3.1628)	Arch Beta Loss 537.6800 (522.4150)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.7%, 52.3%)	
11/22 08:54:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.9091 (3.0958)	Arch Loss -5466.6348 (-5267.0587)	Arch Hard Loss 2.9670 (3.1372)	Arch Beta Loss 546.9601 (527.0196)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.8%, 52.4%)	
11/22 08:54:08午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 22.8560%
11/22 08:54:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.0822	Prec@(1,5) (23.0%, 53.3%)
11/22 08:54:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0464	Prec@(1,5) (23.7%, 54.2%)
11/22 08:54:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0389	Prec@(1,5) (23.9%, 54.5%)
11/22 08:54:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0401	Prec@(1,5) (24.0%, 54.6%)
11/22 08:54:38午後 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 24.0480%
11/22 08:54:39午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/22 08:54:39午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 24.0480%
11/22 08:55:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.0975 (2.9470)	Arch Loss -5571.0898 (-5519.8683)	Arch Hard Loss 3.0484 (3.0109)	Arch Beta Loss 557.4138 (552.2879)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.0%, 56.6%)	
11/22 08:56:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.3092 (2.9354)	Arch Loss -5674.4233 (-5571.6642)	Arch Hard Loss 3.4009 (3.0225)	Arch Beta Loss 567.7824 (557.4687)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.7%, 56.8%)	
11/22 08:57:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.7281 (2.9328)	Arch Loss -5778.6860 (-5623.5136)	Arch Hard Loss 2.7699 (3.0017)	Arch Beta Loss 578.1456 (562.6515)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.6%, 56.5%)	
11/22 08:58:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.5200 (2.9193)	Arch Loss -5871.6763 (-5670.1514)	Arch Hard Loss 2.8083 (2.9789)	Arch Beta Loss 587.4484 (567.3130)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (26.0%, 57.2%)	
11/22 08:58:10午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 25.9520%
11/22 08:58:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.8999	Prec@(1,5) (26.3%, 58.2%)
11/22 08:58:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9067	Prec@(1,5) (26.6%, 57.8%)
11/22 08:58:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9091	Prec@(1,5) (26.4%, 57.8%)
11/22 08:58:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9025	Prec@(1,5) (26.5%, 57.8%)
11/22 08:58:41午後 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 26.5280%
11/22 08:58:41午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 08:58:42午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 26.5280%
11/22 08:59:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.8600 (2.7862)	Arch Loss -5975.4214 (-5924.6251)	Arch Hard Loss 2.9767 (2.8964)	Arch Beta Loss 597.8398 (592.7521)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.6%, 60.5%)	
11/22 09:00:30午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.9139 (2.7903)	Arch Loss -6077.6094 (-5975.9125)	Arch Hard Loss 2.9619 (2.8796)	Arch Beta Loss 608.0571 (597.8792)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.6%, 60.3%)	
11/22 09:01:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.0226 (2.7795)	Arch Loss -6179.2314 (-6026.9451)	Arch Hard Loss 2.6101 (2.8481)	Arch Beta Loss 618.1842 (602.9793)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.8%, 60.6%)	
11/22 09:02:12午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.8605 (2.7780)	Arch Loss -6269.2534 (-6072.5751)	Arch Hard Loss 2.8158 (2.8507)	Arch Beta Loss 627.2069 (607.5426)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.0%, 60.7%)	
11/22 09:02:13午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 28.9880%
11/22 09:02:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.8283	Prec@(1,5) (28.9%, 59.5%)
11/22 09:02:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.8082	Prec@(1,5) (29.2%, 60.3%)
11/22 09:02:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.8150	Prec@(1,5) (29.1%, 60.1%)
11/22 09:02:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.8251	Prec@(1,5) (28.9%, 59.9%)
11/22 09:02:44午後 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 28.8880%
11/22 09:02:44午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 09:02:44午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 28.8880%
11/22 09:03:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.7944 (2.6232)	Arch Loss -6369.3804 (-6320.4207)	Arch Hard Loss 2.7614 (2.7872)	Arch Beta Loss 637.2142 (632.3208)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.9%, 64.3%)	
11/22 09:04:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.5750 (2.6390)	Arch Loss -6467.1929 (-6369.6372)	Arch Hard Loss 2.6497 (2.7672)	Arch Beta Loss 646.9843 (637.2404)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.2%, 64.1%)	
11/22 09:05:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.9624 (2.6316)	Arch Loss -6562.8467 (-6418.3570)	Arch Hard Loss 3.1826 (2.7623)	Arch Beta Loss 656.6029 (642.1119)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.7%, 64.3%)	
11/22 09:06:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.5850 (2.6242)	Arch Loss -6648.3853 (-6461.7829)	Arch Hard Loss 2.8107 (2.7398)	Arch Beta Loss 665.1196 (646.4523)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.8%, 64.4%)	
11/22 09:06:15午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 31.7600%
11/22 09:06:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6877	Prec@(1,5) (30.9%, 62.7%)
11/22 09:06:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.6702	Prec@(1,5) (31.0%, 63.8%)
11/22 09:06:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6654	Prec@(1,5) (31.2%, 63.9%)
11/22 09:06:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6804	Prec@(1,5) (31.0%, 63.6%)
11/22 09:06:46午後 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 30.9760%
11/22 09:06:46午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 09:06:47午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 30.9760%
11/22 09:07:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.2807 (2.5066)	Arch Loss -6742.2793 (-6696.5697)	Arch Hard Loss 2.8045 (2.6535)	Arch Beta Loss 674.5084 (669.9223)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.4%, 67.4%)	
11/22 09:08:35午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 1.9823 (2.5063)	Arch Loss -6833.0981 (-6742.5757)	Arch Hard Loss 3.0954 (2.6612)	Arch Beta Loss 683.6193 (674.5237)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.8%, 67.4%)	
11/22 09:09:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.5537 (2.5104)	Arch Loss -6922.7598 (-6787.9692)	Arch Hard Loss 2.6387 (2.6563)	Arch Beta Loss 692.5399 (679.0626)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.9%, 67.3%)	
11/22 09:10:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.1128 (2.5078)	Arch Loss -7001.3984 (-6828.2920)	Arch Hard Loss 2.6200 (2.6379)	Arch Beta Loss 700.4019 (683.0930)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.0%, 67.4%)	
11/22 09:10:18午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 34.0360%
11/22 09:10:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5381	Prec@(1,5) (33.7%, 67.4%)
11/22 09:10:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.5534	Prec@(1,5) (33.4%, 66.9%)
11/22 09:10:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.5633	Prec@(1,5) (33.3%, 66.4%)
11/22 09:10:49午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.5678	Prec@(1,5) (33.1%, 66.2%)
11/22 09:10:49午後 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 33.1360%
11/22 09:10:49午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 09:10:50午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.1360%
11/22 09:11:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.5618 (2.4037)	Arch Loss -7087.5527 (-7045.6068)	Arch Hard Loss 2.7987 (2.6017)	Arch Beta Loss 709.0352 (704.8208)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.0%, 69.7%)	
11/22 09:12:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.4827 (2.4152)	Arch Loss -7171.2778 (-7087.8772)	Arch Hard Loss 2.5773 (2.5688)	Arch Beta Loss 717.3855 (709.0446)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.5%, 69.1%)	
11/22 09:13:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.2390 (2.4042)	Arch Loss -7253.0176 (-7129.4640)	Arch Hard Loss 2.3898 (2.5668)	Arch Beta Loss 725.5407 (713.2031)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.5%, 69.2%)	
11/22 09:14:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.2256 (2.4011)	Arch Loss -7324.5586 (-7166.3475)	Arch Hard Loss 2.6024 (2.5559)	Arch Beta Loss 732.7161 (716.8903)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.4%, 69.4%)	
11/22 09:14:21午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 36.3920%
11/22 09:14:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.5214	Prec@(1,5) (34.7%, 67.4%)
11/22 09:14:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.5143	Prec@(1,5) (34.6%, 67.4%)
11/22 09:14:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.5237	Prec@(1,5) (34.5%, 67.0%)
11/22 09:14:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.5256	Prec@(1,5) (34.6%, 67.1%)
11/22 09:14:52午後 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 34.5760%
11/22 09:14:52午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 09:14:53午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 34.5760%
11/22 09:15:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.4559 (2.2752)	Arch Loss -7403.3804 (-7364.9165)	Arch Hard Loss 2.4780 (2.5330)	Arch Beta Loss 740.5858 (736.7449)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.9%, 72.5%)	
11/22 09:16:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.4809 (2.2986)	Arch Loss -7479.6553 (-7403.4149)	Arch Hard Loss 2.2518 (2.5183)	Arch Beta Loss 748.1907 (740.5933)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.7%, 72.0%)	
11/22 09:17:35午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.2236 (2.3060)	Arch Loss -7553.6924 (-7441.3047)	Arch Hard Loss 2.4429 (2.4986)	Arch Beta Loss 755.6135 (744.3803)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.6%, 71.7%)	
11/22 09:18:23午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.3575 (2.2997)	Arch Loss -7619.0791 (-7474.8795)	Arch Hard Loss 2.3292 (2.4892)	Arch Beta Loss 762.1408 (747.7369)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.8%, 71.8%)	
11/22 09:18:24午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 38.7760%
11/22 09:18:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.4735	Prec@(1,5) (35.5%, 68.1%)
11/22 09:18:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.4481	Prec@(1,5) (36.1%, 68.5%)
11/22 09:18:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.4566	Prec@(1,5) (36.2%, 68.2%)
11/22 09:18:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4455	Prec@(1,5) (36.3%, 68.4%)
11/22 09:18:55午後 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 36.3040%
11/22 09:18:55午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:18:55午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 36.3040%
11/22 09:19:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.0229 (2.2253)	Arch Loss -7690.1499 (-7655.5929)	Arch Hard Loss 2.8138 (2.4506)	Arch Beta Loss 769.2964 (765.8044)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.8%, 73.3%)	
11/22 09:20:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.4536 (2.2098)	Arch Loss -7759.5054 (-7690.5944)	Arch Hard Loss 2.5769 (2.4328)	Arch Beta Loss 776.2083 (769.3027)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.4%, 73.5%)	
11/22 09:21:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.1805 (2.2070)	Arch Loss -7827.1328 (-7725.0218)	Arch Hard Loss 2.3913 (2.4224)	Arch Beta Loss 782.9524 (772.7444)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.5%, 73.6%)	
11/22 09:22:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.0178 (2.2155)	Arch Loss -7886.5625 (-7755.5235)	Arch Hard Loss 2.2658 (2.4214)	Arch Beta Loss 788.8828 (775.7945)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.2%, 73.4%)	
11/22 09:22:26午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 40.2360%
11/22 09:22:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.3854	Prec@(1,5) (37.6%, 70.4%)
11/22 09:22:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.3665	Prec@(1,5) (38.0%, 70.7%)
11/22 09:22:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.3616	Prec@(1,5) (38.1%, 70.9%)
11/22 09:22:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.3628	Prec@(1,5) (38.2%, 70.8%)
11/22 09:22:57午後 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 38.2200%
11/22 09:22:57午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:22:58午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 38.2200%
11/22 09:23:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.0711 (2.1026)	Arch Loss -7951.5542 (-7919.7232)	Arch Hard Loss 2.3091 (2.3988)	Arch Beta Loss 795.3864 (792.2122)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.4%, 75.8%)	
11/22 09:24:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.1987 (2.1186)	Arch Loss -8014.5635 (-7951.5413)	Arch Hard Loss 2.1797 (2.3896)	Arch Beta Loss 801.6743 (795.3931)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.4%, 75.6%)	
11/22 09:25:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.1733 (2.1287)	Arch Loss -8075.8110 (-7982.8779)	Arch Hard Loss 2.3811 (2.3729)	Arch Beta Loss 807.8192 (798.5251)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.4%, 75.3%)	
11/22 09:26:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.9134 (2.1340)	Arch Loss -8129.9658 (-8010.6742)	Arch Hard Loss 2.3846 (2.3634)	Arch Beta Loss 813.2350 (801.3038)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.2%, 75.1%)	
11/22 09:26:29午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 42.2120%
11/22 09:26:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.2965	Prec@(1,5) (39.5%, 72.2%)
11/22 09:26:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.2994	Prec@(1,5) (39.0%, 72.1%)
11/22 09:26:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.3000	Prec@(1,5) (38.9%, 72.1%)
11/22 09:27:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3103	Prec@(1,5) (38.7%, 71.8%)
11/22 09:27:00午後 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 38.7040%
11/22 09:27:00午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:27:00午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 38.7040%
11/22 09:27:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.0545 (2.0174)	Arch Loss -8189.5557 (-8160.5229)	Arch Hard Loss 2.3710 (2.3082)	Arch Beta Loss 819.1927 (816.2831)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.8%, 78.1%)	
11/22 09:28:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.7416 (2.0654)	Arch Loss -8247.4082 (-8189.7245)	Arch Hard Loss 2.3569 (2.3037)	Arch Beta Loss 824.9764 (819.2028)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.4%, 76.6%)	
11/22 09:29:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.9400 (2.0624)	Arch Loss -8304.4307 (-8218.5617)	Arch Hard Loss 2.1516 (2.3032)	Arch Beta Loss 830.6582 (822.0865)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.7%, 76.6%)	
11/22 09:30:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.9757 (2.0633)	Arch Loss -8354.7529 (-8244.2381)	Arch Hard Loss 2.1965 (2.2969)	Arch Beta Loss 835.6949 (824.6535)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.7%, 76.4%)	
11/22 09:30:32午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 43.6640%
11/22 09:30:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2684	Prec@(1,5) (40.9%, 72.4%)
11/22 09:30:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2757	Prec@(1,5) (40.3%, 72.2%)
11/22 09:30:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2604	Prec@(1,5) (40.5%, 72.3%)
11/22 09:31:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.2644	Prec@(1,5) (40.3%, 72.5%)
11/22 09:31:03午後 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 40.3120%
11/22 09:31:03午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:31:03午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.3120%
11/22 09:31:58午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.0011 (1.9648)	Arch Loss -8410.4678 (-8383.1698)	Arch Hard Loss 2.2656 (2.2837)	Arch Beta Loss 841.2733 (838.5454)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.2%, 78.7%)	
11/22 09:32:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.2167 (2.0014)	Arch Loss -8465.1406 (-8410.6245)	Arch Hard Loss 2.1954 (2.2789)	Arch Beta Loss 846.7336 (841.2904)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.4%, 77.6%)	
11/22 09:33:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 2.0522 (1.9971)	Arch Loss -8519.2637 (-8437.8994)	Arch Hard Loss 2.1984 (2.2705)	Arch Beta Loss 852.1462 (844.0170)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.3%, 77.7%)	
11/22 09:34:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.8515 (1.9998)	Arch Loss -8567.6738 (-8462.3161)	Arch Hard Loss 2.2267 (2.2685)	Arch Beta Loss 856.9900 (846.4585)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.3%, 77.7%)	
11/22 09:34:35午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 45.2480%
11/22 09:34:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.2225	Prec@(1,5) (41.4%, 73.6%)
11/22 09:34:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.2442	Prec@(1,5) (41.2%, 73.4%)
11/22 09:34:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.2497	Prec@(1,5) (40.9%, 73.3%)
11/22 09:35:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.2523	Prec@(1,5) (40.7%, 73.2%)
11/22 09:35:06午後 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 40.6760%
11/22 09:35:06午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:35:06午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.6760%
11/22 09:36:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.1939 (1.9048)	Arch Loss -8621.8262 (-8595.2806)	Arch Hard Loss 2.2632 (2.2593)	Arch Beta Loss 862.4090 (859.7540)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.3%, 79.5%)	
11/22 09:36:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.6650 (1.9136)	Arch Loss -8675.5400 (-8622.1037)	Arch Hard Loss 2.1831 (2.2511)	Arch Beta Loss 867.7723 (862.4355)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.0%, 79.4%)	
11/22 09:37:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.8884 (1.9177)	Arch Loss -8729.4453 (-8648.9382)	Arch Hard Loss 2.0381 (2.2500)	Arch Beta Loss 873.1484 (865.1188)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.6%, 79.4%)	
11/22 09:38:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.6536 (1.9219)	Arch Loss -8778.1094 (-8673.1457)	Arch Hard Loss 1.9820 (2.2382)	Arch Beta Loss 878.0092 (867.5384)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.3%, 79.2%)	
11/22 09:38:37午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 47.2520%
11/22 09:38:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.2463	Prec@(1,5) (41.0%, 73.9%)
11/22 09:38:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.2255	Prec@(1,5) (41.5%, 74.1%)
11/22 09:39:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.2362	Prec@(1,5) (41.1%, 73.8%)
11/22 09:39:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.2284	Prec@(1,5) (41.3%, 74.0%)
11/22 09:39:08午後 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 41.2560%
11/22 09:39:08午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:39:09午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.2560%
11/22 09:40:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.6808 (1.8538)	Arch Loss -8833.3174 (-8805.8462)	Arch Hard Loss 1.6934 (2.2112)	Arch Beta Loss 883.5010 (880.8057)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.6%, 80.8%)	
11/22 09:40:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.0417 (1.8750)	Arch Loss -8887.5078 (-8833.1585)	Arch Hard Loss 2.3602 (2.2046)	Arch Beta Loss 888.9868 (883.5363)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.1%, 80.5%)	
11/22 09:41:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.9083 (1.8873)	Arch Loss -8942.9395 (-8860.6279)	Arch Hard Loss 2.3496 (2.2161)	Arch Beta Loss 894.5289 (886.2844)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.8%, 80.1%)	
11/22 09:42:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9143 (1.8895)	Arch Loss -8993.3652 (-8885.5337)	Arch Hard Loss 2.3316 (2.2073)	Arch Beta Loss 899.5697 (888.7741)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.8%, 80.1%)	
11/22 09:42:39午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 47.8040%
11/22 09:42:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.2629	Prec@(1,5) (40.5%, 72.7%)
11/22 09:42:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.2339	Prec@(1,5) (41.0%, 73.2%)
11/22 09:43:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.2297	Prec@(1,5) (41.0%, 73.5%)
11/22 09:43:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.2248	Prec@(1,5) (41.1%, 73.5%)
11/22 09:43:10午後 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 41.0800%
11/22 09:43:10午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:43:10午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.2560%
11/22 09:44:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.7630 (1.7899)	Arch Loss -9050.7197 (-9022.6257)	Arch Hard Loss 2.1711 (2.1781)	Arch Beta Loss 905.2891 (902.4804)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.2%, 82.0%)	
11/22 09:44:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.7954 (1.8082)	Arch Loss -9108.1787 (-9051.1109)	Arch Hard Loss 2.0020 (2.1748)	Arch Beta Loss 911.0181 (905.3286)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.9%, 81.5%)	
11/22 09:45:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 2.0248 (1.8180)	Arch Loss -9166.1367 (-9079.7997)	Arch Hard Loss 1.9782 (2.1854)	Arch Beta Loss 916.8115 (908.1985)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.8%, 81.3%)	
11/22 09:46:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.5914 (1.8318)	Arch Loss -9218.7451 (-9105.8128)	Arch Hard Loss 2.0452 (2.1846)	Arch Beta Loss 922.0790 (910.7997)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.2%, 81.0%)	
11/22 09:46:42午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 49.2200%
11/22 09:46:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.1532	Prec@(1,5) (42.3%, 74.7%)
11/22 09:46:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.1464	Prec@(1,5) (42.8%, 74.6%)
11/22 09:47:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.1500	Prec@(1,5) (42.9%, 74.4%)
11/22 09:47:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.1507	Prec@(1,5) (43.0%, 74.4%)
11/22 09:47:12午後 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 42.9680%
11/22 09:47:12午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:47:13午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.9680%
11/22 09:48:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.6349 (1.7325)	Arch Loss -9278.0781 (-9249.0249)	Arch Hard Loss 2.3831 (2.1437)	Arch Beta Loss 928.0461 (925.1169)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 83.2%)	
11/22 09:49:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.6400 (1.7715)	Arch Loss -9337.8564 (-9278.6720)	Arch Hard Loss 2.2135 (2.1735)	Arch Beta Loss 934.0070 (928.0845)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.3%, 82.4%)	
11/22 09:49:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.9796 (1.7807)	Arch Loss -9398.0791 (-9308.5126)	Arch Hard Loss 2.0719 (2.1762)	Arch Beta Loss 940.0151 (931.0689)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.2%, 82.0%)	
11/22 09:50:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.8330 (1.7926)	Arch Loss -9453.0703 (-9335.5157)	Arch Hard Loss 1.4840 (2.1599)	Arch Beta Loss 945.4554 (933.7676)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.8%, 81.8%)	
11/22 09:50:44午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 49.8320%
11/22 09:50:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.1304	Prec@(1,5) (43.9%, 75.2%)
11/22 09:51:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.1279	Prec@(1,5) (43.6%, 75.6%)
11/22 09:51:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.1293	Prec@(1,5) (43.9%, 75.4%)
11/22 09:51:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.1304	Prec@(1,5) (43.5%, 75.6%)
11/22 09:51:15午後 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 43.5040%
11/22 09:51:15午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:51:15午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.5040%
11/22 09:52:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.4818 (1.6691)	Arch Loss -9513.6631 (-9483.6599)	Arch Hard Loss 2.2567 (2.1607)	Arch Beta Loss 951.5920 (948.5821)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.7%, 83.8%)	
11/22 09:53:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.8825 (1.7067)	Arch Loss -9574.4854 (-9514.1144)	Arch Hard Loss 2.4356 (2.1498)	Arch Beta Loss 957.6921 (951.6264)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.5%, 83.2%)	
11/22 09:53:58午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 1.8739 (1.7288)	Arch Loss -9636.1846 (-9544.6429)	Arch Hard Loss 1.9031 (2.1332)	Arch Beta Loss 963.8088 (954.6776)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 82.9%)	
11/22 09:54:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.7984 (1.7449)	Arch Loss -9690.9521 (-9572.1518)	Arch Hard Loss 2.2796 (2.1287)	Arch Beta Loss 969.3232 (957.4281)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.8%, 82.6%)	
11/22 09:54:47午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 50.7960%
11/22 09:54:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.1525	Prec@(1,5) (43.2%, 74.8%)
11/22 09:55:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.1577	Prec@(1,5) (43.5%, 74.6%)
11/22 09:55:11午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.1485	Prec@(1,5) (43.5%, 74.7%)
11/22 09:55:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.1475	Prec@(1,5) (43.5%, 74.7%)
11/22 09:55:18午後 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 43.4920%
11/22 09:55:18午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:55:18午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.5040%
11/22 09:56:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.8688 (1.6344)	Arch Loss -9753.0508 (-9722.6714)	Arch Hard Loss 2.1091 (2.1347)	Arch Beta Loss 975.5160 (972.4806)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.3%, 84.7%)	
11/22 09:57:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.6755 (1.6611)	Arch Loss -9813.8994 (-9753.3391)	Arch Hard Loss 2.5837 (2.1289)	Arch Beta Loss 981.6483 (975.5468)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.9%, 84.2%)	
11/22 09:58:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3501 (1.6829)	Arch Loss -9875.7998 (-9783.9958)	Arch Hard Loss 1.9747 (2.1292)	Arch Beta Loss 987.7775 (978.6125)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.3%, 83.7%)	
11/22 09:58:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.7380 (1.7064)	Arch Loss -9930.9355 (-9811.5763)	Arch Hard Loss 1.9439 (2.1288)	Arch Beta Loss 993.2880 (981.3705)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.9%, 83.2%)	
11/22 09:58:50午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 51.9120%
11/22 09:58:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.0849	Prec@(1,5) (45.3%, 75.8%)
11/22 09:59:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.0836	Prec@(1,5) (45.4%, 75.8%)
11/22 09:59:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.0918	Prec@(1,5) (45.3%, 75.9%)
11/22 09:59:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.0986	Prec@(1,5) (45.2%, 75.9%)
11/22 09:59:21午後 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 45.2360%
11/22 09:59:21午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 09:59:22午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.2360%
11/22 10:00:16午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.6924 (1.5689)	Arch Loss -9992.0605 (-9962.2758)	Arch Hard Loss 2.5575 (2.0960)	Arch Beta Loss 999.4619 (996.4372)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.0%, 85.9%)	
11/22 10:01:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.5689 (1.6248)	Arch Loss -10053.0010 (-9992.7724)	Arch Hard Loss 2.5795 (2.1238)	Arch Beta Loss 1005.5580 (999.4896)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.7%, 84.7%)	
11/22 10:02:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.7582 (1.6510)	Arch Loss -10114.3965 (-10023.2466)	Arch Hard Loss 1.9072 (2.1072)	Arch Beta Loss 1011.6304 (1002.5354)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.1%, 84.3%)	
11/22 10:02:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.6185 (1.6605)	Arch Loss -10168.5762 (-10050.5869)	Arch Hard Loss 2.0982 (2.1045)	Arch Beta Loss 1017.0674 (1005.2691)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.0%, 84.0%)	
11/22 10:02:54午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 53.0040%
11/22 10:03:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.1111	Prec@(1,5) (45.0%, 75.1%)
11/22 10:03:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.1215	Prec@(1,5) (44.7%, 75.6%)
11/22 10:03:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.1263	Prec@(1,5) (44.5%, 75.4%)
11/22 10:03:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.1165	Prec@(1,5) (44.6%, 75.6%)
11/22 10:03:25午後 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 44.5520%
11/22 10:03:25午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:03:25午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.2360%
11/22 10:04:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.6850 (1.5285)	Arch Loss -10228.9053 (-10199.5005)	Arch Hard Loss 2.3719 (2.1161)	Arch Beta Loss 1023.1277 (1020.1617)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.5%, 86.2%)	
11/22 10:05:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.6307 (1.5752)	Arch Loss -10288.7070 (-10229.3997)	Arch Hard Loss 2.0441 (2.0882)	Arch Beta Loss 1029.0751 (1023.1488)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.3%, 85.7%)	
11/22 10:06:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.5138 (1.5947)	Arch Loss -10347.0947 (-10259.0835)	Arch Hard Loss 2.5053 (2.0834)	Arch Beta Loss 1034.9600 (1026.1167)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.9%, 85.3%)	
11/22 10:06:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.8475 (1.6107)	Arch Loss -10399.9785 (-10285.6131)	Arch Hard Loss 1.9852 (2.0819)	Arch Beta Loss 1040.1964 (1028.7695)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.3%, 84.9%)	
11/22 10:06:57午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 54.3240%
11/22 10:07:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.1161	Prec@(1,5) (43.9%, 76.1%)
11/22 10:07:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.1116	Prec@(1,5) (43.7%, 76.5%)
11/22 10:07:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.1125	Prec@(1,5) (43.9%, 76.6%)
11/22 10:07:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.1174	Prec@(1,5) (43.8%, 76.4%)
11/22 10:07:28午後 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 43.8280%
11/22 10:07:28午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:07:29午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.2360%
11/22 10:08:23午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.6007 (1.4835)	Arch Loss -10458.1758 (-10429.5500)	Arch Hard Loss 1.8218 (2.0729)	Arch Beta Loss 1045.9998 (1043.1623)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.7%, 87.6%)	
11/22 10:09:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.5880 (1.5406)	Arch Loss -10514.5732 (-10458.0756)	Arch Hard Loss 2.0865 (2.0761)	Arch Beta Loss 1051.6660 (1046.0152)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.3%, 86.2%)	
11/22 10:10:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.7851 (1.5528)	Arch Loss -10570.5771 (-10486.3263)	Arch Hard Loss 1.9401 (2.0866)	Arch Beta Loss 1057.2517 (1048.8413)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.8%, 86.1%)	
11/22 10:11:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.6059 (1.5687)	Arch Loss -10619.6924 (-10511.5401)	Arch Hard Loss 2.4163 (2.0791)	Arch Beta Loss 1062.2108 (1051.3619)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.3%, 85.7%)	
11/22 10:11:00午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 55.3280%
11/22 10:11:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.0163	Prec@(1,5) (47.0%, 77.3%)
11/22 10:11:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.0090	Prec@(1,5) (47.2%, 77.7%)
11/22 10:11:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.0238	Prec@(1,5) (46.9%, 77.6%)
11/22 10:11:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.0270	Prec@(1,5) (46.8%, 77.8%)
11/22 10:11:31午後 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 46.8680%
11/22 10:11:31午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:11:32午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8680%
11/22 10:12:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.3219 (1.4541)	Arch Loss -10674.6426 (-10648.1168)	Arch Hard Loss 2.3807 (2.0559)	Arch Beta Loss 1067.7024 (1065.0173)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.4%, 87.4%)	
11/22 10:13:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.5918 (1.4851)	Arch Loss -10728.5625 (-10675.1171)	Arch Hard Loss 2.1227 (2.0590)	Arch Beta Loss 1073.0686 (1067.7176)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.6%, 87.0%)	
11/22 10:14:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.8217 (1.5051)	Arch Loss -10781.8682 (-10701.8924)	Arch Hard Loss 1.8447 (2.0627)	Arch Beta Loss 1078.3713 (1070.3955)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.9%, 86.6%)	
11/22 10:15:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.3283 (1.5198)	Arch Loss -10828.9160 (-10725.8138)	Arch Hard Loss 2.0512 (2.0678)	Arch Beta Loss 1083.0967 (1072.7882)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.5%, 86.4%)	
11/22 10:15:04午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 56.5000%
11/22 10:15:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.0846	Prec@(1,5) (45.6%, 77.2%)
11/22 10:15:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.0729	Prec@(1,5) (45.6%, 77.3%)
11/22 10:15:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.0779	Prec@(1,5) (45.5%, 77.2%)
11/22 10:15:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.0640	Prec@(1,5) (45.8%, 77.3%)
11/22 10:15:35午後 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 45.7680%
11/22 10:15:35午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:15:35午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8680%
11/22 10:16:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.5510 (1.4185)	Arch Loss -10881.3555 (-10855.7915)	Arch Hard Loss 2.2041 (2.0266)	Arch Beta Loss 1088.3560 (1085.7818)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.1%, 87.6%)	
11/22 10:17:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.5946 (1.4462)	Arch Loss -10932.7471 (-10881.7101)	Arch Hard Loss 2.5161 (2.0464)	Arch Beta Loss 1093.5262 (1088.3756)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.3%, 87.2%)	
11/22 10:18:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.4405 (1.4660)	Arch Loss -10985.1455 (-10907.5330)	Arch Hard Loss 1.5458 (2.0559)	Arch Beta Loss 1098.6692 (1090.9589)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.1%, 86.9%)	
11/22 10:19:06午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.4634 (1.4753)	Arch Loss -11031.0186 (-10930.7145)	Arch Hard Loss 1.7842 (2.0499)	Arch Beta Loss 1103.2803 (1093.2764)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.8%, 86.8%)	
11/22 10:19:07午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 57.8240%
11/22 10:19:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.0639	Prec@(1,5) (46.4%, 77.6%)
11/22 10:19:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.0642	Prec@(1,5) (46.4%, 77.5%)
11/22 10:19:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.0406	Prec@(1,5) (46.7%, 78.0%)
11/22 10:19:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.0442	Prec@(1,5) (46.5%, 77.9%)
11/22 10:19:37午後 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 46.4960%
11/22 10:19:37午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:19:38午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8680%
11/22 10:20:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.6874 (1.3711)	Arch Loss -11082.7002 (-11057.1435)	Arch Hard Loss 1.6883 (1.9757)	Arch Beta Loss 1108.4388 (1105.9119)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.7%, 88.8%)	
11/22 10:21:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.0575 (1.4104)	Arch Loss -11133.1006 (-11082.6132)	Arch Hard Loss 2.1983 (2.0026)	Arch Beta Loss 1113.5299 (1108.4616)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.6%, 88.2%)	
11/22 10:22:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.2622 (1.4099)	Arch Loss -11184.3135 (-11108.0437)	Arch Hard Loss 1.7001 (2.0096)	Arch Beta Loss 1118.6013 (1111.0053)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.5%, 88.3%)	
11/22 10:23:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.2776 (1.4296)	Arch Loss -11229.1875 (-11130.8672)	Arch Hard Loss 2.2366 (2.0180)	Arch Beta Loss 1123.1423 (1113.2885)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.1%, 87.9%)	
11/22 10:23:09午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 59.0920%
11/22 10:23:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.1420	Prec@(1,5) (44.8%, 76.0%)
11/22 10:23:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.1305	Prec@(1,5) (45.1%, 76.2%)
11/22 10:23:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.1269	Prec@(1,5) (45.0%, 76.2%)
11/22 10:23:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.1317	Prec@(1,5) (44.7%, 76.0%)
11/22 10:23:40午後 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 44.7560%
11/22 10:23:40午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:23:40午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8680%
11/22 10:24:35午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.2200 (1.2930)	Arch Loss -11279.3252 (-11255.2195)	Arch Hard Loss 2.7053 (2.0446)	Arch Beta Loss 1128.2030 (1125.7264)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.7%, 89.7%)	
11/22 10:25:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.6990 (1.3447)	Arch Loss -11329.8047 (-11280.1638)	Arch Hard Loss 1.8446 (2.0350)	Arch Beta Loss 1133.1649 (1128.2199)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.7%, 88.8%)	
11/22 10:26:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.4008 (1.3547)	Arch Loss -11378.2920 (-11304.9105)	Arch Hard Loss 2.3563 (2.0390)	Arch Beta Loss 1138.0648 (1130.6949)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.6%, 88.7%)	
11/22 10:27:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.3200 (1.3775)	Arch Loss -11422.4873 (-11327.0125)	Arch Hard Loss 1.6245 (2.0274)	Arch Beta Loss 1142.4111 (1132.9040)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.1%, 88.4%)	
11/22 10:27:11午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 60.0880%
11/22 10:27:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.0621	Prec@(1,5) (47.1%, 77.2%)
11/22 10:27:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.0619	Prec@(1,5) (46.9%, 77.5%)
11/22 10:27:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.0609	Prec@(1,5) (46.7%, 77.4%)
11/22 10:27:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.0546	Prec@(1,5) (46.8%, 77.6%)
11/22 10:27:42午後 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 46.7960%
11/22 10:27:42午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:27:42午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8680%
11/22 10:28:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.6927 (1.2701)	Arch Loss -11470.1562 (-11446.6723)	Arch Hard Loss 1.8952 (1.9620)	Arch Beta Loss 1147.2052 (1144.8634)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.6%, 89.9%)	
11/22 10:29:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.0043 (1.2937)	Arch Loss -11516.8428 (-11470.1239)	Arch Hard Loss 1.6982 (2.0004)	Arch Beta Loss 1151.8541 (1147.2124)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.5%, 89.9%)	
11/22 10:30:25午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.3034 (1.3057)	Arch Loss -11562.0088 (-11493.2602)	Arch Hard Loss 1.9565 (2.0136)	Arch Beta Loss 1156.3966 (1149.5274)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.1%, 89.7%)	
11/22 10:31:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.4764 (1.3331)	Arch Loss -11602.2422 (-11513.7783)	Arch Hard Loss 1.6306 (2.0216)	Arch Beta Loss 1160.3873 (1151.5800)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.2%, 89.2%)	
11/22 10:31:14午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 61.2200%
11/22 10:31:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.9834	Prec@(1,5) (48.0%, 78.5%)
11/22 10:31:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.0013	Prec@(1,5) (47.4%, 78.6%)
11/22 10:31:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.9808	Prec@(1,5) (48.3%, 78.7%)
11/22 10:31:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.9739	Prec@(1,5) (48.3%, 78.8%)
11/22 10:31:45午後 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 48.2800%
11/22 10:31:45午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:31:45午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:32:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.4103 (1.2183)	Arch Loss -11645.5615 (-11624.1731)	Arch Hard Loss 1.9388 (2.0504)	Arch Beta Loss 1164.7500 (1162.6223)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.5%, 90.9%)	
11/22 10:33:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.8804 (1.2294)	Arch Loss -11687.5508 (-11645.4550)	Arch Hard Loss 1.8961 (2.0506)	Arch Beta Loss 1168.9447 (1164.7506)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.7%)	
11/22 10:34:27午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.2481 (1.2548)	Arch Loss -11727.5020 (-11666.3362)	Arch Hard Loss 2.6164 (2.0318)	Arch Beta Loss 1173.0118 (1166.8368)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.5%, 90.4%)	
11/22 10:35:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.2389 (1.2710)	Arch Loss -11763.4951 (-11684.7566)	Arch Hard Loss 2.1219 (2.0232)	Arch Beta Loss 1176.5618 (1168.6780)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.9%, 90.2%)	
11/22 10:35:16午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 62.9160%
11/22 10:35:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.1481	Prec@(1,5) (47.4%, 76.2%)
11/22 10:35:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.1383	Prec@(1,5) (46.8%, 76.5%)
11/22 10:35:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.1472	Prec@(1,5) (46.4%, 76.6%)
11/22 10:35:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.1341	Prec@(1,5) (46.7%, 76.9%)
11/22 10:35:47午後 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 46.6320%
11/22 10:35:47午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:35:47午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:36:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.4803 (1.1864)	Arch Loss -11802.6143 (-11783.4118)	Arch Hard Loss 1.5889 (1.9918)	Arch Beta Loss 1180.4203 (1178.5404)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.6%, 91.4%)	
11/22 10:37:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.4131 (1.1889)	Arch Loss -11838.8096 (-11802.1563)	Arch Hard Loss 2.2936 (2.0181)	Arch Beta Loss 1184.1104 (1180.4174)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.5%, 91.3%)	
11/22 10:38:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.4391 (1.2118)	Arch Loss -11874.9580 (-11820.5054)	Arch Hard Loss 1.7592 (2.0089)	Arch Beta Loss 1187.6716 (1182.2514)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.9%)	
11/22 10:39:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.5609 (1.2345)	Arch Loss -11905.0547 (-11836.6424)	Arch Hard Loss 2.6234 (2.0119)	Arch Beta Loss 1190.7678 (1183.8654)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.6%, 90.7%)	
11/22 10:39:18午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 63.6400%
11/22 10:39:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.2882	Prec@(1,5) (44.0%, 74.7%)
11/22 10:39:34午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.2632	Prec@(1,5) (44.4%, 75.0%)
11/22 10:39:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.2751	Prec@(1,5) (44.1%, 74.8%)
11/22 10:39:49午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.2722	Prec@(1,5) (44.1%, 74.9%)
11/22 10:39:49午後 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 44.1520%
11/22 10:39:49午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:39:49午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:40:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.9964 (1.1386)	Arch Loss -11939.2500 (-11922.9171)	Arch Hard Loss 1.9694 (1.9707)	Arch Beta Loss 1194.1219 (1192.4888)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.3%, 91.6%)	
11/22 10:41:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.9364 (1.1481)	Arch Loss -11971.3037 (-11939.1685)	Arch Hard Loss 1.8948 (2.0101)	Arch Beta Loss 1197.3198 (1194.1179)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 91.8%)	
11/22 10:42:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.2733 (1.1705)	Arch Loss -12001.5254 (-11955.0486)	Arch Hard Loss 2.4544 (2.0170)	Arch Beta Loss 1200.3979 (1195.7066)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.7%, 91.3%)	
11/22 10:43:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.6055 (1.1846)	Arch Loss -12028.7236 (-11969.0122)	Arch Hard Loss 1.9598 (2.0129)	Arch Beta Loss 1203.0684 (1197.1025)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.4%, 91.2%)	
11/22 10:43:20午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 65.3880%
11/22 10:43:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.3547	Prec@(1,5) (44.7%, 74.8%)
11/22 10:43:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.3837	Prec@(1,5) (44.4%, 74.4%)
11/22 10:43:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.4042	Prec@(1,5) (44.2%, 74.2%)
11/22 10:43:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.4179	Prec@(1,5) (44.3%, 74.0%)
11/22 10:43:51午後 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 44.2480%
11/22 10:43:51午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:43:51午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:44:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.3536 (1.1014)	Arch Loss -12057.7031 (-12043.5068)	Arch Hard Loss 1.8594 (1.9972)	Arch Beta Loss 1205.9563 (1204.5504)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.4%, 92.3%)	
11/22 10:45:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.8046 (1.1103)	Arch Loss -12084.9453 (-12057.5138)	Arch Hard Loss 2.1034 (2.0045)	Arch Beta Loss 1208.7048 (1205.9518)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.0%, 92.3%)	
11/22 10:46:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.0334 (1.1178)	Arch Loss -12111.5811 (-12071.1591)	Arch Hard Loss 1.8937 (2.0128)	Arch Beta Loss 1211.3474 (1207.3172)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.9%, 92.2%)	
11/22 10:47:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.0697 (1.1297)	Arch Loss -12134.5879 (-12083.1539)	Arch Hard Loss 1.7882 (2.0058)	Arch Beta Loss 1213.6376 (1208.5160)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.6%, 92.0%)	
11/22 10:47:22午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 66.5960%
11/22 10:47:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.3733	Prec@(1,5) (43.2%, 75.0%)
11/22 10:47:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.3758	Prec@(1,5) (43.3%, 74.6%)
11/22 10:47:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.3845	Prec@(1,5) (43.2%, 74.4%)
11/22 10:47:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.3944	Prec@(1,5) (43.3%, 74.2%)
11/22 10:47:53午後 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 43.2960%
11/22 10:47:53午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:47:53午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:48:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.9239 (0.9907)	Arch Loss -12159.0586 (-12147.0894)	Arch Hard Loss 2.0586 (1.9854)	Arch Beta Loss 1216.1117 (1214.9075)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.5%, 93.8%)	
11/22 10:49:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.9182 (1.0335)	Arch Loss -12182.9727 (-12159.1056)	Arch Hard Loss 1.6821 (1.9730)	Arch Beta Loss 1218.4655 (1216.1079)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.6%, 93.2%)	
11/22 10:50:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.2734 (1.0588)	Arch Loss -12204.9541 (-12170.7648)	Arch Hard Loss 2.3133 (2.0038)	Arch Beta Loss 1220.7268 (1217.2769)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.2%, 92.9%)	
11/22 10:51:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.3746 (1.0719)	Arch Loss -12224.8164 (-12181.0252)	Arch Hard Loss 2.0426 (2.0037)	Arch Beta Loss 1222.6859 (1218.3029)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.0%, 92.7%)	
11/22 10:51:25午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 67.9960%
11/22 10:51:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.4237	Prec@(1,5) (42.7%, 74.1%)
11/22 10:51:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.3991	Prec@(1,5) (43.4%, 74.3%)
11/22 10:51:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.3960	Prec@(1,5) (43.5%, 74.4%)
11/22 10:51:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.4042	Prec@(1,5) (43.4%, 74.4%)
11/22 10:51:56午後 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 43.4080%
11/22 10:51:56午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:51:56午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:52:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.9399 (0.9666)	Arch Loss -12246.0762 (-12235.7613)	Arch Hard Loss 1.9457 (1.9596)	Arch Beta Loss 1224.8021 (1223.7721)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.5%, 94.1%)	
11/22 10:53:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.8612 (0.9960)	Arch Loss -12265.6357 (-12245.9932)	Arch Hard Loss 2.5100 (1.9929)	Arch Beta Loss 1226.8146 (1224.7986)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.5%)	
11/22 10:54:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.0776 (1.0044)	Arch Loss -12285.1816 (-12255.9883)	Arch Hard Loss 2.3026 (1.9943)	Arch Beta Loss 1228.7484 (1225.7983)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.5%, 93.5%)	
11/22 10:55:27午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.2137 (1.0215)	Arch Loss -12302.2832 (-12264.7585)	Arch Hard Loss 1.9528 (1.9981)	Arch Beta Loss 1230.4236 (1226.6757)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.1%, 93.3%)	
11/22 10:55:27午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 69.0760%
11/22 10:55:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.5085	Prec@(1,5) (42.9%, 72.2%)
11/22 10:55:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.5074	Prec@(1,5) (42.8%, 72.5%)
11/22 10:55:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.5033	Prec@(1,5) (43.0%, 72.9%)
11/22 10:55:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.5067	Prec@(1,5) (43.1%, 72.9%)
11/22 10:55:58午後 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 43.0680%
11/22 10:55:58午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/22 10:55:58午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2800%
11/22 10:56:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.9976 (0.9163)	Arch Loss -12320.2969 (-12311.5322)	Arch Hard Loss 2.0366 (1.9930)	Arch Beta Loss 1232.2333 (1231.3525)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.6%)	
11/22 10:57:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.2803 (0.9465)	Arch Loss -12337.7100 (-12320.3011)	Arch Hard Loss 1.8370 (2.0033)	Arch Beta Loss 1233.9547 (1232.2304)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.9%, 94.0%)	
11/22 10:58:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.3477 (0.9661)	Arch Loss -12354.2051 (-12328.8581)	Arch Hard Loss 1.8842 (1.9964)	Arch Beta Loss 1235.6089 (1233.0855)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.2%, 93.9%)	
11/22 10:59:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.2527 (0.9739)	Arch Loss -12368.8232 (-12336.3533)	Arch Hard Loss 1.6018 (2.0067)	Arch Beta Loss 1237.0425 (1233.8360)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.8%, 93.9%)	
11/22 10:59:29午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 70.8040%
11/22 10:59:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.0387	Prec@(1,5) (49.6%, 79.0%)
11/22 10:59:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.0552	Prec@(1,5) (49.0%, 78.9%)
11/22 10:59:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.0610	Prec@(1,5) (49.4%, 78.8%)
11/22 11:00:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.0603	Prec@(1,5) (49.4%, 78.7%)
11/22 11:00:00午後 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 49.3960%
11/22 11:00:00午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:00:01午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:00:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.8035 (0.8698)	Arch Loss -12383.7305 (-12376.3696)	Arch Hard Loss 2.1837 (2.0048)	Arch Beta Loss 1238.5914 (1237.8374)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 94.8%)	
11/22 11:01:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.7488 (0.8999)	Arch Loss -12399.0459 (-12383.8815)	Arch Hard Loss 1.6080 (2.0086)	Arch Beta Loss 1240.0654 (1238.5890)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.0%, 94.5%)	
11/22 11:02:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.8131 (0.9131)	Arch Loss -12412.9082 (-12391.2014)	Arch Hard Loss 1.9144 (2.0103)	Arch Beta Loss 1241.4822 (1239.3212)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.4%)	
11/22 11:03:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.8549 (0.9225)	Arch Loss -12425.2979 (-12397.6246)	Arch Hard Loss 1.8078 (2.0156)	Arch Beta Loss 1242.7106 (1239.9640)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.4%)	
11/22 11:03:32午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 72.3720%
11/22 11:03:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.2607	Prec@(1,5) (45.9%, 76.4%)
11/22 11:03:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.2534	Prec@(1,5) (46.8%, 76.3%)
11/22 11:03:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.2596	Prec@(1,5) (46.4%, 76.2%)
11/22 11:04:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.2579	Prec@(1,5) (46.3%, 76.2%)
11/22 11:04:03午後 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 46.2680%
11/22 11:04:03午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:04:03午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:04:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.7357 (0.8234)	Arch Loss -12437.9072 (-12431.8920)	Arch Hard Loss 2.4774 (2.0284)	Arch Beta Loss 1244.0385 (1243.3920)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.2%, 95.3%)	
11/22 11:05:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9295 (0.8395)	Arch Loss -12450.9453 (-12438.3606)	Arch Hard Loss 2.0765 (2.0036)	Arch Beta Loss 1245.3021 (1244.0364)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.8%, 95.2%)	
11/22 11:06:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.6719 (0.8480)	Arch Loss -12463.4053 (-12444.6260)	Arch Hard Loss 1.7722 (2.0171)	Arch Beta Loss 1246.5178 (1244.6643)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.2%)	
11/22 11:07:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.6728 (0.8572)	Arch Loss -12473.9854 (-12450.1517)	Arch Hard Loss 1.7358 (2.0059)	Arch Beta Loss 1247.5720 (1245.2158)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.1%, 95.1%)	
11/22 11:07:34午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 74.1240%
11/22 11:07:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 3.2218	Prec@(1,5) (33.5%, 65.1%)
11/22 11:07:49午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 3.2234	Prec@(1,5) (33.7%, 64.9%)
11/22 11:07:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 3.2127	Prec@(1,5) (33.9%, 65.0%)
11/22 11:08:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 3.2192	Prec@(1,5) (33.5%, 64.9%)
11/22 11:08:04午後 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 33.5400%
11/22 11:08:04午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:08:05午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:08:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7348 (0.7817)	Arch Loss -12485.3789 (-12479.5416)	Arch Hard Loss 1.7444 (2.0307)	Arch Beta Loss 1248.7123 (1248.1572)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.7%, 95.5%)	
11/22 11:09:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.7488 (0.7915)	Arch Loss -12495.8984 (-12485.1246)	Arch Hard Loss 2.0841 (1.9822)	Arch Beta Loss 1249.7982 (1248.7107)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 95.5%)	
11/22 11:10:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.9116 (0.8017)	Arch Loss -12506.3486 (-12490.5170)	Arch Hard Loss 2.0787 (1.9844)	Arch Beta Loss 1250.8428 (1249.2501)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.6%, 95.7%)	
11/22 11:11:35午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.5775 (0.8112)	Arch Loss -12515.3994 (-12495.2325)	Arch Hard Loss 2.0919 (2.0077)	Arch Beta Loss 1251.7491 (1249.7240)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.2%, 95.6%)	
11/22 11:11:36午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 75.1960%
11/22 11:11:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.8750	Prec@(1,5) (39.4%, 69.9%)
11/22 11:11:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.9051	Prec@(1,5) (38.9%, 69.4%)
11/22 11:12:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.9057	Prec@(1,5) (38.9%, 69.6%)
11/22 11:12:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.8999	Prec@(1,5) (38.9%, 69.8%)
11/22 11:12:07午後 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 38.9120%
11/22 11:12:07午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:12:07午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:13:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.7877 (0.7214)	Arch Loss -12525.6328 (-12520.4899)	Arch Hard Loss 1.6637 (2.0343)	Arch Beta Loss 1252.7297 (1252.2524)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.4%, 96.5%)	
11/22 11:13:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.5485 (0.7454)	Arch Loss -12534.8018 (-12525.2772)	Arch Hard Loss 1.8400 (2.0079)	Arch Beta Loss 1253.6642 (1252.7285)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.3%)	
11/22 11:14:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.5924 (0.7510)	Arch Loss -12543.5684 (-12529.9016)	Arch Hard Loss 2.0664 (2.0260)	Arch Beta Loss 1254.5635 (1253.1928)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.6%, 96.3%)	
11/22 11:15:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.8674 (0.7569)	Arch Loss -12551.2607 (-12533.9810)	Arch Hard Loss 2.1813 (2.0262)	Arch Beta Loss 1255.3442 (1253.6007)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.5%, 96.2%)	
11/22 11:15:38午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 76.4440%
11/22 11:15:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.6000	Prec@(1,5) (43.5%, 72.9%)
11/22 11:15:54午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.5927	Prec@(1,5) (43.8%, 73.0%)
11/22 11:16:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.5924	Prec@(1,5) (43.6%, 73.1%)
11/22 11:16:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.6081	Prec@(1,5) (43.5%, 73.1%)
11/22 11:16:09午後 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 43.4920%
11/22 11:16:09午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:16:09午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:17:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.7527 (0.6811)	Arch Loss -12559.5830 (-12555.7200)	Arch Hard Loss 2.3116 (2.0603)	Arch Beta Loss 1256.1895 (1255.7780)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.7%, 97.0%)	
11/22 11:17:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.9891 (0.6956)	Arch Loss -12567.8340 (-12559.8305)	Arch Hard Loss 2.1170 (2.0541)	Arch Beta Loss 1256.9951 (1256.1885)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.9%, 96.8%)	
11/22 11:18:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.5550 (0.7119)	Arch Loss -12575.5566 (-12563.8410)	Arch Hard Loss 2.1540 (2.0467)	Arch Beta Loss 1257.7711 (1256.5888)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.2%, 96.7%)	
11/22 11:19:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.8656 (0.7174)	Arch Loss -12582.6309 (-12567.3717)	Arch Hard Loss 1.8159 (2.0346)	Arch Beta Loss 1258.4447 (1256.9406)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.0%, 96.6%)	
11/22 11:19:40午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 78.0120%
11/22 11:19:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 3.9926	Prec@(1,5) (29.1%, 57.6%)
11/22 11:19:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 4.0086	Prec@(1,5) (29.2%, 57.6%)
11/22 11:20:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 4.0016	Prec@(1,5) (29.5%, 57.8%)
11/22 11:20:11午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 3.9936	Prec@(1,5) (29.5%, 58.0%)
11/22 11:20:11午後 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 29.4720%
11/22 11:20:11午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:20:11午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:21:06午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.5076 (0.6523)	Arch Loss -12589.6631 (-12586.1569)	Arch Hard Loss 2.0794 (2.0332)	Arch Beta Loss 1259.1742 (1258.8190)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.0%, 97.3%)	
11/22 11:22:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.5834 (0.6645)	Arch Loss -12596.8076 (-12589.6977)	Arch Hard Loss 1.8895 (2.0356)	Arch Beta Loss 1259.8698 (1259.1733)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.5%, 97.2%)	
11/22 11:22:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.5040 (0.6630)	Arch Loss -12603.8154 (-12593.1504)	Arch Hard Loss 1.5838 (2.0395)	Arch Beta Loss 1260.5399 (1259.5190)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.7%, 97.1%)	
11/22 11:23:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.9202 (0.6721)	Arch Loss -12609.2568 (-12596.1859)	Arch Hard Loss 1.9643 (2.0430)	Arch Beta Loss 1261.1221 (1259.8229)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.4%, 97.0%)	
11/22 11:23:42午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 79.3400%
11/22 11:23:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.6123	Prec@(1,5) (44.4%, 73.6%)
11/22 11:23:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.6447	Prec@(1,5) (43.8%, 73.3%)
11/22 11:24:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.6396	Prec@(1,5) (43.9%, 73.4%)
11/22 11:24:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.6442	Prec@(1,5) (43.8%, 73.3%)
11/22 11:24:13午後 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 43.8360%
11/22 11:24:13午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:24:13午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:25:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.5851 (0.6213)	Arch Loss -12615.2637 (-12612.3934)	Arch Hard Loss 2.2656 (2.0628)	Arch Beta Loss 1261.7529 (1261.4456)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.0%, 97.5%)	
11/22 11:26:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.6539 (0.6302)	Arch Loss -12621.5654 (-12615.4935)	Arch Hard Loss 1.9782 (2.0264)	Arch Beta Loss 1262.3544 (1261.7520)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.7%, 97.4%)	
11/22 11:26:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.5554 (0.6406)	Arch Loss -12626.7803 (-12618.4687)	Arch Hard Loss 2.5562 (2.0405)	Arch Beta Loss 1262.9337 (1262.0509)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.4%, 97.3%)	
11/22 11:27:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.6684 (0.6438)	Arch Loss -12631.8984 (-12621.1012)	Arch Hard Loss 2.4816 (2.0369)	Arch Beta Loss 1263.4380 (1262.3138)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.4%, 97.2%)	
11/22 11:27:44午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 80.4120%
11/22 11:27:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 3.3348	Prec@(1,5) (42.2%, 72.2%)
11/22 11:28:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 3.4436	Prec@(1,5) (41.6%, 71.4%)
11/22 11:28:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 3.4617	Prec@(1,5) (41.7%, 71.5%)
11/22 11:28:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 3.4313	Prec@(1,5) (41.3%, 71.4%)
11/22 11:28:15午後 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 41.3320%
11/22 11:28:15午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:28:15午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:29:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.4919 (0.5988)	Arch Loss -12637.5342 (-12635.1484)	Arch Hard Loss 2.3035 (2.0314)	Arch Beta Loss 1263.9838 (1263.7180)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.7%, 97.6%)	
11/22 11:30:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.4276 (0.6075)	Arch Loss -12643.2314 (-12637.7978)	Arch Hard Loss 1.8172 (2.0347)	Arch Beta Loss 1264.5049 (1263.9833)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.5%, 97.4%)	
11/22 11:30:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.7096 (0.6015)	Arch Loss -12647.4473 (-12640.3875)	Arch Hard Loss 2.6247 (2.0344)	Arch Beta Loss 1265.0072 (1264.2422)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.7%, 97.5%)	
11/22 11:31:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.5578 (0.6038)	Arch Loss -12652.5371 (-12642.6616)	Arch Hard Loss 1.9011 (2.0377)	Arch Beta Loss 1265.4438 (1264.4699)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.6%, 97.4%)	
11/22 11:31:46午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 81.5920%
11/22 11:31:54午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 3.1476	Prec@(1,5) (37.0%, 67.2%)
11/22 11:32:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 3.1457	Prec@(1,5) (36.8%, 66.9%)
11/22 11:32:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 3.1342	Prec@(1,5) (36.8%, 67.1%)
11/22 11:32:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 3.1458	Prec@(1,5) (36.7%, 67.1%)
11/22 11:32:17午後 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 36.6600%
11/22 11:32:17午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:32:17午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:33:12午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.5677 (0.5721)	Arch Loss -12656.7871 (-12654.8211)	Arch Hard Loss 2.3827 (2.0443)	Arch Beta Loss 1265.9170 (1265.6865)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.7%, 97.8%)	
11/22 11:34:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.8200 (0.5768)	Arch Loss -12661.2988 (-12657.1241)	Arch Hard Loss 2.3872 (2.0408)	Arch Beta Loss 1266.3687 (1265.9165)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.7%)	
11/22 11:34:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.5535 (0.5814)	Arch Loss -12665.6211 (-12659.3647)	Arch Hard Loss 2.4252 (2.0455)	Arch Beta Loss 1266.8046 (1266.1410)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.7%)	
11/22 11:35:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.4602 (0.5815)	Arch Loss -12669.6191 (-12661.3416)	Arch Hard Loss 2.2161 (2.0440)	Arch Beta Loss 1267.1835 (1266.3386)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.3%, 97.7%)	
11/22 11:35:48午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 82.3160%
11/22 11:35:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 3.9619	Prec@(1,5) (29.8%, 60.8%)
11/22 11:36:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 3.9770	Prec@(1,5) (30.4%, 60.4%)
11/22 11:36:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 3.9726	Prec@(1,5) (30.5%, 60.5%)
11/22 11:36:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 3.9886	Prec@(1,5) (30.4%, 60.3%)
11/22 11:36:19午後 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 30.3960%
11/22 11:36:19午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:36:19午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:37:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.5546 (0.5536)	Arch Loss -12673.9697 (-12671.9080)	Arch Hard Loss 1.9722 (2.0319)	Arch Beta Loss 1267.5942 (1267.3940)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.1%, 98.3%)	
11/22 11:38:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.6030 (0.5480)	Arch Loss -12677.8408 (-12673.8856)	Arch Hard Loss 2.0219 (2.0510)	Arch Beta Loss 1267.9862 (1267.5937)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.2%, 98.1%)	
11/22 11:39:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.7561 (0.5557)	Arch Loss -12681.3818 (-12675.8358)	Arch Hard Loss 2.2648 (2.0501)	Arch Beta Loss 1268.3646 (1267.7886)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.1%, 98.0%)	
11/22 11:39:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.5340 (0.5596)	Arch Loss -12684.9268 (-12677.5416)	Arch Hard Loss 2.0087 (2.0594)	Arch Beta Loss 1268.6936 (1267.9601)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.1%, 97.9%)	
11/22 11:39:50午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 83.0680%
11/22 11:39:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 3.5761	Prec@(1,5) (33.7%, 64.1%)
11/22 11:40:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 3.6355	Prec@(1,5) (32.9%, 63.4%)
11/22 11:40:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 3.6150	Prec@(1,5) (33.1%, 63.6%)
11/22 11:40:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 3.6219	Prec@(1,5) (33.0%, 63.4%)
11/22 11:40:20午後 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 33.0360%
11/22 11:40:20午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:40:21午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:41:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.6291 (0.5156)	Arch Loss -12688.4307 (-12686.6851)	Arch Hard Loss 2.0755 (2.0808)	Arch Beta Loss 1269.0505 (1268.8766)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.4%, 98.1%)	
11/22 11:42:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.3632 (0.5262)	Arch Loss -12691.9971 (-12688.4482)	Arch Hard Loss 1.9140 (2.0526)	Arch Beta Loss 1269.3911 (1269.0501)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.2%, 98.0%)	
11/22 11:43:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.3843 (0.5288)	Arch Loss -12695.0264 (-12690.1317)	Arch Hard Loss 2.1733 (2.0630)	Arch Beta Loss 1269.7200 (1269.2195)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.1%, 98.0%)	
11/22 11:43:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.4749 (0.5376)	Arch Loss -12697.5400 (-12691.6152)	Arch Hard Loss 2.5228 (2.0703)	Arch Beta Loss 1270.0062 (1269.3686)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.9%, 98.0%)	
11/22 11:43:52午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 83.9000%
11/22 11:44:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.3327	Prec@(1,5) (48.4%, 78.2%)
11/22 11:44:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.3226	Prec@(1,5) (48.4%, 78.3%)
11/22 11:44:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.3240	Prec@(1,5) (48.4%, 78.1%)
11/22 11:44:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.3399	Prec@(1,5) (48.2%, 77.8%)
11/22 11:44:22午後 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 48.2320%
11/22 11:44:23午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:44:23午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:45:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.4289 (0.5156)	Arch Loss -12701.5049 (-12699.5743)	Arch Hard Loss 1.6591 (2.0789)	Arch Beta Loss 1270.3164 (1270.1653)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.2%, 98.5%)	
11/22 11:46:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.4033 (0.5150)	Arch Loss -12704.3701 (-12701.0632)	Arch Hard Loss 1.7612 (2.0989)	Arch Beta Loss 1270.6130 (1270.3162)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.4%)	
11/22 11:47:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.5856 (0.5266)	Arch Loss -12707.1211 (-12702.5610)	Arch Hard Loss 1.8685 (2.0745)	Arch Beta Loss 1270.8989 (1270.4635)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.1%, 98.2%)	
11/22 11:47:54午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.6432 (0.5268)	Arch Loss -12709.5752 (-12703.8568)	Arch Hard Loss 1.9043 (2.0755)	Arch Beta Loss 1271.1479 (1270.5932)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.0%, 98.2%)	
11/22 11:47:55午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 83.9920%
11/22 11:48:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 7.1961	Prec@(1,5) (11.5%, 31.0%)
11/22 11:48:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 7.1932	Prec@(1,5) (11.5%, 31.1%)
11/22 11:48:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 7.2051	Prec@(1,5) (11.4%, 31.0%)
11/22 11:48:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 7.1796	Prec@(1,5) (11.6%, 31.1%)
11/22 11:48:25午後 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 11.6000%
11/22 11:48:25午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:48:26午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:49:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.5214 (0.5000)	Arch Loss -12712.0469 (-12710.8183)	Arch Hard Loss 2.1344 (2.0467)	Arch Beta Loss 1271.4182 (1271.2865)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.0%, 98.2%)	
11/22 11:50:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.4614 (0.5056)	Arch Loss -12714.3584 (-12712.0933)	Arch Hard Loss 2.4002 (2.0845)	Arch Beta Loss 1271.6759 (1271.4178)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.8%, 98.2%)	
11/22 11:51:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.5484 (0.5113)	Arch Loss -12717.4014 (-12713.3764)	Arch Hard Loss 1.8491 (2.0839)	Arch Beta Loss 1271.9250 (1271.5460)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.7%, 98.2%)	
11/22 11:51:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.4756 (0.5174)	Arch Loss -12719.4180 (-12714.5078)	Arch Hard Loss 1.9993 (2.0815)	Arch Beta Loss 1272.1417 (1271.6589)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.4%, 98.2%)	
11/22 11:51:58午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 84.4080%
11/22 11:52:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 4.9101	Prec@(1,5) (21.0%, 47.2%)
11/22 11:52:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 4.8697	Prec@(1,5) (21.6%, 48.2%)
11/22 11:52:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 4.8451	Prec@(1,5) (21.6%, 48.6%)
11/22 11:52:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 4.8600	Prec@(1,5) (21.3%, 48.4%)
11/22 11:52:28午後 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 21.2880%
11/22 11:52:29午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:52:29午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:53:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.5009 (0.5029)	Arch Loss -12721.6279 (-12720.5593)	Arch Hard Loss 2.1426 (2.0653)	Arch Beta Loss 1272.3771 (1272.2625)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.9%, 98.3%)	
11/22 11:54:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.3023 (0.5003)	Arch Loss -12724.1318 (-12721.6967)	Arch Hard Loss 1.8861 (2.0717)	Arch Beta Loss 1272.6018 (1272.3768)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.9%, 98.3%)	
11/22 11:55:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.4407 (0.5038)	Arch Loss -12726.0254 (-12722.8077)	Arch Hard Loss 2.1603 (2.0781)	Arch Beta Loss 1272.8186 (1272.4886)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.9%, 98.3%)	
11/22 11:56:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.6241 (0.5109)	Arch Loss -12728.5928 (-12723.7884)	Arch Hard Loss 1.4838 (2.0809)	Arch Beta Loss 1273.0076 (1272.5869)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.2%)	
11/22 11:56:00午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 84.5200%
11/22 11:56:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 8.2687	Prec@(1,5) (10.7%, 29.7%)
11/22 11:56:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 8.2179	Prec@(1,5) (10.8%, 30.1%)
11/22 11:56:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 8.2367	Prec@(1,5) (11.0%, 30.1%)
11/22 11:56:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 8.2140	Prec@(1,5) (11.1%, 30.0%)
11/22 11:56:31午後 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 11.1240%
11/22 11:56:31午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 11:56:31午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.3960%
11/22 11:56:31午後 trainer_runner.py:110 [INFO] Final best Prec@1 = 49.3960%
11/22 11:56:31午後 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('avg_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
