12/26 10:42:31PM parser.py:28 [INFO] 
12/26 10:42:31PM parser.py:29 [INFO] Parameters:
12/26 10:42:31PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g0.01/DAG
12/26 10:42:31PM parser.py:31 [INFO] T=10.0
12/26 10:42:31PM parser.py:31 [INFO] ADVANCED=1
12/26 10:42:31PM parser.py:31 [INFO] ALPHA_LR=0.0003
12/26 10:42:31PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
12/26 10:42:31PM parser.py:31 [INFO] ARCH_CRITERION=expected
12/26 10:42:31PM parser.py:31 [INFO] BATCH_SIZE=64
12/26 10:42:31PM parser.py:31 [INFO] CASCADE=0
12/26 10:42:31PM parser.py:31 [INFO] CHECKPOINT_RESET=False
12/26 10:42:31PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[30, 30]
12/26 10:42:31PM parser.py:31 [INFO] CUTOUT_LENGTH=0
12/26 10:42:31PM parser.py:31 [INFO] DATA_PATH=../data/
12/26 10:42:31PM parser.py:31 [INFO] DATASET=cifar100
12/26 10:42:31PM parser.py:31 [INFO] DEPTH_COEF=0.01
12/26 10:42:31PM parser.py:31 [INFO] DESCRIPTION=search_and_evaluation_curriculum_learning_with_-Beta-expected-Depth-ecpected-constraint_slidewindow-3
12/26 10:42:31PM parser.py:31 [INFO] DISCRETE=1
12/26 10:42:31PM parser.py:31 [INFO] EPOCHS=50
12/26 10:42:31PM parser.py:31 [INFO] EVAL_EPOCHS=100
12/26 10:42:31PM parser.py:31 [INFO] EXP_NAME=s0-depth-sw3-g0.01
12/26 10:42:31PM parser.py:31 [INFO] FINAL_L=0.0
12/26 10:42:31PM parser.py:31 [INFO] G=0.0
12/26 10:42:31PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
12/26 10:42:31PM parser.py:31 [INFO] GPUS=[0]
12/26 10:42:31PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
12/26 10:42:31PM parser.py:31 [INFO] INIT_CHANNELS=16
12/26 10:42:31PM parser.py:31 [INFO] L=0.0
12/26 10:42:31PM parser.py:31 [INFO] LAYERS=32
12/26 10:42:31PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
12/26 10:42:31PM parser.py:31 [INFO] NAME=Pruning
12/26 10:42:31PM parser.py:31 [INFO] NONKD=1
12/26 10:42:31PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g0.01
12/26 10:42:31PM parser.py:31 [INFO] PCDARTS=0
12/26 10:42:31PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-depth-sw3-g0.01/plots
12/26 10:42:31PM parser.py:31 [INFO] PRINT_FREQ=100
12/26 10:42:31PM parser.py:31 [INFO] RESET=0
12/26 10:42:31PM parser.py:31 [INFO] RESUME_PATH=None
12/26 10:42:31PM parser.py:31 [INFO] SAVE=s0-depth-sw3-g0.01
12/26 10:42:31PM parser.py:31 [INFO] SEED=0
12/26 10:42:31PM parser.py:31 [INFO] SHARE_STAGE=0
12/26 10:42:31PM parser.py:31 [INFO] SLIDE_WINDOW=3
12/26 10:42:31PM parser.py:31 [INFO] SPEC_CELL=1
12/26 10:42:31PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
12/26 10:42:31PM parser.py:31 [INFO] TEACHER_NAME=none
12/26 10:42:31PM parser.py:31 [INFO] TEACHER_PATH=none
12/26 10:42:31PM parser.py:31 [INFO] TRAIN_PORTION=0.5
12/26 10:42:31PM parser.py:31 [INFO] TYPE=Distribution
12/26 10:42:31PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
12/26 10:42:31PM parser.py:31 [INFO] W_LR=0.025
12/26 10:42:31PM parser.py:31 [INFO] W_LR_MIN=0.001
12/26 10:42:31PM parser.py:31 [INFO] W_MOMENTUM=0.9
12/26 10:42:31PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
12/26 10:42:31PM parser.py:31 [INFO] WORKERS=4
12/26 10:42:31PM parser.py:32 [INFO] 
12/26 10:42:34PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
12/26 10:43:17PM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.5094 (4.5167)	Arch Loss 4.1755 (4.5005)	Arch Hard Loss 4.1755 (4.5005)	Arch Beta Loss 2.8233 (2.8233)	Arch depth Loss -0.0282 (-0.0282)	Prec@(1,5) (2.2%, 9.5%)	
12/26 10:43:57PM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.1512 (4.4258)	Arch Loss 4.1913 (4.4149)	Arch Hard Loss 4.1913 (4.4149)	Arch Beta Loss 2.8247 (2.8236)	Arch depth Loss -0.0282 (-0.0282)	Prec@(1,5) (3.1%, 12.6%)	
12/26 10:44:38PM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0545 (4.3439)	Arch Loss 4.2459 (4.3302)	Arch Hard Loss 4.2459 (4.3302)	Arch Beta Loss 2.8269 (2.8242)	Arch depth Loss -0.0283 (-0.0282)	Prec@(1,5) (3.7%, 15.0%)	
12/26 10:45:14PM searchDistribution_trainer.py:152 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 4.0286 (4.2827)	Arch Loss 4.1396 (4.2690)	Arch Hard Loss 4.1396 (4.2690)	Arch Beta Loss 2.8317 (2.8254)	Arch depth Loss -0.0283 (-0.0283)	Prec@(1,5) (4.2%, 17.0%)	
12/26 10:45:15PM searchDistribution_trainer.py:166 [INFO] Train: [  0/49] Final Prec@1 4.2520%
12/26 10:45:22PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9705	Prec@(1,5) (7.7%, 26.7%)
12/26 10:45:28PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9829	Prec@(1,5) (7.3%, 25.9%)
12/26 10:45:34PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9839	Prec@(1,5) (7.3%, 25.9%)
12/26 10:45:39PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9847	Prec@(1,5) (7.3%, 25.8%)
12/26 10:45:39PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 7.3360%
12/26 10:45:39PM trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=range(10, 12), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=range(10, 12))
12/26 10:45:40PM trainer_runner.py:108 [INFO] Until now, best Prec@1 = 7.3360%
12/26 10:46:23午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.9890 (3.9944)	Arch Loss 3.8054 (3.9957)	Arch Hard Loss 3.8054 (3.9957)	Arch Beta Loss 2.8395 (2.8360)	Arch depth Loss -0.0284 (-0.0284)	Prec@(1,5) (7.3%, 25.8%)	
12/26 10:47:05午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.9975 (3.9653)	Arch Loss 3.8957 (3.9494)	Arch Hard Loss 3.8957 (3.9494)	Arch Beta Loss 2.8483 (2.8401)	Arch depth Loss -0.0285 (-0.0284)	Prec@(1,5) (7.5%, 26.9%)	
12/26 10:47:47午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.9330 (3.9306)	Arch Loss 3.8364 (3.9207)	Arch Hard Loss 3.8364 (3.9207)	Arch Beta Loss 2.8575 (2.8444)	Arch depth Loss -0.0286 (-0.0284)	Prec@(1,5) (8.1%, 28.1%)	
12/26 10:48:25午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.6900 (3.9053)	Arch Loss 3.9945 (3.8767)	Arch Hard Loss 3.9945 (3.8767)	Arch Beta Loss 2.8658 (2.8484)	Arch depth Loss -0.0287 (-0.0285)	Prec@(1,5) (8.4%, 28.8%)	
12/26 10:48:25午後 searchDistribution_trainer.py:166 [INFO] Train: [  1/49] Final Prec@1 8.3960%
12/26 10:48:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.8023	Prec@(1,5) (10.9%, 32.7%)
12/26 10:48:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.7781	Prec@(1,5) (11.1%, 33.6%)
12/26 10:48:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.7783	Prec@(1,5) (11.2%, 33.6%)
12/26 10:48:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.7822	Prec@(1,5) (11.1%, 33.5%)
12/26 10:48:49午後 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 11.0680%
12/26 10:48:49午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=range(10, 12), DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 10:48:49午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 11.0680%
12/26 10:49:32午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.8353 (3.7082)	Arch Loss 4.1124 (3.7182)	Arch Hard Loss 4.1124 (3.7182)	Arch Beta Loss 2.8753 (2.8707)	Arch depth Loss -0.0288 (-0.0287)	Prec@(1,5) (11.1%, 35.2%)	
12/26 10:50:14午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.6314 (3.6977)	Arch Loss 3.6950 (3.6950)	Arch Hard Loss 3.6950 (3.6950)	Arch Beta Loss 2.8849 (2.8754)	Arch depth Loss -0.0288 (-0.0288)	Prec@(1,5) (11.6%, 35.4%)	
12/26 10:50:56午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5963 (3.6807)	Arch Loss 3.5638 (3.6603)	Arch Hard Loss 3.5638 (3.6603)	Arch Beta Loss 2.8936 (2.8800)	Arch depth Loss -0.0289 (-0.0288)	Prec@(1,5) (11.7%, 36.1%)	
12/26 10:51:33午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.5285 (3.6572)	Arch Loss 3.5083 (3.6372)	Arch Hard Loss 3.5083 (3.6372)	Arch Beta Loss 2.9019 (2.8841)	Arch depth Loss -0.0290 (-0.0288)	Prec@(1,5) (12.2%, 36.8%)	
12/26 10:51:34午後 searchDistribution_trainer.py:166 [INFO] Train: [  2/49] Final Prec@1 12.1680%
12/26 10:51:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.5952	Prec@(1,5) (13.8%, 40.1%)
12/26 10:51:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.5977	Prec@(1,5) (13.6%, 40.0%)
12/26 10:51:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.6056	Prec@(1,5) (13.6%, 40.0%)
12/26 10:51:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.6017	Prec@(1,5) (13.9%, 40.0%)
12/26 10:51:57午後 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 13.9000%
12/26 10:51:57午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(10, 12), DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 10:51:58午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 13.9000%
12/26 10:52:40午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.4018 (3.4926)	Arch Loss 3.3054 (3.5098)	Arch Hard Loss 3.3054 (3.5098)	Arch Beta Loss 2.9114 (2.9069)	Arch depth Loss -0.0291 (-0.0291)	Prec@(1,5) (14.8%, 41.5%)	
12/26 10:53:21午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.2847 (3.4855)	Arch Loss 3.5931 (3.4859)	Arch Hard Loss 3.5931 (3.4859)	Arch Beta Loss 2.9200 (2.9113)	Arch depth Loss -0.0292 (-0.0291)	Prec@(1,5) (15.5%, 42.3%)	
12/26 10:54:02午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.6493 (3.4793)	Arch Loss 3.5892 (3.4610)	Arch Hard Loss 3.5892 (3.4610)	Arch Beta Loss 2.9288 (2.9157)	Arch depth Loss -0.0293 (-0.0292)	Prec@(1,5) (15.5%, 42.5%)	
12/26 10:54:39午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.1047 (3.4566)	Arch Loss 3.3044 (3.4464)	Arch Hard Loss 3.3044 (3.4464)	Arch Beta Loss 2.9363 (2.9196)	Arch depth Loss -0.0294 (-0.0292)	Prec@(1,5) (16.0%, 43.2%)	
12/26 10:54:40午後 searchDistribution_trainer.py:166 [INFO] Train: [  3/49] Final Prec@1 15.9720%
12/26 10:54:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.4160	Prec@(1,5) (17.3%, 44.2%)
12/26 10:54:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.4229	Prec@(1,5) (17.1%, 43.9%)
12/26 10:54:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.4171	Prec@(1,5) (16.8%, 44.2%)
12/26 10:55:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.4234	Prec@(1,5) (16.8%, 43.9%)
12/26 10:55:04午後 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 16.8080%
12/26 10:55:04午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=range(10, 12), DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=range(9, 11))
12/26 10:55:04午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 16.8080%
12/26 10:55:47午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.1976 (3.3272)	Arch Loss 3.0435 (3.3704)	Arch Hard Loss 3.0435 (3.3704)	Arch Beta Loss 2.9442 (2.9404)	Arch depth Loss -0.0294 (-0.0294)	Prec@(1,5) (17.8%, 47.1%)	
12/26 10:56:29午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 2.9244 (3.3196)	Arch Loss 3.1613 (3.3388)	Arch Hard Loss 3.1613 (3.3388)	Arch Beta Loss 2.9516 (2.9442)	Arch depth Loss -0.0295 (-0.0294)	Prec@(1,5) (18.1%, 46.7%)	
12/26 10:57:10午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 3.4182 (3.3055)	Arch Loss 3.1450 (3.3252)	Arch Hard Loss 3.1450 (3.3252)	Arch Beta Loss 2.9590 (2.9479)	Arch depth Loss -0.0296 (-0.0295)	Prec@(1,5) (18.6%, 47.2%)	
12/26 10:57:48午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.3118 (3.2931)	Arch Loss 3.1102 (3.3045)	Arch Hard Loss 3.1102 (3.3045)	Arch Beta Loss 2.9658 (2.9512)	Arch depth Loss -0.0297 (-0.0295)	Prec@(1,5) (18.9%, 47.5%)	
12/26 10:57:48午後 searchDistribution_trainer.py:166 [INFO] Train: [  4/49] Final Prec@1 18.9200%
12/26 10:57:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.2481	Prec@(1,5) (20.7%, 49.0%)
12/26 10:58:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.2145	Prec@(1,5) (20.9%, 49.8%)
12/26 10:58:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.2229	Prec@(1,5) (20.4%, 49.7%)
12/26 10:58:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.2157	Prec@(1,5) (20.4%, 50.0%)
12/26 10:58:12午後 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 20.4240%
12/26 10:58:12午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(10, 12), DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 10:58:13午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 20.4240%
12/26 10:58:55午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.2532 (3.1581)	Arch Loss 3.2701 (3.2187)	Arch Hard Loss 3.2701 (3.2187)	Arch Beta Loss 2.9726 (2.9694)	Arch depth Loss -0.0297 (-0.0297)	Prec@(1,5) (21.1%, 51.3%)	
12/26 10:59:37午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.1561 (3.1581)	Arch Loss 3.1561 (3.2085)	Arch Hard Loss 3.1561 (3.2085)	Arch Beta Loss 2.9792 (2.9726)	Arch depth Loss -0.0298 (-0.0297)	Prec@(1,5) (21.0%, 51.5%)	
12/26 11:00:19午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 3.5977 (3.1549)	Arch Loss 3.0625 (3.1890)	Arch Hard Loss 3.0625 (3.1890)	Arch Beta Loss 2.9856 (2.9759)	Arch depth Loss -0.0299 (-0.0298)	Prec@(1,5) (21.0%, 51.4%)	
12/26 11:00:56午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.9440 (3.1445)	Arch Loss 3.0976 (3.1810)	Arch Hard Loss 3.0976 (3.1810)	Arch Beta Loss 2.9911 (2.9788)	Arch depth Loss -0.0299 (-0.0298)	Prec@(1,5) (21.1%, 51.5%)	
12/26 11:00:57午後 searchDistribution_trainer.py:166 [INFO] Train: [  5/49] Final Prec@1 21.1360%
12/26 11:01:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 3.0838	Prec@(1,5) (22.5%, 52.5%)
12/26 11:01:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 3.1013	Prec@(1,5) (22.3%, 52.3%)
12/26 11:01:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 3.1048	Prec@(1,5) (22.3%, 52.4%)
12/26 11:01:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 3.1089	Prec@(1,5) (22.2%, 52.4%)
12/26 11:01:21午後 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 22.2440%
12/26 11:01:21午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 11:01:22午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 22.2440%
12/26 11:02:05午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.7591 (3.0170)	Arch Loss 2.8912 (3.1045)	Arch Hard Loss 2.8912 (3.1045)	Arch Beta Loss 2.9975 (2.9946)	Arch depth Loss -0.0300 (-0.0299)	Prec@(1,5) (23.4%, 54.7%)	
12/26 11:02:46午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.8083 (3.0292)	Arch Loss 2.7571 (3.0705)	Arch Hard Loss 2.7571 (3.0705)	Arch Beta Loss 3.0032 (2.9973)	Arch depth Loss -0.0300 (-0.0300)	Prec@(1,5) (23.7%, 54.5%)	
12/26 11:03:28午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.1576 (3.0171)	Arch Loss 2.9110 (3.0652)	Arch Hard Loss 2.9110 (3.0652)	Arch Beta Loss 3.0091 (3.0003)	Arch depth Loss -0.0301 (-0.0300)	Prec@(1,5) (24.0%, 54.9%)	
12/26 11:04:05午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 3.0384 (3.0198)	Arch Loss 3.1134 (3.0624)	Arch Hard Loss 3.1134 (3.0624)	Arch Beta Loss 3.0124 (3.0027)	Arch depth Loss -0.0301 (-0.0300)	Prec@(1,5) (23.9%, 54.8%)	
12/26 11:04:06午後 searchDistribution_trainer.py:166 [INFO] Train: [  6/49] Final Prec@1 23.8600%
12/26 11:04:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 3.0343	Prec@(1,5) (23.8%, 54.3%)
12/26 11:04:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 3.0282	Prec@(1,5) (23.5%, 54.9%)
12/26 11:04:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 3.0385	Prec@(1,5) (23.5%, 54.6%)
12/26 11:04:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 3.0404	Prec@(1,5) (23.6%, 54.5%)
12/26 11:04:30午後 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 23.6320%
12/26 11:04:30午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 11:04:31午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 23.6320%
12/26 11:05:14午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.9473 (2.9158)	Arch Loss 2.9703 (2.9916)	Arch Hard Loss 2.9703 (2.9916)	Arch Beta Loss 3.0178 (3.0151)	Arch depth Loss -0.0302 (-0.0302)	Prec@(1,5) (26.1%, 57.5%)	
12/26 11:05:56午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.8522 (2.9016)	Arch Loss 2.8875 (2.9865)	Arch Hard Loss 2.8875 (2.9865)	Arch Beta Loss 3.0228 (3.0175)	Arch depth Loss -0.0302 (-0.0302)	Prec@(1,5) (26.4%, 57.9%)	
12/26 11:06:38午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 3.0776 (2.9017)	Arch Loss 2.8796 (2.9788)	Arch Hard Loss 2.8796 (2.9788)	Arch Beta Loss 3.0287 (3.0203)	Arch depth Loss -0.0303 (-0.0302)	Prec@(1,5) (26.3%, 57.9%)	
12/26 11:07:15午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.6917 (2.8967)	Arch Loss 2.5988 (2.9669)	Arch Hard Loss 2.5988 (2.9669)	Arch Beta Loss 3.0322 (3.0226)	Arch depth Loss -0.0303 (-0.0302)	Prec@(1,5) (26.4%, 58.1%)	
12/26 11:07:16午後 searchDistribution_trainer.py:166 [INFO] Train: [  7/49] Final Prec@1 26.3840%
12/26 11:07:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.8858	Prec@(1,5) (27.3%, 58.8%)
12/26 11:07:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.8906	Prec@(1,5) (27.5%, 58.5%)
12/26 11:07:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.9013	Prec@(1,5) (27.2%, 58.0%)
12/26 11:07:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.9129	Prec@(1,5) (26.9%, 57.8%)
12/26 11:07:40午後 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 26.9400%
12/26 11:07:40午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 11:07:41午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 26.9400%
12/26 11:08:23午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.8683 (2.7909)	Arch Loss 2.8065 (2.9440)	Arch Hard Loss 2.8065 (2.9440)	Arch Beta Loss 3.0365 (3.0342)	Arch depth Loss -0.0304 (-0.0303)	Prec@(1,5) (28.4%, 60.2%)	
12/26 11:09:03午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.7656 (2.7958)	Arch Loss 2.6116 (2.9268)	Arch Hard Loss 2.6116 (2.9268)	Arch Beta Loss 3.0409 (3.0363)	Arch depth Loss -0.0304 (-0.0304)	Prec@(1,5) (28.6%, 60.1%)	
12/26 11:09:45午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.7447 (2.7972)	Arch Loss 3.2382 (2.8998)	Arch Hard Loss 3.2382 (2.8998)	Arch Beta Loss 3.0455 (3.0385)	Arch depth Loss -0.0305 (-0.0304)	Prec@(1,5) (28.4%, 60.5%)	
12/26 11:10:22午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.6542 (2.7986)	Arch Loss 3.1085 (2.8882)	Arch Hard Loss 3.1085 (2.8882)	Arch Beta Loss 3.0485 (3.0405)	Arch depth Loss -0.0305 (-0.0304)	Prec@(1,5) (28.2%, 60.6%)	
12/26 11:10:23午後 searchDistribution_trainer.py:166 [INFO] Train: [  8/49] Final Prec@1 28.2240%
12/26 11:10:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.8289	Prec@(1,5) (27.6%, 60.0%)
12/26 11:10:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.8285	Prec@(1,5) (27.7%, 59.9%)
12/26 11:10:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.8169	Prec@(1,5) (28.1%, 60.4%)
12/26 11:10:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.8181	Prec@(1,5) (28.2%, 60.4%)
12/26 11:10:46午後 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 28.2360%
12/26 11:10:46午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 11:10:47午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 28.2360%
12/26 11:11:30午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.7120 (2.7121)	Arch Loss 2.8527 (2.8287)	Arch Hard Loss 2.8527 (2.8287)	Arch Beta Loss 3.0533 (3.0510)	Arch depth Loss -0.0305 (-0.0305)	Prec@(1,5) (30.9%, 62.3%)	
12/26 11:12:13午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.7191 (2.7167)	Arch Loss 3.2240 (2.8335)	Arch Hard Loss 3.2240 (2.8335)	Arch Beta Loss 3.0563 (3.0529)	Arch depth Loss -0.0306 (-0.0305)	Prec@(1,5) (30.3%, 62.1%)	
12/26 11:12:55午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.6667 (2.7091)	Arch Loss 2.8550 (2.8210)	Arch Hard Loss 2.8550 (2.8210)	Arch Beta Loss 3.0595 (3.0546)	Arch depth Loss -0.0306 (-0.0305)	Prec@(1,5) (30.1%, 62.4%)	
12/26 11:13:32午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.5584 (2.7050)	Arch Loss 3.0634 (2.8156)	Arch Hard Loss 3.0634 (2.8156)	Arch Beta Loss 3.0628 (3.0561)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (30.2%, 62.5%)	
12/26 11:13:33午後 searchDistribution_trainer.py:166 [INFO] Train: [  9/49] Final Prec@1 30.1880%
12/26 11:13:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.8088	Prec@(1,5) (28.8%, 60.8%)
12/26 11:13:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.7820	Prec@(1,5) (28.8%, 61.1%)
12/26 11:13:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.7838	Prec@(1,5) (29.0%, 61.0%)
12/26 11:13:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.7757	Prec@(1,5) (29.0%, 61.2%)
12/26 11:13:58午後 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 28.9440%
12/26 11:13:58午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=range(9, 11))
12/26 11:13:58午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 28.9440%
12/26 11:14:41午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.8787 (2.6128)	Arch Loss 2.8136 (2.7568)	Arch Hard Loss 2.8136 (2.7568)	Arch Beta Loss 3.0648 (3.0639)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (31.8%, 64.2%)	
12/26 11:15:24午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.6448 (2.6086)	Arch Loss 2.6698 (2.7388)	Arch Hard Loss 2.6698 (2.7388)	Arch Beta Loss 3.0664 (3.0648)	Arch depth Loss -0.0307 (-0.0306)	Prec@(1,5) (32.2%, 64.8%)	
12/26 11:16:06午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.3702 (2.6228)	Arch Loss 3.0537 (2.7470)	Arch Hard Loss 3.0537 (2.7470)	Arch Beta Loss 3.0703 (3.0661)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (31.8%, 64.5%)	
12/26 11:16:44午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.5112 (2.6165)	Arch Loss 2.8861 (2.7341)	Arch Hard Loss 2.8861 (2.7341)	Arch Beta Loss 3.0737 (3.0675)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (31.8%, 64.7%)	
12/26 11:16:44午後 searchDistribution_trainer.py:166 [INFO] Train: [ 10/49] Final Prec@1 31.8280%
12/26 11:16:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.6997	Prec@(1,5) (31.3%, 63.1%)
12/26 11:16:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.7148	Prec@(1,5) (31.1%, 62.7%)
12/26 11:17:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.7190	Prec@(1,5) (30.8%, 62.5%)
12/26 11:17:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.7166	Prec@(1,5) (30.9%, 62.5%)
12/26 11:17:09午後 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 30.9200%
12/26 11:17:09午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/26 11:17:10午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 30.9200%
12/26 11:17:53午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.7458 (2.5130)	Arch Loss 2.5186 (2.7066)	Arch Hard Loss 2.5186 (2.7066)	Arch Beta Loss 3.0767 (3.0754)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (33.7%, 67.0%)	
12/26 11:18:34午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.4866 (2.5184)	Arch Loss 2.7161 (2.6739)	Arch Hard Loss 2.7161 (2.6739)	Arch Beta Loss 3.0787 (3.0767)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (33.9%, 66.5%)	
12/26 11:19:15午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.6586 (2.5136)	Arch Loss 2.5517 (2.6720)	Arch Hard Loss 2.5517 (2.6720)	Arch Beta Loss 3.0807 (3.0777)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (34.0%, 67.0%)	
12/26 11:19:53午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.4842 (2.5184)	Arch Loss 2.4725 (2.6661)	Arch Hard Loss 2.4725 (2.6661)	Arch Beta Loss 3.0822 (3.0785)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (34.0%, 66.8%)	
12/26 11:19:53午後 searchDistribution_trainer.py:166 [INFO] Train: [ 11/49] Final Prec@1 33.9840%
12/26 11:20:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.6978	Prec@(1,5) (31.7%, 62.8%)
12/26 11:20:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.6938	Prec@(1,5) (31.7%, 62.8%)
12/26 11:20:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.7010	Prec@(1,5) (31.2%, 62.8%)
12/26 11:20:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.7087	Prec@(1,5) (31.1%, 62.6%)
12/26 11:20:17午後 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 31.0920%
12/26 11:20:18午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/26 11:20:18午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 31.0920%
12/26 11:21:01午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.2177 (2.4214)	Arch Loss 2.7376 (2.6303)	Arch Hard Loss 2.7376 (2.6303)	Arch Beta Loss 3.0849 (3.0834)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (35.8%, 69.3%)	
12/26 11:21:43午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.3687 (2.4303)	Arch Loss 2.5963 (2.6347)	Arch Hard Loss 2.5963 (2.6347)	Arch Beta Loss 3.0854 (3.0845)	Arch depth Loss -0.0309 (-0.0308)	Prec@(1,5) (36.0%, 68.9%)	
12/26 11:22:25午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.7569 (2.4500)	Arch Loss 2.5683 (2.6314)	Arch Hard Loss 2.5683 (2.6314)	Arch Beta Loss 3.0862 (3.0849)	Arch depth Loss -0.0309 (-0.0308)	Prec@(1,5) (35.6%, 68.4%)	
12/26 11:23:03午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 2.6746 (2.4432)	Arch Loss 2.7919 (2.6151)	Arch Hard Loss 2.7919 (2.6151)	Arch Beta Loss 3.0875 (3.0852)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (35.6%, 68.6%)	
12/26 11:23:03午後 searchDistribution_trainer.py:166 [INFO] Train: [ 12/49] Final Prec@1 35.6240%
12/26 11:23:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.6217	Prec@(1,5) (32.6%, 65.0%)
12/26 11:23:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.6256	Prec@(1,5) (32.3%, 65.1%)
12/26 11:23:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.6270	Prec@(1,5) (32.5%, 64.8%)
12/26 11:23:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.6237	Prec@(1,5) (32.4%, 64.8%)
12/26 11:23:28午後 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 32.4440%
12/26 11:23:28午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/26 11:23:28午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 32.4440%
12/26 11:24:11午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.2200 (2.3477)	Arch Loss 2.1202 (2.5613)	Arch Hard Loss 2.1202 (2.5613)	Arch Beta Loss 3.0894 (3.0885)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (38.1%, 70.5%)	
12/26 11:24:53午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.5156 (2.3621)	Arch Loss 2.6555 (2.5532)	Arch Hard Loss 2.6555 (2.5532)	Arch Beta Loss 3.0906 (3.0892)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (37.6%, 70.4%)	
12/26 11:25:35午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 2.2391 (2.3628)	Arch Loss 2.7441 (2.5518)	Arch Hard Loss 2.7441 (2.5518)	Arch Beta Loss 3.0922 (3.0898)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (37.4%, 70.4%)	
12/26 11:26:12午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.9444 (2.3609)	Arch Loss 2.5203 (2.5467)	Arch Hard Loss 2.5203 (2.5467)	Arch Beta Loss 3.0946 (3.0905)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (37.4%, 70.5%)	
12/26 11:26:13午後 searchDistribution_trainer.py:166 [INFO] Train: [ 13/49] Final Prec@1 37.4120%
12/26 11:26:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.5963	Prec@(1,5) (33.4%, 65.6%)
12/26 11:26:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.5872	Prec@(1,5) (33.8%, 66.2%)
12/26 11:26:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.5934	Prec@(1,5) (33.8%, 66.0%)
12/26 11:26:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.5907	Prec@(1,5) (33.9%, 66.1%)
12/26 11:26:36午後 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 33.9240%
12/26 11:26:36午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/26 11:26:37午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 33.9240%
12/26 11:27:20午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.3219 (2.2462)	Arch Loss 2.3413 (2.5224)	Arch Hard Loss 2.3413 (2.5224)	Arch Beta Loss 3.0949 (3.0948)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (39.1%, 72.7%)	
12/26 11:28:02午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.0727 (2.2744)	Arch Loss 2.5319 (2.5079)	Arch Hard Loss 2.5319 (2.5079)	Arch Beta Loss 3.0955 (3.0952)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (38.6%, 72.2%)	
12/26 11:28:44午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.9900 (2.2798)	Arch Loss 2.5777 (2.5115)	Arch Hard Loss 2.5777 (2.5115)	Arch Beta Loss 3.0962 (3.0953)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (38.4%, 72.0%)	
12/26 11:29:20午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.1071 (2.2800)	Arch Loss 2.6685 (2.5095)	Arch Hard Loss 2.6685 (2.5095)	Arch Beta Loss 3.0966 (3.0954)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (38.7%, 72.0%)	
12/26 11:29:21午後 searchDistribution_trainer.py:166 [INFO] Train: [ 14/49] Final Prec@1 38.7040%
12/26 11:29:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.5160	Prec@(1,5) (35.1%, 67.6%)
12/26 11:29:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.4955	Prec@(1,5) (35.4%, 67.5%)
12/26 11:29:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.5008	Prec@(1,5) (35.3%, 67.4%)
12/26 11:29:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.4956	Prec@(1,5) (35.3%, 67.7%)
12/26 11:29:46午後 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 35.3520%
12/26 11:29:46午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:29:46午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 35.3520%
12/26 11:30:28午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.2726 (2.1800)	Arch Loss 2.4954 (2.4603)	Arch Hard Loss 2.4954 (2.4603)	Arch Beta Loss 3.0973 (3.0969)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (40.5%, 74.5%)	
12/26 11:31:10午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.7442 (2.2059)	Arch Loss 2.6780 (2.4521)	Arch Hard Loss 2.6780 (2.4521)	Arch Beta Loss 3.0945 (3.0965)	Arch depth Loss -0.0309 (-0.0310)	Prec@(1,5) (40.4%, 73.6%)	
12/26 11:31:51午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.9900 (2.2098)	Arch Loss 2.5333 (2.4449)	Arch Hard Loss 2.5333 (2.4449)	Arch Beta Loss 3.0951 (3.0958)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (40.3%, 73.5%)	
12/26 11:32:29午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 2.5298 (2.2089)	Arch Loss 2.2594 (2.4415)	Arch Hard Loss 2.2594 (2.4415)	Arch Beta Loss 3.0941 (3.0955)	Arch depth Loss -0.0309 (-0.0310)	Prec@(1,5) (40.5%, 73.6%)	
12/26 11:32:30午後 searchDistribution_trainer.py:166 [INFO] Train: [ 15/49] Final Prec@1 40.4800%
12/26 11:32:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.4981	Prec@(1,5) (36.3%, 68.0%)
12/26 11:32:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.4865	Prec@(1,5) (36.4%, 68.1%)
12/26 11:32:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.4868	Prec@(1,5) (36.0%, 68.2%)
12/26 11:32:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.4963	Prec@(1,5) (35.9%, 68.1%)
12/26 11:32:53午後 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 35.8520%
12/26 11:32:53午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:32:54午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 35.8520%
12/26 11:33:37午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 2.0533 (2.1045)	Arch Loss 2.6334 (2.4349)	Arch Hard Loss 2.6334 (2.4349)	Arch Beta Loss 3.0931 (3.0936)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (43.5%, 75.3%)	
12/26 11:34:19午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.9529 (2.1249)	Arch Loss 2.5535 (2.4275)	Arch Hard Loss 2.5535 (2.4275)	Arch Beta Loss 3.0924 (3.0931)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (42.6%, 75.4%)	
12/26 11:35:00午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 2.3046 (2.1426)	Arch Loss 2.5146 (2.4121)	Arch Hard Loss 2.5146 (2.4121)	Arch Beta Loss 3.0927 (3.0930)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (42.1%, 75.1%)	
12/26 11:35:37午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9318 (2.1398)	Arch Loss 2.5996 (2.4053)	Arch Hard Loss 2.5996 (2.4053)	Arch Beta Loss 3.0931 (3.0930)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (42.2%, 75.2%)	
12/26 11:35:38午後 searchDistribution_trainer.py:166 [INFO] Train: [ 16/49] Final Prec@1 42.2080%
12/26 11:35:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.3852	Prec@(1,5) (37.8%, 70.2%)
12/26 11:35:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.3953	Prec@(1,5) (37.5%, 70.0%)
12/26 11:35:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.3797	Prec@(1,5) (37.9%, 70.3%)
12/26 11:36:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.3798	Prec@(1,5) (37.8%, 70.3%)
12/26 11:36:01午後 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 37.7920%
12/26 11:36:01午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:36:02午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 37.7920%
12/26 11:36:45午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 2.0815 (2.0218)	Arch Loss 2.4544 (2.3634)	Arch Hard Loss 2.4544 (2.3634)	Arch Beta Loss 3.0912 (3.0919)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (46.0%, 77.4%)	
12/26 11:37:26午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 2.1366 (2.0451)	Arch Loss 2.1997 (2.3815)	Arch Hard Loss 2.1997 (2.3815)	Arch Beta Loss 3.0920 (3.0920)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (44.2%, 77.1%)	
12/26 11:38:08午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 2.1875 (2.0661)	Arch Loss 2.2756 (2.3557)	Arch Hard Loss 2.2756 (2.3557)	Arch Beta Loss 3.0936 (3.0921)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (43.7%, 76.7%)	
12/26 11:38:45午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 2.2082 (2.0593)	Arch Loss 2.4026 (2.3522)	Arch Hard Loss 2.4026 (2.3522)	Arch Beta Loss 3.0929 (3.0924)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (43.9%, 76.6%)	
12/26 11:38:46午後 searchDistribution_trainer.py:166 [INFO] Train: [ 17/49] Final Prec@1 43.8680%
12/26 11:38:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.3661	Prec@(1,5) (37.5%, 70.2%)
12/26 11:38:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.3557	Prec@(1,5) (37.8%, 70.5%)
12/26 11:39:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.3582	Prec@(1,5) (37.9%, 70.6%)
12/26 11:39:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.3602	Prec@(1,5) (37.7%, 70.7%)
12/26 11:39:10午後 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 37.7360%
12/26 11:39:10午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:39:10午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 37.7920%
12/26 11:39:53午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 2.0039 (2.0217)	Arch Loss 2.2140 (2.3170)	Arch Hard Loss 2.2140 (2.3170)	Arch Beta Loss 3.0926 (3.0926)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (43.3%, 77.7%)	
12/26 11:40:35午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.9824 (1.9836)	Arch Loss 2.1371 (2.3351)	Arch Hard Loss 2.1371 (2.3351)	Arch Beta Loss 3.0914 (3.0923)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (44.7%, 78.1%)	
12/26 11:41:17午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.9188 (1.9788)	Arch Loss 2.2995 (2.3309)	Arch Hard Loss 2.2995 (2.3309)	Arch Beta Loss 3.0912 (3.0921)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (45.4%, 78.2%)	
12/26 11:41:55午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 2.0579 (1.9849)	Arch Loss 2.1135 (2.3171)	Arch Hard Loss 2.1135 (2.3171)	Arch Beta Loss 3.0908 (3.0918)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (45.2%, 78.1%)	
12/26 11:41:56午後 searchDistribution_trainer.py:166 [INFO] Train: [ 18/49] Final Prec@1 45.2280%
12/26 11:42:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.4124	Prec@(1,5) (37.4%, 70.5%)
12/26 11:42:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.4459	Prec@(1,5) (36.9%, 69.7%)
12/26 11:42:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.4333	Prec@(1,5) (37.0%, 69.7%)
12/26 11:42:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.4433	Prec@(1,5) (36.9%, 69.5%)
12/26 11:42:20午後 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 36.8320%
12/26 11:42:20午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:42:20午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 37.7920%
12/26 11:43:04午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.7929 (1.8733)	Arch Loss 2.0259 (2.2865)	Arch Hard Loss 2.0259 (2.2865)	Arch Beta Loss 3.0889 (3.0908)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (48.1%, 80.0%)	
12/26 11:43:45午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.4379 (1.8909)	Arch Loss 2.1811 (2.2713)	Arch Hard Loss 2.1811 (2.2713)	Arch Beta Loss 3.0886 (3.0900)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (47.9%, 79.9%)	
12/26 11:44:28午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.3707 (1.9172)	Arch Loss 2.0993 (2.2767)	Arch Hard Loss 2.0993 (2.2767)	Arch Beta Loss 3.0878 (3.0894)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (47.3%, 79.2%)	
12/26 11:45:06午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.8535 (1.9236)	Arch Loss 2.6546 (2.2757)	Arch Hard Loss 2.6546 (2.2757)	Arch Beta Loss 3.0875 (3.0889)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (46.9%, 79.2%)	
12/26 11:45:06午後 searchDistribution_trainer.py:166 [INFO] Train: [ 19/49] Final Prec@1 46.8400%
12/26 11:45:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.2798	Prec@(1,5) (40.1%, 72.2%)
12/26 11:45:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.2819	Prec@(1,5) (40.3%, 72.2%)
12/26 11:45:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.2859	Prec@(1,5) (40.1%, 72.1%)
12/26 11:45:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.2946	Prec@(1,5) (40.0%, 72.0%)
12/26 11:45:30午後 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 40.0360%
12/26 11:45:30午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:45:31午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 40.0360%
12/26 11:46:13午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.6416 (1.7653)	Arch Loss 2.2414 (2.2460)	Arch Hard Loss 2.2414 (2.2460)	Arch Beta Loss 3.0866 (3.0871)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (50.8%, 82.6%)	
12/26 11:46:55午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 2.2558 (1.8200)	Arch Loss 1.6605 (2.2374)	Arch Hard Loss 1.6605 (2.2374)	Arch Beta Loss 3.0859 (3.0868)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (49.2%, 81.4%)	
12/26 11:47:37午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 2.0792 (1.8361)	Arch Loss 2.0501 (2.2327)	Arch Hard Loss 2.0501 (2.2327)	Arch Beta Loss 3.0851 (3.0862)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (48.9%, 80.9%)	
12/26 11:48:14午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.9254 (1.8507)	Arch Loss 2.0262 (2.2319)	Arch Hard Loss 2.0262 (2.2319)	Arch Beta Loss 3.0851 (3.0860)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (48.5%, 80.6%)	
12/26 11:48:15午後 searchDistribution_trainer.py:166 [INFO] Train: [ 20/49] Final Prec@1 48.5360%
12/26 11:48:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.2428	Prec@(1,5) (41.2%, 72.9%)
12/26 11:48:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.2841	Prec@(1,5) (40.0%, 72.3%)
12/26 11:48:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.2914	Prec@(1,5) (40.1%, 72.6%)
12/26 11:48:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.2904	Prec@(1,5) (40.0%, 72.6%)
12/26 11:48:39午後 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 40.0320%
12/26 11:48:39午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:48:39午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 40.0360%
12/26 11:49:22午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.8317 (1.7886)	Arch Loss 2.4792 (2.2321)	Arch Hard Loss 2.4792 (2.2321)	Arch Beta Loss 3.0837 (3.0847)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (49.2%, 81.7%)	
12/26 11:50:04午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 2.3733 (1.7781)	Arch Loss 2.3982 (2.2152)	Arch Hard Loss 2.3982 (2.2152)	Arch Beta Loss 3.0844 (3.0844)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (49.7%, 81.9%)	
12/26 11:50:45午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.8101 (1.7932)	Arch Loss 2.4406 (2.2161)	Arch Hard Loss 2.4406 (2.2161)	Arch Beta Loss 3.0839 (3.0843)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (49.7%, 81.8%)	
12/26 11:51:23午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.7173 (1.8021)	Arch Loss 2.3101 (2.2148)	Arch Hard Loss 2.3101 (2.2148)	Arch Beta Loss 3.0818 (3.0839)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (49.7%, 81.7%)	
12/26 11:51:24午後 searchDistribution_trainer.py:166 [INFO] Train: [ 21/49] Final Prec@1 49.6920%
12/26 11:51:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.2368	Prec@(1,5) (41.1%, 73.3%)
12/26 11:51:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.2206	Prec@(1,5) (41.4%, 73.4%)
12/26 11:51:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.2311	Prec@(1,5) (41.3%, 73.2%)
12/26 11:51:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.2407	Prec@(1,5) (41.1%, 73.1%)
12/26 11:51:48午後 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 41.1080%
12/26 11:51:48午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:51:49午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 41.1080%
12/26 11:52:32午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 2.1267 (1.6624)	Arch Loss 2.0256 (2.2056)	Arch Hard Loss 2.0256 (2.2056)	Arch Beta Loss 3.0817 (3.0817)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (52.6%, 84.2%)	
12/26 11:53:13午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.4205 (1.7062)	Arch Loss 1.8807 (2.2035)	Arch Hard Loss 1.8807 (2.2035)	Arch Beta Loss 3.0819 (3.0817)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (51.5%, 83.4%)	
12/26 11:53:55午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.8490 (1.7184)	Arch Loss 2.2329 (2.1795)	Arch Hard Loss 2.2329 (2.1795)	Arch Beta Loss 3.0805 (3.0816)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (51.5%, 82.9%)	
12/26 11:54:32午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.8565 (1.7329)	Arch Loss 2.0427 (2.1730)	Arch Hard Loss 2.0427 (2.1730)	Arch Beta Loss 3.0783 (3.0810)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (51.4%, 82.5%)	
12/26 11:54:33午後 searchDistribution_trainer.py:166 [INFO] Train: [ 22/49] Final Prec@1 51.3640%
12/26 11:54:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.1375	Prec@(1,5) (44.1%, 75.5%)
12/26 11:54:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.1570	Prec@(1,5) (43.8%, 75.0%)
12/26 11:54:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.1648	Prec@(1,5) (43.6%, 75.0%)
12/26 11:54:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.1617	Prec@(1,5) (43.6%, 75.1%)
12/26 11:54:57午後 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 43.5800%
12/26 11:54:57午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG3_concat=range(9, 11))
12/26 11:54:57午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.5800%
12/26 11:55:40午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.6492 (1.6156)	Arch Loss 1.8685 (2.1381)	Arch Hard Loss 1.8685 (2.1381)	Arch Beta Loss 3.0767 (3.0769)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (53.4%, 85.0%)	
12/26 11:56:21午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.8529 (1.6561)	Arch Loss 2.2899 (2.1529)	Arch Hard Loss 2.2899 (2.1529)	Arch Beta Loss 3.0758 (3.0766)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (52.9%, 84.0%)	
12/26 11:57:03午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 2.1646 (1.6737)	Arch Loss 2.0768 (2.1556)	Arch Hard Loss 2.0768 (2.1556)	Arch Beta Loss 3.0756 (3.0763)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (52.5%, 83.9%)	
12/26 11:57:40午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.7224 (1.6882)	Arch Loss 2.3692 (2.1406)	Arch Hard Loss 2.3692 (2.1406)	Arch Beta Loss 3.0764 (3.0762)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (52.4%, 83.6%)	
12/26 11:57:40午後 searchDistribution_trainer.py:166 [INFO] Train: [ 23/49] Final Prec@1 52.4040%
12/26 11:57:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.1674	Prec@(1,5) (42.9%, 74.7%)
12/26 11:57:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.1711	Prec@(1,5) (42.7%, 74.7%)
12/26 11:57:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.1700	Prec@(1,5) (43.1%, 74.9%)
12/26 11:58:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.1643	Prec@(1,5) (43.1%, 75.0%)
12/26 11:58:04午後 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 43.0720%
12/26 11:58:04午後 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=range(9, 11))
12/26 11:58:05午後 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.5800%
12/26 11:58:47午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.3787 (1.5651)	Arch Loss 2.0317 (2.1364)	Arch Hard Loss 2.0317 (2.1364)	Arch Beta Loss 3.0764 (3.0770)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (55.1%, 85.8%)	
12/26 11:59:29午後 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.7793 (1.5867)	Arch Loss 2.1555 (2.1376)	Arch Hard Loss 2.1555 (2.1376)	Arch Beta Loss 3.0753 (3.0765)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (54.9%, 85.3%)	
12/27 12:00:11午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 2.0138 (1.6099)	Arch Loss 1.9746 (2.1391)	Arch Hard Loss 1.9746 (2.1391)	Arch Beta Loss 3.0736 (3.0757)	Arch depth Loss -0.0307 (-0.0308)	Prec@(1,5) (54.2%, 85.0%)	
12/27 12:00:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.8420 (1.6207)	Arch Loss 2.3293 (2.1413)	Arch Hard Loss 2.3293 (2.1413)	Arch Beta Loss 3.0750 (3.0753)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (54.1%, 84.7%)	
12/27 12:00:49午前 searchDistribution_trainer.py:166 [INFO] Train: [ 24/49] Final Prec@1 54.1520%
12/27 12:00:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.4549	Prec@(1,5) (38.4%, 69.7%)
12/27 12:01:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.4523	Prec@(1,5) (38.0%, 69.9%)
12/27 12:01:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.4578	Prec@(1,5) (37.8%, 69.9%)
12/27 12:01:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.4663	Prec@(1,5) (37.6%, 69.8%)
12/27 12:01:13午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 37.6400%
12/27 12:01:13午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=range(9, 11))
12/27 12:01:14午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 43.5800%
12/27 12:01:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.3466 (1.5063)	Arch Loss 1.9738 (2.1335)	Arch Hard Loss 1.9738 (2.1335)	Arch Beta Loss 3.0748 (3.0748)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (56.6%, 86.2%)	
12/27 12:02:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.7600 (1.5452)	Arch Loss 1.7708 (2.1269)	Arch Hard Loss 1.7708 (2.1269)	Arch Beta Loss 3.0749 (3.0750)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (56.0%, 85.8%)	
12/27 12:03:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.5180 (1.5567)	Arch Loss 1.9043 (2.1032)	Arch Hard Loss 1.9043 (2.1032)	Arch Beta Loss 3.0748 (3.0750)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (55.8%, 85.3%)	
12/27 12:03:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.6929 (1.5636)	Arch Loss 2.4769 (2.1131)	Arch Hard Loss 2.4769 (2.1131)	Arch Beta Loss 3.0746 (3.0748)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (55.5%, 85.4%)	
12/27 12:03:58午前 searchDistribution_trainer.py:166 [INFO] Train: [ 25/49] Final Prec@1 55.4640%
12/27 12:04:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.1449	Prec@(1,5) (43.8%, 75.7%)
12/27 12:04:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.1390	Prec@(1,5) (44.2%, 75.7%)
12/27 12:04:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.1485	Prec@(1,5) (44.1%, 75.5%)
12/27 12:04:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.1536	Prec@(1,5) (44.4%, 75.3%)
12/27 12:04:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 44.3800%
12/27 12:04:22午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=range(9, 11))
12/27 12:04:23午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:05:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.7068 (1.4898)	Arch Loss 2.1486 (2.0850)	Arch Hard Loss 2.1486 (2.0850)	Arch Beta Loss 3.0749 (3.0750)	Arch depth Loss -0.0307 (-0.0308)	Prec@(1,5) (56.7%, 87.3%)	
12/27 12:05:47午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.4480 (1.4885)	Arch Loss 2.3358 (2.1168)	Arch Hard Loss 2.3358 (2.1168)	Arch Beta Loss 3.0742 (3.0749)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (56.9%, 87.2%)	
12/27 12:06:28午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.2863 (1.5007)	Arch Loss 1.9510 (2.0884)	Arch Hard Loss 1.9510 (2.0884)	Arch Beta Loss 3.0738 (3.0746)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (56.7%, 86.7%)	
12/27 12:07:05午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.6881 (1.5183)	Arch Loss 2.0480 (2.0918)	Arch Hard Loss 2.0480 (2.0918)	Arch Beta Loss 3.0724 (3.0743)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (56.3%, 86.4%)	
12/27 12:07:06午前 searchDistribution_trainer.py:166 [INFO] Train: [ 26/49] Final Prec@1 56.2880%
12/27 12:07:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.3159	Prec@(1,5) (41.0%, 72.7%)
12/27 12:07:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.3045	Prec@(1,5) (41.5%, 72.7%)
12/27 12:07:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.2966	Prec@(1,5) (41.5%, 73.0%)
12/27 12:07:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.2964	Prec@(1,5) (41.6%, 72.8%)
12/27 12:07:30午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 41.6000%
12/27 12:07:30午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=range(9, 11))
12/27 12:07:30午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:08:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.5892 (1.3594)	Arch Loss 1.9070 (2.0713)	Arch Hard Loss 1.9070 (2.0713)	Arch Beta Loss 3.0712 (3.0717)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (59.7%, 88.8%)	
12/27 12:08:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.5201 (1.3995)	Arch Loss 2.2450 (2.0819)	Arch Hard Loss 2.2450 (2.0819)	Arch Beta Loss 3.0709 (3.0716)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (59.1%, 88.1%)	
12/27 12:09:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.7650 (1.4301)	Arch Loss 1.6465 (2.0631)	Arch Hard Loss 1.6465 (2.0631)	Arch Beta Loss 3.0708 (3.0713)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (58.8%, 87.6%)	
12/27 12:10:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.3771 (1.4501)	Arch Loss 2.4311 (2.0688)	Arch Hard Loss 2.4311 (2.0688)	Arch Beta Loss 3.0700 (3.0711)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (58.4%, 87.3%)	
12/27 12:10:13午前 searchDistribution_trainer.py:166 [INFO] Train: [ 27/49] Final Prec@1 58.3960%
12/27 12:10:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.3219	Prec@(1,5) (40.9%, 73.0%)
12/27 12:10:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.3387	Prec@(1,5) (40.3%, 72.8%)
12/27 12:10:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.3443	Prec@(1,5) (40.5%, 72.8%)
12/27 12:10:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.3368	Prec@(1,5) (40.7%, 72.8%)
12/27 12:10:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 40.6720%
12/27 12:10:38午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=range(9, 11))
12/27 12:10:39午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:11:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.4660 (1.3640)	Arch Loss 2.1087 (2.0659)	Arch Hard Loss 2.1087 (2.0659)	Arch Beta Loss 3.0704 (3.0698)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (61.0%, 88.7%)	
12/27 12:12:02午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.6159 (1.3816)	Arch Loss 2.0159 (2.0666)	Arch Hard Loss 2.0159 (2.0666)	Arch Beta Loss 3.0697 (3.0699)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (60.3%, 88.3%)	
12/27 12:12:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.3928 (1.3961)	Arch Loss 2.0026 (2.0740)	Arch Hard Loss 2.0026 (2.0740)	Arch Beta Loss 3.0700 (3.0699)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (59.9%, 88.2%)	
12/27 12:13:19午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.9930 (1.4055)	Arch Loss 1.8220 (2.0556)	Arch Hard Loss 1.8220 (2.0556)	Arch Beta Loss 3.0697 (3.0698)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (59.6%, 88.0%)	
12/27 12:13:19午前 searchDistribution_trainer.py:166 [INFO] Train: [ 28/49] Final Prec@1 59.5880%
12/27 12:13:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.3510	Prec@(1,5) (42.7%, 72.4%)
12/27 12:13:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.3385	Prec@(1,5) (42.1%, 73.1%)
12/27 12:13:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.3247	Prec@(1,5) (42.2%, 73.2%)
12/27 12:13:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.3252	Prec@(1,5) (42.2%, 73.3%)
12/27 12:13:42午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 42.2280%
12/27 12:13:42午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 12:13:43午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:14:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.1544 (1.2759)	Arch Loss 1.7746 (2.0412)	Arch Hard Loss 1.7746 (2.0412)	Arch Beta Loss 3.0690 (3.0697)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (62.2%, 89.9%)	
12/27 12:15:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.0857 (1.2967)	Arch Loss 1.4768 (2.0212)	Arch Hard Loss 1.4768 (2.0212)	Arch Beta Loss 3.0685 (3.0694)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (61.7%, 89.8%)	
12/27 12:15:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.3765 (1.3241)	Arch Loss 2.5611 (2.0217)	Arch Hard Loss 2.5611 (2.0217)	Arch Beta Loss 3.0671 (3.0689)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (61.2%, 89.2%)	
12/27 12:16:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.4201 (1.3389)	Arch Loss 1.9546 (2.0344)	Arch Hard Loss 1.9546 (2.0344)	Arch Beta Loss 3.0671 (3.0684)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (60.8%, 89.0%)	
12/27 12:16:26午前 searchDistribution_trainer.py:166 [INFO] Train: [ 29/49] Final Prec@1 60.8160%
12/27 12:16:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.4870	Prec@(1,5) (39.4%, 70.2%)
12/27 12:16:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.5195	Prec@(1,5) (39.0%, 69.7%)
12/27 12:16:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.5340	Prec@(1,5) (38.8%, 69.7%)
12/27 12:16:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.5309	Prec@(1,5) (38.6%, 69.8%)
12/27 12:16:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 38.5720%
12/27 12:16:49午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 12:16:50午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:17:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.3403 (1.2410)	Arch Loss 2.4599 (2.0765)	Arch Hard Loss 2.4599 (2.0765)	Arch Beta Loss 3.0657 (3.0666)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (63.3%, 90.3%)	
12/27 12:18:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.0882 (1.2630)	Arch Loss 2.4098 (2.0579)	Arch Hard Loss 2.4098 (2.0579)	Arch Beta Loss 3.0664 (3.0663)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (62.8%, 90.3%)	
12/27 12:18:56午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.0761 (1.2693)	Arch Loss 2.4480 (2.0632)	Arch Hard Loss 2.4480 (2.0632)	Arch Beta Loss 3.0650 (3.0661)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (63.0%, 90.2%)	
12/27 12:19:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.2860 (1.2900)	Arch Loss 1.9252 (2.0477)	Arch Hard Loss 1.9252 (2.0477)	Arch Beta Loss 3.0640 (3.0659)	Arch depth Loss -0.0306 (-0.0307)	Prec@(1,5) (62.4%, 89.9%)	
12/27 12:19:35午前 searchDistribution_trainer.py:166 [INFO] Train: [ 30/49] Final Prec@1 62.4120%
12/27 12:19:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.4435	Prec@(1,5) (39.9%, 71.3%)
12/27 12:19:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.4829	Prec@(1,5) (39.4%, 70.9%)
12/27 12:19:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.5003	Prec@(1,5) (39.1%, 70.5%)
12/27 12:19:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.5038	Prec@(1,5) (39.1%, 70.6%)
12/27 12:19:59午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 39.0840%
12/27 12:19:59午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 12:19:59午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:20:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.6059 (1.1751)	Arch Loss 1.8888 (1.9836)	Arch Hard Loss 1.8888 (1.9836)	Arch Beta Loss 3.0634 (3.0635)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (65.6%, 91.5%)	
12/27 12:21:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.2290 (1.1925)	Arch Loss 2.0463 (2.0273)	Arch Hard Loss 2.0463 (2.0273)	Arch Beta Loss 3.0638 (3.0634)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (65.1%, 91.3%)	
12/27 12:22:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.0508 (1.2137)	Arch Loss 2.0173 (2.0354)	Arch Hard Loss 2.0173 (2.0354)	Arch Beta Loss 3.0650 (3.0638)	Arch depth Loss -0.0307 (-0.0306)	Prec@(1,5) (64.6%, 90.8%)	
12/27 12:22:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.1069 (1.2348)	Arch Loss 2.1596 (2.0410)	Arch Hard Loss 2.1596 (2.0410)	Arch Beta Loss 3.0640 (3.0640)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (64.0%, 90.6%)	
12/27 12:22:45午前 searchDistribution_trainer.py:166 [INFO] Train: [ 31/49] Final Prec@1 64.0080%
12/27 12:22:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.6815	Prec@(1,5) (37.1%, 68.2%)
12/27 12:22:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.6690	Prec@(1,5) (37.6%, 68.4%)
12/27 12:23:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.6649	Prec@(1,5) (37.5%, 68.4%)
12/27 12:23:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.6691	Prec@(1,5) (37.6%, 68.3%)
12/27 12:23:10午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 37.5640%
12/27 12:23:10午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(10, 12), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 12:23:11午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:23:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.0171 (1.1316)	Arch Loss 2.0944 (1.9998)	Arch Hard Loss 2.0944 (1.9998)	Arch Beta Loss 3.0650 (3.0642)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (66.6%, 91.8%)	
12/27 12:24:36午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.3797 (1.1508)	Arch Loss 2.0348 (2.0307)	Arch Hard Loss 2.0348 (2.0307)	Arch Beta Loss 3.0640 (3.0644)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (66.1%, 91.5%)	
12/27 12:25:18午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.0238 (1.1801)	Arch Loss 2.3811 (2.0281)	Arch Hard Loss 2.3811 (2.0281)	Arch Beta Loss 3.0636 (3.0642)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (65.3%, 91.2%)	
12/27 12:25:56午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.0024 (1.1992)	Arch Loss 2.0191 (2.0268)	Arch Hard Loss 2.0191 (2.0268)	Arch Beta Loss 3.0632 (3.0640)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (64.8%, 90.9%)	
12/27 12:25:57午前 searchDistribution_trainer.py:166 [INFO] Train: [ 32/49] Final Prec@1 64.7960%
12/27 12:26:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.4231	Prec@(1,5) (41.7%, 72.0%)
12/27 12:26:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.4266	Prec@(1,5) (41.5%, 72.1%)
12/27 12:26:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.4301	Prec@(1,5) (41.6%, 72.0%)
12/27 12:26:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.4104	Prec@(1,5) (42.0%, 72.3%)
12/27 12:26:20午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 42.0480%
12/27 12:26:20午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=range(9, 11))
12/27 12:26:21午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:27:04午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.1608 (1.0810)	Arch Loss 2.5989 (1.9974)	Arch Hard Loss 2.5989 (1.9974)	Arch Beta Loss 3.0621 (3.0628)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (67.8%, 92.9%)	
12/27 12:27:46午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.7300 (1.1049)	Arch Loss 1.9671 (2.0349)	Arch Hard Loss 1.9671 (2.0349)	Arch Beta Loss 3.0636 (3.0627)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (67.3%, 92.2%)	
12/27 12:28:27午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.0599 (1.1216)	Arch Loss 1.5243 (2.0312)	Arch Hard Loss 1.5243 (2.0312)	Arch Beta Loss 3.0634 (3.0630)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (66.6%, 92.1%)	
12/27 12:29:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.3053 (1.1315)	Arch Loss 2.0343 (2.0252)	Arch Hard Loss 2.0343 (2.0252)	Arch Beta Loss 3.0632 (3.0629)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (66.2%, 91.9%)	
12/27 12:29:04午前 searchDistribution_trainer.py:166 [INFO] Train: [ 33/49] Final Prec@1 66.2240%
12/27 12:29:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.3624	Prec@(1,5) (42.7%, 72.9%)
12/27 12:29:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.3397	Prec@(1,5) (43.0%, 73.3%)
12/27 12:29:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.3332	Prec@(1,5) (43.2%, 73.4%)
12/27 12:29:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.3250	Prec@(1,5) (43.3%, 73.5%)
12/27 12:29:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 43.2680%
12/27 12:29:29午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:29:29午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 44.3800%
12/27 12:30:10午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 1.2390 (0.9956)	Arch Loss 1.8179 (1.9741)	Arch Hard Loss 1.8179 (1.9741)	Arch Beta Loss 3.0623 (3.0630)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (70.6%, 94.1%)	
12/27 12:30:51午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 1.1753 (1.0396)	Arch Loss 1.9086 (2.0156)	Arch Hard Loss 1.9086 (2.0156)	Arch Beta Loss 3.0623 (3.0623)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (69.2%, 93.3%)	
12/27 12:31:31午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.0831 (1.0568)	Arch Loss 1.6637 (2.0060)	Arch Hard Loss 1.6637 (2.0060)	Arch Beta Loss 3.0632 (3.0625)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (68.7%, 93.0%)	
12/27 12:32:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.4258 (1.0703)	Arch Loss 2.0047 (2.0039)	Arch Hard Loss 2.0047 (2.0039)	Arch Beta Loss 3.0631 (3.0627)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (68.1%, 92.8%)	
12/27 12:32:07午前 searchDistribution_trainer.py:166 [INFO] Train: [ 34/49] Final Prec@1 68.1520%
12/27 12:32:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.0637	Prec@(1,5) (48.1%, 78.3%)
12/27 12:32:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.0649	Prec@(1,5) (48.3%, 78.5%)
12/27 12:32:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.0633	Prec@(1,5) (48.6%, 78.4%)
12/27 12:32:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.0527	Prec@(1,5) (48.6%, 78.5%)
12/27 12:32:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 48.6040%
12/27 12:32:31午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:32:32午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.6040%
12/27 12:33:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.8978 (0.9579)	Arch Loss 2.0286 (2.0082)	Arch Hard Loss 2.0286 (2.0082)	Arch Beta Loss 3.0645 (3.0639)	Arch depth Loss -0.0306 (-0.0306)	Prec@(1,5) (71.4%, 93.9%)	
12/27 12:33:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.3128 (0.9899)	Arch Loss 1.8518 (2.0070)	Arch Hard Loss 1.8518 (2.0070)	Arch Beta Loss 3.0654 (3.0645)	Arch depth Loss -0.0307 (-0.0306)	Prec@(1,5) (70.6%, 93.7%)	
12/27 12:34:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.1388 (1.0053)	Arch Loss 2.3143 (2.0147)	Arch Hard Loss 2.3143 (2.0147)	Arch Beta Loss 3.0663 (3.0648)	Arch depth Loss -0.0307 (-0.0306)	Prec@(1,5) (70.2%, 93.4%)	
12/27 12:35:17午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.0692 (1.0125)	Arch Loss 1.8788 (2.0178)	Arch Hard Loss 1.8788 (2.0178)	Arch Beta Loss 3.0656 (3.0650)	Arch depth Loss -0.0307 (-0.0306)	Prec@(1,5) (69.9%, 93.3%)	
12/27 12:35:17午前 searchDistribution_trainer.py:166 [INFO] Train: [ 35/49] Final Prec@1 69.8720%
12/27 12:35:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.0655	Prec@(1,5) (48.0%, 78.8%)
12/27 12:35:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.0624	Prec@(1,5) (48.5%, 78.9%)
12/27 12:35:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.0754	Prec@(1,5) (48.4%, 78.7%)
12/27 12:35:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.0848	Prec@(1,5) (48.4%, 78.5%)
12/27 12:35:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 48.3760%
12/27 12:35:41午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:35:42午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.6040%
12/27 12:36:24午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.8011 (0.9207)	Arch Loss 1.5552 (2.0231)	Arch Hard Loss 1.5552 (2.0231)	Arch Beta Loss 3.0653 (3.0658)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (72.9%, 94.4%)	
12/27 12:37:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 1.2586 (0.9405)	Arch Loss 1.9699 (2.0253)	Arch Hard Loss 1.9699 (2.0253)	Arch Beta Loss 3.0662 (3.0656)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (72.1%, 94.2%)	
12/27 12:37:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.8620 (0.9544)	Arch Loss 2.0910 (1.9995)	Arch Hard Loss 2.0910 (1.9995)	Arch Beta Loss 3.0677 (3.0660)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (71.5%, 94.1%)	
12/27 12:38:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 1.1221 (0.9624)	Arch Loss 1.8995 (2.0046)	Arch Hard Loss 1.8995 (2.0046)	Arch Beta Loss 3.0682 (3.0665)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (71.3%, 94.1%)	
12/27 12:38:26午前 searchDistribution_trainer.py:166 [INFO] Train: [ 36/49] Final Prec@1 71.2760%
12/27 12:38:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.1709	Prec@(1,5) (47.9%, 77.2%)
12/27 12:38:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.1803	Prec@(1,5) (47.8%, 77.4%)
12/27 12:38:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.1848	Prec@(1,5) (47.7%, 77.2%)
12/27 12:38:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.1956	Prec@(1,5) (47.5%, 77.2%)
12/27 12:38:51午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 47.5080%
12/27 12:38:51午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:38:51午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.6040%
12/27 12:39:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.9834 (0.8756)	Arch Loss 1.6061 (2.0039)	Arch Hard Loss 1.6061 (2.0039)	Arch Beta Loss 3.0681 (3.0682)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (73.8%, 95.1%)	
12/27 12:40:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9305 (0.8927)	Arch Loss 2.1096 (2.0226)	Arch Hard Loss 2.1096 (2.0226)	Arch Beta Loss 3.0695 (3.0685)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (73.6%, 94.7%)	
12/27 12:40:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.9660 (0.9029)	Arch Loss 2.3604 (2.0208)	Arch Hard Loss 2.3604 (2.0208)	Arch Beta Loss 3.0720 (3.0693)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (73.1%, 94.7%)	
12/27 12:41:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 1.0596 (0.9118)	Arch Loss 1.8779 (2.0150)	Arch Hard Loss 1.8779 (2.0150)	Arch Beta Loss 3.0734 (3.0701)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (72.8%, 94.6%)	
12/27 12:41:36午前 searchDistribution_trainer.py:166 [INFO] Train: [ 37/49] Final Prec@1 72.8120%
12/27 12:41:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.4854	Prec@(1,5) (42.1%, 73.2%)
12/27 12:41:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.4248	Prec@(1,5) (43.5%, 74.2%)
12/27 12:41:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.4157	Prec@(1,5) (43.7%, 74.1%)
12/27 12:42:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.4106	Prec@(1,5) (43.7%, 74.2%)
12/27 12:42:00午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 43.6600%
12/27 12:42:00午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:42:00午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.6040%
12/27 12:42:43午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7026 (0.8235)	Arch Loss 1.7991 (2.0411)	Arch Hard Loss 1.7991 (2.0411)	Arch Beta Loss 3.0736 (3.0733)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (74.9%, 96.0%)	
12/27 12:43:25午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 1.1664 (0.8541)	Arch Loss 1.9300 (2.0113)	Arch Hard Loss 1.9300 (2.0113)	Arch Beta Loss 3.0734 (3.0734)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (74.3%, 95.5%)	
12/27 12:44:07午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.9136 (0.8536)	Arch Loss 2.1365 (2.0227)	Arch Hard Loss 2.1365 (2.0227)	Arch Beta Loss 3.0739 (3.0733)	Arch depth Loss -0.0307 (-0.0307)	Prec@(1,5) (74.3%, 95.4%)	
12/27 12:44:44午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.8088 (0.8592)	Arch Loss 2.2571 (2.0221)	Arch Hard Loss 2.2571 (2.0221)	Arch Beta Loss 3.0763 (3.0737)	Arch depth Loss -0.0308 (-0.0307)	Prec@(1,5) (74.0%, 95.4%)	
12/27 12:44:45午前 searchDistribution_trainer.py:166 [INFO] Train: [ 38/49] Final Prec@1 73.9720%
12/27 12:44:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.1354	Prec@(1,5) (48.6%, 77.9%)
12/27 12:44:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.1115	Prec@(1,5) (48.5%, 78.5%)
12/27 12:45:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.1041	Prec@(1,5) (48.9%, 78.7%)
12/27 12:45:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.1153	Prec@(1,5) (48.9%, 78.5%)
12/27 12:45:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 48.9120%
12/27 12:45:09午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:45:09午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 48.9120%
12/27 12:45:52午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.8930 (0.7776)	Arch Loss 1.6578 (2.0669)	Arch Hard Loss 1.6578 (2.0669)	Arch Beta Loss 3.0789 (3.0780)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (77.1%, 96.3%)	
12/27 12:46:34午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.7158 (0.7907)	Arch Loss 1.9802 (2.0354)	Arch Hard Loss 1.9802 (2.0354)	Arch Beta Loss 3.0809 (3.0791)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (76.3%, 96.1%)	
12/27 12:47:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.9726 (0.8001)	Arch Loss 1.7772 (2.0237)	Arch Hard Loss 1.7772 (2.0237)	Arch Beta Loss 3.0806 (3.0796)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (76.1%, 96.0%)	
12/27 12:47:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.5434 (0.8160)	Arch Loss 1.9786 (2.0294)	Arch Hard Loss 1.9786 (2.0294)	Arch Beta Loss 3.0817 (3.0799)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (75.6%, 95.8%)	
12/27 12:47:55午前 searchDistribution_trainer.py:166 [INFO] Train: [ 39/49] Final Prec@1 75.5520%
12/27 12:48:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.0577	Prec@(1,5) (49.9%, 79.2%)
12/27 12:48:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.1003	Prec@(1,5) (49.5%, 78.7%)
12/27 12:48:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.1017	Prec@(1,5) (49.5%, 78.8%)
12/27 12:48:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.0947	Prec@(1,5) (49.7%, 79.0%)
12/27 12:48:20午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 49.6600%
12/27 12:48:20午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:48:20午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 49.6600%
12/27 12:49:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.6616 (0.7154)	Arch Loss 2.2758 (1.9896)	Arch Hard Loss 2.2758 (1.9896)	Arch Beta Loss 3.0836 (3.0824)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (78.6%, 97.3%)	
12/27 12:49:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.6033 (0.7425)	Arch Loss 2.0907 (2.0225)	Arch Hard Loss 2.0907 (2.0225)	Arch Beta Loss 3.0850 (3.0832)	Arch depth Loss -0.0308 (-0.0308)	Prec@(1,5) (77.7%, 96.7%)	
12/27 12:50:26午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.8861 (0.7595)	Arch Loss 1.8851 (2.0218)	Arch Hard Loss 1.8851 (2.0218)	Arch Beta Loss 3.0851 (3.0837)	Arch depth Loss -0.0309 (-0.0308)	Prec@(1,5) (77.4%, 96.4%)	
12/27 12:51:04午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.6291 (0.7657)	Arch Loss 2.0211 (2.0170)	Arch Hard Loss 2.0211 (2.0170)	Arch Beta Loss 3.0868 (3.0842)	Arch depth Loss -0.0309 (-0.0308)	Prec@(1,5) (77.2%, 96.3%)	
12/27 12:51:04午前 searchDistribution_trainer.py:166 [INFO] Train: [ 40/49] Final Prec@1 77.1680%
12/27 12:51:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 2.1004	Prec@(1,5) (49.6%, 78.7%)
12/27 12:51:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 2.0729	Prec@(1,5) (49.9%, 79.3%)
12/27 12:51:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 2.0732	Prec@(1,5) (49.9%, 79.3%)
12/27 12:51:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 2.0892	Prec@(1,5) (49.5%, 79.1%)
12/27 12:51:28午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 49.5280%
12/27 12:51:28午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:51:29午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 49.6600%
12/27 12:52:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.6980 (0.6957)	Arch Loss 1.5949 (2.0232)	Arch Hard Loss 1.5949 (2.0232)	Arch Beta Loss 3.0878 (3.0876)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (79.6%, 96.9%)	
12/27 12:52:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.7177 (0.7134)	Arch Loss 1.9271 (2.0315)	Arch Hard Loss 1.9271 (2.0315)	Arch Beta Loss 3.0878 (3.0877)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (79.0%, 96.8%)	
12/27 12:53:35午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.6380 (0.7209)	Arch Loss 1.8871 (2.0385)	Arch Hard Loss 1.8871 (2.0385)	Arch Beta Loss 3.0911 (3.0883)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (78.5%, 96.8%)	
12/27 12:54:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.6691 (0.7273)	Arch Loss 1.8515 (2.0306)	Arch Hard Loss 1.8515 (2.0306)	Arch Beta Loss 3.0918 (3.0889)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (78.1%, 96.8%)	
12/27 12:54:14午前 searchDistribution_trainer.py:166 [INFO] Train: [ 41/49] Final Prec@1 78.0880%
12/27 12:54:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.0945	Prec@(1,5) (49.4%, 78.5%)
12/27 12:54:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.0956	Prec@(1,5) (49.1%, 78.7%)
12/27 12:54:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.0846	Prec@(1,5) (49.5%, 78.8%)
12/27 12:54:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.0977	Prec@(1,5) (49.4%, 78.6%)
12/27 12:54:37午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 49.4440%
12/27 12:54:37午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:54:38午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 49.6600%
12/27 12:55:21午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.8530 (0.6536)	Arch Loss 2.0606 (1.9978)	Arch Hard Loss 2.0606 (1.9978)	Arch Beta Loss 3.0913 (3.0921)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (80.8%, 97.3%)	
12/27 12:56:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.8264 (0.6602)	Arch Loss 1.9392 (2.0298)	Arch Hard Loss 1.9392 (2.0298)	Arch Beta Loss 3.0932 (3.0920)	Arch depth Loss -0.0309 (-0.0309)	Prec@(1,5) (80.5%, 97.3%)	
12/27 12:56:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.8058 (0.6775)	Arch Loss 1.7396 (2.0215)	Arch Hard Loss 1.7396 (2.0215)	Arch Beta Loss 3.0953 (3.0930)	Arch depth Loss -0.0310 (-0.0309)	Prec@(1,5) (79.8%, 97.1%)	
12/27 12:57:23午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.7424 (0.6854)	Arch Loss 1.7376 (2.0284)	Arch Hard Loss 1.7376 (2.0284)	Arch Beta Loss 3.0970 (3.0937)	Arch depth Loss -0.0310 (-0.0309)	Prec@(1,5) (79.6%, 97.1%)	
12/27 12:57:23午前 searchDistribution_trainer.py:166 [INFO] Train: [ 42/49] Final Prec@1 79.5320%
12/27 12:57:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.1125	Prec@(1,5) (49.7%, 78.7%)
12/27 12:57:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.0793	Prec@(1,5) (50.3%, 79.1%)
12/27 12:57:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.0575	Prec@(1,5) (50.5%, 79.5%)
12/27 12:57:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.0562	Prec@(1,5) (50.4%, 79.5%)
12/27 12:57:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 50.3680%
12/27 12:57:49午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 12:57:49午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.3680%
12/27 12:58:33午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.4909 (0.6425)	Arch Loss 2.1893 (2.0025)	Arch Hard Loss 2.1893 (2.0025)	Arch Beta Loss 3.0979 (3.0973)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (80.9%, 97.5%)	
12/27 12:59:13午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.5568 (0.6508)	Arch Loss 2.5313 (2.0343)	Arch Hard Loss 2.5313 (2.0343)	Arch Beta Loss 3.0995 (3.0980)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (80.7%, 97.4%)	
12/27 12:59:54午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.7033 (0.6546)	Arch Loss 2.2752 (2.0280)	Arch Hard Loss 2.2752 (2.0280)	Arch Beta Loss 3.0999 (3.0987)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (80.5%, 97.4%)	
12/27 01:00:32午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.4228 (0.6616)	Arch Loss 1.7810 (2.0359)	Arch Hard Loss 1.7810 (2.0359)	Arch Beta Loss 3.0992 (3.0989)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (80.3%, 97.3%)	
12/27 01:00:32午前 searchDistribution_trainer.py:166 [INFO] Train: [ 43/49] Final Prec@1 80.3000%
12/27 01:00:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 2.0656	Prec@(1,5) (50.6%, 79.7%)
12/27 01:00:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.0554	Prec@(1,5) (50.4%, 79.5%)
12/27 01:00:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.0676	Prec@(1,5) (50.3%, 79.3%)
12/27 01:00:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.0620	Prec@(1,5) (50.6%, 79.4%)
12/27 01:00:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 50.5840%
12/27 01:00:56午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:00:57午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 50.5840%
12/27 01:01:40午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.7379 (0.5985)	Arch Loss 1.7823 (2.0308)	Arch Hard Loss 1.7823 (2.0308)	Arch Beta Loss 3.1003 (3.1002)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (82.6%, 97.6%)	
12/27 01:02:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.5162 (0.6091)	Arch Loss 2.2745 (2.0283)	Arch Hard Loss 2.2745 (2.0283)	Arch Beta Loss 3.1008 (3.1004)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (81.9%, 97.7%)	
12/27 01:03:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.7065 (0.6176)	Arch Loss 1.8603 (2.0373)	Arch Hard Loss 1.8603 (2.0373)	Arch Beta Loss 3.1021 (3.1007)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (81.6%, 97.6%)	
12/27 01:03:41午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.7263 (0.6255)	Arch Loss 1.8831 (2.0367)	Arch Hard Loss 1.8831 (2.0367)	Arch Beta Loss 3.1034 (3.1011)	Arch depth Loss -0.0310 (-0.0310)	Prec@(1,5) (81.5%, 97.5%)	
12/27 01:03:41午前 searchDistribution_trainer.py:166 [INFO] Train: [ 44/49] Final Prec@1 81.4440%
12/27 01:03:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.0162	Prec@(1,5) (51.4%, 80.6%)
12/27 01:03:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.0056	Prec@(1,5) (51.4%, 80.7%)
12/27 01:04:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.0108	Prec@(1,5) (51.4%, 80.3%)
12/27 01:04:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.0036	Prec@(1,5) (51.6%, 80.5%)
12/27 01:04:05午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 51.5720%
12/27 01:04:05午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:04:06午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.5720%
12/27 01:04:49午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.4282 (0.5867)	Arch Loss 2.4280 (2.0680)	Arch Hard Loss 2.4280 (2.0680)	Arch Beta Loss 3.1060 (3.1044)	Arch depth Loss -0.0311 (-0.0310)	Prec@(1,5) (83.2%, 97.9%)	
12/27 01:05:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.5068 (0.5924)	Arch Loss 1.6920 (2.0481)	Arch Hard Loss 1.6920 (2.0481)	Arch Beta Loss 3.1058 (3.1053)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (82.9%, 97.8%)	
12/27 01:06:12午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.5779 (0.5965)	Arch Loss 1.8644 (2.0445)	Arch Hard Loss 1.8644 (2.0445)	Arch Beta Loss 3.1077 (3.1056)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (82.6%, 97.8%)	
12/27 01:06:50午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.5370 (0.5983)	Arch Loss 1.8633 (2.0406)	Arch Hard Loss 1.8633 (2.0406)	Arch Beta Loss 3.1074 (3.1061)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (82.4%, 97.7%)	
12/27 01:06:51午前 searchDistribution_trainer.py:166 [INFO] Train: [ 45/49] Final Prec@1 82.4280%
12/27 01:06:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 2.0627	Prec@(1,5) (50.3%, 79.7%)
12/27 01:07:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 2.1102	Prec@(1,5) (49.9%, 79.0%)
12/27 01:07:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 2.1260	Prec@(1,5) (49.8%, 79.0%)
12/27 01:07:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.1594	Prec@(1,5) (49.2%, 78.6%)
12/27 01:07:14午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 49.2680%
12/27 01:07:14午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:07:15午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.5720%
12/27 01:07:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.4961 (0.5738)	Arch Loss 2.0503 (2.0198)	Arch Hard Loss 2.0503 (2.0198)	Arch Beta Loss 3.1081 (3.1076)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.4%, 97.9%)	
12/27 01:08:39午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.6192 (0.5760)	Arch Loss 2.0837 (2.0265)	Arch Hard Loss 2.0837 (2.0265)	Arch Beta Loss 3.1095 (3.1082)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.1%, 97.9%)	
12/27 01:09:20午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.5422 (0.5759)	Arch Loss 2.3651 (2.0483)	Arch Hard Loss 2.3651 (2.0483)	Arch Beta Loss 3.1102 (3.1087)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.1%, 97.9%)	
12/27 01:09:57午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.5493 (0.5766)	Arch Loss 2.0899 (2.0484)	Arch Hard Loss 2.0899 (2.0484)	Arch Beta Loss 3.1107 (3.1091)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.1%, 97.9%)	
12/27 01:09:58午前 searchDistribution_trainer.py:166 [INFO] Train: [ 46/49] Final Prec@1 83.0760%
12/27 01:10:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.0586	Prec@(1,5) (52.1%, 80.0%)
12/27 01:10:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.0464	Prec@(1,5) (52.1%, 80.1%)
12/27 01:10:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.0495	Prec@(1,5) (52.0%, 79.9%)
12/27 01:10:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.0536	Prec@(1,5) (51.6%, 80.1%)
12/27 01:10:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 51.5680%
12/27 01:10:22午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:10:22午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.5720%
12/27 01:11:06午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.6829 (0.5197)	Arch Loss 1.7194 (2.0441)	Arch Hard Loss 1.7194 (2.0441)	Arch Beta Loss 3.1108 (3.1108)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (85.2%, 98.6%)	
12/27 01:11:48午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.6800 (0.5472)	Arch Loss 2.8282 (2.0480)	Arch Hard Loss 2.8282 (2.0480)	Arch Beta Loss 3.1118 (3.1110)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.3%, 98.3%)	
12/27 01:12:30午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.5795 (0.5535)	Arch Loss 2.0575 (2.0548)	Arch Hard Loss 2.0575 (2.0548)	Arch Beta Loss 3.1124 (3.1113)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.9%, 98.2%)	
12/27 01:13:08午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.5226 (0.5618)	Arch Loss 1.6931 (2.0471)	Arch Hard Loss 1.6931 (2.0471)	Arch Beta Loss 3.1134 (3.1116)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.6%, 98.1%)	
12/27 01:13:08午前 searchDistribution_trainer.py:166 [INFO] Train: [ 47/49] Final Prec@1 83.6360%
12/27 01:13:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.1086	Prec@(1,5) (50.9%, 79.4%)
12/27 01:13:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.1224	Prec@(1,5) (50.5%, 79.3%)
12/27 01:13:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.1118	Prec@(1,5) (50.8%, 79.5%)
12/27 01:13:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.1144	Prec@(1,5) (50.7%, 79.5%)
12/27 01:13:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 50.6560%
12/27 01:13:32午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:13:33午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.5720%
12/27 01:14:16午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.3344 (0.5308)	Arch Loss 1.7252 (2.0470)	Arch Hard Loss 1.7252 (2.0470)	Arch Beta Loss 3.1146 (3.1142)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.7%, 98.2%)	
12/27 01:14:58午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.7870 (0.5502)	Arch Loss 1.9545 (2.0627)	Arch Hard Loss 1.9545 (2.0627)	Arch Beta Loss 3.1151 (3.1148)	Arch depth Loss -0.0312 (-0.0311)	Prec@(1,5) (83.9%, 98.0%)	
12/27 01:15:38午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.4290 (0.5471)	Arch Loss 2.1551 (2.0441)	Arch Hard Loss 2.1551 (2.0441)	Arch Beta Loss 3.1128 (3.1144)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.0%, 98.1%)	
12/27 01:16:15午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.5834 (0.5565)	Arch Loss 1.9846 (2.0474)	Arch Hard Loss 1.9846 (2.0474)	Arch Beta Loss 3.1139 (3.1141)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (83.8%, 98.0%)	
12/27 01:16:15午前 searchDistribution_trainer.py:166 [INFO] Train: [ 48/49] Final Prec@1 83.7680%
12/27 01:16:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.0722	Prec@(1,5) (51.0%, 79.1%)
12/27 01:16:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 2.0360	Prec@(1,5) (51.2%, 79.9%)
12/27 01:16:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 2.0257	Prec@(1,5) (51.4%, 80.0%)
12/27 01:16:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 2.0190	Prec@(1,5) (51.7%, 80.1%)
12/27 01:16:39午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 51.6880%
12/27 01:16:39午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:16:40午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.6880%
12/27 01:17:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.4239 (0.5252)	Arch Loss 2.4079 (2.0498)	Arch Hard Loss 2.4079 (2.0498)	Arch Beta Loss 3.1132 (3.1138)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.8%, 98.5%)	
12/27 01:18:03午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.4739 (0.5251)	Arch Loss 2.2134 (2.0723)	Arch Hard Loss 2.2134 (2.0723)	Arch Beta Loss 3.1140 (3.1138)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.7%, 98.5%)	
12/27 01:18:45午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.6563 (0.5378)	Arch Loss 1.8018 (2.0458)	Arch Hard Loss 1.8018 (2.0458)	Arch Beta Loss 3.1136 (3.1137)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.3%, 98.3%)	
12/27 01:19:22午前 searchDistribution_trainer.py:152 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.5141 (0.5421)	Arch Loss 1.8866 (2.0521)	Arch Hard Loss 1.8866 (2.0521)	Arch Beta Loss 3.1136 (3.1137)	Arch depth Loss -0.0311 (-0.0311)	Prec@(1,5) (84.3%, 98.2%)	
12/27 01:19:23午前 searchDistribution_trainer.py:166 [INFO] Train: [ 49/49] Final Prec@1 84.3240%
12/27 01:19:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0433	Prec@(1,5) (50.2%, 80.1%)
12/27 01:19:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.0110	Prec@(1,5) (51.2%, 80.5%)
12/27 01:19:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.0134	Prec@(1,5) (51.3%, 80.6%)
12/27 01:19:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.0182	Prec@(1,5) (51.5%, 80.6%)
12/27 01:19:48午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 51.5040%
12/27 01:19:48午前 trainer_runner.py:77 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
12/27 01:19:48午前 trainer_runner.py:108 [INFO] Until now, best Prec@1 = 51.6880%
12/27 01:19:48午前 trainer_runner.py:113 [INFO] Final best Prec@1 = 51.6880%
12/27 01:19:48午前 trainer_runner.py:114 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=range(9, 11), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=range(9, 11), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=range(9, 11))
