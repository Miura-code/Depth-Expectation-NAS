11/22 11:56:40PM parser.py:28 [INFO] 
11/22 11:56:40PM parser.py:29 [INFO] Parameters:
11/22 11:56:40PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-1/DAG
11/22 11:56:40PM parser.py:31 [INFO] T=10.0
11/22 11:56:40PM parser.py:31 [INFO] ADVANCED=1
11/22 11:56:40PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/22 11:56:40PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/22 11:56:40PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/22 11:56:40PM parser.py:31 [INFO] BATCH_SIZE=64
11/22 11:56:40PM parser.py:31 [INFO] CASCADE=0
11/22 11:56:40PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/22 11:56:40PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/22 11:56:40PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/22 11:56:40PM parser.py:31 [INFO] DATA_PATH=../data/
11/22 11:56:40PM parser.py:31 [INFO] DATASET=cifar100
11/22 11:56:40PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/22 11:56:40PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/22 11:56:40PM parser.py:31 [INFO] DISCRETE=0
11/22 11:56:40PM parser.py:31 [INFO] EPOCHS=50
11/22 11:56:40PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/22 11:56:40PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g-1
11/22 11:56:40PM parser.py:31 [INFO] FINAL_L=0.0
11/22 11:56:40PM parser.py:31 [INFO] G=-1.0
11/22 11:56:40PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/22 11:56:40PM parser.py:31 [INFO] GPUS=[0]
11/22 11:56:40PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/22 11:56:40PM parser.py:31 [INFO] INIT_CHANNELS=16
11/22 11:56:40PM parser.py:31 [INFO] L=0.0
11/22 11:56:40PM parser.py:31 [INFO] LAYERS=32
11/22 11:56:40PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/22 11:56:40PM parser.py:31 [INFO] NAME=Pruning
11/22 11:56:40PM parser.py:31 [INFO] NONKD=1
11/22 11:56:40PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-1
11/22 11:56:40PM parser.py:31 [INFO] PCDARTS=0
11/22 11:56:40PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-1/plots
11/22 11:56:40PM parser.py:31 [INFO] PRINT_FREQ=100
11/22 11:56:40PM parser.py:31 [INFO] RESET=0
11/22 11:56:40PM parser.py:31 [INFO] RESUME_PATH=None
11/22 11:56:40PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g-1
11/22 11:56:40PM parser.py:31 [INFO] SEED=0
11/22 11:56:40PM parser.py:31 [INFO] SHARE_STAGE=0
11/22 11:56:40PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/22 11:56:40PM parser.py:31 [INFO] SPEC_CELL=1
11/22 11:56:40PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/22 11:56:40PM parser.py:31 [INFO] TEACHER_NAME=none
11/22 11:56:40PM parser.py:31 [INFO] TEACHER_PATH=none
11/22 11:56:40PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/22 11:56:40PM parser.py:31 [INFO] TYPE=Pruning
11/22 11:56:40PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/22 11:56:40PM parser.py:31 [INFO] W_LR=0.025
11/22 11:56:40PM parser.py:31 [INFO] W_LR_MIN=0.001
11/22 11:56:40PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/22 11:56:40PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/22 11:56:40PM parser.py:31 [INFO] WORKERS=4
11/22 11:56:40PM parser.py:32 [INFO] 
11/22 11:56:41PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/22 11:57:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.4220 (4.5364)	Arch Loss -364.2997 (-359.9205)	Arch Hard Loss 4.4689 (4.5449)	Arch Beta Loss 368.7685 (364.4654)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.1%, 9.5%)	
11/22 11:58:30PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0653 (4.4065)	Arch Loss -373.2557 (-364.4307)	Arch Hard Loss 4.3238 (4.4052)	Arch Beta Loss 377.5794 (368.8359)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.4%, 13.3%)	
11/22 11:59:23PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 3.9920 (4.2986)	Arch Loss -382.4842 (-368.9482)	Arch Hard Loss 4.0468 (4.3051)	Arch Beta Loss 386.5311 (373.2533)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.3%, 16.5%)	
11/23 12:00:11AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.7969 (4.2347)	Arch Loss -390.8287 (-373.0332)	Arch Hard Loss 3.8810 (4.2363)	Arch Beta Loss 394.7097 (377.2694)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.9%, 18.6%)	
11/23 12:00:13AM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 4.9400%
11/23 12:00:21AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9550	Prec@(1,5) (7.5%, 27.7%)
11/23 12:00:29AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9461	Prec@(1,5) (7.9%, 28.2%)
11/23 12:00:36AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9470	Prec@(1,5) (7.8%, 27.9%)
11/23 12:00:44AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9483	Prec@(1,5) (7.8%, 27.6%)
11/23 12:00:44AM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 7.7760%
11/23 12:00:44AM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG2_concat=[5, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 2)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[4, 11])
11/23 12:00:44AM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 7.7760%
11/23 12:01:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 4.0149 (3.9370)	Arch Loss -399.8148 (-395.5206)	Arch Hard Loss 4.1942 (3.9209)	Arch Beta Loss 404.0090 (399.4415)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.0%, 28.0%)	
11/23 12:02:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.8045 (3.9011)	Arch Loss -409.3837 (-400.1918)	Arch Hard Loss 3.9370 (3.8809)	Arch Beta Loss 413.3207 (404.0727)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.7%, 29.8%)	
11/23 12:03:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7439 (3.8584)	Arch Loss -419.1915 (-404.8948)	Arch Hard Loss 3.5221 (3.8397)	Arch Beta Loss 422.7136 (408.7345)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.6%, 30.9%)	
11/23 12:04:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.7499 (3.8176)	Arch Loss -427.3074 (-409.1447)	Arch Hard Loss 3.9151 (3.8075)	Arch Beta Loss 431.2225 (412.9522)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.2%, 32.3%)	
11/23 12:04:15午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 10.2480%
11/23 12:04:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6674	Prec@(1,5) (12.7%, 36.8%)
11/23 12:04:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6590	Prec@(1,5) (13.0%, 37.1%)
11/23 12:04:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6662	Prec@(1,5) (12.9%, 36.7%)
11/23 12:04:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6673	Prec@(1,5) (12.9%, 36.7%)
11/23 12:04:46午前 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 12.9400%
11/23 12:04:46午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[10, 11])
11/23 12:04:47午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 12.9400%
11/23 12:05:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.7071 (3.6135)	Arch Loss -437.1884 (-432.5008)	Arch Hard Loss 3.6335 (3.6124)	Arch Beta Loss 440.8220 (436.1132)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.3%, 38.0%)	
11/23 12:06:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5775 (3.5915)	Arch Loss -446.8939 (-437.2876)	Arch Hard Loss 3.4772 (3.5894)	Arch Beta Loss 450.3711 (440.8770)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.8%, 38.6%)	
11/23 12:07:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.2378 (3.5508)	Arch Loss -456.4054 (-442.1016)	Arch Hard Loss 3.5584 (3.5536)	Arch Beta Loss 459.9638 (445.6552)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.6%, 40.1%)	
11/23 12:08:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.5088 (3.5264)	Arch Loss -465.1505 (-446.4365)	Arch Hard Loss 3.4897 (3.5321)	Arch Beta Loss 468.6402 (449.9686)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.1%, 40.8%)	
11/23 12:08:17午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 15.0840%
11/23 12:08:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4616	Prec@(1,5) (16.7%, 42.8%)
11/23 12:08:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4782	Prec@(1,5) (16.3%, 42.2%)
11/23 12:08:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4808	Prec@(1,5) (16.2%, 41.9%)
11/23 12:08:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4811	Prec@(1,5) (16.2%, 41.9%)
11/23 12:08:48午前 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 16.2360%
11/23 12:08:48午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/23 12:08:48午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 16.2360%
11/23 12:09:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.6058 (3.3973)	Arch Loss -475.0976 (-470.2567)	Arch Hard Loss 3.3409 (3.3738)	Arch Beta Loss 478.4385 (473.6304)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.8%, 44.7%)	
11/23 12:10:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.1038 (3.3540)	Arch Loss -484.6764 (-475.1453)	Arch Hard Loss 3.5424 (3.3551)	Arch Beta Loss 488.2188 (478.5004)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.9%, 45.8%)	
11/23 12:11:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1673 (3.3181)	Arch Loss -494.5760 (-480.0647)	Arch Hard Loss 3.5188 (3.3348)	Arch Beta Loss 498.0948 (483.3995)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.6%, 46.5%)	
11/23 12:12:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 2.9861 (3.2980)	Arch Loss -503.8044 (-484.5213)	Arch Hard Loss 3.2675 (3.3150)	Arch Beta Loss 507.0720 (487.8364)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (19.1%, 47.1%)	
11/23 12:12:19午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 19.1240%
11/23 12:12:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2205	Prec@(1,5) (21.2%, 49.0%)
11/23 12:12:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2142	Prec@(1,5) (20.9%, 49.4%)
11/23 12:12:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2189	Prec@(1,5) (20.8%, 49.2%)
11/23 12:12:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2242	Prec@(1,5) (20.7%, 49.0%)
11/23 12:12:50午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 20.7200%
11/23 12:12:50午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/23 12:12:50午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 20.7200%
11/23 12:13:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.0308 (3.1142)	Arch Loss -514.2602 (-509.0151)	Arch Hard Loss 2.9867 (3.2362)	Arch Beta Loss 517.2468 (512.2513)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.9%, 52.3%)	
11/23 12:14:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0031 (3.1005)	Arch Loss -524.4311 (-514.1297)	Arch Hard Loss 2.9862 (3.1837)	Arch Beta Loss 527.4173 (517.3134)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.8%, 52.8%)	
11/23 12:15:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.6735 (3.0920)	Arch Loss -534.5162 (-519.2436)	Arch Hard Loss 3.1519 (3.1614)	Arch Beta Loss 537.6680 (522.4050)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.8%, 52.8%)	
11/23 12:16:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.8884 (3.0892)	Arch Loss -544.0934 (-523.8705)	Arch Hard Loss 2.8535 (3.1384)	Arch Beta Loss 546.9469 (527.0089)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.8%, 53.1%)	
11/23 12:16:21午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 22.8520%
11/23 12:16:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.0628	Prec@(1,5) (22.7%, 53.7%)
11/23 12:16:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0378	Prec@(1,5) (23.4%, 54.5%)
11/23 12:16:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0304	Prec@(1,5) (23.7%, 54.6%)
11/23 12:16:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0285	Prec@(1,5) (24.0%, 54.8%)
11/23 12:16:52午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 23.9960%
11/23 12:16:52午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/23 12:16:53午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 23.9960%
11/23 12:17:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1750 (2.9461)	Arch Loss -554.4073 (-549.2496)	Arch Hard Loss 2.9924 (3.0248)	Arch Beta Loss 557.3997 (552.2743)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.5%, 55.9%)	
11/23 12:18:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.1792 (2.9468)	Arch Loss -564.3109 (-554.4147)	Arch Hard Loss 3.4569 (3.0400)	Arch Beta Loss 567.7678 (557.4547)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.8%, 56.0%)	
11/23 12:19:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.8483 (2.9459)	Arch Loss -575.4037 (-559.6205)	Arch Hard Loss 2.7264 (3.0167)	Arch Beta Loss 578.1301 (562.6372)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.9%, 55.9%)	
11/23 12:20:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.6346 (2.9335)	Arch Loss -584.5498 (-564.3031)	Arch Hard Loss 2.8829 (2.9953)	Arch Beta Loss 587.4327 (567.2984)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.9%, 56.5%)	
11/23 12:20:24午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 25.8560%
11/23 12:20:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.9419	Prec@(1,5) (25.8%, 58.0%)
11/23 12:20:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9474	Prec@(1,5) (25.7%, 57.1%)
11/23 12:20:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9522	Prec@(1,5) (25.5%, 56.9%)
11/23 12:20:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9465	Prec@(1,5) (25.7%, 57.1%)
11/23 12:20:55午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 25.7280%
11/23 12:20:55午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/23 12:20:55午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 25.7280%
11/23 12:21:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.8729 (2.8014)	Arch Loss -594.8626 (-589.8305)	Arch Hard Loss 2.9606 (2.9056)	Arch Beta Loss 597.8232 (592.7361)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.8%, 60.3%)	
11/23 12:22:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.9453 (2.8118)	Arch Loss -605.1594 (-594.9779)	Arch Hard Loss 2.8808 (2.8851)	Arch Beta Loss 608.0402 (597.8630)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.1%, 59.8%)	
11/23 12:23:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.9460 (2.8000)	Arch Loss -615.3456 (-600.1018)	Arch Hard Loss 2.8212 (2.8610)	Arch Beta Loss 618.1668 (602.9629)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.3%, 60.3%)	
11/23 12:24:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.9708 (2.7911)	Arch Loss -624.3428 (-604.6642)	Arch Hard Loss 2.8464 (2.8616)	Arch Beta Loss 627.1892 (607.5258)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.7%, 60.5%)	
11/23 12:24:26午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 28.7080%
11/23 12:24:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.8008	Prec@(1,5) (29.0%, 60.0%)
11/23 12:24:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.7925	Prec@(1,5) (29.1%, 60.5%)
11/23 12:24:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.7962	Prec@(1,5) (28.9%, 60.5%)
11/23 12:24:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.8036	Prec@(1,5) (28.6%, 60.4%)
11/23 12:24:57午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 28.6560%
11/23 12:24:57午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/23 12:24:58午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 28.6560%
11/23 12:25:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.8611 (2.6617)	Arch Loss -634.4195 (-629.5032)	Arch Hard Loss 2.7760 (2.7996)	Arch Beta Loss 637.1956 (632.3027)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.5%, 63.3%)	
11/23 12:26:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.5746 (2.6697)	Arch Loss -644.1993 (-634.4429)	Arch Hard Loss 2.7667 (2.7793)	Arch Beta Loss 646.9659 (637.2222)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.3%, 63.3%)	
11/23 12:27:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.6997 (2.6562)	Arch Loss -653.3710 (-639.3187)	Arch Hard Loss 3.2136 (2.7751)	Arch Beta Loss 656.5846 (642.0937)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.0%, 63.8%)	
11/23 12:28:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.6555 (2.6506)	Arch Loss -662.3890 (-643.6793)	Arch Hard Loss 2.7121 (2.7547)	Arch Beta Loss 665.1011 (646.4340)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.1%, 63.9%)	
11/23 12:28:29午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 31.1400%
11/23 12:28:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6908	Prec@(1,5) (31.4%, 62.6%)
11/23 12:28:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.6719	Prec@(1,5) (31.2%, 63.2%)
11/23 12:28:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6693	Prec@(1,5) (31.1%, 63.2%)
11/23 12:28:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6811	Prec@(1,5) (30.9%, 63.0%)
11/23 12:29:00午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 30.8720%
11/23 12:29:00午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 12:29:00午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 30.8720%
11/23 12:29:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.4772 (2.5206)	Arch Loss -671.6046 (-667.2263)	Arch Hard Loss 2.8850 (2.6775)	Arch Beta Loss 674.4896 (669.9038)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.7%, 66.9%)	
11/23 12:30:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.1341 (2.5181)	Arch Loss -680.5529 (-671.8247)	Arch Hard Loss 3.0466 (2.6801)	Arch Beta Loss 683.5994 (674.5048)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.7%, 67.1%)	
11/23 12:31:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.5370 (2.5283)	Arch Loss -689.8288 (-676.3682)	Arch Hard Loss 2.6909 (2.6750)	Arch Beta Loss 692.5197 (679.0432)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.6%, 66.9%)	
11/23 12:32:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.2025 (2.5240)	Arch Loss -697.7827 (-680.4143)	Arch Hard Loss 2.5987 (2.6591)	Arch Beta Loss 700.3815 (683.0734)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.8%, 67.0%)	
11/23 12:32:31午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 33.7600%
11/23 12:32:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5477	Prec@(1,5) (34.1%, 66.6%)
11/23 12:32:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.5629	Prec@(1,5) (33.6%, 66.1%)
11/23 12:32:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.5761	Prec@(1,5) (33.5%, 65.7%)
11/23 12:33:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.5756	Prec@(1,5) (33.3%, 65.6%)
11/23 12:33:02午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 33.3080%
11/23 12:33:02午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 12:33:03午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.3080%
11/23 12:33:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.4014 (2.4350)	Arch Loss -706.2188 (-702.1986)	Arch Hard Loss 2.7952 (2.6014)	Arch Beta Loss 709.0139 (704.8000)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.7%, 68.5%)	
11/23 12:34:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.4100 (2.4410)	Arch Loss -714.6325 (-706.4368)	Arch Hard Loss 2.7312 (2.5866)	Arch Beta Loss 717.3637 (709.0234)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.5%, 68.6%)	
11/23 12:35:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.3420 (2.4292)	Arch Loss -723.1915 (-710.6014)	Arch Hard Loss 2.3273 (2.5802)	Arch Beta Loss 725.5188 (713.1816)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.8%, 68.8%)	
11/23 12:36:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.1174 (2.4277)	Arch Loss -730.0170 (-714.2993)	Arch Hard Loss 2.6766 (2.5694)	Arch Beta Loss 732.6936 (716.8687)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.9%, 68.9%)	
11/23 12:36:33午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 35.8840%
11/23 12:36:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.4958	Prec@(1,5) (34.6%, 68.1%)
11/23 12:36:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.4855	Prec@(1,5) (34.8%, 68.1%)
11/23 12:36:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.5003	Prec@(1,5) (34.6%, 67.7%)
11/23 12:37:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.5008	Prec@(1,5) (34.6%, 67.6%)
11/23 12:37:04午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 34.5760%
11/23 12:37:04午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:37:05午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 34.5760%
11/23 12:37:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.5085 (2.3010)	Arch Loss -737.9784 (-734.1727)	Arch Hard Loss 2.5846 (2.5497)	Arch Beta Loss 740.5630 (736.7224)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.4%, 71.5%)	
11/23 12:38:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.5360 (2.3207)	Arch Loss -745.8532 (-738.0535)	Arch Hard Loss 2.3142 (2.5170)	Arch Beta Loss 748.1674 (740.5705)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.0%, 71.2%)	
11/23 12:39:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.4097 (2.3335)	Arch Loss -753.3173 (-741.8470)	Arch Hard Loss 2.2719 (2.5102)	Arch Beta Loss 755.5892 (744.3572)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.7%, 70.9%)	
11/23 12:40:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.5248 (2.3262)	Arch Loss -759.6339 (-745.2164)	Arch Hard Loss 2.4821 (2.4969)	Arch Beta Loss 762.1159 (747.7134)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.1%, 71.0%)	
11/23 12:40:35午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 38.0760%
11/23 12:40:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.4598	Prec@(1,5) (36.2%, 68.2%)
11/23 12:40:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.4415	Prec@(1,5) (36.4%, 68.3%)
11/23 12:40:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.4559	Prec@(1,5) (36.3%, 68.2%)
11/23 12:41:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4504	Prec@(1,5) (36.2%, 68.3%)
11/23 12:41:06午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 36.2080%
11/23 12:41:06午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:41:07午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 36.2080%
11/23 12:42:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 1.9889 (2.2212)	Arch Loss -766.4921 (-763.3179)	Arch Hard Loss 2.7789 (2.4613)	Arch Beta Loss 769.2711 (765.7792)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.5%, 73.3%)	
11/23 12:42:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.4433 (2.2194)	Arch Loss -773.4572 (-766.8279)	Arch Hard Loss 2.7249 (2.4493)	Arch Beta Loss 776.1821 (769.2772)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.5%, 73.1%)	
11/23 12:43:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.1200 (2.2215)	Arch Loss -780.7986 (-770.2836)	Arch Hard Loss 2.1268 (2.4350)	Arch Beta Loss 782.9254 (772.7186)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.4%, 73.0%)	
11/23 12:44:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.0184 (2.2293)	Arch Loss -786.4446 (-773.3403)	Arch Hard Loss 2.4107 (2.4280)	Arch Beta Loss 788.8553 (775.7683)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.3%, 73.0%)	
11/23 12:44:37午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 40.2680%
11/23 12:44:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.3960	Prec@(1,5) (37.2%, 69.7%)
11/23 12:44:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.3907	Prec@(1,5) (37.3%, 69.9%)
11/23 12:45:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.3860	Prec@(1,5) (37.5%, 70.0%)
11/23 12:45:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.3874	Prec@(1,5) (37.5%, 70.0%)
11/23 12:45:08午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 37.4800%
11/23 12:45:08午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:45:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.4800%
11/23 12:46:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.0984 (2.1279)	Arch Loss -793.1446 (-789.7940)	Arch Hard Loss 2.2137 (2.3905)	Arch Beta Loss 795.3583 (792.1845)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.9%, 75.2%)	
11/23 12:46:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.1393 (2.1496)	Arch Loss -799.4423 (-792.9789)	Arch Hard Loss 2.2034 (2.3861)	Arch Beta Loss 801.6457 (795.3650)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.9%, 74.6%)	
11/23 12:47:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.1531 (2.1525)	Arch Loss -805.3460 (-796.1197)	Arch Hard Loss 2.4429 (2.3769)	Arch Beta Loss 807.7889 (798.4966)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.9%, 74.5%)	
11/23 12:48:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.8444 (2.1580)	Arch Loss -810.9706 (-798.9094)	Arch Hard Loss 2.2332 (2.3654)	Arch Beta Loss 813.2037 (801.2747)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.7%, 74.4%)	
11/23 12:48:39午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 41.7080%
11/23 12:48:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.3183	Prec@(1,5) (38.6%, 71.2%)
11/23 12:48:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.2959	Prec@(1,5) (39.0%, 72.0%)
11/23 12:49:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.2961	Prec@(1,5) (39.1%, 72.0%)
11/23 12:49:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3026	Prec@(1,5) (39.1%, 71.7%)
11/23 12:49:10午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 39.1280%
11/23 12:49:10午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:49:10午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 39.1280%
11/23 12:50:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1295 (2.0517)	Arch Loss -816.8343 (-813.9094)	Arch Hard Loss 2.3256 (2.3416)	Arch Beta Loss 819.1599 (816.2510)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.5%, 76.7%)	
11/23 12:50:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.0793 (2.0858)	Arch Loss -822.6509 (-816.8447)	Arch Hard Loss 2.2917 (2.3255)	Arch Beta Loss 824.9426 (819.1702)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.2%, 75.9%)	
11/23 12:51:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8586 (2.0812)	Arch Loss -828.3984 (-819.7310)	Arch Hard Loss 2.2250 (2.3222)	Arch Beta Loss 830.6234 (822.0532)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.3%, 76.1%)	
11/23 12:52:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.0629 (2.0820)	Arch Loss -833.4916 (-822.3052)	Arch Hard Loss 2.1676 (2.3146)	Arch Beta Loss 835.6592 (824.6198)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.3%, 76.0%)	
11/23 12:52:42午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 43.2920%
11/23 12:52:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2486	Prec@(1,5) (40.1%, 72.6%)
11/23 12:52:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2747	Prec@(1,5) (39.6%, 72.2%)
11/23 12:53:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2508	Prec@(1,5) (40.2%, 72.6%)
11/23 12:53:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.2518	Prec@(1,5) (40.2%, 72.7%)
11/23 12:53:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 40.1640%
11/23 12:53:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:53:13午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.1640%
11/23 12:54:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.1584 (1.9672)	Arch Loss -838.8224 (-836.1969)	Arch Hard Loss 2.4128 (2.3115)	Arch Beta Loss 841.2353 (838.5084)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.5%, 78.7%)	
11/23 12:55:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.0884 (1.9976)	Arch Loss -844.4304 (-838.9629)	Arch Hard Loss 2.2642 (2.2898)	Arch Beta Loss 846.6946 (841.2527)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.2%, 77.9%)	
11/23 12:55:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 2.0289 (2.0033)	Arch Loss -849.8608 (-841.6934)	Arch Hard Loss 2.2447 (2.2852)	Arch Beta Loss 852.1055 (843.9786)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.3%, 78.0%)	
11/23 12:56:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.7805 (2.0067)	Arch Loss -854.6922 (-844.1417)	Arch Hard Loss 2.2553 (2.2776)	Arch Beta Loss 856.9475 (846.4193)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.1%, 77.8%)	
11/23 12:56:44午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 45.1200%
11/23 12:56:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.2052	Prec@(1,5) (41.7%, 73.8%)
11/23 12:57:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.2188	Prec@(1,5) (41.4%, 73.4%)
11/23 12:57:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.2253	Prec@(1,5) (41.4%, 73.2%)
11/23 12:57:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.2265	Prec@(1,5) (41.1%, 73.3%)
11/23 12:57:15午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 41.0800%
11/23 12:57:15午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 12:57:15午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.0800%
11/23 12:58:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.2555 (1.9114)	Arch Loss -860.1395 (-857.4283)	Arch Hard Loss 2.2249 (2.2820)	Arch Beta Loss 862.3643 (859.7104)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.1%, 79.3%)	
11/23 12:59:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.9260 (1.9267)	Arch Loss -865.7119 (-860.1265)	Arch Hard Loss 2.0135 (2.2642)	Arch Beta Loss 867.7254 (862.3907)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.4%, 79.2%)	
11/23 12:59:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 2.0508 (1.9367)	Arch Loss -870.8286 (-862.8136)	Arch Hard Loss 2.2704 (2.2593)	Arch Beta Loss 873.0991 (865.0729)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.9%, 79.0%)	
11/23 01:00:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.5035 (1.9424)	Arch Loss -875.9511 (-865.2412)	Arch Hard Loss 2.0062 (2.2502)	Arch Beta Loss 877.9573 (867.4914)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.7%, 78.9%)	
11/23 01:00:46午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 46.6960%
11/23 01:00:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.2113	Prec@(1,5) (41.7%, 73.7%)
11/23 01:01:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.1951	Prec@(1,5) (42.1%, 73.9%)
11/23 01:01:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.2073	Prec@(1,5) (41.6%, 73.7%)
11/23 01:01:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.2010	Prec@(1,5) (41.5%, 73.8%)
11/23 01:01:17午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 41.4600%
11/23 01:01:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:01:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.4600%
11/23 01:02:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.8807 (1.8601)	Arch Loss -881.5162 (-878.5444)	Arch Hard Loss 1.9296 (2.2076)	Arch Beta Loss 883.4459 (880.7520)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.6%, 80.3%)	
11/23 01:03:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.1375 (1.8805)	Arch Loss -886.6697 (-881.2782)	Arch Hard Loss 2.2595 (2.2030)	Arch Beta Loss 888.9292 (883.4811)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.8%, 80.2%)	
11/23 01:03:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.8368 (1.8851)	Arch Loss -892.2527 (-884.0133)	Arch Hard Loss 2.2166 (2.2148)	Arch Beta Loss 894.4693 (886.2281)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.8%, 79.9%)	
11/23 01:04:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9566 (1.8893)	Arch Loss -897.1349 (-886.5114)	Arch Hard Loss 2.3732 (2.2054)	Arch Beta Loss 899.5081 (888.7168)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.7%, 79.8%)	
11/23 01:04:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 47.7360%
11/23 01:04:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1720	Prec@(1,5) (42.3%, 74.7%)
11/23 01:05:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1440	Prec@(1,5) (43.0%, 75.2%)
11/23 01:05:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1481	Prec@(1,5) (42.8%, 75.0%)
11/23 01:05:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1460	Prec@(1,5) (42.8%, 75.1%)
11/23 01:05:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 42.8400%
11/23 01:05:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:05:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.8400%
11/23 01:06:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.6827 (1.7697)	Arch Loss -903.0987 (-900.2357)	Arch Hard Loss 2.1262 (2.1817)	Arch Beta Loss 905.2249 (902.4174)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.1%, 82.6%)	
11/23 01:07:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.8928 (1.8146)	Arch Loss -908.8796 (-903.0891)	Arch Hard Loss 2.0719 (2.1753)	Arch Beta Loss 910.9515 (905.2644)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.5%, 81.4%)	
11/23 01:08:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.8952 (1.8220)	Arch Loss -914.7193 (-905.9435)	Arch Hard Loss 2.0228 (2.1896)	Arch Beta Loss 916.7421 (908.1331)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.3%, 81.0%)	
11/23 01:08:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.4466 (1.8401)	Arch Loss -919.8456 (-908.5492)	Arch Hard Loss 2.1623 (2.1840)	Arch Beta Loss 922.0079 (910.7332)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.8%, 80.7%)	
11/23 01:08:51午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 48.7520%
11/23 01:08:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.1543	Prec@(1,5) (43.1%, 74.5%)
11/23 01:09:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.1455	Prec@(1,5) (42.9%, 75.0%)
11/23 01:09:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.1505	Prec@(1,5) (43.0%, 74.9%)
11/23 01:09:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.1456	Prec@(1,5) (42.9%, 75.0%)
11/23 01:09:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 42.9480%
11/23 01:09:22午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:09:22午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.9480%
11/23 01:10:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.6464 (1.7496)	Arch Loss -925.7136 (-922.8980)	Arch Hard Loss 2.2608 (2.1477)	Arch Beta Loss 927.9744 (925.0457)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.4%, 82.8%)	
11/23 01:11:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.4479 (1.7754)	Arch Loss -931.8608 (-925.8484)	Arch Hard Loss 2.0725 (2.1643)	Arch Beta Loss 933.9332 (928.0126)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.5%, 82.2%)	
11/23 01:12:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 2.0237 (1.7840)	Arch Loss -937.7587 (-928.8251)	Arch Hard Loss 2.1803 (2.1709)	Arch Beta Loss 939.9390 (930.9959)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.2%, 82.1%)	
11/23 01:12:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.9889 (1.7961)	Arch Loss -943.7014 (-931.5371)	Arch Hard Loss 1.6769 (2.1566)	Arch Beta Loss 945.3784 (933.6937)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.9%, 81.9%)	
11/23 01:12:54午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 49.9120%
11/23 01:13:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.2036	Prec@(1,5) (42.3%, 73.4%)
11/23 01:13:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.1973	Prec@(1,5) (42.7%, 73.8%)
11/23 01:13:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.1990	Prec@(1,5) (42.9%, 73.8%)
11/23 01:13:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.2004	Prec@(1,5) (42.7%, 74.0%)
11/23 01:13:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 42.6560%
11/23 01:13:25午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:13:25午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.9480%
11/23 01:14:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.2954 (1.6834)	Arch Loss -949.2062 (-946.3601)	Arch Hard Loss 2.3076 (2.1443)	Arch Beta Loss 951.5139 (948.5045)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.3%, 83.8%)	
11/23 01:15:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.8598 (1.7193)	Arch Loss -955.2341 (-949.4061)	Arch Hard Loss 2.3793 (2.1423)	Arch Beta Loss 957.6134 (951.5484)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.8%, 83.1%)	
11/23 01:16:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.0203 (1.7300)	Arch Loss -961.9018 (-952.4676)	Arch Hard Loss 1.8270 (2.1316)	Arch Beta Loss 963.7288 (954.5991)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.5%, 82.9%)	
11/23 01:16:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.6041 (1.7506)	Arch Loss -967.0202 (-955.2211)	Arch Hard Loss 2.2229 (2.1280)	Arch Beta Loss 969.2431 (957.3492)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.1%, 82.5%)	
11/23 01:16:56午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 51.0920%
11/23 01:17:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.1825	Prec@(1,5) (42.8%, 74.2%)
11/23 01:17:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.1929	Prec@(1,5) (42.4%, 74.0%)
11/23 01:17:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.1787	Prec@(1,5) (42.5%, 74.5%)
11/23 01:17:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.1736	Prec@(1,5) (42.5%, 74.6%)
11/23 01:17:26午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 42.5360%
11/23 01:17:26午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:17:27午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.9480%
11/23 01:18:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.6612 (1.6713)	Arch Loss -973.2262 (-970.2438)	Arch Hard Loss 2.2095 (2.1565)	Arch Beta Loss 975.4357 (972.4002)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.0%, 83.9%)	
11/23 01:19:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.8257 (1.6850)	Arch Loss -978.8062 (-973.3135)	Arch Hard Loss 2.7614 (2.1527)	Arch Beta Loss 981.5675 (975.4662)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.5%, 83.6%)	
11/23 01:20:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3563 (1.6970)	Arch Loss -985.7455 (-976.3923)	Arch Hard Loss 1.9505 (2.1393)	Arch Beta Loss 987.6960 (978.5317)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.2%, 83.5%)	
11/23 01:20:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.6099 (1.7118)	Arch Loss -991.3651 (-979.1530)	Arch Hard Loss 1.8414 (2.1365)	Arch Beta Loss 993.2064 (981.2895)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.8%, 83.1%)	
11/23 01:20:58午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 51.7920%
11/23 01:21:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.1033	Prec@(1,5) (44.7%, 76.0%)
11/23 01:21:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.1057	Prec@(1,5) (44.7%, 75.9%)
11/23 01:21:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.1125	Prec@(1,5) (44.3%, 75.9%)
11/23 01:21:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.1159	Prec@(1,5) (44.2%, 75.8%)
11/23 01:21:28午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 44.2120%
11/23 01:21:28午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:21:29午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.2120%
11/23 01:22:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.7317 (1.5862)	Arch Loss -996.9373 (-994.2374)	Arch Hard Loss 2.4423 (2.1178)	Arch Beta Loss 999.3796 (996.3552)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.9%, 85.8%)	
11/23 01:23:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.7346 (1.6404)	Arch Loss -1003.1373 (-997.2845)	Arch Hard Loss 2.3386 (2.1231)	Arch Beta Loss 1005.4758 (999.4077)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.3%, 84.6%)	
11/23 01:24:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.6706 (1.6619)	Arch Loss -1009.6212 (-1000.3420)	Arch Hard Loss 1.9266 (2.1114)	Arch Beta Loss 1011.5478 (1002.4534)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.0%, 84.2%)	
11/23 01:25:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.6523 (1.6729)	Arch Loss -1014.9316 (-1003.0761)	Arch Hard Loss 2.0536 (2.1109)	Arch Beta Loss 1016.9852 (1005.1870)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.7%, 83.9%)	
11/23 01:25:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 52.7000%
11/23 01:25:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.0873	Prec@(1,5) (44.8%, 75.9%)
11/23 01:25:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.0907	Prec@(1,5) (44.4%, 76.0%)
11/23 01:25:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.0868	Prec@(1,5) (44.5%, 76.1%)
11/23 01:25:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.0787	Prec@(1,5) (44.7%, 76.4%)
11/23 01:25:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 44.6360%
11/23 01:25:31午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:25:32午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.6360%
11/23 01:26:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.7380 (1.5516)	Arch Loss -1020.6775 (-1017.9519)	Arch Hard Loss 2.3681 (2.1273)	Arch Beta Loss 1023.0457 (1020.0792)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.8%, 85.6%)	
11/23 01:27:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.7495 (1.5876)	Arch Loss -1026.7413 (-1020.9551)	Arch Hard Loss 2.2520 (2.1115)	Arch Beta Loss 1028.9933 (1023.0666)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.6%, 85.0%)	
11/23 01:28:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.6161 (1.6044)	Arch Loss -1032.6300 (-1023.9304)	Arch Hard Loss 2.2475 (2.1042)	Arch Beta Loss 1034.8776 (1026.0345)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.4%, 84.8%)	
11/23 01:29:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.7898 (1.6234)	Arch Loss -1037.9553 (-1026.5889)	Arch Hard Loss 2.1599 (2.0986)	Arch Beta Loss 1040.1152 (1028.6875)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.9%, 84.4%)	
11/23 01:29:04午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 53.8520%
11/23 01:29:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.1004	Prec@(1,5) (44.8%, 76.4%)
11/23 01:29:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.0991	Prec@(1,5) (44.6%, 76.5%)
11/23 01:29:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.0986	Prec@(1,5) (44.7%, 76.5%)
11/23 01:29:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.1084	Prec@(1,5) (44.7%, 76.2%)
11/23 01:29:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 44.7280%
11/23 01:29:34午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:29:35午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.7280%
11/23 01:30:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.6113 (1.4791)	Arch Loss -1044.0283 (-1041.0087)	Arch Hard Loss 1.8891 (2.0716)	Arch Beta Loss 1045.9174 (1043.0803)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.1%, 86.7%)	
11/23 01:31:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.3493 (1.5295)	Arch Loss -1049.2130 (-1043.8599)	Arch Hard Loss 2.3705 (2.0729)	Arch Beta Loss 1051.5835 (1045.9329)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.4%, 86.2%)	
11/23 01:32:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.6067 (1.5518)	Arch Loss -1055.0476 (-1046.6685)	Arch Hard Loss 2.1218 (2.0905)	Arch Beta Loss 1057.1694 (1048.7589)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.7%, 85.9%)	
11/23 01:33:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.7079 (1.5735)	Arch Loss -1060.0109 (-1049.1890)	Arch Hard Loss 2.1180 (2.0907)	Arch Beta Loss 1062.1289 (1051.2797)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.1%, 85.4%)	
11/23 01:33:07午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 55.0880%
11/23 01:33:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.0232	Prec@(1,5) (46.7%, 77.0%)
11/23 01:33:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.0200	Prec@(1,5) (46.4%, 77.7%)
11/23 01:33:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.0292	Prec@(1,5) (46.2%, 77.5%)
11/23 01:33:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.0261	Prec@(1,5) (46.2%, 77.7%)
11/23 01:33:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 46.2040%
11/23 01:33:38午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:33:38午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.2040%
11/23 01:34:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.2872 (1.4676)	Arch Loss -1065.3987 (-1062.8948)	Arch Hard Loss 2.2228 (2.0413)	Arch Beta Loss 1067.6215 (1064.9361)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.9%, 87.0%)	
11/23 01:35:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.4525 (1.5030)	Arch Loss -1070.7833 (-1065.5781)	Arch Hard Loss 2.2047 (2.0587)	Arch Beta Loss 1072.9880 (1067.6368)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.6%, 86.6%)	
11/23 01:36:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.8036 (1.5112)	Arch Loss -1076.1981 (-1068.2542)	Arch Hard Loss 2.0924 (2.0606)	Arch Beta Loss 1078.2905 (1070.3148)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.5%, 86.2%)	
11/23 01:37:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.3916 (1.5304)	Arch Loss -1081.0071 (-1070.6412)	Arch Hard Loss 2.0087 (2.0662)	Arch Beta Loss 1083.0157 (1072.7074)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.9%, 85.9%)	
11/23 01:37:10午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 56.9360%
11/23 01:37:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.0741	Prec@(1,5) (45.8%, 77.5%)
11/23 01:37:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.0604	Prec@(1,5) (46.2%, 77.8%)
11/23 01:37:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.0570	Prec@(1,5) (46.1%, 77.7%)
11/23 01:37:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.0494	Prec@(1,5) (46.3%, 77.6%)
11/23 01:37:40午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 46.3160%
11/23 01:37:40午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:37:41午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.3160%
11/23 01:38:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.5095 (1.4285)	Arch Loss -1086.0878 (-1083.6687)	Arch Hard Loss 2.1870 (2.0325)	Arch Beta Loss 1088.2748 (1085.7012)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.7%, 87.7%)	
11/23 01:39:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.5533 (1.4550)	Arch Loss -1091.2114 (-1086.2465)	Arch Hard Loss 2.2317 (2.0479)	Arch Beta Loss 1093.4431 (1088.2944)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.9%, 87.2%)	
11/23 01:40:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.4695 (1.4731)	Arch Loss -1097.0308 (-1088.8156)	Arch Hard Loss 1.5546 (2.0612)	Arch Beta Loss 1098.5853 (1090.8769)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.5%, 86.9%)	
11/23 01:41:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.3325 (1.4836)	Arch Loss -1101.2898 (-1091.1394)	Arch Hard Loss 1.9065 (2.0546)	Arch Beta Loss 1103.1963 (1093.1939)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.4%, 86.8%)	
11/23 01:41:12午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 57.3640%
11/23 01:41:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.0655	Prec@(1,5) (45.3%, 77.5%)
11/23 01:41:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.0579	Prec@(1,5) (46.0%, 77.5%)
11/23 01:41:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.0337	Prec@(1,5) (46.3%, 78.0%)
11/23 01:41:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.0388	Prec@(1,5) (46.3%, 77.9%)
11/23 01:41:43午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 46.3040%
11/23 01:41:43午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:41:43午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.3160%
11/23 01:42:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.8207 (1.3730)	Arch Loss -1106.5780 (-1103.8386)	Arch Hard Loss 1.7768 (1.9895)	Arch Beta Loss 1108.3547 (1105.8281)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.8%, 88.5%)	
11/23 01:43:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.1293 (1.4137)	Arch Loss -1111.2900 (-1106.3735)	Arch Hard Loss 2.1546 (2.0037)	Arch Beta Loss 1113.4446 (1108.3772)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.3%, 87.9%)	
11/23 01:44:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.4169 (1.4225)	Arch Loss -1116.6678 (-1108.8941)	Arch Hard Loss 1.8466 (2.0262)	Arch Beta Loss 1118.5145 (1110.9203)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.1%, 87.7%)	
11/23 01:45:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.1197 (1.4432)	Arch Loss -1120.7086 (-1111.1652)	Arch Hard Loss 2.3469 (2.0381)	Arch Beta Loss 1123.0555 (1113.2033)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.5%, 87.4%)	
11/23 01:45:15午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 58.5480%
11/23 01:45:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.0565	Prec@(1,5) (46.4%, 77.4%)
11/23 01:45:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.0340	Prec@(1,5) (46.9%, 77.6%)
11/23 01:45:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.0332	Prec@(1,5) (47.0%, 77.6%)
11/23 01:45:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.0368	Prec@(1,5) (46.9%, 77.5%)
11/23 01:45:45午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 46.9480%
11/23 01:45:45午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:45:46午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.9480%
11/23 01:46:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.3936 (1.2939)	Arch Loss -1125.5941 (-1123.5812)	Arch Hard Loss 2.5206 (2.0576)	Arch Beta Loss 1128.1147 (1125.6388)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.1%, 89.9%)	
11/23 01:47:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.6079 (1.3520)	Arch Loss -1131.3313 (-1126.0951)	Arch Hard Loss 1.7447 (2.0367)	Arch Beta Loss 1133.0759 (1128.1318)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.5%, 89.0%)	
11/23 01:48:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.4362 (1.3782)	Arch Loss -1135.8472 (-1128.5646)	Arch Hard Loss 2.1286 (2.0419)	Arch Beta Loss 1137.9757 (1130.6065)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.8%, 88.5%)	
11/23 01:49:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.2485 (1.3941)	Arch Loss -1140.5964 (-1130.7865)	Arch Hard Loss 1.7258 (2.0289)	Arch Beta Loss 1142.3223 (1132.8154)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.6%, 88.2%)	
11/23 01:49:17午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 59.5400%
11/23 01:49:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.0882	Prec@(1,5) (45.5%, 77.5%)
11/23 01:49:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.0777	Prec@(1,5) (46.2%, 77.4%)
11/23 01:49:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.0885	Prec@(1,5) (46.2%, 77.3%)
11/23 01:49:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.0816	Prec@(1,5) (46.3%, 77.5%)
11/23 01:49:48午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 46.2800%
11/23 01:49:48午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:49:48午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.9480%
11/23 01:50:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.6199 (1.2863)	Arch Loss -1144.9080 (-1142.8126)	Arch Hard Loss 2.2084 (1.9619)	Arch Beta Loss 1147.1163 (1144.7744)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.8%, 90.1%)	
11/23 01:51:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.3822 (1.3291)	Arch Loss -1150.2175 (-1145.1228)	Arch Hard Loss 1.5464 (2.0003)	Arch Beta Loss 1151.7639 (1147.1230)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.0%, 89.5%)	
11/23 01:52:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.4420 (1.3411)	Arch Loss -1154.4379 (-1147.4218)	Arch Hard Loss 1.8691 (2.0160)	Arch Beta Loss 1156.3070 (1149.4379)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.1%, 89.1%)	
11/23 01:53:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.3081 (1.3638)	Arch Loss -1158.6075 (-1149.4586)	Arch Hard Loss 1.6879 (2.0316)	Arch Beta Loss 1160.2954 (1151.4902)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.6%, 88.6%)	
11/23 01:53:19午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 60.5520%
11/23 01:53:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.0509	Prec@(1,5) (47.1%, 77.7%)
11/23 01:53:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.0636	Prec@(1,5) (46.9%, 77.7%)
11/23 01:53:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.0377	Prec@(1,5) (47.4%, 78.0%)
11/23 01:53:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.0313	Prec@(1,5) (47.4%, 78.1%)
11/23 01:53:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 47.4440%
11/23 01:53:49午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:53:50午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4440%
11/23 01:54:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.4698 (1.2407)	Arch Loss -1162.8003 (-1160.5010)	Arch Hard Loss 1.8590 (2.0302)	Arch Beta Loss 1164.6593 (1162.5313)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.4%)	
11/23 01:55:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.1287 (1.2564)	Arch Loss -1167.0808 (-1162.6294)	Arch Hard Loss 1.7731 (2.0304)	Arch Beta Loss 1168.8539 (1164.6598)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.5%, 90.3%)	
11/23 01:56:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.3798 (1.2787)	Arch Loss -1170.6335 (-1164.7178)	Arch Hard Loss 2.2866 (2.0280)	Arch Beta Loss 1172.9202 (1166.7459)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.0%, 89.9%)	
11/23 01:57:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.1948 (1.2934)	Arch Loss -1174.4509 (-1166.5682)	Arch Hard Loss 2.0179 (2.0186)	Arch Beta Loss 1176.4688 (1168.5868)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.4%, 89.8%)	
11/23 01:57:21午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 62.3840%
11/23 01:57:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.1616	Prec@(1,5) (45.6%, 76.5%)
11/23 01:57:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.1461	Prec@(1,5) (45.9%, 76.7%)
11/23 01:57:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.1576	Prec@(1,5) (45.6%, 76.4%)
11/23 01:57:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.1457	Prec@(1,5) (45.6%, 76.6%)
11/23 01:57:51午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 45.5960%
11/23 01:57:51午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 01:57:52午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4440%
11/23 01:58:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.3640 (1.1963)	Arch Loss -1178.4288 (-1176.4530)	Arch Hard Loss 1.8982 (1.9943)	Arch Beta Loss 1180.3270 (1178.4473)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.9%, 91.3%)	
11/23 01:59:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.4596 (1.1958)	Arch Loss -1181.7347 (-1178.3077)	Arch Hard Loss 2.2817 (2.0163)	Arch Beta Loss 1184.0165 (1180.3241)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.7%, 91.1%)	
11/23 02:00:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.4318 (1.2325)	Arch Loss -1185.5990 (-1180.1486)	Arch Hard Loss 1.9805 (2.0097)	Arch Beta Loss 1187.5796 (1182.1582)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.9%, 90.5%)	
11/23 02:01:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.4289 (1.2628)	Arch Loss -1188.3365 (-1181.7635)	Arch Hard Loss 2.3390 (2.0090)	Arch Beta Loss 1190.6755 (1183.7725)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.8%, 90.2%)	
11/23 02:01:22午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 62.7800%
11/23 02:01:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.1464	Prec@(1,5) (45.2%, 76.1%)
11/23 02:01:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.1537	Prec@(1,5) (45.2%, 76.2%)
11/23 02:01:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.1710	Prec@(1,5) (44.9%, 76.0%)
11/23 02:01:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.1648	Prec@(1,5) (45.1%, 76.1%)
11/23 02:01:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 45.1360%
11/23 02:01:53午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:01:53午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4440%
11/23 02:02:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.0974 (1.1726)	Arch Loss -1191.5504 (-1190.4235)	Arch Hard Loss 2.4803 (1.9736)	Arch Beta Loss 1194.0308 (1192.3971)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.8%, 91.2%)	
11/23 02:03:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.9672 (1.1882)	Arch Loss -1195.2020 (-1192.0297)	Arch Hard Loss 2.0273 (1.9967)	Arch Beta Loss 1197.2294 (1194.0265)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.4%, 91.0%)	
11/23 02:04:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.3569 (1.2133)	Arch Loss -1198.1044 (-1193.6205)	Arch Hard Loss 2.2037 (1.9951)	Arch Beta Loss 1200.3081 (1195.6156)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.5%, 90.7%)	
11/23 02:05:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.3992 (1.2271)	Arch Loss -1200.7557 (-1195.0196)	Arch Hard Loss 2.2230 (1.9922)	Arch Beta Loss 1202.9788 (1197.0118)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.6%)	
11/23 02:05:24午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 64.2360%
11/23 02:05:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.2810	Prec@(1,5) (43.9%, 75.4%)
11/23 02:05:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.2839	Prec@(1,5) (44.4%, 75.4%)
11/23 02:05:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.2891	Prec@(1,5) (44.4%, 75.0%)
11/23 02:05:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.2983	Prec@(1,5) (44.0%, 75.0%)
11/23 02:05:55午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 44.0000%
11/23 02:05:55午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:05:55午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4440%
11/23 02:06:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.4379 (1.1195)	Arch Loss -1204.1522 (-1202.4613)	Arch Hard Loss 1.7145 (1.9996)	Arch Beta Loss 1205.8667 (1204.4609)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.1%, 91.9%)	
11/23 02:07:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.0360 (1.1341)	Arch Loss -1206.2352 (-1203.8644)	Arch Hard Loss 2.3810 (1.9983)	Arch Beta Loss 1208.6162 (1205.8627)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.4%, 91.7%)	
11/23 02:08:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.0000 (1.1470)	Arch Loss -1209.4733 (-1205.2223)	Arch Hard Loss 1.7868 (2.0062)	Arch Beta Loss 1211.2601 (1207.2285)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.2%, 91.6%)	
11/23 02:09:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.3311 (1.1595)	Arch Loss -1211.9247 (-1206.4250)	Arch Hard Loss 1.6246 (2.0026)	Arch Beta Loss 1213.5492 (1208.4276)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.7%, 91.5%)	
11/23 02:09:26午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 65.7480%
11/23 02:09:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.3027	Prec@(1,5) (44.7%, 76.0%)
11/23 02:09:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.2791	Prec@(1,5) (45.0%, 76.1%)
11/23 02:09:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.2703	Prec@(1,5) (45.1%, 75.9%)
11/23 02:09:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.2645	Prec@(1,5) (45.1%, 75.9%)
11/23 02:09:57午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 45.0760%
11/23 02:09:57午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:09:57午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4440%
11/23 02:10:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.9006 (1.0409)	Arch Loss -1214.1863 (-1212.8324)	Arch Hard Loss 1.8372 (1.9866)	Arch Beta Loss 1216.0234 (1214.8190)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.4%, 93.2%)	
11/23 02:11:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.7948 (1.0800)	Arch Loss -1216.6344 (-1214.0476)	Arch Hard Loss 1.7428 (1.9719)	Arch Beta Loss 1218.3772 (1216.0195)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.7%, 92.4%)	
11/23 02:12:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.2581 (1.1031)	Arch Loss -1218.1396 (-1215.1874)	Arch Hard Loss 2.4993 (2.0012)	Arch Beta Loss 1220.6389 (1217.1886)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.0%, 92.3%)	
11/23 02:13:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.1829 (1.1197)	Arch Loss -1220.9199 (-1216.2168)	Arch Hard Loss 1.6783 (1.9979)	Arch Beta Loss 1222.5981 (1218.2147)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.5%, 92.1%)	
11/23 02:13:28午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 66.4720%
11/23 02:13:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.0833	Prec@(1,5) (47.9%, 79.2%)
11/23 02:13:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.0690	Prec@(1,5) (48.2%, 78.7%)
11/23 02:13:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.0687	Prec@(1,5) (48.4%, 78.6%)
11/23 02:13:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.0733	Prec@(1,5) (48.4%, 78.6%)
11/23 02:13:58午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 48.3840%
11/23 02:13:58午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:13:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:14:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.7009 (1.0004)	Arch Loss -1222.5349 (-1221.7313)	Arch Hard Loss 2.1785 (1.9525)	Arch Beta Loss 1224.7134 (1223.6838)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.2%, 93.7%)	
11/23 02:15:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.9550 (1.0253)	Arch Loss -1224.5901 (-1222.7335)	Arch Hard Loss 2.1357 (1.9766)	Arch Beta Loss 1226.7258 (1224.7100)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.9%, 93.3%)	
11/23 02:16:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.3025 (1.0355)	Arch Loss -1226.2230 (-1223.7304)	Arch Hard Loss 2.4363 (1.9791)	Arch Beta Loss 1228.6593 (1225.7096)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.8%, 93.1%)	
11/23 02:17:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.1297 (1.0555)	Arch Loss -1228.3656 (-1224.6042)	Arch Hard Loss 1.9691 (1.9827)	Arch Beta Loss 1230.3347 (1226.5870)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.3%, 92.9%)	
11/23 02:17:29午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 68.3000%
11/23 02:17:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.5072	Prec@(1,5) (42.0%, 72.8%)
11/23 02:17:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.5199	Prec@(1,5) (41.3%, 72.6%)
11/23 02:17:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.5141	Prec@(1,5) (41.3%, 72.7%)
11/23 02:18:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.5111	Prec@(1,5) (41.4%, 72.6%)
11/23 02:18:00午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 41.3960%
11/23 02:18:00午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:18:00午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:18:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.9300 (0.9626)	Arch Loss -1230.2920 (-1229.2612)	Arch Hard Loss 1.8527 (2.0024)	Arch Beta Loss 1232.1447 (1231.2636)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.1%, 94.0%)	
11/23 02:19:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.0505 (0.9925)	Arch Loss -1232.0178 (-1230.1452)	Arch Hard Loss 1.8478 (1.9962)	Arch Beta Loss 1233.8657 (1232.1414)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.5%, 93.6%)	
11/23 02:20:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.2423 (0.9985)	Arch Loss -1233.2159 (-1231.0181)	Arch Hard Loss 2.3039 (1.9784)	Arch Beta Loss 1235.5199 (1232.9965)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.1%, 93.6%)	
11/23 02:21:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.3203 (1.0025)	Arch Loss -1235.0629 (-1231.7529)	Arch Hard Loss 1.8893 (1.9939)	Arch Beta Loss 1236.9521 (1233.7469)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.0%, 93.5%)	
11/23 02:21:31午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 69.9760%
11/23 02:21:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.2371	Prec@(1,5) (45.9%, 77.2%)
11/23 02:21:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.2411	Prec@(1,5) (45.8%, 77.0%)
11/23 02:21:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.2418	Prec@(1,5) (46.0%, 77.0%)
11/23 02:22:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.2470	Prec@(1,5) (46.0%, 77.0%)
11/23 02:22:01午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 45.9800%
11/23 02:22:01午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:22:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:22:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.7992 (0.9032)	Arch Loss -1236.2001 (-1235.7137)	Arch Hard Loss 2.3010 (2.0336)	Arch Beta Loss 1238.5011 (1237.7472)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.7%, 94.6%)	
11/23 02:23:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 1.0773 (0.9347)	Arch Loss -1238.3479 (-1236.4986)	Arch Hard Loss 1.6266 (2.0001)	Arch Beta Loss 1239.9746 (1238.4986)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.6%, 94.2%)	
11/23 02:24:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.7775 (0.9491)	Arch Loss -1239.2261 (-1237.2241)	Arch Hard Loss 2.1648 (2.0067)	Arch Beta Loss 1241.3909 (1239.2307)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.3%, 94.0%)	
11/23 02:25:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.8676 (0.9606)	Arch Loss -1240.9412 (-1237.8667)	Arch Hard Loss 1.6776 (2.0067)	Arch Beta Loss 1242.6188 (1239.8734)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.0%, 93.9%)	
11/23 02:25:32午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 70.9840%
11/23 02:25:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.1456	Prec@(1,5) (48.1%, 78.2%)
11/23 02:25:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.1307	Prec@(1,5) (48.5%, 78.6%)
11/23 02:25:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.1444	Prec@(1,5) (48.1%, 78.4%)
11/23 02:26:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.1558	Prec@(1,5) (47.9%, 78.1%)
11/23 02:26:03午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 47.9040%
11/23 02:26:03午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:26:03午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:26:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.6825 (0.8535)	Arch Loss -1241.8146 (-1241.3011)	Arch Hard Loss 2.1323 (1.9993)	Arch Beta Loss 1243.9468 (1243.3004)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.0%, 95.4%)	
11/23 02:27:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9892 (0.8781)	Arch Loss -1242.7151 (-1241.9762)	Arch Hard Loss 2.4949 (1.9682)	Arch Beta Loss 1245.2100 (1243.9444)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.7%, 95.0%)	
11/23 02:28:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.8762 (0.8902)	Arch Loss -1244.7321 (-1242.5913)	Arch Hard Loss 1.6936 (1.9810)	Arch Beta Loss 1246.4257 (1244.5723)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.3%, 94.9%)	
11/23 02:29:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.5812 (0.8956)	Arch Loss -1245.6556 (-1243.1427)	Arch Hard Loss 1.8242 (1.9810)	Arch Beta Loss 1247.4799 (1245.1236)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.9%, 94.9%)	
11/23 02:29:34午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 72.9400%
11/23 02:29:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.1616	Prec@(1,5) (48.3%, 78.2%)
11/23 02:29:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.1614	Prec@(1,5) (48.2%, 78.5%)
11/23 02:29:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.1540	Prec@(1,5) (48.5%, 78.7%)
11/23 02:30:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.1599	Prec@(1,5) (48.3%, 78.5%)
11/23 02:30:05午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 48.3000%
11/23 02:30:05午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:30:05午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:31:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7338 (0.8334)	Arch Loss -1247.0089 (-1246.0968)	Arch Hard Loss 1.6113 (1.9683)	Arch Beta Loss 1248.6202 (1248.0651)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.4%)	
11/23 02:31:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.9614 (0.8450)	Arch Loss -1247.5596 (-1246.6672)	Arch Hard Loss 2.1465 (1.9513)	Arch Beta Loss 1249.7062 (1248.6186)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.2%)	
11/23 02:32:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.9526 (0.8491)	Arch Loss -1248.6104 (-1247.1905)	Arch Hard Loss 2.1403 (1.9675)	Arch Beta Loss 1250.7506 (1249.1580)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.0%, 95.3%)	
11/23 02:33:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.6060 (0.8549)	Arch Loss -1249.4359 (-1247.6379)	Arch Hard Loss 2.2208 (1.9939)	Arch Beta Loss 1251.6567 (1249.6319)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 95.2%)	
11/23 02:33:36午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 73.8720%
11/23 02:33:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.2636	Prec@(1,5) (47.7%, 77.8%)
11/23 02:33:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.2276	Prec@(1,5) (47.9%, 78.2%)
11/23 02:34:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.2105	Prec@(1,5) (48.0%, 78.3%)
11/23 02:34:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.2083	Prec@(1,5) (48.1%, 78.3%)
11/23 02:34:07午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 48.0640%
11/23 02:34:07午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:34:07午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:35:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.6604 (0.7682)	Arch Loss -1250.7802 (-1250.1476)	Arch Hard Loss 1.8568 (2.0123)	Arch Beta Loss 1252.6370 (1252.1599)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.7%, 96.0%)	
11/23 02:35:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.6487 (0.7827)	Arch Loss -1251.6709 (-1250.6467)	Arch Hard Loss 1.8997 (1.9890)	Arch Beta Loss 1253.5707 (1252.6357)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 96.0%)	
11/23 02:36:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.7482 (0.8009)	Arch Loss -1252.1459 (-1251.1067)	Arch Hard Loss 2.3236 (1.9930)	Arch Beta Loss 1254.4695 (1253.0996)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.4%, 95.8%)	
11/23 02:37:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 1.0721 (0.8085)	Arch Loss -1252.7386 (-1251.5133)	Arch Hard Loss 2.5114 (1.9940)	Arch Beta Loss 1255.2500 (1253.5073)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.7%)	
11/23 02:37:38午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 75.1240%
11/23 02:37:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.5609	Prec@(1,5) (43.5%, 73.1%)
11/23 02:37:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.5568	Prec@(1,5) (43.8%, 73.2%)
11/23 02:38:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.5504	Prec@(1,5) (43.7%, 73.2%)
11/23 02:38:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.5518	Prec@(1,5) (43.5%, 73.3%)
11/23 02:38:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 43.5280%
11/23 02:38:09午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:38:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:39:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.5854 (0.7184)	Arch Loss -1253.5858 (-1253.6487)	Arch Hard Loss 2.5095 (2.0345)	Arch Beta Loss 1256.0953 (1255.6832)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.5%, 96.3%)	
11/23 02:39:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.7123 (0.7301)	Arch Loss -1254.9734 (-1254.0672)	Arch Hard Loss 1.9259 (2.0264)	Arch Beta Loss 1256.8993 (1256.0936)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.9%, 96.4%)	
11/23 02:40:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.5237 (0.7491)	Arch Loss -1255.6475 (-1254.4705)	Arch Hard Loss 2.0276 (2.0230)	Arch Beta Loss 1257.6750 (1256.4936)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.4%, 96.3%)	
11/23 02:41:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.9446 (0.7554)	Arch Loss -1256.6730 (-1254.8382)	Arch Hard Loss 1.6752 (2.0070)	Arch Beta Loss 1258.3483 (1256.8452)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.2%)	
11/23 02:41:40午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 77.1160%
11/23 02:41:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 4.8701	Prec@(1,5) (26.2%, 52.0%)
11/23 02:41:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 4.8754	Prec@(1,5) (26.1%, 52.2%)
11/23 02:42:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 4.8692	Prec@(1,5) (26.5%, 52.3%)
11/23 02:42:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 4.8895	Prec@(1,5) (26.5%, 52.0%)
11/23 02:42:11午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 26.5320%
11/23 02:42:11午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:42:11午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.3840%
11/23 02:43:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.6351 (0.6865)	Arch Loss -1257.0978 (-1256.6974)	Arch Hard Loss 1.9790 (2.0248)	Arch Beta Loss 1259.0768 (1258.7222)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.7%, 97.0%)	
11/23 02:43:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.5618 (0.7100)	Arch Loss -1257.6504 (-1257.0453)	Arch Hard Loss 2.1216 (2.0310)	Arch Beta Loss 1259.7720 (1259.0762)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.5%, 96.7%)	
11/23 02:44:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.6247 (0.7140)	Arch Loss -1258.4395 (-1257.3953)	Arch Hard Loss 2.0030 (2.0264)	Arch Beta Loss 1260.4424 (1259.4217)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.4%, 96.6%)	
11/23 02:45:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.9016 (0.7250)	Arch Loss -1259.2062 (-1257.7008)	Arch Hard Loss 1.8183 (2.0247)	Arch Beta Loss 1261.0244 (1259.7255)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.9%, 96.5%)	
11/23 02:45:42午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 77.8840%
11/23 02:45:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.2113	Prec@(1,5) (48.6%, 77.8%)
11/23 02:45:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.2287	Prec@(1,5) (48.0%, 77.7%)
11/23 02:46:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.2234	Prec@(1,5) (48.4%, 77.9%)
11/23 02:46:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.2340	Prec@(1,5) (48.5%, 77.8%)
11/23 02:46:13午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 48.5160%
11/23 02:46:13午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:46:13午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 02:47:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.7215 (0.6650)	Arch Loss -1259.6124 (-1259.3063)	Arch Hard Loss 2.0424 (2.0417)	Arch Beta Loss 1261.6549 (1261.3480)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.5%, 97.1%)	
11/23 02:48:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.9909 (0.6753)	Arch Loss -1260.4556 (-1259.6367)	Arch Hard Loss 1.8003 (2.0174)	Arch Beta Loss 1262.2559 (1261.6541)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.4%, 96.9%)	
11/23 02:48:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.7578 (0.6856)	Arch Loss -1260.5848 (-1259.9330)	Arch Hard Loss 2.2500 (2.0198)	Arch Beta Loss 1262.8348 (1261.9528)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.8%, 96.8%)	
11/23 02:49:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.7650 (0.6868)	Arch Loss -1261.1439 (-1260.1963)	Arch Hard Loss 2.1946 (2.0192)	Arch Beta Loss 1263.3385 (1262.2155)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.8%, 96.8%)	
11/23 02:49:44午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 78.8440%
11/23 02:49:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.7608	Prec@(1,5) (43.8%, 72.9%)
11/23 02:50:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.7630	Prec@(1,5) (43.3%, 72.5%)
11/23 02:50:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.7467	Prec@(1,5) (43.4%, 72.6%)
11/23 02:50:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.7657	Prec@(1,5) (43.0%, 72.5%)
11/23 02:50:15午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 43.0000%
11/23 02:50:15午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:50:15午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 02:51:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.5342 (0.6272)	Arch Loss -1261.5780 (-1261.6231)	Arch Hard Loss 2.3061 (1.9954)	Arch Beta Loss 1263.8840 (1263.6185)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.2%, 97.5%)	
11/23 02:52:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.6055 (0.6470)	Arch Loss -1262.6447 (-1261.8573)	Arch Hard Loss 1.7585 (2.0256)	Arch Beta Loss 1264.4032 (1263.8829)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.4%, 97.1%)	
11/23 02:52:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.8074 (0.6518)	Arch Loss -1262.6598 (-1262.1230)	Arch Hard Loss 2.2456 (2.0185)	Arch Beta Loss 1264.9054 (1264.1415)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.1%, 97.1%)	
11/23 02:53:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.4408 (0.6574)	Arch Loss -1263.5582 (-1262.3552)	Arch Hard Loss 1.7830 (2.0137)	Arch Beta Loss 1265.3413 (1264.3689)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.0%, 97.0%)	
11/23 02:53:46午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 79.9440%
11/23 02:53:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 3.3621	Prec@(1,5) (35.8%, 66.5%)
11/23 02:54:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 3.3025	Prec@(1,5) (36.2%, 67.2%)
11/23 02:54:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 3.3063	Prec@(1,5) (36.4%, 67.2%)
11/23 02:54:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 3.3145	Prec@(1,5) (36.3%, 67.1%)
11/23 02:54:17午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 36.3480%
11/23 02:54:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:54:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 02:55:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.5882 (0.6278)	Arch Loss -1263.7291 (-1263.5959)	Arch Hard Loss 2.0847 (1.9880)	Arch Beta Loss 1265.8137 (1265.5839)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.3%, 97.2%)	
11/23 02:56:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.7734 (0.6253)	Arch Loss -1263.7935 (-1263.8132)	Arch Hard Loss 2.4716 (2.0001)	Arch Beta Loss 1266.2651 (1265.8133)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.1%, 97.3%)	
11/23 02:57:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.7384 (0.6287)	Arch Loss -1264.8126 (-1264.0191)	Arch Hard Loss 1.8884 (2.0187)	Arch Beta Loss 1266.7010 (1266.0378)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.0%, 97.3%)	
11/23 02:57:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.5413 (0.6280)	Arch Loss -1265.0154 (-1264.2210)	Arch Hard Loss 2.0634 (2.0142)	Arch Beta Loss 1267.0787 (1266.2351)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.1%, 97.3%)	
11/23 02:57:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 81.0680%
11/23 02:57:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.7163	Prec@(1,5) (42.8%, 72.7%)
11/23 02:58:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.7415	Prec@(1,5) (42.6%, 72.2%)
11/23 02:58:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.7200	Prec@(1,5) (42.7%, 72.6%)
11/23 02:58:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.7191	Prec@(1,5) (42.7%, 72.7%)
11/23 02:58:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 42.6840%
11/23 02:58:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 02:58:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 02:59:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.3767 (0.5935)	Arch Loss -1266.1277 (-1265.2744)	Arch Hard Loss 1.3615 (2.0149)	Arch Beta Loss 1267.4891 (1267.2893)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.1%, 98.0%)	
11/23 03:00:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.8330 (0.5962)	Arch Loss -1265.6256 (-1265.4619)	Arch Hard Loss 2.2547 (2.0266)	Arch Beta Loss 1267.8802 (1267.4885)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.8%, 97.7%)	
11/23 03:01:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.6412 (0.6043)	Arch Loss -1265.7781 (-1265.6496)	Arch Hard Loss 2.4800 (2.0335)	Arch Beta Loss 1268.2581 (1267.6831)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.5%, 97.6%)	
11/23 03:01:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.6295 (0.6072)	Arch Loss -1266.6986 (-1265.8224)	Arch Hard Loss 1.8879 (2.0319)	Arch Beta Loss 1268.5865 (1267.8543)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.5%, 97.5%)	
11/23 03:01:50午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 81.4600%
11/23 03:01:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 2.3815	Prec@(1,5) (46.5%, 76.7%)
11/23 03:02:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 2.4367	Prec@(1,5) (45.9%, 75.8%)
11/23 03:02:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 2.4440	Prec@(1,5) (45.9%, 75.6%)
11/23 03:02:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.4335	Prec@(1,5) (46.1%, 75.8%)
11/23 03:02:21午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 46.1360%
11/23 03:02:21午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 03:02:21午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 03:03:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.7150 (0.5558)	Arch Loss -1266.8796 (-1266.7660)	Arch Hard Loss 2.0640 (2.0036)	Arch Beta Loss 1268.9436 (1268.7696)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.2%, 97.9%)	
11/23 03:04:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.4140 (0.5736)	Arch Loss -1267.6592 (-1266.9339)	Arch Hard Loss 1.6234 (2.0086)	Arch Beta Loss 1269.2826 (1268.9425)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.7%)	
11/23 03:05:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.4279 (0.5793)	Arch Loss -1267.7583 (-1267.0988)	Arch Hard Loss 1.8515 (2.0125)	Arch Beta Loss 1269.6097 (1269.1113)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.7%)	
11/23 03:05:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.5661 (0.5815)	Arch Loss -1267.2206 (-1267.2357)	Arch Hard Loss 2.6745 (2.0241)	Arch Beta Loss 1269.8951 (1269.2598)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.6%)	
11/23 03:05:52午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 82.5800%
11/23 03:06:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.8141	Prec@(1,5) (41.8%, 71.8%)
11/23 03:06:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.7712	Prec@(1,5) (42.0%, 71.9%)
11/23 03:06:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.7729	Prec@(1,5) (41.9%, 71.9%)
11/23 03:06:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.7825	Prec@(1,5) (41.6%, 71.7%)
11/23 03:06:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 41.6760%
11/23 03:06:23午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 03:06:23午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 03:07:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.4430 (0.5727)	Arch Loss -1268.9756 (-1268.0516)	Arch Hard Loss 1.2287 (2.0019)	Arch Beta Loss 1270.2043 (1270.0535)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.5%, 97.9%)	
11/23 03:08:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.4306 (0.5722)	Arch Loss -1268.9857 (-1268.1607)	Arch Hard Loss 1.5136 (2.0431)	Arch Beta Loss 1270.4993 (1270.2038)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.5%, 98.0%)	
11/23 03:09:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.4069 (0.5781)	Arch Loss -1268.6238 (-1268.3211)	Arch Hard Loss 2.1601 (2.0293)	Arch Beta Loss 1270.7839 (1270.3504)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.8%)	
11/23 03:09:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.5893 (0.5787)	Arch Loss -1268.8862 (-1268.4470)	Arch Hard Loss 2.1455 (2.0326)	Arch Beta Loss 1271.0317 (1270.4796)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.8%)	
11/23 03:09:55午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 82.3960%
11/23 03:10:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.6147	Prec@(1,5) (44.2%, 74.3%)
11/23 03:10:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.6017	Prec@(1,5) (44.0%, 74.5%)
11/23 03:10:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.5912	Prec@(1,5) (44.1%, 74.7%)
11/23 03:10:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.6032	Prec@(1,5) (43.9%, 74.5%)
11/23 03:10:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 43.8680%
11/23 03:10:25午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 03:10:26午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 03:11:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.4966 (0.5561)	Arch Loss -1269.4800 (-1269.1868)	Arch Hard Loss 1.8209 (1.9830)	Arch Beta Loss 1271.3009 (1271.1699)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.9%, 97.9%)	
11/23 03:12:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.5835 (0.5612)	Arch Loss -1269.0413 (-1269.2689)	Arch Hard Loss 2.5163 (2.0316)	Arch Beta Loss 1271.5576 (1271.3005)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.0%, 97.8%)	
11/23 03:13:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.6069 (0.5722)	Arch Loss -1269.7102 (-1269.3957)	Arch Hard Loss 2.0946 (2.0324)	Arch Beta Loss 1271.8048 (1271.4281)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.7%, 97.8%)	
11/23 03:13:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.5877 (0.5765)	Arch Loss -1269.9877 (-1269.5060)	Arch Hard Loss 2.0338 (2.0344)	Arch Beta Loss 1272.0215 (1271.5404)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.7%)	
11/23 03:13:56午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 82.6200%
11/23 03:14:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 3.6458	Prec@(1,5) (31.5%, 60.8%)
11/23 03:14:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 3.5985	Prec@(1,5) (32.0%, 61.5%)
11/23 03:14:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 3.5742	Prec@(1,5) (32.2%, 61.9%)
11/23 03:14:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 3.5859	Prec@(1,5) (32.0%, 61.7%)
11/23 03:14:27午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 31.9960%
11/23 03:14:27午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 03:14:27午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 03:15:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.4971 (0.5553)	Arch Loss -1270.0934 (-1270.0968)	Arch Hard Loss 2.1619 (2.0447)	Arch Beta Loss 1272.2552 (1272.1415)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.2%, 97.9%)	
11/23 03:16:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.4389 (0.5556)	Arch Loss -1270.4821 (-1270.2101)	Arch Hard Loss 1.9967 (2.0451)	Arch Beta Loss 1272.4788 (1272.2552)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.3%, 97.8%)	
11/23 03:17:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.4787 (0.5607)	Arch Loss -1270.2610 (-1270.3200)	Arch Hard Loss 2.4346 (2.0465)	Arch Beta Loss 1272.6956 (1272.3665)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.0%, 97.9%)	
11/23 03:17:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.6343 (0.5663)	Arch Loss -1271.4196 (-1270.4206)	Arch Hard Loss 1.4636 (2.0438)	Arch Beta Loss 1272.8831 (1272.4644)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.8%, 97.9%)	
11/23 03:17:58午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 82.8080%
11/23 03:18:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.7909	Prec@(1,5) (40.0%, 72.3%)
11/23 03:18:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.8046	Prec@(1,5) (40.1%, 72.3%)
11/23 03:18:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.8332	Prec@(1,5) (39.9%, 71.9%)
11/23 03:18:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.8211	Prec@(1,5) (40.1%, 72.0%)
11/23 03:18:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 40.1200%
11/23 03:18:29午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 03:18:29午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.5160%
11/23 03:18:29午前 trainer_runner.py:110 [INFO] Final best Prec@1 = 48.5160%
11/23 03:18:29午前 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
