11/23 06:40:54AM parser.py:28 [INFO] 
11/23 06:40:54AM parser.py:29 [INFO] Parameters:
11/23 06:40:54AM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-0.01/DAG
11/23 06:40:54AM parser.py:31 [INFO] T=10.0
11/23 06:40:54AM parser.py:31 [INFO] ADVANCED=1
11/23 06:40:54AM parser.py:31 [INFO] ALPHA_LR=0.0003
11/23 06:40:54AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/23 06:40:54AM parser.py:31 [INFO] ARCH_CRITERION=expected
11/23 06:40:54AM parser.py:31 [INFO] BATCH_SIZE=64
11/23 06:40:54AM parser.py:31 [INFO] CASCADE=0
11/23 06:40:54AM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/23 06:40:54AM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/23 06:40:54AM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/23 06:40:54AM parser.py:31 [INFO] DATA_PATH=../data/
11/23 06:40:54AM parser.py:31 [INFO] DATASET=cifar100
11/23 06:40:54AM parser.py:31 [INFO] DEPTH_COEF=0.0
11/23 06:40:54AM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/23 06:40:54AM parser.py:31 [INFO] DISCRETE=0
11/23 06:40:54AM parser.py:31 [INFO] EPOCHS=50
11/23 06:40:54AM parser.py:31 [INFO] EVAL_EPOCHS=100
11/23 06:40:54AM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g-0.01
11/23 06:40:54AM parser.py:31 [INFO] FINAL_L=0.0
11/23 06:40:54AM parser.py:31 [INFO] G=-0.01
11/23 06:40:54AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/23 06:40:54AM parser.py:31 [INFO] GPUS=[0]
11/23 06:40:54AM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/23 06:40:54AM parser.py:31 [INFO] INIT_CHANNELS=16
11/23 06:40:54AM parser.py:31 [INFO] L=0.0
11/23 06:40:54AM parser.py:31 [INFO] LAYERS=32
11/23 06:40:54AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/23 06:40:54AM parser.py:31 [INFO] NAME=Pruning
11/23 06:40:54AM parser.py:31 [INFO] NONKD=1
11/23 06:40:54AM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-0.01
11/23 06:40:54AM parser.py:31 [INFO] PCDARTS=0
11/23 06:40:54AM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-0.01/plots
11/23 06:40:54AM parser.py:31 [INFO] PRINT_FREQ=100
11/23 06:40:54AM parser.py:31 [INFO] RESET=0
11/23 06:40:54AM parser.py:31 [INFO] RESUME_PATH=None
11/23 06:40:54AM parser.py:31 [INFO] SAVE=s0-expected-sw3-g-0.01
11/23 06:40:54AM parser.py:31 [INFO] SEED=0
11/23 06:40:54AM parser.py:31 [INFO] SHARE_STAGE=0
11/23 06:40:54AM parser.py:31 [INFO] SLIDE_WINDOW=3
11/23 06:40:54AM parser.py:31 [INFO] SPEC_CELL=1
11/23 06:40:54AM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/23 06:40:54AM parser.py:31 [INFO] TEACHER_NAME=none
11/23 06:40:54AM parser.py:31 [INFO] TEACHER_PATH=none
11/23 06:40:54AM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/23 06:40:54AM parser.py:31 [INFO] TYPE=Pruning
11/23 06:40:54AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/23 06:40:54AM parser.py:31 [INFO] W_LR=0.025
11/23 06:40:54AM parser.py:31 [INFO] W_LR_MIN=0.001
11/23 06:40:54AM parser.py:31 [INFO] W_MOMENTUM=0.9
11/23 06:40:54AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/23 06:40:54AM parser.py:31 [INFO] WORKERS=4
11/23 06:40:54AM parser.py:32 [INFO] 
11/23 06:40:55AM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/23 06:41:50AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.3015 (4.5292)	Arch Loss 0.6745 (0.8780)	Arch Hard Loss 4.3611 (4.5222)	Arch Beta Loss 368.6648 (364.4153)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.2%, 9.1%)	
11/23 06:42:43AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0952 (4.4064)	Arch Loss 0.5203 (0.7173)	Arch Hard Loss 4.2943 (4.4046)	Arch Beta Loss 377.3988 (368.7386)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.1%, 12.9%)	
11/23 06:43:37AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0569 (4.3058)	Arch Loss 0.1514 (0.5717)	Arch Hard Loss 4.0137 (4.3028)	Arch Beta Loss 386.2218 (373.1102)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.1%, 16.0%)	
11/23 06:44:24AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.9276 (4.2397)	Arch Loss -0.0141 (0.4640)	Arch Hard Loss 3.9290 (4.2348)	Arch Beta Loss 394.3068 (377.0789)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.0%, 18.2%)	
11/23 06:44:26AM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 4.9640%
11/23 06:44:34AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9731	Prec@(1,5) (7.8%, 26.4%)
11/23 06:44:42AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9675	Prec@(1,5) (8.1%, 26.4%)
11/23 06:44:49AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9674	Prec@(1,5) (8.1%, 26.6%)
11/23 06:44:57AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9660	Prec@(1,5) (8.1%, 26.5%)
11/23 06:44:57AM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 8.1240%
11/23 06:44:57AM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG2_concat=[3, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG3_concat=[2, 11])
11/23 06:44:57AM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 8.1240%
11/23 06:45:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.8497 (3.9237)	Arch Loss -0.0652 (-0.0617)	Arch Hard Loss 3.9694 (3.9278)	Arch Beta Loss 403.4599 (398.9579)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.4%, 27.9%)	
11/23 06:46:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.8310 (3.8923)	Arch Loss -0.2811 (-0.1560)	Arch Hard Loss 3.8453 (3.8792)	Arch Beta Loss 412.6388 (403.5257)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.8%, 29.2%)	
11/23 06:47:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7449 (3.8571)	Arch Loss -0.6635 (-0.2370)	Arch Hard Loss 3.5553 (3.8442)	Arch Beta Loss 421.8771 (408.1159)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.5%, 30.5%)	
11/23 06:48:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.9570 (3.8182)	Arch Loss -0.4189 (-0.3134)	Arch Hard Loss 3.8840 (3.8094)	Arch Beta Loss 430.2905 (412.2729)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.1%, 31.9%)	
11/23 06:48:28午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 10.1000%
11/23 06:48:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6397	Prec@(1,5) (13.6%, 37.5%)
11/23 06:48:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6362	Prec@(1,5) (13.6%, 37.6%)
11/23 06:48:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6381	Prec@(1,5) (13.6%, 37.6%)
11/23 06:48:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6375	Prec@(1,5) (13.6%, 37.5%)
11/23 06:48:58午前 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 13.6360%
11/23 06:48:58午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG3_concat=[10, 11])
11/23 06:48:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 13.6360%
11/23 06:49:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.9093 (3.6306)	Arch Loss -0.7729 (-0.7307)	Arch Hard Loss 3.6248 (3.6204)	Arch Beta Loss 439.7641 (435.1109)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.1%, 37.7%)	
11/23 06:50:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.7097 (3.6057)	Arch Loss -1.0064 (-0.7907)	Arch Hard Loss 3.4852 (3.6074)	Arch Beta Loss 449.1667 (439.8140)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.8%, 38.6%)	
11/23 06:51:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5551 (3.5730)	Arch Loss -0.9020 (-0.8692)	Arch Hard Loss 3.6837 (3.5759)	Arch Beta Loss 458.5716 (444.5107)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.3%, 39.6%)	
11/23 06:52:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.5255 (3.5507)	Arch Loss -1.3203 (-0.9372)	Arch Hard Loss 3.3502 (3.5502)	Arch Beta Loss 467.0505 (448.7423)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.7%, 40.1%)	
11/23 06:52:30午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 14.6960%
11/23 06:52:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4902	Prec@(1,5) (15.2%, 42.8%)
11/23 06:52:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4883	Prec@(1,5) (15.3%, 42.7%)
11/23 06:52:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4948	Prec@(1,5) (15.2%, 42.5%)
11/23 06:53:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4927	Prec@(1,5) (15.4%, 42.6%)
11/23 06:53:00午前 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 15.4000%
11/23 06:53:00午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 6)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/23 06:53:01午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 15.4000%
11/23 06:53:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.6639 (3.4102)	Arch Loss -1.3836 (-1.3125)	Arch Hard Loss 3.3832 (3.4070)	Arch Beta Loss 476.6760 (471.9496)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.7%, 44.2%)	
11/23 06:54:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.2628 (3.3849)	Arch Loss -1.4580 (-1.3846)	Arch Hard Loss 3.4052 (3.3830)	Arch Beta Loss 486.3267 (476.7567)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.4%, 45.1%)	
11/23 06:55:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.2287 (3.3517)	Arch Loss -1.4538 (-1.4519)	Arch Hard Loss 3.5052 (3.3636)	Arch Beta Loss 495.9055 (481.5562)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.1%, 45.9%)	
11/23 06:56:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.2801 (3.3311)	Arch Loss -1.7925 (-1.5097)	Arch Hard Loss 3.2537 (3.3491)	Arch Beta Loss 504.6199 (485.8819)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.5%, 46.3%)	
11/23 06:56:32午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 18.5120%
11/23 06:56:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2968	Prec@(1,5) (19.2%, 47.6%)
11/23 06:56:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2956	Prec@(1,5) (19.1%, 47.7%)
11/23 06:56:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.3037	Prec@(1,5) (19.0%, 47.4%)
11/23 06:57:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.3057	Prec@(1,5) (19.0%, 47.5%)
11/23 06:57:03午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 18.9720%
11/23 06:57:03午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG3_concat=[10, 11])
11/23 06:57:03午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 18.9720%
11/23 06:57:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.2168 (3.1844)	Arch Loss -2.0737 (-1.8239)	Arch Hard Loss 3.0707 (3.2723)	Arch Beta Loss 514.4384 (509.6199)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.4%, 50.9%)	
11/23 06:58:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.2272 (3.1713)	Arch Loss -2.2485 (-1.9134)	Arch Hard Loss 2.9947 (3.2318)	Arch Beta Loss 524.3223 (514.5211)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.5%, 50.9%)	
11/23 06:59:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.6868 (3.1634)	Arch Loss -2.1793 (-1.9816)	Arch Hard Loss 3.1626 (3.2129)	Arch Beta Loss 534.1918 (519.4443)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.5%, 51.1%)	
11/23 07:00:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.0268 (3.1509)	Arch Loss -2.4776 (-2.0523)	Arch Hard Loss 2.9538 (3.1866)	Arch Beta Loss 543.1444 (523.8915)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.7%, 51.5%)	
11/23 07:00:34午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 21.6920%
11/23 07:00:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.1982	Prec@(1,5) (21.3%, 52.2%)
11/23 07:00:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.1654	Prec@(1,5) (21.9%, 52.5%)
11/23 07:00:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.1579	Prec@(1,5) (22.0%, 52.6%)
11/23 07:01:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.1570	Prec@(1,5) (22.0%, 52.7%)
11/23 07:01:05午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 22.0400%
11/23 07:01:05午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG3_concat=[10, 11])
11/23 07:01:06午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 22.0400%
11/23 07:02:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1969 (2.9891)	Arch Loss -2.4509 (-2.4534)	Arch Hard Loss 3.0814 (3.0296)	Arch Beta Loss 553.2307 (548.2959)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.6%, 55.0%)	
11/23 07:02:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.2015 (2.9896)	Arch Loss -2.2178 (-2.4872)	Arch Hard Loss 3.4142 (3.0456)	Arch Beta Loss 563.1967 (553.2805)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.6%, 55.3%)	
11/23 07:03:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.9205 (2.9831)	Arch Loss -2.9989 (-2.5518)	Arch Hard Loss 2.7331 (3.0309)	Arch Beta Loss 573.1959 (558.2638)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.8%, 55.3%)	
11/23 07:04:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.6800 (2.9693)	Arch Loss -2.9536 (-2.6093)	Arch Hard Loss 2.8683 (3.0182)	Arch Beta Loss 582.1872 (562.7559)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.0%, 56.0%)	
11/23 07:04:37午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 25.0040%
11/23 07:04:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.9375	Prec@(1,5) (25.2%, 57.9%)
11/23 07:04:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9497	Prec@(1,5) (25.4%, 57.3%)
11/23 07:05:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9526	Prec@(1,5) (25.2%, 57.2%)
11/23 07:05:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9442	Prec@(1,5) (25.4%, 57.4%)
11/23 07:05:07午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 25.4160%
11/23 07:05:07午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/23 07:05:08午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 25.4160%
11/23 07:06:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.8414 (2.8308)	Arch Loss -2.8942 (-2.9575)	Arch Hard Loss 3.0274 (2.9156)	Arch Beta Loss 592.1664 (587.3036)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.9%, 60.0%)	
11/23 07:06:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.9603 (2.8337)	Arch Loss -3.1008 (-3.0218)	Arch Hard Loss 2.9192 (2.9005)	Arch Beta Loss 601.9972 (592.2311)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.8%, 59.7%)	
11/23 07:07:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.8643 (2.8285)	Arch Loss -3.3303 (-3.0890)	Arch Hard Loss 2.7880 (2.8825)	Arch Beta Loss 611.8304 (597.1446)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.8%, 59.8%)	
11/23 07:08:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.8372 (2.8229)	Arch Loss -3.3390 (-3.1299)	Arch Hard Loss 2.8667 (2.8856)	Arch Beta Loss 620.5645 (601.5537)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.0%, 60.0%)	
11/23 07:08:39午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 28.0360%
11/23 07:08:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.7800	Prec@(1,5) (29.0%, 60.5%)
11/23 07:08:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.7704	Prec@(1,5) (29.0%, 61.3%)
11/23 07:09:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.7813	Prec@(1,5) (29.0%, 61.1%)
11/23 07:09:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.7879	Prec@(1,5) (28.9%, 60.8%)
11/23 07:09:10午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 28.8880%
11/23 07:09:10午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/23 07:09:10午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 28.8880%
11/23 07:10:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.8268 (2.6864)	Arch Loss -3.3820 (-3.4419)	Arch Hard Loss 2.9203 (2.8133)	Arch Beta Loss 630.2322 (625.5270)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.3%, 63.4%)	
11/23 07:10:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.7473 (2.6973)	Arch Loss -3.5869 (-3.5039)	Arch Hard Loss 2.8095 (2.7987)	Arch Beta Loss 639.6354 (630.2550)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.8%, 63.0%)	
11/23 07:11:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.9045 (2.6911)	Arch Loss -3.1936 (-3.5583)	Arch Hard Loss 3.2962 (2.7913)	Arch Beta Loss 648.9764 (634.9583)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.1%, 63.1%)	
11/23 07:12:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.6891 (2.6843)	Arch Loss -3.8450 (-3.6193)	Arch Hard Loss 2.7283 (2.7724)	Arch Beta Loss 657.3345 (639.1701)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.4%, 63.2%)	
11/23 07:12:41午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 30.3560%
11/23 07:12:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.7261	Prec@(1,5) (30.2%, 62.5%)
11/23 07:12:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.7132	Prec@(1,5) (30.6%, 62.7%)
11/23 07:13:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.7142	Prec@(1,5) (30.4%, 62.6%)
11/23 07:13:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.7252	Prec@(1,5) (30.4%, 62.3%)
11/23 07:13:12午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 30.3760%
11/23 07:13:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:13:12午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 30.3760%
11/23 07:14:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.6433 (2.5574)	Arch Loss -3.6277 (-3.9117)	Arch Hard Loss 3.0365 (2.7081)	Arch Beta Loss 666.4113 (661.9783)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.5%, 66.6%)	
11/23 07:15:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.0998 (2.5694)	Arch Loss -3.7481 (-3.9614)	Arch Hard Loss 3.0043 (2.7031)	Arch Beta Loss 675.2421 (666.4430)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.5%, 66.1%)	
11/23 07:15:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.5731 (2.5663)	Arch Loss -3.9486 (-4.0102)	Arch Hard Loss 2.8911 (2.6982)	Arch Beta Loss 683.9708 (670.8491)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.9%, 66.2%)	
11/23 07:16:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.2743 (2.5614)	Arch Loss -4.2190 (-4.0662)	Arch Hard Loss 2.6974 (2.6815)	Arch Beta Loss 691.6343 (674.7749)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.1%, 66.3%)	
11/23 07:16:43午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 33.1000%
11/23 07:16:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5899	Prec@(1,5) (33.0%, 66.2%)
11/23 07:16:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.6043	Prec@(1,5) (32.3%, 65.5%)
11/23 07:17:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.6155	Prec@(1,5) (32.2%, 65.0%)
11/23 07:17:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.6193	Prec@(1,5) (32.2%, 64.8%)
11/23 07:17:14午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 32.2160%
11/23 07:17:14午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:17:15午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 32.2160%
11/23 07:18:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.6354 (2.4612)	Arch Loss -4.1502 (-4.3246)	Arch Hard Loss 2.8500 (2.6345)	Arch Beta Loss 700.0214 (695.9121)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.9%, 68.7%)	
11/23 07:19:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.4308 (2.4663)	Arch Loss -4.4504 (-4.3941)	Arch Hard Loss 2.6313 (2.6061)	Arch Beta Loss 708.1682 (700.0236)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.1%, 68.4%)	
11/23 07:19:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.3691 (2.4575)	Arch Loss -4.7573 (-4.4407)	Arch Hard Loss 2.4043 (2.6002)	Arch Beta Loss 716.1575 (704.0876)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.4%, 68.4%)	
11/23 07:20:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.2835 (2.4537)	Arch Loss -4.7373 (-4.4831)	Arch Hard Loss 2.4947 (2.5939)	Arch Beta Loss 723.1945 (707.6960)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.5%, 68.6%)	
11/23 07:20:46午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 35.5240%
11/23 07:20:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.5615	Prec@(1,5) (33.3%, 66.1%)
11/23 07:21:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.5454	Prec@(1,5) (33.6%, 66.4%)
11/23 07:21:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.5586	Prec@(1,5) (33.6%, 66.5%)
11/23 07:21:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.5572	Prec@(1,5) (33.8%, 66.5%)
11/23 07:21:17午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 33.8120%
11/23 07:21:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:21:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.8120%
11/23 07:22:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.4747 (2.3507)	Arch Loss -4.6263 (-4.6882)	Arch Hard Loss 2.6828 (2.5833)	Arch Beta Loss 730.9128 (727.1502)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.5%, 70.6%)	
11/23 07:23:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.4654 (2.3673)	Arch Loss -5.0305 (-4.7605)	Arch Hard Loss 2.3539 (2.5489)	Arch Beta Loss 738.4366 (730.9408)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.4%, 70.3%)	
11/23 07:24:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.4464 (2.3713)	Arch Loss -5.0563 (-4.8158)	Arch Hard Loss 2.4015 (2.5311)	Arch Beta Loss 745.7750 (734.6816)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.7%, 70.1%)	
11/23 07:24:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.3505 (2.3651)	Arch Loss -5.0199 (-4.8597)	Arch Hard Loss 2.5018 (2.5202)	Arch Beta Loss 752.1739 (737.9901)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.0%, 70.3%)	
11/23 07:24:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 36.9440%
11/23 07:24:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.5307	Prec@(1,5) (34.2%, 67.0%)
11/23 07:25:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.4834	Prec@(1,5) (35.0%, 67.9%)
11/23 07:25:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.4994	Prec@(1,5) (35.0%, 67.8%)
11/23 07:25:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4892	Prec@(1,5) (35.1%, 68.1%)
11/23 07:25:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 35.1320%
11/23 07:25:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:25:20午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 35.1320%
11/23 07:26:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.2088 (2.2705)	Arch Loss -4.6707 (-5.0651)	Arch Hard Loss 2.9212 (2.4925)	Arch Beta Loss 759.1885 (755.7528)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.3%, 72.7%)	
11/23 07:27:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.7612 (2.2666)	Arch Loss -4.9826 (-5.1036)	Arch Hard Loss 2.6774 (2.4883)	Arch Beta Loss 765.9966 (759.1991)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.2%, 72.7%)	
11/23 07:28:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.2636 (2.2667)	Arch Loss -5.5238 (-5.1589)	Arch Hard Loss 2.2028 (2.4670)	Arch Beta Loss 772.6603 (762.5897)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.5%, 72.8%)	
11/23 07:28:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.1060 (2.2706)	Arch Loss -5.5487 (-5.2006)	Arch Hard Loss 2.2365 (2.4554)	Arch Beta Loss 778.5256 (765.5968)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.4%, 72.7%)	
11/23 07:28:51午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 39.4480%
11/23 07:28:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.4071	Prec@(1,5) (37.9%, 69.7%)
11/23 07:29:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.3970	Prec@(1,5) (37.2%, 69.8%)
11/23 07:29:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.3835	Prec@(1,5) (37.6%, 70.1%)
11/23 07:29:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.3858	Prec@(1,5) (37.6%, 70.0%)
11/23 07:29:22午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 37.5440%
11/23 07:29:22午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 7), ('avg_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:29:22午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.5440%
11/23 07:30:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.1622 (2.1450)	Arch Loss -5.4534 (-5.4030)	Arch Hard Loss 2.3962 (2.4153)	Arch Beta Loss 784.9611 (781.8316)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.6%, 74.8%)	
11/23 07:31:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.2540 (2.1681)	Arch Loss -5.8305 (-5.4445)	Arch Hard Loss 2.0815 (2.4051)	Arch Beta Loss 791.2003 (784.9661)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.1%, 74.6%)	
11/23 07:32:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 1.9317 (2.1697)	Arch Loss -5.4479 (-5.4928)	Arch Hard Loss 2.5241 (2.3878)	Arch Beta Loss 797.2001 (788.0615)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.3%, 74.4%)	
11/23 07:32:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.7892 (2.1767)	Arch Loss -5.5697 (-5.5268)	Arch Hard Loss 2.4553 (2.3811)	Arch Beta Loss 802.5021 (790.7917)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.3%, 74.2%)	
11/23 07:32:53午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 41.3000%
11/23 07:33:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.3567	Prec@(1,5) (37.6%, 70.9%)
11/23 07:33:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.3442	Prec@(1,5) (37.8%, 71.0%)
11/23 07:33:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.3452	Prec@(1,5) (37.8%, 70.7%)
11/23 07:33:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3527	Prec@(1,5) (37.8%, 70.6%)
11/23 07:33:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 37.7760%
11/23 07:33:24午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:33:24午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.7760%
11/23 07:34:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1329 (2.0500)	Arch Loss -5.7228 (-5.7254)	Arch Hard Loss 2.3611 (2.3298)	Arch Beta Loss 808.3969 (805.5205)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.8%, 77.1%)	
11/23 07:35:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.1321 (2.1063)	Arch Loss -5.7440 (-5.7563)	Arch Hard Loss 2.3968 (2.3278)	Arch Beta Loss 814.0790 (808.4068)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.5%, 75.8%)	
11/23 07:36:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.9285 (2.1022)	Arch Loss -5.9683 (-5.7872)	Arch Hard Loss 2.2291 (2.3254)	Arch Beta Loss 819.7413 (811.2564)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.7%, 76.0%)	
11/23 07:36:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.0090 (2.1068)	Arch Loss -5.9490 (-5.8239)	Arch Hard Loss 2.2987 (2.3141)	Arch Beta Loss 824.7767 (813.8039)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.6%, 76.0%)	
11/23 07:36:55午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 42.6120%
11/23 07:37:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2271	Prec@(1,5) (40.9%, 73.4%)
11/23 07:37:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2490	Prec@(1,5) (40.4%, 73.2%)
11/23 07:37:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2322	Prec@(1,5) (40.8%, 73.4%)
11/23 07:37:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.2394	Prec@(1,5) (40.6%, 73.2%)
11/23 07:37:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 40.6000%
11/23 07:37:25午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:37:26午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.6000%
11/23 07:38:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 1.9183 (1.9787)	Arch Loss -5.8363 (-5.9663)	Arch Hard Loss 2.4666 (2.3097)	Arch Beta Loss 830.2970 (827.5990)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.7%, 78.3%)	
11/23 07:39:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.1718 (2.0176)	Arch Loss -6.2127 (-6.0154)	Arch Hard Loss 2.1441 (2.2877)	Arch Beta Loss 835.6786 (830.3062)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.8%, 77.2%)	
11/23 07:40:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.9549 (2.0234)	Arch Loss -5.8388 (-6.0472)	Arch Hard Loss 2.5710 (2.2827)	Arch Beta Loss 840.9809 (832.9931)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.7%, 77.2%)	
11/23 07:40:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.0686 (2.0221)	Arch Loss -6.3174 (-6.0818)	Arch Hard Loss 2.1403 (2.2722)	Arch Beta Loss 845.7722 (835.3963)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.6%, 77.2%)	
11/23 07:40:57午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 44.6320%
11/23 07:41:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.2519	Prec@(1,5) (41.3%, 72.7%)
11/23 07:41:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.2691	Prec@(1,5) (40.5%, 72.3%)
11/23 07:41:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.2715	Prec@(1,5) (40.4%, 72.4%)
11/23 07:41:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.2727	Prec@(1,5) (40.2%, 72.2%)
11/23 07:41:28午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 40.2040%
11/23 07:41:28午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:41:28午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.6000%
11/23 07:42:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.3362 (1.9217)	Arch Loss -6.3964 (-6.2413)	Arch Hard Loss 2.1143 (2.2435)	Arch Beta Loss 851.0698 (848.4819)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.2%, 79.2%)	
11/23 07:43:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.7668 (1.9417)	Arch Loss -6.3623 (-6.2563)	Arch Hard Loss 2.2005 (2.2545)	Arch Beta Loss 856.2737 (851.0776)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (46.8%, 78.8%)	
11/23 07:44:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.9416 (1.9501)	Arch Loss -6.6244 (-6.2904)	Arch Hard Loss 1.9898 (2.2464)	Arch Beta Loss 861.4163 (853.6748)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (46.5%, 78.7%)	
11/23 07:44:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.7179 (1.9565)	Arch Loss -6.6712 (-6.3172)	Arch Hard Loss 1.9893 (2.2429)	Arch Beta Loss 866.0531 (856.0031)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (46.2%, 78.6%)	
11/23 07:44:59午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 46.1440%
11/23 07:45:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.1969	Prec@(1,5) (41.0%, 75.0%)
11/23 07:45:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.1751	Prec@(1,5) (41.5%, 74.9%)
11/23 07:45:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.1874	Prec@(1,5) (41.3%, 74.7%)
11/23 07:45:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.1801	Prec@(1,5) (41.5%, 74.7%)
11/23 07:45:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 41.5160%
11/23 07:45:29午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:45:30午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.5160%
11/23 07:46:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.7240 (1.8713)	Arch Loss -6.6782 (-6.4800)	Arch Hard Loss 2.0345 (2.2072)	Arch Beta Loss 871.2673 (868.7141)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.5%, 80.2%)	
11/23 07:47:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.0626 (1.8899)	Arch Loss -6.3734 (-6.5101)	Arch Hard Loss 2.3907 (2.2027)	Arch Beta Loss 876.4113 (871.2815)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.9%, 80.0%)	
11/23 07:48:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 2.1106 (1.9017)	Arch Loss -6.6265 (-6.5239)	Arch Hard Loss 2.1887 (2.2145)	Arch Beta Loss 881.5169 (873.8493)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.8%, 79.8%)	
11/23 07:49:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9027 (1.8995)	Arch Loss -6.7275 (-6.5665)	Arch Hard Loss 2.1345 (2.1951)	Arch Beta Loss 886.2059 (876.1650)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.9%, 79.9%)	
11/23 07:49:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 47.8920%
11/23 07:49:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.2028	Prec@(1,5) (41.8%, 73.5%)
11/23 07:49:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1791	Prec@(1,5) (42.1%, 74.0%)
11/23 07:49:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1826	Prec@(1,5) (42.0%, 74.1%)
11/23 07:49:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1778	Prec@(1,5) (42.0%, 74.1%)
11/23 07:49:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 42.0560%
11/23 07:49:32午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:49:32午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.0560%
11/23 07:50:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.8492 (1.7752)	Arch Loss -6.8819 (-6.7365)	Arch Hard Loss 2.0325 (2.1523)	Arch Beta Loss 891.4393 (888.8761)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.4%, 82.4%)	
11/23 07:51:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.8096 (1.8005)	Arch Loss -6.7920 (-6.7516)	Arch Hard Loss 2.1743 (2.1631)	Arch Beta Loss 896.6315 (891.4678)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.6%, 81.8%)	
11/23 07:52:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.9578 (1.8283)	Arch Loss -7.1079 (-6.7685)	Arch Hard Loss 1.9110 (2.1722)	Arch Beta Loss 901.8890 (894.0700)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.9%, 81.2%)	
11/23 07:53:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.5649 (1.8394)	Arch Loss -6.9798 (-6.7938)	Arch Hard Loss 2.0854 (2.1704)	Arch Beta Loss 906.5176 (896.4144)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.5%, 81.1%)	
11/23 07:53:03午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 48.5080%
11/23 07:53:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.1028	Prec@(1,5) (43.9%, 76.1%)
11/23 07:53:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.0984	Prec@(1,5) (43.9%, 76.2%)
11/23 07:53:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.1074	Prec@(1,5) (43.8%, 75.9%)
11/23 07:53:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.1078	Prec@(1,5) (43.8%, 75.9%)
11/23 07:53:34午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 43.8320%
11/23 07:53:34午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[10, 11])
11/23 07:53:34午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.8320%
11/23 07:54:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.6336 (1.7264)	Arch Loss -6.9424 (-6.9414)	Arch Hard Loss 2.1755 (2.1508)	Arch Beta Loss 911.7834 (909.2254)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 83.4%)	
11/23 07:55:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.4749 (1.7695)	Arch Loss -7.1142 (-6.9593)	Arch Hard Loss 2.0558 (2.1589)	Arch Beta Loss 917.0035 (911.8196)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.5%, 82.3%)	
11/23 07:56:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 2.0388 (1.7765)	Arch Loss -7.1180 (-6.9876)	Arch Hard Loss 2.1059 (2.1570)	Arch Beta Loss 922.3896 (914.4550)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.3%, 82.0%)	
11/23 07:57:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.8690 (1.7873)	Arch Loss -7.6057 (-7.0250)	Arch Hard Loss 1.6661 (2.1434)	Arch Beta Loss 927.1797 (916.8408)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.1%, 82.0%)	
11/23 07:57:05午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 50.1480%
11/23 07:57:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.1161	Prec@(1,5) (43.9%, 75.9%)
11/23 07:57:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.1021	Prec@(1,5) (44.0%, 76.0%)
11/23 07:57:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.0945	Prec@(1,5) (44.5%, 76.0%)
11/23 07:57:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.0964	Prec@(1,5) (44.2%, 76.1%)
11/23 07:57:36午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 44.1640%
11/23 07:57:36午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 07:57:37午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.1640%
11/23 07:58:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.4956 (1.6805)	Arch Loss -6.9443 (-7.1722)	Arch Hard Loss 2.3808 (2.1265)	Arch Beta Loss 932.5069 (929.8793)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.9%, 83.6%)	
11/23 07:59:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.8309 (1.7132)	Arch Loss -7.1528 (-7.1952)	Arch Hard Loss 2.2264 (2.1302)	Arch Beta Loss 937.9176 (932.5359)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.2%, 83.2%)	
11/23 08:00:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.0683 (1.7183)	Arch Loss -7.7119 (-7.2431)	Arch Hard Loss 1.7205 (2.1092)	Arch Beta Loss 943.2375 (935.2247)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.0%, 83.1%)	
11/23 08:01:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.7002 (1.7364)	Arch Loss -7.6185 (-7.2644)	Arch Hard Loss 1.8622 (2.1120)	Arch Beta Loss 948.0735 (937.6352)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.6%, 82.9%)	
11/23 08:01:07午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 51.5840%
11/23 08:01:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.0955	Prec@(1,5) (44.9%, 75.4%)
11/23 08:01:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.1008	Prec@(1,5) (44.7%, 75.6%)
11/23 08:01:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.0927	Prec@(1,5) (44.7%, 75.9%)
11/23 08:01:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.0900	Prec@(1,5) (44.6%, 76.0%)
11/23 08:01:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 44.6080%
11/23 08:01:38午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 5)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:01:39午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.6080%
11/23 08:02:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.8084 (1.6223)	Arch Loss -7.4479 (-7.4005)	Arch Hard Loss 2.0862 (2.1074)	Arch Beta Loss 953.4013 (950.7910)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.6%, 84.9%)	
11/23 08:03:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.7491 (1.6455)	Arch Loss -6.9550 (-7.4428)	Arch Hard Loss 2.6342 (2.0922)	Arch Beta Loss 958.9171 (953.4909)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.5%, 84.2%)	
11/23 08:04:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3645 (1.6657)	Arch Loss -7.8230 (-7.4712)	Arch Hard Loss 1.8184 (2.0906)	Arch Beta Loss 964.1374 (956.1749)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.0%, 84.0%)	
11/23 08:05:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.7659 (1.6845)	Arch Loss -7.7614 (-7.5005)	Arch Hard Loss 1.9281 (2.0852)	Arch Beta Loss 968.9459 (958.5707)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.6%, 83.6%)	
11/23 08:05:09午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 52.5680%
11/23 08:05:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.0592	Prec@(1,5) (45.9%, 76.7%)
11/23 08:05:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.0375	Prec@(1,5) (46.2%, 77.0%)
11/23 08:05:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.0350	Prec@(1,5) (46.2%, 77.2%)
11/23 08:05:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.0370	Prec@(1,5) (45.9%, 77.0%)
11/23 08:05:40午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 45.9160%
11/23 08:05:40午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:05:41午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.9160%
11/23 08:06:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.6399 (1.5488)	Arch Loss -7.4334 (-7.6249)	Arch Hard Loss 2.3098 (2.0921)	Arch Beta Loss 974.3237 (971.6999)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.4%, 86.5%)	
11/23 08:07:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.6447 (1.6028)	Arch Loss -7.4451 (-7.6489)	Arch Hard Loss 2.3511 (2.0947)	Arch Beta Loss 979.6164 (974.3585)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.2%, 85.1%)	
11/23 08:08:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.5698 (1.6313)	Arch Loss -7.8908 (-7.6869)	Arch Hard Loss 1.9583 (2.0831)	Arch Beta Loss 984.9095 (977.0040)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.7%, 84.6%)	
11/23 08:09:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.5994 (1.6449)	Arch Loss -7.8087 (-7.7124)	Arch Hard Loss 2.0885 (2.0815)	Arch Beta Loss 989.7171 (979.3844)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.3%, 84.4%)	
11/23 08:09:12午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 53.3400%
11/23 08:09:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.0221	Prec@(1,5) (46.9%, 77.1%)
11/23 08:09:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.0219	Prec@(1,5) (46.4%, 77.2%)
11/23 08:09:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.0300	Prec@(1,5) (46.1%, 77.0%)
11/23 08:09:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.0217	Prec@(1,5) (46.1%, 77.2%)
11/23 08:09:43午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 46.1160%
11/23 08:09:43午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:09:43午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.1160%
11/23 08:10:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.7872 (1.5198)	Arch Loss -7.6540 (-7.8673)	Arch Hard Loss 2.2963 (2.0571)	Arch Beta Loss 995.0376 (992.4365)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.4%, 86.7%)	
11/23 08:11:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.6129 (1.5552)	Arch Loss -7.9939 (-7.9026)	Arch Hard Loss 2.0088 (2.0481)	Arch Beta Loss 1000.2697 (995.0757)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.9%, 86.0%)	
11/23 08:12:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.5912 (1.5770)	Arch Loss -7.8307 (-7.9244)	Arch Hard Loss 2.2243 (2.0524)	Arch Beta Loss 1005.5040 (997.6819)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.2%, 85.7%)	
11/23 08:13:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.8392 (1.5898)	Arch Loss -8.0213 (-7.9550)	Arch Hard Loss 2.0798 (2.0452)	Arch Beta Loss 1010.1080 (1000.0222)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.9%, 85.3%)	
11/23 08:13:15午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 54.8880%
11/23 08:13:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.0221	Prec@(1,5) (46.7%, 77.7%)
11/23 08:13:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.0218	Prec@(1,5) (46.5%, 77.7%)
11/23 08:13:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.0175	Prec@(1,5) (46.5%, 77.7%)
11/23 08:13:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.0248	Prec@(1,5) (46.5%, 77.6%)
11/23 08:13:46午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 46.5440%
11/23 08:13:46午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:13:47午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.5440%
11/23 08:14:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.6407 (1.4589)	Arch Loss -8.3054 (-8.0928)	Arch Hard Loss 1.8475 (2.0348)	Arch Beta Loss 1015.2942 (1012.7627)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.7%, 87.6%)	
11/23 08:15:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.3790 (1.5086)	Arch Loss -8.1454 (-8.1136)	Arch Hard Loss 2.0575 (2.0394)	Arch Beta Loss 1020.2846 (1015.2996)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.6%, 86.6%)	
11/23 08:16:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.6157 (1.5305)	Arch Loss -8.3002 (-8.1209)	Arch Hard Loss 1.9527 (2.0571)	Arch Beta Loss 1025.2809 (1017.8051)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.2%, 86.2%)	
11/23 08:17:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.6387 (1.5451)	Arch Loss -8.0412 (-8.1497)	Arch Hard Loss 2.2568 (2.0509)	Arch Beta Loss 1029.7997 (1020.0588)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.9%, 85.9%)	
11/23 08:17:18午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 55.9320%
11/23 08:17:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.9581	Prec@(1,5) (47.5%, 78.5%)
11/23 08:17:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.9784	Prec@(1,5) (47.4%, 78.2%)
11/23 08:17:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.9879	Prec@(1,5) (47.2%, 78.1%)
11/23 08:17:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.9962	Prec@(1,5) (47.0%, 78.1%)
11/23 08:17:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 47.0200%
11/23 08:17:49午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:17:49午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.0200%
11/23 08:18:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.1805 (1.4383)	Arch Loss -8.1035 (-8.3087)	Arch Hard Loss 2.2446 (2.0148)	Arch Beta Loss 1034.8126 (1032.3469)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.0%, 87.6%)	
11/23 08:19:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.5788 (1.4745)	Arch Loss -8.1757 (-8.3231)	Arch Hard Loss 2.2205 (2.0249)	Arch Beta Loss 1039.6184 (1034.8000)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.0%, 87.0%)	
11/23 08:20:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.7346 (1.4827)	Arch Loss -8.5169 (-8.3520)	Arch Hard Loss 1.9268 (2.0202)	Arch Beta Loss 1044.3734 (1037.2141)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.9%, 86.9%)	
11/23 08:21:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.4730 (1.4986)	Arch Loss -8.7456 (-8.3654)	Arch Hard Loss 1.7409 (2.0283)	Arch Beta Loss 1048.6517 (1039.3668)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.6%, 86.6%)	
11/23 08:21:21午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 57.5960%
11/23 08:21:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.0358	Prec@(1,5) (46.8%, 77.7%)
11/23 08:21:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.0178	Prec@(1,5) (46.9%, 78.0%)
11/23 08:21:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.0310	Prec@(1,5) (46.5%, 78.0%)
11/23 08:21:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.0217	Prec@(1,5) (46.7%, 78.1%)
11/23 08:21:52午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 46.6720%
11/23 08:21:52午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:21:52午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.0200%
11/23 08:22:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.4648 (1.4095)	Arch Loss -8.4062 (-8.5318)	Arch Hard Loss 2.1287 (1.9793)	Arch Beta Loss 1053.4863 (1051.1166)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.8%, 88.2%)	
11/23 08:23:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.4913 (1.4238)	Arch Loss -8.3213 (-8.5238)	Arch Hard Loss 2.2598 (2.0110)	Arch Beta Loss 1058.1104 (1053.4852)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.0%, 88.0%)	
11/23 08:24:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.2623 (1.4395)	Arch Loss -9.1443 (-8.5462)	Arch Hard Loss 1.4829 (2.0117)	Arch Beta Loss 1062.7186 (1055.7928)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.6%, 87.9%)	
11/23 08:25:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.3202 (1.4524)	Arch Loss -9.0851 (-8.5699)	Arch Hard Loss 1.5841 (2.0089)	Arch Beta Loss 1066.9191 (1057.8818)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.2%, 87.5%)	
11/23 08:25:24午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 58.2160%
11/23 08:25:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.0450	Prec@(1,5) (46.4%, 77.6%)
11/23 08:25:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.0428	Prec@(1,5) (46.9%, 77.8%)
11/23 08:25:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.0262	Prec@(1,5) (47.2%, 78.0%)
11/23 08:25:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.0286	Prec@(1,5) (47.2%, 77.9%)
11/23 08:25:54午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 47.1640%
11/23 08:25:54午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:25:55午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.1640%
11/23 08:26:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.6824 (1.3681)	Arch Loss -8.9488 (-8.7546)	Arch Hard Loss 1.7665 (1.9382)	Arch Beta Loss 1071.5305 (1069.2827)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.6%, 89.0%)	
11/23 08:27:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.7944 (1.3914)	Arch Loss -8.6043 (-8.7425)	Arch Hard Loss 2.1565 (1.9731)	Arch Beta Loss 1076.0833 (1071.5593)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.1%, 88.5%)	
11/23 08:28:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.2891 (1.3925)	Arch Loss -9.0332 (-8.7523)	Arch Hard Loss 1.7722 (1.9859)	Arch Beta Loss 1080.5354 (1073.8234)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.9%, 88.5%)	
11/23 08:29:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.1853 (1.4146)	Arch Loss -8.5482 (-8.7605)	Arch Hard Loss 2.2982 (1.9980)	Arch Beta Loss 1084.6356 (1075.8490)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.3%, 88.0%)	
11/23 08:29:26午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 59.2680%
11/23 08:29:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.9555	Prec@(1,5) (47.8%, 79.1%)
11/23 08:29:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.9493	Prec@(1,5) (48.2%, 79.0%)
11/23 08:29:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.9623	Prec@(1,5) (48.0%, 78.8%)
11/23 08:29:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.9636	Prec@(1,5) (47.8%, 78.9%)
11/23 08:29:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 47.8080%
11/23 08:29:56午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:29:57午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.8080%
11/23 08:30:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.2158 (1.2726)	Arch Loss -8.1795 (-8.8911)	Arch Hard Loss 2.7121 (1.9784)	Arch Beta Loss 1089.1544 (1086.9498)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.6%, 89.8%)	
11/23 08:31:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.6963 (1.3189)	Arch Loss -9.0923 (-8.9153)	Arch Hard Loss 1.8434 (1.9763)	Arch Beta Loss 1093.5729 (1089.1547)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.2%, 89.2%)	
11/23 08:32:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.2557 (1.3383)	Arch Loss -8.9615 (-8.9311)	Arch Hard Loss 2.0183 (1.9825)	Arch Beta Loss 1097.9856 (1091.3659)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.9%, 89.0%)	
11/23 08:33:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.2216 (1.3593)	Arch Loss -9.3228 (-8.9533)	Arch Hard Loss 1.6959 (1.9803)	Arch Beta Loss 1101.8722 (1093.3583)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.5%, 88.6%)	
11/23 08:33:28午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 60.5000%
11/23 08:33:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.9368	Prec@(1,5) (48.3%, 79.3%)
11/23 08:33:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.9379	Prec@(1,5) (48.6%, 79.3%)
11/23 08:33:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.9511	Prec@(1,5) (48.6%, 79.1%)
11/23 08:33:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.9461	Prec@(1,5) (48.5%, 79.2%)
11/23 08:33:59午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 48.4520%
11/23 08:33:59午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:33:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.4520%
11/23 08:34:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.6101 (1.2396)	Arch Loss -9.0473 (-9.1129)	Arch Hard Loss 2.0159 (1.9287)	Arch Beta Loss 1106.3147 (1104.1613)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.7%, 90.4%)	
11/23 08:35:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.1512 (1.2698)	Arch Loss -9.5468 (-9.1097)	Arch Hard Loss 1.5584 (1.9534)	Arch Beta Loss 1110.5162 (1106.3032)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.8%, 90.0%)	
11/23 08:36:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.2725 (1.2905)	Arch Loss -9.2209 (-9.1182)	Arch Hard Loss 1.9263 (1.9659)	Arch Beta Loss 1114.7231 (1108.4150)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.3%, 89.7%)	
11/23 08:37:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.3273 (1.3138)	Arch Loss -9.3867 (-9.1289)	Arch Hard Loss 1.7977 (1.9742)	Arch Beta Loss 1118.4326 (1110.3071)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.7%, 89.4%)	
11/23 08:37:30午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 61.6680%
11/23 08:37:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.9417	Prec@(1,5) (48.8%, 79.0%)
11/23 08:37:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 1.9570	Prec@(1,5) (48.2%, 78.8%)
11/23 08:37:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.9327	Prec@(1,5) (49.0%, 79.3%)
11/23 08:38:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.9281	Prec@(1,5) (49.2%, 79.4%)
11/23 08:38:01午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 49.1960%
11/23 08:38:01午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:38:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1960%
11/23 08:38:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.4405 (1.2073)	Arch Loss -9.3714 (-9.2500)	Arch Hard Loss 1.8545 (1.9556)	Arch Beta Loss 1122.5920 (1120.5571)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.6%, 91.1%)	
11/23 08:39:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.0331 (1.2171)	Arch Loss -9.4813 (-9.2489)	Arch Hard Loss 1.7840 (1.9770)	Arch Beta Loss 1126.5322 (1122.5840)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.4%, 91.0%)	
11/23 08:40:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.2302 (1.2475)	Arch Loss -9.3282 (-9.2784)	Arch Hard Loss 1.9769 (1.9674)	Arch Beta Loss 1130.5127 (1124.5817)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.6%, 90.5%)	
11/23 08:41:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.1713 (1.2615)	Arch Loss -9.4106 (-9.3047)	Arch Hard Loss 1.9283 (1.9588)	Arch Beta Loss 1133.8868 (1126.3445)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.1%, 90.3%)	
11/23 08:41:33午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 63.1280%
11/23 08:41:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.0278	Prec@(1,5) (49.1%, 78.4%)
11/23 08:41:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.0107	Prec@(1,5) (49.5%, 78.9%)
11/23 08:41:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.0207	Prec@(1,5) (49.0%, 78.9%)
11/23 08:42:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.0154	Prec@(1,5) (48.8%, 79.1%)
11/23 08:42:04午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 48.8320%
11/23 08:42:04午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:42:04午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1960%
11/23 08:42:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.2448 (1.1700)	Arch Loss -9.7417 (-9.4282)	Arch Hard Loss 1.6349 (1.9301)	Arch Beta Loss 1137.6599 (1135.8339)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.7%, 91.7%)	
11/23 08:43:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.5221 (1.1754)	Arch Loss -9.0127 (-9.4023)	Arch Hard Loss 2.3991 (1.9741)	Arch Beta Loss 1141.1842 (1137.6429)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.4%, 91.6%)	
11/23 08:44:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.3482 (1.1968)	Arch Loss -9.5926 (-9.4318)	Arch Hard Loss 1.8547 (1.9623)	Arch Beta Loss 1144.7273 (1139.4103)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.7%, 91.2%)	
11/23 08:45:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.4294 (1.2182)	Arch Loss -9.1642 (-9.4453)	Arch Hard Loss 2.3144 (1.9648)	Arch Beta Loss 1147.8593 (1141.0071)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.1%, 90.9%)	
11/23 08:45:36午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 64.1160%
11/23 08:45:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 1.9700	Prec@(1,5) (49.7%, 78.9%)
11/23 08:45:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 1.9647	Prec@(1,5) (49.8%, 79.0%)
11/23 08:45:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 1.9624	Prec@(1,5) (49.7%, 79.2%)
11/23 08:46:07午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 1.9614	Prec@(1,5) (49.7%, 79.3%)
11/23 08:46:07午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 49.7080%
11/23 08:46:07午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:46:07午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.7080%
11/23 08:47:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.0613 (1.1256)	Arch Loss -9.1105 (-9.5648)	Arch Hard Loss 2.4019 (1.9311)	Arch Beta Loss 1151.2474 (1149.5886)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.1%, 92.0%)	
11/23 08:47:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.1291 (1.1388)	Arch Loss -9.5216 (-9.5444)	Arch Hard Loss 2.0220 (1.9676)	Arch Beta Loss 1154.3571 (1151.1960)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 91.9%)	
11/23 08:48:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.3206 (1.1679)	Arch Loss -9.3253 (-9.5517)	Arch Hard Loss 2.2494 (1.9761)	Arch Beta Loss 1157.4695 (1152.7775)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.4%, 91.6%)	
11/23 08:49:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.3927 (1.1811)	Arch Loss -9.7045 (-9.5727)	Arch Hard Loss 1.8980 (1.9691)	Arch Beta Loss 1160.2438 (1154.1796)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.0%, 91.4%)	
11/23 08:49:39午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 64.9680%
11/23 08:49:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 1.8931	Prec@(1,5) (51.0%, 80.7%)
11/23 08:49:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 1.8989	Prec@(1,5) (50.9%, 80.7%)
11/23 08:50:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 1.9067	Prec@(1,5) (50.8%, 80.6%)
11/23 08:50:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 1.9180	Prec@(1,5) (50.5%, 80.4%)
11/23 08:50:10午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 50.5240%
11/23 08:50:10午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:50:10午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.5240%
11/23 08:51:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.3473 (1.0657)	Arch Loss -10.0838 (-9.6834)	Arch Hard Loss 1.5490 (1.9345)	Arch Beta Loss 1163.2831 (1161.7871)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.8%, 92.9%)	
11/23 08:51:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.0743 (1.0938)	Arch Loss -9.4209 (-9.6875)	Arch Hard Loss 2.2399 (1.9450)	Arch Beta Loss 1166.0721 (1163.2538)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.3%, 92.5%)	
11/23 08:52:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.1027 (1.1013)	Arch Loss -9.7803 (-9.6920)	Arch Hard Loss 1.9084 (1.9547)	Arch Beta Loss 1168.8752 (1164.6771)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.3%, 92.4%)	
11/23 08:53:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.1933 (1.1179)	Arch Loss -9.8615 (-9.7019)	Arch Hard Loss 1.8507 (1.9573)	Arch Beta Loss 1171.2217 (1165.9180)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.0%, 92.2%)	
11/23 08:53:42午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 67.0400%
11/23 08:53:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.0082	Prec@(1,5) (48.8%, 79.6%)
11/23 08:53:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 1.9950	Prec@(1,5) (49.2%, 79.7%)
11/23 08:54:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 1.9866	Prec@(1,5) (49.0%, 79.7%)
11/23 08:54:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 1.9894	Prec@(1,5) (49.3%, 79.4%)
11/23 08:54:13午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 49.2960%
11/23 08:54:13午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:54:13午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.5240%
11/23 08:55:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.1489 (1.0193)	Arch Loss -9.9204 (-9.7879)	Arch Hard Loss 1.8183 (1.9378)	Arch Beta Loss 1173.8682 (1172.5686)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.3%, 93.5%)	
11/23 08:56:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.9584 (1.0448)	Arch Loss -10.1399 (-9.8185)	Arch Hard Loss 1.6253 (1.9205)	Arch Beta Loss 1176.5171 (1173.9002)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.7%, 93.2%)	
11/23 08:56:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.0390 (1.0628)	Arch Loss -9.1637 (-9.7992)	Arch Hard Loss 2.6252 (1.9525)	Arch Beta Loss 1178.8870 (1175.1681)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.1%, 92.9%)	
11/23 08:57:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.2878 (1.0799)	Arch Loss -10.1211 (-9.8155)	Arch Hard Loss 1.6897 (1.9473)	Arch Beta Loss 1181.0800 (1176.2809)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.6%, 92.7%)	
11/23 08:57:45午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 67.5800%
11/23 08:57:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.0353	Prec@(1,5) (48.3%, 78.4%)
11/23 08:58:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.0070	Prec@(1,5) (49.2%, 78.9%)
11/23 08:58:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.0046	Prec@(1,5) (49.3%, 79.0%)
11/23 08:58:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.0021	Prec@(1,5) (49.3%, 79.2%)
11/23 08:58:16午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 49.3080%
11/23 08:58:16午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 08:58:16午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.5240%
11/23 08:59:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.8463 (0.9662)	Arch Loss -9.6158 (-9.9269)	Arch Hard Loss 2.2182 (1.8958)	Arch Beta Loss 1183.4026 (1182.2716)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.1%, 93.8%)	
11/23 09:00:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.9047 (1.0019)	Arch Loss -9.5686 (-9.8954)	Arch Hard Loss 2.2871 (1.9387)	Arch Beta Loss 1185.5726 (1183.4014)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.1%, 93.5%)	
11/23 09:00:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.9838 (1.0081)	Arch Loss -9.6731 (-9.9038)	Arch Hard Loss 2.2051 (1.9412)	Arch Beta Loss 1187.8164 (1184.4994)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.9%, 93.4%)	
11/23 09:01:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.0863 (1.0257)	Arch Loss -9.8172 (-9.9036)	Arch Hard Loss 2.0801 (1.9513)	Arch Beta Loss 1189.7245 (1185.4896)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.2%, 93.3%)	
11/23 09:01:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 69.2360%
11/23 09:01:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 1.9996	Prec@(1,5) (50.5%, 80.0%)
11/23 09:02:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.0344	Prec@(1,5) (49.6%, 79.6%)
11/23 09:02:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.0202	Prec@(1,5) (49.7%, 79.7%)
11/23 09:02:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.0117	Prec@(1,5) (49.8%, 79.7%)
11/23 09:02:19午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 49.8000%
11/23 09:02:19午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:02:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.5240%
11/23 09:03:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.9801 (0.9209)	Arch Loss -10.2244 (-9.9441)	Arch Hard Loss 1.6941 (1.9641)	Arch Beta Loss 1191.8521 (1190.8175)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.9%, 94.7%)	
11/23 09:04:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.1341 (0.9542)	Arch Loss -10.2022 (-9.9585)	Arch Hard Loss 1.7355 (1.9599)	Arch Beta Loss 1193.7722 (1191.8437)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.3%, 94.2%)	
11/23 09:05:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.1543 (0.9608)	Arch Loss -9.9169 (-9.9880)	Arch Hard Loss 2.0403 (1.9401)	Arch Beta Loss 1195.7202 (1192.8155)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.0%, 94.1%)	
11/23 09:05:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.2187 (0.9665)	Arch Loss -10.1190 (-9.9875)	Arch Hard Loss 1.8541 (1.9492)	Arch Beta Loss 1197.3037 (1193.6660)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.9%, 93.9%)	
11/23 09:05:51午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 70.8800%
11/23 09:05:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 1.8991	Prec@(1,5) (52.7%, 81.2%)
11/23 09:06:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 1.9135	Prec@(1,5) (52.2%, 81.2%)
11/23 09:06:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 1.9230	Prec@(1,5) (52.3%, 81.0%)
11/23 09:06:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 1.9317	Prec@(1,5) (52.2%, 80.9%)
11/23 09:06:21午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 52.2480%
11/23 09:06:21午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:06:22午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.2480%
11/23 09:07:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.9053 (0.8658)	Arch Loss -9.6257 (-10.0008)	Arch Hard Loss 2.3657 (1.9815)	Arch Beta Loss 1199.1421 (1198.2269)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 95.1%)	
11/23 09:08:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.9706 (0.8947)	Arch Loss -10.5332 (-10.0391)	Arch Hard Loss 1.4748 (1.9518)	Arch Beta Loss 1200.8015 (1199.0938)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.8%, 94.8%)	
11/23 09:09:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.9196 (0.9058)	Arch Loss -10.1209 (-10.0382)	Arch Hard Loss 1.9024 (1.9611)	Arch Beta Loss 1202.3260 (1199.9329)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.8%)	
11/23 09:09:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.7816 (0.9161)	Arch Loss -10.5269 (-10.0460)	Arch Hard Loss 1.5100 (1.9604)	Arch Beta Loss 1203.6919 (1200.6435)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.1%, 94.6%)	
11/23 09:09:53午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 72.0320%
11/23 09:10:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 1.9979	Prec@(1,5) (50.9%, 79.6%)
11/23 09:10:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 1.9916	Prec@(1,5) (50.9%, 79.8%)
11/23 09:10:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.0029	Prec@(1,5) (50.8%, 79.6%)
11/23 09:10:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.0039	Prec@(1,5) (50.5%, 79.7%)
11/23 09:10:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 50.5280%
11/23 09:10:24午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:10:24午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.2480%
11/23 09:11:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.7841 (0.8194)	Arch Loss -9.9887 (-10.0655)	Arch Hard Loss 2.0641 (1.9796)	Arch Beta Loss 1205.2842 (1204.5054)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.3%, 95.8%)	
11/23 09:12:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.8209 (0.8392)	Arch Loss -9.5891 (-10.0906)	Arch Hard Loss 2.4784 (1.9621)	Arch Beta Loss 1206.7415 (1205.2714)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.5%, 95.4%)	
11/23 09:13:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.9715 (0.8539)	Arch Loss -10.1988 (-10.0926)	Arch Hard Loss 1.8822 (1.9673)	Arch Beta Loss 1208.1001 (1205.9935)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.1%, 95.1%)	
11/23 09:13:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.7940 (0.8631)	Arch Loss -10.6177 (-10.1033)	Arch Hard Loss 1.4772 (1.9632)	Arch Beta Loss 1209.4960 (1206.6470)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.7%, 95.0%)	
11/23 09:13:55午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 73.7560%
11/23 09:14:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 1.9519	Prec@(1,5) (52.0%, 81.3%)
11/23 09:14:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 1.9426	Prec@(1,5) (52.3%, 81.0%)
11/23 09:14:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 1.9269	Prec@(1,5) (52.5%, 81.0%)
11/23 09:14:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 1.9381	Prec@(1,5) (52.2%, 81.0%)
11/23 09:14:26午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 52.2040%
11/23 09:14:26午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:14:26午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.2480%
11/23 09:15:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.6486 (0.8108)	Arch Loss -10.2346 (-10.1650)	Arch Hard Loss 1.8755 (1.9377)	Arch Beta Loss 1211.0118 (1210.2708)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.7%)	
11/23 09:16:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.6689 (0.8127)	Arch Loss -10.3049 (-10.1784)	Arch Hard Loss 1.8189 (1.9315)	Arch Beta Loss 1212.3763 (1210.9903)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.7%)	
11/23 09:17:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.8929 (0.8140)	Arch Loss -10.4363 (-10.1737)	Arch Hard Loss 1.6990 (1.9428)	Arch Beta Loss 1213.5367 (1211.6490)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.7%)	
11/23 09:17:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.7894 (0.8238)	Arch Loss -9.8412 (-10.1517)	Arch Hard Loss 2.3038 (1.9702)	Arch Beta Loss 1214.4957 (1212.1950)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.8%, 95.6%)	
11/23 09:17:58午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 74.8320%
11/23 09:18:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.0728	Prec@(1,5) (51.6%, 79.9%)
11/23 09:18:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.0620	Prec@(1,5) (51.4%, 80.4%)
11/23 09:18:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.0575	Prec@(1,5) (51.5%, 80.6%)
11/23 09:18:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.0598	Prec@(1,5) (51.4%, 80.6%)
11/23 09:18:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 51.4000%
11/23 09:18:29午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:18:29午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.2480%
11/23 09:19:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.7901 (0.7460)	Arch Loss -10.4617 (-10.1924)	Arch Hard Loss 1.6953 (1.9586)	Arch Beta Loss 1215.6925 (1215.1043)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.6%, 96.2%)	
11/23 09:20:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.7454 (0.7604)	Arch Loss -10.1470 (-10.2003)	Arch Hard Loss 2.0213 (1.9566)	Arch Beta Loss 1216.8264 (1215.6873)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.1%)	
11/23 09:21:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.4921 (0.7653)	Arch Loss -10.0062 (-10.2099)	Arch Hard Loss 2.1734 (1.9526)	Arch Beta Loss 1217.9662 (1216.2541)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.9%, 96.1%)	
11/23 09:21:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.9364 (0.7702)	Arch Loss -10.1484 (-10.2102)	Arch Hard Loss 2.0419 (1.9576)	Arch Beta Loss 1219.0338 (1216.7751)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 96.0%)	
11/23 09:22:00午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 76.7760%
11/23 09:22:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.0234	Prec@(1,5) (51.8%, 80.8%)
11/23 09:22:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.0233	Prec@(1,5) (51.8%, 80.4%)
11/23 09:22:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.0116	Prec@(1,5) (51.8%, 80.6%)
11/23 09:22:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.0116	Prec@(1,5) (51.7%, 80.5%)
11/23 09:22:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 51.7480%
11/23 09:22:31午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:22:31午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.2480%
11/23 09:23:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.6993 (0.6873)	Arch Loss -9.8979 (-10.2018)	Arch Hard Loss 2.3021 (1.9935)	Arch Beta Loss 1219.9983 (1219.5338)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.4%, 96.9%)	
11/23 09:24:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.8674 (0.6958)	Arch Loss -10.3535 (-10.2155)	Arch Hard Loss 1.8567 (1.9846)	Arch Beta Loss 1221.0214 (1220.0125)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.9%, 96.7%)	
11/23 09:25:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.5780 (0.7141)	Arch Loss -10.2721 (-10.2123)	Arch Hard Loss 1.9469 (1.9929)	Arch Beta Loss 1221.9032 (1220.5133)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.3%, 96.6%)	
11/23 09:26:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.9583 (0.7196)	Arch Loss -10.4466 (-10.2330)	Arch Hard Loss 1.7817 (1.9764)	Arch Beta Loss 1222.8396 (1220.9361)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.1%, 96.6%)	
11/23 09:26:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 78.0600%
11/23 09:26:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 1.9404	Prec@(1,5) (52.7%, 81.9%)
11/23 09:26:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 1.9410	Prec@(1,5) (53.0%, 81.6%)
11/23 09:26:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 1.9341	Prec@(1,5) (53.0%, 81.7%)
11/23 09:26:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 1.9385	Prec@(1,5) (53.0%, 81.7%)
11/23 09:26:32午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 52.9840%
11/23 09:26:32午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:26:33午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.9840%
11/23 09:27:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.5102 (0.6550)	Arch Loss -10.2901 (-10.2534)	Arch Hard Loss 1.9462 (1.9792)	Arch Beta Loss 1223.6313 (1223.2682)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.2%, 97.1%)	
11/23 09:28:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.6074 (0.6682)	Arch Loss -10.4079 (-10.2585)	Arch Hard Loss 1.8381 (1.9785)	Arch Beta Loss 1224.6045 (1223.6982)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.8%, 97.0%)	
11/23 09:29:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.5291 (0.6705)	Arch Loss -10.3354 (-10.2622)	Arch Hard Loss 1.9200 (1.9793)	Arch Beta Loss 1225.5416 (1224.1571)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.7%, 96.9%)	
11/23 09:30:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.9969 (0.6782)	Arch Loss -10.3894 (-10.2556)	Arch Hard Loss 1.8720 (1.9898)	Arch Beta Loss 1226.1411 (1224.5393)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.5%, 96.9%)	
11/23 09:30:04午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 79.4480%
11/23 09:30:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.0197	Prec@(1,5) (51.7%, 80.5%)
11/23 09:30:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.0149	Prec@(1,5) (52.1%, 80.5%)
11/23 09:30:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 1.9980	Prec@(1,5) (52.5%, 80.7%)
11/23 09:30:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 1.9927	Prec@(1,5) (52.6%, 80.8%)
11/23 09:30:35午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 52.5760%
11/23 09:30:35午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:30:35午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.9840%
11/23 09:31:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.6249 (0.6214)	Arch Loss -10.4464 (-10.2672)	Arch Hard Loss 1.8237 (1.9988)	Arch Beta Loss 1227.0049 (1226.5974)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.3%, 97.5%)	
11/23 09:32:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.6978 (0.6239)	Arch Loss -10.4473 (-10.3084)	Arch Hard Loss 1.8314 (1.9619)	Arch Beta Loss 1227.8730 (1227.0297)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.1%, 97.4%)	
11/23 09:33:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.4909 (0.6293)	Arch Loss -9.8265 (-10.2943)	Arch Hard Loss 2.4584 (1.9800)	Arch Beta Loss 1228.4849 (1227.4311)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.9%, 97.4%)	
11/23 09:34:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.7785 (0.6376)	Arch Loss -9.9517 (-10.2927)	Arch Hard Loss 2.3396 (1.9848)	Arch Beta Loss 1229.1249 (1227.7479)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.6%, 97.3%)	
11/23 09:34:06午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 80.6400%
11/23 09:34:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 1.9704	Prec@(1,5) (52.3%, 81.7%)
11/23 09:34:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.0192	Prec@(1,5) (51.8%, 80.9%)
11/23 09:34:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.0092	Prec@(1,5) (52.4%, 80.8%)
11/23 09:34:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.0110	Prec@(1,5) (52.3%, 80.8%)
11/23 09:34:36午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 52.2560%
11/23 09:34:36午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:34:36午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.9840%
11/23 09:35:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.6915 (0.5861)	Arch Loss -10.0969 (-10.3304)	Arch Hard Loss 2.2022 (1.9647)	Arch Beta Loss 1229.9048 (1229.5122)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.0%, 97.8%)	
11/23 09:36:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.4972 (0.6141)	Arch Loss -10.4638 (-10.3225)	Arch Hard Loss 1.8427 (1.9764)	Arch Beta Loss 1230.6508 (1229.8859)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.2%, 97.4%)	
11/23 09:37:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.7616 (0.6108)	Arch Loss -9.9189 (-10.3137)	Arch Hard Loss 2.3936 (1.9886)	Arch Beta Loss 1231.2557 (1230.2336)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.3%, 97.4%)	
11/23 09:38:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.4940 (0.6119)	Arch Loss -10.6859 (-10.3169)	Arch Hard Loss 1.6320 (1.9885)	Arch Beta Loss 1231.7942 (1230.5403)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.3%, 97.4%)	
11/23 09:38:07午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 81.2800%
11/23 09:38:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 1.9581	Prec@(1,5) (53.4%, 81.2%)
11/23 09:38:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 1.9576	Prec@(1,5) (53.2%, 81.3%)
11/23 09:38:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 1.9691	Prec@(1,5) (53.0%, 81.2%)
11/23 09:38:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 1.9813	Prec@(1,5) (52.8%, 81.2%)
11/23 09:38:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 52.8480%
11/23 09:38:38午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:38:38午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.9840%
11/23 09:39:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.6561 (0.5601)	Arch Loss -9.8739 (-10.3347)	Arch Hard Loss 2.4512 (1.9873)	Arch Beta Loss 1232.5034 (1232.2021)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.9%, 98.0%)	
11/23 09:40:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.7205 (0.5690)	Arch Loss -9.7498 (-10.3413)	Arch Hard Loss 2.5823 (1.9839)	Arch Beta Loss 1233.2130 (1232.5226)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.8%, 97.8%)	
11/23 09:41:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.5387 (0.5815)	Arch Loss -10.3113 (-10.3292)	Arch Hard Loss 2.0275 (1.9995)	Arch Beta Loss 1233.8854 (1232.8711)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.8%)	
11/23 09:42:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.4100 (0.5835)	Arch Loss -10.1202 (-10.3356)	Arch Hard Loss 2.2231 (1.9960)	Arch Beta Loss 1234.3279 (1233.1600)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.2%, 97.8%)	
11/23 09:42:09午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 82.2040%
11/23 09:42:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.0212	Prec@(1,5) (52.1%, 81.4%)
11/23 09:42:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.0505	Prec@(1,5) (52.1%, 80.7%)
11/23 09:42:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.0334	Prec@(1,5) (52.3%, 80.9%)
11/23 09:42:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.0414	Prec@(1,5) (52.2%, 80.8%)
11/23 09:42:39午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 52.1720%
11/23 09:42:39午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:42:40午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.9840%
11/23 09:43:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.4432 (0.5542)	Arch Loss -10.7175 (-10.3412)	Arch Hard Loss 1.6325 (2.0054)	Arch Beta Loss 1235.0024 (1234.6592)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.7%, 98.1%)	
11/23 09:44:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.6828 (0.5507)	Arch Loss -10.1558 (-10.3471)	Arch Hard Loss 2.1991 (2.0025)	Arch Beta Loss 1235.4883 (1234.9665)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.5%, 98.0%)	
11/23 09:45:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.6534 (0.5545)	Arch Loss -10.3984 (-10.3528)	Arch Hard Loss 1.9616 (1.9996)	Arch Beta Loss 1236.0017 (1235.2384)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.4%, 97.9%)	
11/23 09:46:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.4023 (0.5565)	Arch Loss -10.6936 (-10.3572)	Arch Hard Loss 1.6709 (1.9974)	Arch Beta Loss 1236.4551 (1235.4650)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.5%, 97.9%)	
11/23 09:46:11午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 83.4440%
11/23 09:46:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 1.9303	Prec@(1,5) (54.3%, 81.9%)
11/23 09:46:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 1.9905	Prec@(1,5) (53.1%, 81.2%)
11/23 09:46:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 1.9945	Prec@(1,5) (52.9%, 81.3%)
11/23 09:46:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 1.9876	Prec@(1,5) (53.1%, 81.5%)
11/23 09:46:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 53.1000%
11/23 09:46:41午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:46:42午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.1000%
11/23 09:47:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.7026 (0.5047)	Arch Loss -10.3701 (-10.3621)	Arch Hard Loss 2.0006 (2.0059)	Arch Beta Loss 1237.0724 (1236.7935)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.2%, 98.3%)	
11/23 09:48:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.4143 (0.5172)	Arch Loss -10.3523 (-10.3800)	Arch Hard Loss 2.0239 (1.9905)	Arch Beta Loss 1237.6201 (1237.0551)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.6%, 98.2%)	
11/23 09:49:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.4026 (0.5263)	Arch Loss -10.4064 (-10.3749)	Arch Hard Loss 1.9741 (1.9982)	Arch Beta Loss 1238.0529 (1237.3119)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.1%, 98.2%)	
11/23 09:50:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.5767 (0.5335)	Arch Loss -10.1510 (-10.3723)	Arch Hard Loss 2.2332 (2.0030)	Arch Beta Loss 1238.4202 (1237.5310)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.9%, 98.1%)	
11/23 09:50:13午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 83.9320%
11/23 09:50:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.0380	Prec@(1,5) (52.2%, 81.4%)
11/23 09:50:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.0011	Prec@(1,5) (53.0%, 81.7%)
11/23 09:50:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.0109	Prec@(1,5) (52.9%, 81.5%)
11/23 09:50:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.0078	Prec@(1,5) (52.8%, 81.5%)
11/23 09:50:44午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 52.7880%
11/23 09:50:44午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:50:44午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.1000%
11/23 09:51:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.4820 (0.5036)	Arch Loss -10.5239 (-10.3682)	Arch Hard Loss 1.8638 (2.0179)	Arch Beta Loss 1238.7672 (1238.6078)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.6%, 98.4%)	
11/23 09:52:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.5603 (0.5067)	Arch Loss -10.6157 (-10.3627)	Arch Hard Loss 1.7764 (2.0254)	Arch Beta Loss 1239.2058 (1238.8135)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.0%, 98.3%)	
11/23 09:53:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.3485 (0.5159)	Arch Loss -10.6956 (-10.3772)	Arch Hard Loss 1.7017 (2.0131)	Arch Beta Loss 1239.7280 (1239.0265)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.6%, 98.1%)	
11/23 09:54:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.4873 (0.5186)	Arch Loss -10.2824 (-10.3810)	Arch Hard Loss 2.1195 (2.0113)	Arch Beta Loss 1240.1958 (1239.2357)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.1%)	
11/23 09:54:15午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 84.4400%
11/23 09:54:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.0263	Prec@(1,5) (53.0%, 81.7%)
11/23 09:54:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.0201	Prec@(1,5) (53.1%, 81.7%)
11/23 09:54:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.0065	Prec@(1,5) (53.1%, 81.8%)
11/23 09:54:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.0101	Prec@(1,5) (53.4%, 81.7%)
11/23 09:54:46午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 53.3640%
11/23 09:54:46午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:54:47午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.3640%
11/23 09:55:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.4409 (0.4939)	Arch Loss -10.5289 (-10.4088)	Arch Hard Loss 1.8775 (1.9956)	Arch Beta Loss 1240.6407 (1240.4392)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.3%, 98.6%)	
11/23 09:56:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.4917 (0.5050)	Arch Loss -10.3318 (-10.3881)	Arch Hard Loss 2.0791 (2.0184)	Arch Beta Loss 1241.0859 (1240.6561)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.0%, 98.4%)	
11/23 09:57:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.6341 (0.5124)	Arch Loss -10.6294 (-10.3938)	Arch Hard Loss 1.7850 (2.0146)	Arch Beta Loss 1241.4395 (1240.8472)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.9%, 98.3%)	
11/23 09:58:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.5422 (0.5152)	Arch Loss -10.4893 (-10.3915)	Arch Hard Loss 1.9291 (2.0188)	Arch Beta Loss 1241.8447 (1241.0349)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.8%, 98.3%)	
11/23 09:58:18午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 84.7840%
11/23 09:58:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.0037	Prec@(1,5) (54.0%, 81.5%)
11/23 09:58:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 1.9911	Prec@(1,5) (53.9%, 81.6%)
11/23 09:58:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 1.9752	Prec@(1,5) (54.0%, 81.9%)
11/23 09:58:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 1.9849	Prec@(1,5) (53.6%, 81.8%)
11/23 09:58:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 53.6080%
11/23 09:58:49午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 09:58:49午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.6080%
11/23 09:59:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.5133 (0.4969)	Arch Loss -10.3818 (-10.3765)	Arch Hard Loss 2.0405 (2.0438)	Arch Beta Loss 1242.2277 (1242.0267)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.1%, 98.3%)	
11/23 10:00:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.3633 (0.4848)	Arch Loss -10.4654 (-10.3883)	Arch Hard Loss 1.9606 (2.0339)	Arch Beta Loss 1242.5996 (1242.2149)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.7%, 98.3%)	
11/23 10:01:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.3404 (0.4887)	Arch Loss -10.3723 (-10.3982)	Arch Hard Loss 2.0575 (2.0258)	Arch Beta Loss 1242.9871 (1242.4081)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.5%, 98.4%)	
11/23 10:02:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.6147 (0.4960)	Arch Loss -11.2718 (-10.3960)	Arch Hard Loss 1.1613 (2.0298)	Arch Beta Loss 1243.3118 (1242.5757)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (85.3%, 98.3%)	
11/23 10:02:20午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 85.2480%
11/23 10:02:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0493	Prec@(1,5) (53.2%, 81.8%)
11/23 10:02:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.0665	Prec@(1,5) (52.5%, 81.5%)
11/23 10:02:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.0799	Prec@(1,5) (52.5%, 81.2%)
11/23 10:02:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.0681	Prec@(1,5) (52.7%, 81.3%)
11/23 10:02:51午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 52.6920%
11/23 10:02:51午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/23 10:02:51午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.6080%
11/23 10:02:51午前 trainer_runner.py:110 [INFO] Final best Prec@1 = 53.6080%
11/23 10:02:51午前 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
