11/22 05:12:17PM parser.py:28 [INFO] 
11/22 05:12:17PM parser.py:29 [INFO] Parameters:
11/22 05:12:17PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-100/DAG
11/22 05:12:17PM parser.py:31 [INFO] T=10.0
11/22 05:12:17PM parser.py:31 [INFO] ADVANCED=1
11/22 05:12:17PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/22 05:12:17PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/22 05:12:17PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/22 05:12:17PM parser.py:31 [INFO] BATCH_SIZE=64
11/22 05:12:17PM parser.py:31 [INFO] CASCADE=0
11/22 05:12:17PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/22 05:12:17PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/22 05:12:17PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/22 05:12:17PM parser.py:31 [INFO] DATA_PATH=../data/
11/22 05:12:17PM parser.py:31 [INFO] DATASET=cifar100
11/22 05:12:17PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/22 05:12:17PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/22 05:12:17PM parser.py:31 [INFO] DISCRETE=0
11/22 05:12:17PM parser.py:31 [INFO] EPOCHS=50
11/22 05:12:17PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/22 05:12:17PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g-100
11/22 05:12:17PM parser.py:31 [INFO] FINAL_L=0.0
11/22 05:12:17PM parser.py:31 [INFO] G=-100.0
11/22 05:12:17PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/22 05:12:17PM parser.py:31 [INFO] GPUS=[0]
11/22 05:12:17PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/22 05:12:17PM parser.py:31 [INFO] INIT_CHANNELS=16
11/22 05:12:17PM parser.py:31 [INFO] L=0.0
11/22 05:12:17PM parser.py:31 [INFO] LAYERS=32
11/22 05:12:17PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/22 05:12:17PM parser.py:31 [INFO] NAME=Pruning
11/22 05:12:17PM parser.py:31 [INFO] NONKD=1
11/22 05:12:17PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-100
11/22 05:12:17PM parser.py:31 [INFO] PCDARTS=0
11/22 05:12:17PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g-100/plots
11/22 05:12:17PM parser.py:31 [INFO] PRINT_FREQ=100
11/22 05:12:17PM parser.py:31 [INFO] RESET=0
11/22 05:12:17PM parser.py:31 [INFO] RESUME_PATH=None
11/22 05:12:17PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g-100
11/22 05:12:17PM parser.py:31 [INFO] SEED=0
11/22 05:12:17PM parser.py:31 [INFO] SHARE_STAGE=0
11/22 05:12:17PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/22 05:12:17PM parser.py:31 [INFO] SPEC_CELL=1
11/22 05:12:17PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/22 05:12:17PM parser.py:31 [INFO] TEACHER_NAME=none
11/22 05:12:17PM parser.py:31 [INFO] TEACHER_PATH=none
11/22 05:12:17PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/22 05:12:17PM parser.py:31 [INFO] TYPE=Pruning
11/22 05:12:17PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/22 05:12:17PM parser.py:31 [INFO] W_LR=0.025
11/22 05:12:17PM parser.py:31 [INFO] W_LR_MIN=0.001
11/22 05:12:17PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/22 05:12:17PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/22 05:12:17PM parser.py:31 [INFO] WORKERS=4
11/22 05:12:17PM parser.py:32 [INFO] 
11/22 05:12:18PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/22 05:13:13PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.4782 (4.5439)	Arch Loss -36872.4531 (-36442.0090)	Arch Hard Loss 4.4041 (4.5375)	Arch Beta Loss 368.7686 (364.4655)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.4%, 9.3%)	
11/22 05:14:06PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.1568 (4.4095)	Arch Loss -37753.7109 (-36879.1932)	Arch Hard Loss 4.2404 (4.4059)	Arch Beta Loss 377.5795 (368.8360)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.4%, 13.6%)	
11/22 05:14:59PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 3.9832 (4.2849)	Arch Loss -38649.1484 (-37321.0557)	Arch Hard Loss 3.9979 (4.2861)	Arch Beta Loss 386.5314 (373.2534)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.7%, 17.5%)	
11/22 05:15:47PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.8951 (4.2078)	Arch Loss -39467.0312 (-37722.7536)	Arch Hard Loss 3.9467 (4.2045)	Arch Beta Loss 394.7098 (377.2696)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.5%, 20.1%)	
11/22 05:15:48PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 5.5280%
11/22 05:15:57PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 4.0060	Prec@(1,5) (8.2%, 27.4%)
11/22 05:16:04PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9973	Prec@(1,5) (8.4%, 27.6%)
11/22 05:16:12PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9949	Prec@(1,5) (8.3%, 27.7%)
11/22 05:16:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9945	Prec@(1,5) (8.3%, 27.6%)
11/22 05:16:19PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 8.3200%
11/22 05:16:19PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[5, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[4, 11])
11/22 05:16:20PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 8.3200%
11/22 05:17:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.7212 (3.8213)	Arch Loss -40397.0781 (-39940.3773)	Arch Hard Loss 3.8893 (3.8164)	Arch Beta Loss 404.0097 (399.4419)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.2%, 31.7%)	
11/22 05:18:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.6513 (3.7836)	Arch Loss -41328.3945 (-40403.5498)	Arch Hard Loss 3.7330 (3.7643)	Arch Beta Loss 413.3213 (404.0731)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.8%, 33.1%)	
11/22 05:19:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.4215 (3.7445)	Arch Loss -42268.0547 (-40869.7809)	Arch Hard Loss 3.4268 (3.7278)	Arch Beta Loss 422.7148 (408.7351)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (11.5%, 34.3%)	
11/22 05:19:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.6011 (3.7013)	Arch Loss -43118.5781 (-41291.6121)	Arch Hard Loss 3.8638 (3.6954)	Arch Beta Loss 431.2244 (412.9531)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (12.2%, 35.7%)	
11/22 05:19:51午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 12.2520%
11/22 05:19:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.5436	Prec@(1,5) (14.9%, 40.9%)
11/22 05:20:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.5368	Prec@(1,5) (15.1%, 41.0%)
11/22 05:20:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.5441	Prec@(1,5) (15.0%, 40.9%)
11/22 05:20:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.5397	Prec@(1,5) (15.2%, 41.0%)
11/22 05:20:22午後 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 15.1760%
11/22 05:20:22午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:20:22午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 15.1760%
11/22 05:21:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.5775 (3.5267)	Arch Loss -44078.8984 (-43607.9973)	Arch Hard Loss 3.5341 (3.5394)	Arch Beta Loss 440.8243 (436.1154)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.0%, 40.9%)	
11/22 05:22:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.4545 (3.4920)	Arch Loss -45034.0234 (-44084.4324)	Arch Hard Loss 3.3959 (3.5064)	Arch Beta Loss 450.3742 (440.8794)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (15.6%, 41.6%)	
11/22 05:23:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.2517 (3.4573)	Arch Loss -45993.3125 (-44562.3179)	Arch Hard Loss 3.4830 (3.4807)	Arch Beta Loss 459.9680 (445.6580)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.3%, 42.7%)	
11/22 05:23:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.2766 (3.4349)	Arch Loss -46861.3320 (-44993.7260)	Arch Hard Loss 3.2132 (3.4535)	Arch Beta Loss 468.6455 (449.9718)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.8%, 43.3%)	
11/22 05:23:53午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 16.8080%
11/22 05:24:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.3412	Prec@(1,5) (18.5%, 46.7%)
11/22 05:24:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.3621	Prec@(1,5) (18.1%, 45.9%)
11/22 05:24:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.3653	Prec@(1,5) (18.2%, 45.6%)
11/22 05:24:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.3662	Prec@(1,5) (18.3%, 45.7%)
11/22 05:24:24午後 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 18.2560%
11/22 05:24:24午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:24:25午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 18.2560%
11/22 05:25:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.5131 (3.3035)	Arch Loss -47841.0938 (-47360.2788)	Arch Hard Loss 3.3601 (3.3143)	Arch Beta Loss 478.4445 (473.6359)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.7%, 46.9%)	
11/22 05:26:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.0340 (3.2655)	Arch Loss -48819.2812 (-47847.3720)	Arch Hard Loss 3.3671 (3.2780)	Arch Beta Loss 488.2265 (478.5065)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (19.5%, 47.9%)	
11/22 05:27:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1214 (3.2334)	Arch Loss -49806.8086 (-48337.3766)	Arch Hard Loss 3.5380 (3.2542)	Arch Beta Loss 498.1035 (483.4063)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (20.1%, 48.8%)	
11/22 05:27:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.1167 (3.2174)	Arch Loss -50705.0000 (-48781.1344)	Arch Hard Loss 3.1560 (3.2406)	Arch Beta Loss 507.0815 (487.8438)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (20.6%, 49.3%)	
11/22 05:27:56午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 20.5480%
11/22 05:28:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2240	Prec@(1,5) (20.0%, 49.3%)
11/22 05:28:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2161	Prec@(1,5) (20.0%, 49.5%)
11/22 05:28:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2202	Prec@(1,5) (20.0%, 49.5%)
11/22 05:28:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2251	Prec@(1,5) (20.0%, 49.5%)
11/22 05:28:27午後 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 20.0000%
11/22 05:28:27午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 4), ('skip_connect', 6)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:28:27午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 20.0000%
11/22 05:29:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 2.9544 (3.0470)	Arch Loss -51722.7734 (-51222.9771)	Arch Hard Loss 2.9625 (3.1666)	Arch Beta Loss 517.2573 (512.2614)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (23.6%, 53.9%)	
11/22 05:30:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0723 (3.0362)	Arch Loss -52739.8477 (-51729.2691)	Arch Hard Loss 3.0234 (3.1233)	Arch Beta Loss 527.4287 (517.3239)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.2%, 54.3%)	
11/22 05:31:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.7861 (3.0378)	Arch Loss -53765.1172 (-52238.4934)	Arch Hard Loss 3.0016 (3.1082)	Arch Beta Loss 537.6812 (522.4160)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.1%, 54.2%)	
11/22 05:31:58午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.8175 (3.0279)	Arch Loss -54693.1914 (-52698.9761)	Arch Hard Loss 2.9390 (3.0848)	Arch Beta Loss 546.9613 (527.0206)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (24.2%, 54.5%)	
11/22 05:31:58午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 24.2520%
11/22 05:32:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.0672	Prec@(1,5) (23.1%, 54.4%)
11/22 05:32:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0374	Prec@(1,5) (23.3%, 55.1%)
11/22 05:32:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0318	Prec@(1,5) (23.6%, 55.3%)
11/22 05:32:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0291	Prec@(1,5) (23.9%, 55.2%)
11/22 05:32:29午後 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 23.8720%
11/22 05:32:29午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:32:29午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 23.8720%
11/22 05:33:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1522 (2.8786)	Arch Loss -55738.4570 (-55225.9305)	Arch Hard Loss 3.0562 (2.9796)	Arch Beta Loss 557.4151 (552.2891)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (26.7%, 58.3%)	
11/22 05:34:18午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.1463 (2.8776)	Arch Loss -56775.1016 (-55744.0091)	Arch Hard Loss 3.2733 (2.9826)	Arch Beta Loss 567.7838 (557.4699)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.1%, 58.2%)	
11/22 05:35:12午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.9125 (2.8790)	Arch Loss -57812.0898 (-56262.3213)	Arch Hard Loss 2.6123 (2.9611)	Arch Beta Loss 578.1470 (562.6528)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.0%, 58.1%)	
11/22 05:36:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.5980 (2.8737)	Arch Loss -58742.3555 (-56728.4955)	Arch Hard Loss 2.6285 (2.9393)	Arch Beta Loss 587.4498 (567.3143)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.2%, 58.4%)	
11/22 05:36:01午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 27.1800%
11/22 05:36:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.8671	Prec@(1,5) (26.1%, 59.0%)
11/22 05:36:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.8675	Prec@(1,5) (26.5%, 58.7%)
11/22 05:36:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.8701	Prec@(1,5) (26.6%, 58.9%)
11/22 05:36:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.8640	Prec@(1,5) (26.8%, 59.0%)
11/22 05:36:32午後 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 26.8400%
11/22 05:36:32午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:36:32午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 26.8400%
11/22 05:37:27午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.7538 (2.7331)	Arch Loss -59781.1680 (-59272.5102)	Arch Hard Loss 2.9654 (2.8548)	Arch Beta Loss 597.8413 (592.7536)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.7%, 61.5%)	
11/22 05:38:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.8470 (2.7512)	Arch Loss -60802.9844 (-59785.2294)	Arch Hard Loss 2.8825 (2.8442)	Arch Beta Loss 608.0587 (597.8807)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.4%, 61.4%)	
11/22 05:39:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.0417 (2.7449)	Arch Loss -61815.7344 (-60295.2637)	Arch Hard Loss 2.8502 (2.8241)	Arch Beta Loss 618.1859 (602.9809)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.3%, 61.6%)	
11/22 05:40:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.8214 (2.7412)	Arch Loss -62717.9180 (-60751.5897)	Arch Hard Loss 2.9492 (2.8273)	Arch Beta Loss 627.2087 (607.5442)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.7%, 61.7%)	
11/22 05:40:04午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 29.6840%
11/22 05:40:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.7699	Prec@(1,5) (29.9%, 61.2%)
11/22 05:40:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.7583	Prec@(1,5) (29.6%, 61.5%)
11/22 05:40:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.7700	Prec@(1,5) (29.2%, 61.6%)
11/22 05:40:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.7780	Prec@(1,5) (29.1%, 61.2%)
11/22 05:40:35午後 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 29.1160%
11/22 05:40:35午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
11/22 05:40:35午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 29.1160%
11/22 05:41:30午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.7265 (2.6232)	Arch Loss -63718.8555 (-63229.4823)	Arch Hard Loss 2.7357 (2.7677)	Arch Beta Loss 637.2159 (632.3225)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.5%, 65.2%)	
11/22 05:42:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.4421 (2.6146)	Arch Loss -64695.9492 (-63721.4774)	Arch Hard Loss 2.6610 (2.7429)	Arch Beta Loss 646.9861 (637.2422)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.7%, 65.1%)	
11/22 05:43:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.7679 (2.6045)	Arch Loss -65657.3047 (-64208.6385)	Arch Hard Loss 3.1835 (2.7334)	Arch Beta Loss 656.6049 (642.1137)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.2%, 65.5%)	
11/22 05:44:06午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.7352 (2.5988)	Arch Loss -66509.4531 (-64642.6920)	Arch Hard Loss 2.6960 (2.7160)	Arch Beta Loss 665.1215 (646.4541)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.2%, 65.6%)	
11/22 05:44:06午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 32.2000%
11/22 05:44:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6522	Prec@(1,5) (31.9%, 63.1%)
11/22 05:44:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.6368	Prec@(1,5) (32.2%, 64.1%)
11/22 05:44:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6453	Prec@(1,5) (31.9%, 63.9%)
11/22 05:44:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6555	Prec@(1,5) (31.5%, 63.6%)
11/22 05:44:37午後 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 31.5560%
11/22 05:44:37午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 05:44:38午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 31.5560%
11/22 05:45:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.4389 (2.4742)	Arch Loss -67448.1406 (-66989.7715)	Arch Hard Loss 2.8813 (2.6551)	Arch Beta Loss 674.5103 (669.9243)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.3%, 67.3%)	
11/22 05:46:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.1217 (2.4861)	Arch Loss -68359.2344 (-67449.9094)	Arch Hard Loss 2.9030 (2.6555)	Arch Beta Loss 683.6214 (674.5256)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.3%, 67.1%)	
11/22 05:47:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.6766 (2.4878)	Arch Loss -69251.5547 (-67903.8011)	Arch Hard Loss 2.6297 (2.6540)	Arch Beta Loss 692.5419 (679.0646)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.2%, 67.2%)	
11/22 05:48:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.0965 (2.4881)	Arch Loss -70037.5625 (-68306.8593)	Arch Hard Loss 2.8270 (2.6404)	Arch Beta Loss 700.4039 (683.0950)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.3%, 67.3%)	
11/22 05:48:09午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 34.3360%
11/22 05:48:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5141	Prec@(1,5) (35.1%, 67.0%)
11/22 05:48:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.5417	Prec@(1,5) (34.4%, 66.4%)
11/22 05:48:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.5600	Prec@(1,5) (34.1%, 65.7%)
11/22 05:48:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.5586	Prec@(1,5) (34.0%, 65.7%)
11/22 05:48:40午後 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 33.9680%
11/22 05:48:40午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 05:48:41午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.9680%
11/22 05:49:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.4440 (2.3949)	Arch Loss -70900.8672 (-70479.7221)	Arch Hard Loss 2.8569 (2.5702)	Arch Beta Loss 709.0372 (704.8229)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.7%, 69.7%)	
11/22 05:50:30午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.4181 (2.4043)	Arch Loss -71736.0703 (-70902.1111)	Arch Hard Loss 2.6784 (2.5582)	Arch Beta Loss 717.3875 (709.0467)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.1%, 69.5%)	
11/22 05:51:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.1933 (2.3950)	Arch Loss -72551.9766 (-71317.9579)	Arch Hard Loss 2.3241 (2.5601)	Arch Beta Loss 725.5430 (713.2052)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.4%, 69.7%)	
11/22 05:52:12午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.1322 (2.3934)	Arch Loss -73269.2344 (-71686.6930)	Arch Hard Loss 2.5945 (2.5513)	Arch Beta Loss 732.7183 (716.8924)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.4%, 69.7%)	
11/22 05:52:12午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 36.4000%
11/22 05:52:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.5382	Prec@(1,5) (34.2%, 67.1%)
11/22 05:52:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.5232	Prec@(1,5) (34.2%, 67.4%)
11/22 05:52:36午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.5283	Prec@(1,5) (34.2%, 67.2%)
11/22 05:52:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.5262	Prec@(1,5) (34.3%, 67.1%)
11/22 05:52:43午後 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 34.2800%
11/22 05:52:43午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 05:52:44午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 34.2800%
11/22 05:53:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.3241 (2.2859)	Arch Loss -74056.3906 (-73672.1749)	Arch Hard Loss 2.4137 (2.5407)	Arch Beta Loss 740.5880 (736.7472)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.4%, 72.0%)	
11/22 05:54:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.4567 (2.3087)	Arch Loss -74816.9844 (-74057.0436)	Arch Hard Loss 2.3286 (2.5108)	Arch Beta Loss 748.1931 (740.5955)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.1%, 71.5%)	
11/22 05:55:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.3637 (2.3146)	Arch Loss -75559.2422 (-74435.7568)	Arch Hard Loss 2.3439 (2.4996)	Arch Beta Loss 755.6158 (744.3826)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.1%, 71.4%)	
11/22 05:56:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.3880 (2.3106)	Arch Loss -76212.0156 (-74771.4278)	Arch Hard Loss 2.2811 (2.4848)	Arch Beta Loss 762.1430 (747.7391)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.2%, 71.5%)	
11/22 05:56:15午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 38.2160%
11/22 05:56:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.4520	Prec@(1,5) (36.7%, 68.2%)
11/22 05:56:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.4224	Prec@(1,5) (36.6%, 69.1%)
11/22 05:56:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.4393	Prec@(1,5) (36.2%, 68.8%)
11/22 05:56:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4303	Prec@(1,5) (36.4%, 69.0%)
11/22 05:56:45午後 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 36.4120%
11/22 05:56:45午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 05:56:46午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 36.4120%
11/22 05:57:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 1.9537 (2.2091)	Arch Loss -76927.0781 (-76578.2148)	Arch Hard Loss 2.7881 (2.4510)	Arch Beta Loss 769.2987 (765.8067)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.2%, 73.6%)	
11/22 05:58:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.2867 (2.2100)	Arch Loss -77618.4844 (-76928.0619)	Arch Hard Loss 2.5723 (2.4417)	Arch Beta Loss 776.2106 (769.3050)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.2%, 73.5%)	
11/22 05:59:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.2996 (2.2104)	Arch Loss -78293.3438 (-77272.2470)	Arch Hard Loss 2.1411 (2.4301)	Arch Beta Loss 782.9548 (772.7468)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.3%, 73.4%)	
11/22 06:00:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.0868 (2.2197)	Arch Loss -78886.2344 (-77577.2547)	Arch Hard Loss 2.2996 (2.4311)	Arch Beta Loss 788.8853 (775.7969)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.0%, 73.4%)	
11/22 06:00:17午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 40.0440%
11/22 06:00:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.4454	Prec@(1,5) (36.3%, 69.0%)
11/22 06:00:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.4100	Prec@(1,5) (36.8%, 69.7%)
11/22 06:00:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.4055	Prec@(1,5) (37.0%, 69.8%)
11/22 06:00:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.4052	Prec@(1,5) (37.0%, 69.8%)
11/22 06:00:48午後 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 37.0160%
11/22 06:00:48午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:00:48午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.0160%
11/22 06:01:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.0385 (2.1049)	Arch Loss -79536.8047 (-79219.0813)	Arch Hard Loss 2.0840 (2.3928)	Arch Beta Loss 795.3889 (792.2147)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.2%, 75.7%)	
11/22 06:02:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.1571 (2.1388)	Arch Loss -80165.5859 (-79537.1683)	Arch Hard Loss 2.1047 (2.3959)	Arch Beta Loss 801.6769 (795.3956)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.4%, 75.1%)	
11/22 06:03:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 1.9032 (2.1372)	Arch Loss -80779.9219 (-79850.3851)	Arch Hard Loss 2.2685 (2.3785)	Arch Beta Loss 807.8219 (798.5276)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.5%, 74.9%)	
11/22 06:04:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.8067 (2.1435)	Arch Loss -81321.4531 (-80128.2670)	Arch Hard Loss 2.3330 (2.3685)	Arch Beta Loss 813.2379 (801.3064)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.4%, 74.8%)	
11/22 06:04:19午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 42.3520%
11/22 06:04:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.3720	Prec@(1,5) (37.6%, 70.4%)
11/22 06:04:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.3738	Prec@(1,5) (37.2%, 70.2%)
11/22 06:04:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.3740	Prec@(1,5) (37.4%, 70.2%)
11/22 06:04:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3845	Prec@(1,5) (37.1%, 70.0%)
11/22 06:04:50午後 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 37.0880%
11/22 06:04:50午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:04:51午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.0880%
11/22 06:05:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1183 (2.0092)	Arch Loss -81917.0234 (-81626.2452)	Arch Hard Loss 2.5382 (2.3478)	Arch Beta Loss 819.1956 (816.2859)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.8%, 77.6%)	
11/22 06:06:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.8692 (2.0647)	Arch Loss -82495.6406 (-81918.2394)	Arch Hard Loss 2.3241 (2.3316)	Arch Beta Loss 824.9796 (819.2057)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.8%, 76.4%)	
11/22 06:07:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8710 (2.0675)	Arch Loss -83063.9375 (-82206.6154)	Arch Hard Loss 2.1922 (2.3270)	Arch Beta Loss 830.6613 (822.0894)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.8%, 76.4%)	
11/22 06:08:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.0730 (2.0728)	Arch Loss -83567.5078 (-82463.3261)	Arch Hard Loss 2.3057 (2.3227)	Arch Beta Loss 835.6981 (824.6565)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.4%, 76.5%)	
11/22 06:08:22午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 43.4240%
11/22 06:08:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2988	Prec@(1,5) (39.6%, 72.5%)
11/22 06:08:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.3251	Prec@(1,5) (38.9%, 72.0%)
11/22 06:08:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.3129	Prec@(1,5) (39.4%, 72.0%)
11/22 06:08:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.3142	Prec@(1,5) (39.3%, 72.0%)
11/22 06:08:53午後 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 39.3160%
11/22 06:08:53午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:08:53午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 39.3160%
11/22 06:09:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.0530 (1.9601)	Arch Loss -84125.4062 (-83852.5623)	Arch Hard Loss 2.2641 (2.3014)	Arch Beta Loss 841.2767 (838.5486)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.7%, 78.1%)	
11/22 06:10:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.1557 (2.0037)	Arch Loss -84671.7188 (-84127.0738)	Arch Hard Loss 1.9783 (2.2942)	Arch Beta Loss 846.7369 (841.2937)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.1%, 77.3%)	
11/22 06:11:35午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.8936 (2.0088)	Arch Loss -85212.4219 (-84399.7463)	Arch Hard Loss 2.5702 (2.2936)	Arch Beta Loss 852.1499 (844.0204)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.1%, 77.2%)	
11/22 06:12:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.0438 (2.0114)	Arch Loss -85696.9688 (-84643.9078)	Arch Hard Loss 2.3984 (2.2851)	Arch Beta Loss 856.9937 (846.4619)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.0%, 77.3%)	
11/22 06:12:24午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 45.0440%
11/22 06:12:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.2612	Prec@(1,5) (40.1%, 72.4%)
11/22 06:12:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.2662	Prec@(1,5) (40.4%, 72.4%)
11/22 06:12:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.2683	Prec@(1,5) (40.0%, 72.3%)
11/22 06:12:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.2746	Prec@(1,5) (40.1%, 72.3%)
11/22 06:12:55午後 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 40.0800%
11/22 06:12:55午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:12:55午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.0800%
11/22 06:13:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.2764 (1.9209)	Arch Loss -86239.1328 (-85973.5178)	Arch Hard Loss 2.1545 (2.2602)	Arch Beta Loss 862.4129 (859.7578)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.5%, 79.0%)	
11/22 06:14:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.8511 (1.9324)	Arch Loss -86775.5156 (-86241.6760)	Arch Hard Loss 2.1112 (2.2602)	Arch Beta Loss 867.7762 (862.4394)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.3%, 78.7%)	
11/22 06:15:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.9922 (1.9358)	Arch Loss -87313.2188 (-86510.0179)	Arch Hard Loss 2.0384 (2.2609)	Arch Beta Loss 873.1526 (865.1228)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.0%, 78.9%)	
11/22 06:16:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.7194 (1.9440)	Arch Loss -87799.4453 (-86751.9890)	Arch Hard Loss 1.9083 (2.2537)	Arch Beta Loss 878.0135 (867.5424)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.6%, 78.7%)	
11/22 06:16:27午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 46.6040%
11/22 06:16:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.2169	Prec@(1,5) (41.1%, 73.5%)
11/22 06:16:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.2070	Prec@(1,5) (41.2%, 73.5%)
11/22 06:16:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.2187	Prec@(1,5) (40.9%, 73.4%)
11/22 06:16:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.2165	Prec@(1,5) (41.0%, 73.5%)
11/22 06:16:58午後 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 41.0160%
11/22 06:16:58午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:16:58午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.0160%
11/22 06:17:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.6087 (1.8590)	Arch Loss -88348.7656 (-88078.8051)	Arch Hard Loss 1.7911 (2.2180)	Arch Beta Loss 883.5056 (880.8102)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.5%, 80.4%)	
11/22 06:18:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.0625 (1.8784)	Arch Loss -88896.8594 (-88351.8777)	Arch Hard Loss 2.2953 (2.2140)	Arch Beta Loss 888.9916 (883.5409)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.8%, 80.1%)	
11/22 06:19:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.8564 (1.8919)	Arch Loss -89450.9531 (-88626.6783)	Arch Hard Loss 2.4386 (2.2332)	Arch Beta Loss 894.5339 (886.2891)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.2%, 79.9%)	
11/22 06:20:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.8076 (1.9001)	Arch Loss -89955.2500 (-88875.6630)	Arch Hard Loss 2.2283 (2.2238)	Arch Beta Loss 899.5748 (888.7789)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.3%, 79.7%)	
11/22 06:20:29午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 47.2880%
11/22 06:20:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1844	Prec@(1,5) (42.5%, 74.1%)
11/22 06:20:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1719	Prec@(1,5) (42.6%, 74.3%)
11/22 06:20:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1756	Prec@(1,5) (42.5%, 74.2%)
11/22 06:21:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1724	Prec@(1,5) (42.3%, 74.3%)
11/22 06:21:00午後 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 42.3040%
11/22 06:21:00午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 9), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:21:01午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.3040%
11/22 06:21:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.6492 (1.7877)	Arch Loss -90527.1641 (-90246.3374)	Arch Hard Loss 2.2638 (2.2143)	Arch Beta Loss 905.2943 (902.4855)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.5%, 81.9%)	
11/22 06:22:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.8589 (1.8141)	Arch Loss -91100.0938 (-90531.1680)	Arch Hard Loss 2.2325 (2.2078)	Arch Beta Loss 911.0233 (905.3338)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.8%, 81.4%)	
11/22 06:23:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.8977 (1.8313)	Arch Loss -91679.7812 (-90818.1591)	Arch Hard Loss 1.9258 (2.2206)	Arch Beta Loss 916.8171 (908.2038)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.2%, 80.9%)	
11/22 06:24:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.5359 (1.8458)	Arch Loss -92206.1562 (-91078.2894)	Arch Hard Loss 2.2901 (2.2180)	Arch Beta Loss 922.0845 (910.8051)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.9%, 80.8%)	
11/22 06:24:32午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 48.9080%
11/22 06:24:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.2109	Prec@(1,5) (42.0%, 73.9%)
11/22 06:24:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.1908	Prec@(1,5) (42.5%, 74.2%)
11/22 06:24:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.1981	Prec@(1,5) (42.4%, 73.9%)
11/22 06:25:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.1943	Prec@(1,5) (42.4%, 73.8%)
11/22 06:25:03午後 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 42.3520%
11/22 06:25:03午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:25:03午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.3520%
11/22 06:25:58午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.6145 (1.7369)	Arch Loss -92802.9609 (-92510.0705)	Arch Hard Loss 2.2015 (2.1665)	Arch Beta Loss 928.0516 (925.1224)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 83.2%)	
11/22 06:26:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.5646 (1.7916)	Arch Loss -93399.2891 (-92806.8134)	Arch Hard Loss 1.9692 (2.1976)	Arch Beta Loss 934.0126 (928.0901)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.7%, 82.1%)	
11/22 06:27:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.7982 (1.7954)	Arch Loss -93999.9062 (-93105.2459)	Arch Hard Loss 2.1747 (2.2008)	Arch Beta Loss 940.0208 (931.0745)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.7%, 81.9%)	
11/22 06:28:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.9253 (1.8105)	Arch Loss -94544.4688 (-93375.1320)	Arch Hard Loss 1.6511 (2.1872)	Arch Beta Loss 945.4612 (933.7732)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.2%, 81.6%)	
11/22 06:28:35午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 49.1960%
11/22 06:28:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.1941	Prec@(1,5) (42.5%, 74.5%)
11/22 06:28:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.1864	Prec@(1,5) (42.5%, 74.6%)
11/22 06:28:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.1778	Prec@(1,5) (42.7%, 74.7%)
11/22 06:29:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.1844	Prec@(1,5) (42.6%, 74.7%)
11/22 06:29:05午後 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 42.5560%
11/22 06:29:05午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('max_pool_3x3', 5), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:29:06午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.5560%
11/22 06:30:01午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.3567 (1.6923)	Arch Loss -95157.1328 (-94856.5980)	Arch Hard Loss 2.6259 (2.1820)	Arch Beta Loss 951.5976 (948.5878)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.0%, 83.5%)	
11/22 06:30:54午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.7857 (1.7339)	Arch Loss -95767.4609 (-95161.0323)	Arch Hard Loss 2.3310 (2.1868)	Arch Beta Loss 957.6979 (951.6322)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.2%, 82.5%)	
11/22 06:31:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.2574 (1.7562)	Arch Loss -96379.5469 (-95466.1743)	Arch Hard Loss 1.9371 (2.1650)	Arch Beta Loss 963.8148 (954.6834)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.7%, 82.1%)	
11/22 06:32:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.5016 (1.7697)	Arch Loss -96930.7109 (-95741.2228)	Arch Hard Loss 2.1745 (2.1611)	Arch Beta Loss 969.3289 (957.4338)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.4%, 82.0%)	
11/22 06:32:37午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 50.4280%
11/22 06:32:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.1641	Prec@(1,5) (43.3%, 74.6%)
11/22 06:32:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.1736	Prec@(1,5) (43.2%, 74.6%)
11/22 06:33:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.1545	Prec@(1,5) (43.6%, 74.9%)
11/22 06:33:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.1402	Prec@(1,5) (43.6%, 75.1%)
11/22 06:33:08午後 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 43.6400%
11/22 06:33:08午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:33:08午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.6400%
11/22 06:34:03午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.8168 (1.6836)	Arch Loss -97549.9766 (-97246.4730)	Arch Hard Loss 2.2023 (2.1719)	Arch Beta Loss 975.5218 (972.4864)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.7%, 83.8%)	
11/22 06:34:56午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.6037 (1.7165)	Arch Loss -98162.8672 (-97553.0988)	Arch Hard Loss 2.5365 (2.1584)	Arch Beta Loss 981.6541 (975.5526)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.7%, 83.1%)	
11/22 06:35:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3380 (1.7196)	Arch Loss -98776.1562 (-97859.6762)	Arch Hard Loss 2.1600 (2.1490)	Arch Beta Loss 987.7831 (978.6183)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.7%, 83.1%)	
11/22 06:36:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.8528 (1.7355)	Arch Loss -99327.7578 (-98135.4850)	Arch Hard Loss 1.6263 (2.1418)	Arch Beta Loss 993.2938 (981.3763)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.6%, 82.6%)	
11/22 06:36:39午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 51.6040%
11/22 06:36:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.1459	Prec@(1,5) (44.6%, 75.1%)
11/22 06:36:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.1311	Prec@(1,5) (44.3%, 75.6%)
11/22 06:37:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.1371	Prec@(1,5) (44.3%, 75.8%)
11/22 06:37:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.1417	Prec@(1,5) (44.3%, 75.6%)
11/22 06:37:10午後 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 44.2640%
11/22 06:37:10午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:37:11午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.2640%
11/22 06:38:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.6800 (1.6092)	Arch Loss -99944.0859 (-99642.1295)	Arch Hard Loss 2.6739 (2.1636)	Arch Beta Loss 999.4676 (996.4429)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.5%, 85.1%)	
11/22 06:39:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.6698 (1.6540)	Arch Loss -100554.1641 (-99947.3822)	Arch Hard Loss 2.2252 (2.1570)	Arch Beta Loss 1005.5639 (999.4954)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.1%, 84.2%)	
11/22 06:39:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.6483 (1.6720)	Arch Loss -101161.6406 (-100251.9743)	Arch Hard Loss 1.9825 (2.1432)	Arch Beta Loss 1011.6362 (1002.5412)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.9%, 83.8%)	
11/22 06:40:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.7911 (1.6869)	Arch Loss -101705.3594 (-100525.3532)	Arch Hard Loss 1.9374 (2.1390)	Arch Beta Loss 1017.0730 (1005.2749)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.6%, 83.7%)	
11/22 06:40:42午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 52.5720%
11/22 06:40:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.0892	Prec@(1,5) (45.0%, 75.3%)
11/22 06:40:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.0985	Prec@(1,5) (44.4%, 75.6%)
11/22 06:41:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.0992	Prec@(1,5) (44.6%, 75.5%)
11/22 06:41:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.0877	Prec@(1,5) (44.8%, 75.9%)
11/22 06:41:13午後 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 44.7680%
11/22 06:41:13午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:41:14午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.7680%
11/22 06:42:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.8575 (1.5534)	Arch Loss -102310.8438 (-102014.6165)	Arch Hard Loss 2.4836 (2.1184)	Arch Beta Loss 1023.1333 (1020.1673)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.2%, 86.1%)	
11/22 06:43:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.7807 (1.5953)	Arch Loss -102905.8516 (-102313.3234)	Arch Hard Loss 2.2187 (2.1210)	Arch Beta Loss 1029.0807 (1023.1544)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.8%, 85.1%)	
11/22 06:43:56午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.5731 (1.6217)	Arch Loss -103494.1797 (-102610.1098)	Arch Hard Loss 2.3653 (2.1213)	Arch Beta Loss 1034.9655 (1026.1223)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.0%, 84.6%)	
11/22 06:44:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.7771 (1.6379)	Arch Loss -104018.3984 (-102875.3982)	Arch Hard Loss 1.8240 (2.1154)	Arch Beta Loss 1040.2021 (1028.7751)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.6%, 84.4%)	
11/22 06:44:45午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 53.6040%
11/22 06:44:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.0850	Prec@(1,5) (45.0%, 76.8%)
11/22 06:45:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.0780	Prec@(1,5) (45.0%, 76.7%)
11/22 06:45:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.0833	Prec@(1,5) (44.9%, 76.7%)
11/22 06:45:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.0921	Prec@(1,5) (44.7%, 76.4%)
11/22 06:45:15午後 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 44.7360%
11/22 06:45:15午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:45:16午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.7680%
11/22 06:46:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.5937 (1.4946)	Arch Loss -104598.8359 (-104314.6998)	Arch Hard Loss 1.7029 (2.0990)	Arch Beta Loss 1046.0054 (1043.1680)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.6%, 87.0%)	
11/22 06:47:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.6688 (1.5487)	Arch Loss -105164.9141 (-104599.9868)	Arch Hard Loss 2.2579 (2.0998)	Arch Beta Loss 1051.6718 (1046.0209)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.3%, 86.1%)	
11/22 06:47:58午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.7242 (1.5674)	Arch Loss -105723.8359 (-104882.5820)	Arch Hard Loss 1.8990 (2.1179)	Arch Beta Loss 1057.2573 (1048.8470)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.7%, 85.7%)	
11/22 06:48:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.6496 (1.5843)	Arch Loss -106219.1562 (-105134.6466)	Arch Hard Loss 2.4863 (2.1179)	Arch Beta Loss 1062.2164 (1051.3676)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.0%, 85.4%)	
11/22 06:48:47午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 55.0360%
11/22 06:48:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.0173	Prec@(1,5) (46.4%, 76.8%)
11/22 06:49:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.0426	Prec@(1,5) (45.6%, 77.1%)
11/22 06:49:11午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.0424	Prec@(1,5) (45.7%, 77.0%)
11/22 06:49:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.0437	Prec@(1,5) (45.7%, 77.1%)
11/22 06:49:18午後 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 45.7240%
11/22 06:49:18午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:49:18午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.7240%
11/22 06:50:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.2955 (1.5072)	Arch Loss -106768.4141 (-106500.1957)	Arch Hard Loss 2.3998 (2.1078)	Arch Beta Loss 1067.7081 (1065.0230)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.9%, 86.8%)	
11/22 06:51:06午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.6769 (1.5319)	Arch Loss -107305.1875 (-106770.2317)	Arch Hard Loss 2.2534 (2.1051)	Arch Beta Loss 1073.0743 (1067.7234)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.3%, 86.4%)	
11/22 06:52:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.9516 (1.5332)	Arch Loss -107835.5781 (-107038.0358)	Arch Hard Loss 2.1356 (2.0934)	Arch Beta Loss 1078.3771 (1070.4013)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.2%, 86.2%)	
11/22 06:52:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.5218 (1.5526)	Arch Loss -108308.1875 (-107277.2982)	Arch Hard Loss 2.0657 (2.0966)	Arch Beta Loss 1083.1025 (1072.7939)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.8%, 85.9%)	
11/22 06:52:49午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 55.8240%
11/22 06:52:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.0931	Prec@(1,5) (45.9%, 76.5%)
11/22 06:53:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.0808	Prec@(1,5) (45.7%, 76.5%)
11/22 06:53:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.0869	Prec@(1,5) (45.3%, 76.3%)
11/22 06:53:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.0738	Prec@(1,5) (45.5%, 76.6%)
11/22 06:53:20午後 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 45.5080%
11/22 06:53:20午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:53:20午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.7240%
11/22 06:54:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.5027 (1.4255)	Arch Loss -108834.1562 (-108576.7246)	Arch Hard Loss 1.9966 (2.0407)	Arch Beta Loss 1088.3616 (1085.7877)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.5%, 88.0%)	
11/22 06:55:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.6158 (1.4642)	Arch Loss -109350.9375 (-108836.1072)	Arch Hard Loss 2.2772 (2.0416)	Arch Beta Loss 1093.5321 (1088.3815)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.5%, 87.1%)	
11/22 06:56:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.3067 (1.4824)	Arch Loss -109865.9219 (-109094.4197)	Arch Hard Loss 1.5973 (2.0568)	Arch Beta Loss 1098.6752 (1090.9648)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.0%, 87.0%)	
11/22 06:56:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.5397 (1.4975)	Arch Loss -110326.6250 (-109326.1744)	Arch Hard Loss 2.0182 (2.0595)	Arch Beta Loss 1103.2864 (1093.2823)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.8%, 86.8%)	
11/22 06:56:51午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 56.7840%
11/22 06:56:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.0183	Prec@(1,5) (46.7%, 77.5%)
11/22 06:57:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.0351	Prec@(1,5) (46.3%, 77.1%)
11/22 06:57:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.0266	Prec@(1,5) (46.6%, 77.5%)
11/22 06:57:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.0311	Prec@(1,5) (46.4%, 77.4%)
11/22 06:57:22午後 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 46.4120%
11/22 06:57:22午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 06:57:22午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.4120%
11/22 06:58:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.7564 (1.4144)	Arch Loss -110842.6328 (-110589.7954)	Arch Hard Loss 1.8515 (2.0043)	Arch Beta Loss 1108.4448 (1105.9180)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.8%, 88.4%)	
11/22 06:59:10午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.2387 (1.4323)	Arch Loss -111351.5000 (-110844.7394)	Arch Hard Loss 2.0979 (2.0260)	Arch Beta Loss 1113.5360 (1108.4677)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.4%, 88.0%)	
11/22 07:00:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.1709 (1.4384)	Arch Loss -111859.0703 (-111099.0946)	Arch Hard Loss 1.6574 (2.0453)	Arch Beta Loss 1118.6073 (1111.0114)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.3%, 87.9%)	
11/22 07:00:52午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.2583 (1.4576)	Arch Loss -112312.4609 (-111327.4071)	Arch Hard Loss 2.4010 (2.0532)	Arch Beta Loss 1123.1486 (1113.2946)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.9%, 87.5%)	
11/22 07:00:53午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 57.8720%
11/22 07:01:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.0706	Prec@(1,5) (46.2%, 77.5%)
11/22 07:01:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.0515	Prec@(1,5) (46.8%, 77.7%)
11/22 07:01:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.0485	Prec@(1,5) (46.6%, 77.6%)
11/22 07:01:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.0533	Prec@(1,5) (46.1%, 77.5%)
11/22 07:01:24午後 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 46.1040%
11/22 07:01:24午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:01:24午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.4120%
11/22 07:02:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.3426 (1.3013)	Arch Loss -112818.4297 (-112571.2166)	Arch Hard Loss 2.5061 (2.0554)	Arch Beta Loss 1128.2094 (1125.7327)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.3%, 89.5%)	
11/22 07:03:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.6878 (1.3531)	Arch Loss -113315.0625 (-112820.5854)	Arch Hard Loss 2.0742 (2.0406)	Arch Beta Loss 1133.1714 (1128.2263)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.8%, 88.6%)	
11/22 07:04:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.4533 (1.3738)	Arch Loss -113804.7812 (-113068.0920)	Arch Hard Loss 2.3401 (2.0440)	Arch Beta Loss 1138.0713 (1130.7014)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.3%)	
11/22 07:04:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.3048 (1.3949)	Arch Loss -114240.1016 (-113289.0049)	Arch Hard Loss 1.6752 (2.0416)	Arch Beta Loss 1142.4177 (1132.9105)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.8%, 87.9%)	
11/22 07:04:55午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 59.7640%
11/22 07:05:04午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.0489	Prec@(1,5) (46.8%, 77.8%)
11/22 07:05:11午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.0459	Prec@(1,5) (47.0%, 77.7%)
11/22 07:05:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.0440	Prec@(1,5) (47.2%, 77.7%)
11/22 07:05:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.0437	Prec@(1,5) (47.2%, 77.6%)
11/22 07:05:26午後 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 47.2000%
11/22 07:05:26午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:05:27午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:06:21午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.4929 (1.2889)	Arch Loss -114718.9844 (-114485.0149)	Arch Hard Loss 2.2045 (1.9940)	Arch Beta Loss 1147.2119 (1144.8701)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.9%, 90.1%)	
11/22 07:07:15午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.0700 (1.3273)	Arch Loss -115184.4141 (-114719.8745)	Arch Hard Loss 1.6728 (2.0350)	Arch Beta Loss 1151.8608 (1147.2191)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.9%, 89.5%)	
11/22 07:08:09午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.2747 (1.3437)	Arch Loss -115638.4531 (-114951.3639)	Arch Hard Loss 1.8700 (2.0433)	Arch Beta Loss 1156.4032 (1149.5341)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.7%, 89.2%)	
11/22 07:08:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.0980 (1.3699)	Arch Loss -116037.7031 (-115156.6179)	Arch Hard Loss 1.7051 (2.0508)	Arch Beta Loss 1160.3940 (1151.5867)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.1%, 88.9%)	
11/22 07:08:58午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 60.0680%
11/22 07:09:06午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.0665	Prec@(1,5) (46.1%, 77.3%)
11/22 07:09:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.0979	Prec@(1,5) (45.4%, 77.0%)
11/22 07:09:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.0839	Prec@(1,5) (46.1%, 77.4%)
11/22 07:09:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.0767	Prec@(1,5) (46.4%, 77.4%)
11/22 07:09:29午後 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 46.3800%
11/22 07:09:29午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:09:29午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:10:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.4600 (1.2478)	Arch Loss -116473.8828 (-116260.8979)	Arch Hard Loss 1.8041 (2.0120)	Arch Beta Loss 1164.7568 (1162.6291)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.4%, 90.6%)	
11/22 07:11:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.1065 (1.2611)	Arch Loss -116893.2344 (-116473.7013)	Arch Hard Loss 1.9197 (2.0328)	Arch Beta Loss 1168.9515 (1164.7573)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.2%, 90.4%)	
11/22 07:12:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.3846 (1.2847)	Arch Loss -117299.5938 (-116682.3217)	Arch Hard Loss 2.2659 (2.0354)	Arch Beta Loss 1173.0186 (1166.8436)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.5%, 89.7%)	
11/22 07:13:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.1102 (1.3018)	Arch Loss -117654.8203 (-116866.4460)	Arch Hard Loss 2.0394 (2.0300)	Arch Beta Loss 1176.5686 (1168.6848)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.8%, 89.6%)	
11/22 07:13:00午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 61.8520%
11/22 07:13:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.0914	Prec@(1,5) (47.2%, 76.9%)
11/22 07:13:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.0770	Prec@(1,5) (47.5%, 77.4%)
11/22 07:13:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.0976	Prec@(1,5) (46.9%, 77.1%)
11/22 07:13:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.0838	Prec@(1,5) (47.0%, 77.3%)
11/22 07:13:31午後 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 47.0240%
11/22 07:13:31午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:13:31午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:14:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.3444 (1.2275)	Arch Loss -118041.0312 (-117852.7071)	Arch Hard Loss 1.6932 (2.0136)	Arch Beta Loss 1180.4272 (1178.5472)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.0%, 90.7%)	
11/22 07:15:20午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.6715 (1.2261)	Arch Loss -118409.6094 (-118040.3922)	Arch Hard Loss 2.0955 (2.0344)	Arch Beta Loss 1184.1171 (1180.4243)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.9%, 90.7%)	
11/22 07:16:14午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.1914 (1.2473)	Arch Loss -118765.8828 (-118223.8002)	Arch Hard Loss 1.9530 (2.0223)	Arch Beta Loss 1187.6783 (1182.2582)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.4%, 90.4%)	
11/22 07:17:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.1828 (1.2684)	Arch Loss -119075.0156 (-118385.2003)	Arch Hard Loss 2.4364 (2.0210)	Arch Beta Loss 1190.7745 (1183.8722)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.8%, 90.1%)	
11/22 07:17:03午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 62.7520%
11/22 07:17:11午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.1398	Prec@(1,5) (46.7%, 76.6%)
11/22 07:17:18午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.1536	Prec@(1,5) (46.5%, 76.3%)
11/22 07:17:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.1683	Prec@(1,5) (46.4%, 76.3%)
11/22 07:17:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.1631	Prec@(1,5) (46.6%, 76.5%)
11/22 07:17:33午後 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 46.5680%
11/22 07:17:33午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:17:34午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:18:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.1236 (1.1524)	Arch Loss -119411.0078 (-119247.5609)	Arch Hard Loss 1.8794 (1.9903)	Arch Beta Loss 1194.1289 (1192.4955)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.7%, 91.6%)	
11/22 07:19:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.0691 (1.1736)	Arch Loss -119730.9688 (-119410.4332)	Arch Hard Loss 1.6858 (2.0281)	Arch Beta Loss 1197.3265 (1194.1246)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.3%, 91.3%)	
11/22 07:20:16午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.1618 (1.2002)	Arch Loss -120038.2422 (-119569.3049)	Arch Hard Loss 2.2090 (2.0291)	Arch Beta Loss 1200.4045 (1195.7133)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.4%, 91.1%)	
11/22 07:21:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.3761 (1.2168)	Arch Loss -120305.5234 (-119708.9044)	Arch Hard Loss 1.9907 (2.0261)	Arch Beta Loss 1203.0752 (1197.1093)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.1%, 90.9%)	
11/22 07:21:04午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 64.0680%
11/22 07:21:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.1270	Prec@(1,5) (46.5%, 77.1%)
11/22 07:21:20午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.1267	Prec@(1,5) (46.5%, 77.2%)
11/22 07:21:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.1351	Prec@(1,5) (46.0%, 77.1%)
11/22 07:21:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.1407	Prec@(1,5) (45.9%, 77.0%)
11/22 07:21:35午後 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 45.8880%
11/22 07:21:35午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:21:35午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:22:30午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.3056 (1.1062)	Arch Loss -120594.5000 (-120453.7084)	Arch Hard Loss 1.8037 (2.0163)	Arch Beta Loss 1205.9630 (1204.5572)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.9%, 92.7%)	
11/22 07:23:23午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.2966 (1.1368)	Arch Loss -120868.8359 (-120593.8341)	Arch Hard Loss 2.3231 (2.0321)	Arch Beta Loss 1208.7115 (1205.9587)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 92.2%)	
11/22 07:24:17午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.2059 (1.1453)	Arch Loss -121133.4062 (-120730.3654)	Arch Hard Loss 1.9938 (2.0388)	Arch Beta Loss 1211.3540 (1207.3240)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.0%, 91.9%)	
11/22 07:25:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.3095 (1.1630)	Arch Loss -121362.6094 (-120850.2503)	Arch Hard Loss 1.8213 (2.0312)	Arch Beta Loss 1213.6443 (1208.5228)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.4%, 91.7%)	
11/22 07:25:06午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 65.3640%
11/22 07:25:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.1910	Prec@(1,5) (46.2%, 76.7%)
11/22 07:25:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.1930	Prec@(1,5) (46.9%, 76.4%)
11/22 07:25:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.1799	Prec@(1,5) (46.7%, 76.6%)
11/22 07:25:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.1781	Prec@(1,5) (46.7%, 76.6%)
11/22 07:25:37午後 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 46.7360%
11/22 07:25:37午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:25:37午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2000%
11/22 07:26:32午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.0503 (1.0210)	Arch Loss -121609.8438 (-121489.4281)	Arch Hard Loss 2.0315 (2.0058)	Arch Beta Loss 1216.1188 (1214.9143)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.1%, 93.7%)	
11/22 07:27:25午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.9058 (1.0671)	Arch Loss -121845.5781 (-121609.4873)	Arch Hard Loss 1.6414 (1.9809)	Arch Beta Loss 1218.4722 (1216.1147)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.1%, 92.9%)	
11/22 07:28:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.3766 (1.0898)	Arch Loss -122070.9922 (-121726.3601)	Arch Hard Loss 2.3599 (2.0089)	Arch Beta Loss 1220.7335 (1217.2837)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.6%, 92.6%)	
11/22 07:29:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.2472 (1.1034)	Arch Loss -122267.3359 (-121828.9619)	Arch Hard Loss 1.9360 (2.0131)	Arch Beta Loss 1222.6927 (1218.3097)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.1%, 92.4%)	
11/22 07:29:08午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 67.0920%
11/22 07:29:16午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.1137	Prec@(1,5) (47.4%, 77.5%)
11/22 07:29:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.0885	Prec@(1,5) (48.0%, 78.0%)
11/22 07:29:32午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.0944	Prec@(1,5) (47.5%, 78.0%)
11/22 07:29:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.1000	Prec@(1,5) (47.5%, 78.0%)
11/22 07:29:39午後 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 47.4480%
11/22 07:29:39午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:29:40午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4480%
11/22 07:30:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.8231 (0.9677)	Arch Loss -122479.0547 (-122375.9659)	Arch Hard Loss 1.8284 (1.9353)	Arch Beta Loss 1224.8088 (1223.7790)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.9%, 94.0%)	
11/22 07:31:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.8307 (1.0263)	Arch Loss -122680.1719 (-122478.5695)	Arch Hard Loss 1.9839 (1.9845)	Arch Beta Loss 1226.8215 (1224.8055)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.6%, 93.3%)	
11/22 07:32:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.1983 (1.0356)	Arch Loss -122873.0703 (-122578.5256)	Arch Hard Loss 2.4456 (1.9950)	Arch Beta Loss 1228.7551 (1225.8052)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.3%, 93.1%)	
11/22 07:33:11午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.1012 (1.0504)	Arch Loss -123040.9688 (-122666.2553)	Arch Hard Loss 2.0723 (2.0035)	Arch Beta Loss 1230.4304 (1226.6826)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.8%, 93.0%)	
11/22 07:33:11午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 68.8520%
11/22 07:33:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.2176	Prec@(1,5) (46.7%, 77.0%)
11/22 07:33:27午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.2364	Prec@(1,5) (46.2%, 76.6%)
11/22 07:33:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.2119	Prec@(1,5) (46.5%, 76.8%)
11/22 07:33:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.2140	Prec@(1,5) (46.5%, 76.6%)
11/22 07:33:42午後 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 46.5360%
11/22 07:33:42午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:33:42午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4480%
11/22 07:34:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 1.0808 (0.9481)	Arch Loss -123222.1328 (-123133.9273)	Arch Hard Loss 1.8895 (2.0180)	Arch Beta Loss 1232.2402 (1231.3595)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.1%, 94.2%)	
11/22 07:35:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.1877 (0.9811)	Arch Loss -123394.2578 (-123221.7262)	Arch Hard Loss 1.9225 (2.0121)	Arch Beta Loss 1233.9618 (1232.2374)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.3%, 93.7%)	
11/22 07:36:25午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.1210 (0.9906)	Arch Loss -123559.6797 (-123307.2435)	Arch Hard Loss 1.9151 (2.0007)	Arch Beta Loss 1235.6160 (1233.0924)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.1%, 93.7%)	
11/22 07:37:13午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.2355 (0.9978)	Arch Loss -123703.2266 (-123382.2890)	Arch Hard Loss 1.7409 (2.0126)	Arch Beta Loss 1237.0497 (1233.8430)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.7%)	
11/22 07:37:14午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 69.8120%
11/22 07:37:22午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.3506	Prec@(1,5) (45.7%, 75.7%)
11/22 07:37:30午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.3813	Prec@(1,5) (44.9%, 75.3%)
11/22 07:37:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.3826	Prec@(1,5) (45.0%, 75.3%)
11/22 07:37:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.3911	Prec@(1,5) (44.7%, 75.2%)
11/22 07:37:45午後 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 44.7400%
11/22 07:37:45午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:37:45午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4480%
11/22 07:38:40午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 1.0076 (0.8813)	Arch Loss -123857.5781 (-123782.4094)	Arch Hard Loss 2.2849 (2.0449)	Arch Beta Loss 1238.5986 (1237.8445)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.8%, 94.5%)	
11/22 07:39:34午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.9230 (0.9169)	Arch Loss -124005.4219 (-123857.5799)	Arch Hard Loss 1.8279 (2.0344)	Arch Beta Loss 1240.0725 (1238.5961)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.8%, 94.3%)	
11/22 07:40:28午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.8068 (0.9295)	Arch Loss -124146.7500 (-123930.8000)	Arch Hard Loss 2.1914 (2.0324)	Arch Beta Loss 1241.4894 (1239.3283)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.0%, 94.3%)	
11/22 07:41:16午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.8309 (0.9384)	Arch Loss -124269.5547 (-123995.0861)	Arch Hard Loss 2.2249 (2.0326)	Arch Beta Loss 1242.7178 (1239.9712)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.7%, 94.2%)	
11/22 07:41:17午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 71.6720%
11/22 07:41:25午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.5113	Prec@(1,5) (43.5%, 73.6%)
11/22 07:41:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.4829	Prec@(1,5) (43.8%, 73.8%)
11/22 07:41:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.5003	Prec@(1,5) (43.6%, 73.6%)
11/22 07:41:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.5108	Prec@(1,5) (43.5%, 73.4%)
11/22 07:41:48午後 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 43.5280%
11/22 07:41:48午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:41:48午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.4480%
11/22 07:42:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.7657 (0.8462)	Arch Loss -124402.3750 (-124337.8938)	Arch Hard Loss 2.1901 (2.0328)	Arch Beta Loss 1244.0457 (1243.3993)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.4%, 95.4%)	
11/22 07:43:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9821 (0.8608)	Arch Loss -124528.6875 (-124402.3584)	Arch Hard Loss 2.2628 (2.0085)	Arch Beta Loss 1245.3096 (1244.0437)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.9%, 95.2%)	
11/22 07:44:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 1.0257 (0.8767)	Arch Loss -124650.6250 (-124465.1491)	Arch Hard Loss 1.8950 (2.0122)	Arch Beta Loss 1246.5253 (1244.6716)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.5%, 95.0%)	
11/22 07:45:19午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.7811 (0.8830)	Arch Loss -124755.9766 (-124520.2915)	Arch Hard Loss 1.9809 (2.0178)	Arch Beta Loss 1247.5796 (1245.2231)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.2%, 95.0%)	
11/22 07:45:20午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 73.2240%
11/22 07:45:28午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.1060	Prec@(1,5) (48.7%, 78.5%)
11/22 07:45:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.1110	Prec@(1,5) (49.0%, 78.6%)
11/22 07:45:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.1099	Prec@(1,5) (49.2%, 78.6%)
11/22 07:45:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.1097	Prec@(1,5) (49.2%, 78.7%)
11/22 07:45:50午後 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 49.1680%
11/22 07:45:50午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:45:51午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 07:46:46午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.7551 (0.7968)	Arch Loss -124870.1328 (-124814.4859)	Arch Hard Loss 1.8708 (1.9860)	Arch Beta Loss 1248.7200 (1248.1647)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 95.4%)	
11/22 07:47:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.6392 (0.8159)	Arch Loss -124978.4688 (-124869.8641)	Arch Hard Loss 2.0960 (1.9542)	Arch Beta Loss 1249.8057 (1248.7182)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.3%, 95.4%)	
11/22 07:48:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 1.0389 (0.8247)	Arch Loss -125083.0938 (-124923.7701)	Arch Hard Loss 1.9554 (1.9955)	Arch Beta Loss 1250.8505 (1249.2577)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.9%, 95.5%)	
11/22 07:49:22午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.6761 (0.8276)	Arch Loss -125173.2969 (-124971.1322)	Arch Hard Loss 2.4138 (2.0253)	Arch Beta Loss 1251.7571 (1249.7316)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.7%, 95.4%)	
11/22 07:49:22午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 74.7400%
11/22 07:49:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.3708	Prec@(1,5) (46.3%, 75.6%)
11/22 07:49:38午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.3638	Prec@(1,5) (45.9%, 75.6%)
11/22 07:49:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.3784	Prec@(1,5) (45.6%, 75.4%)
11/22 07:49:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.3755	Prec@(1,5) (45.6%, 75.5%)
11/22 07:49:53午後 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 45.5640%
11/22 07:49:53午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:49:54午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 07:50:48午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.7198 (0.7441)	Arch Loss -125272.0156 (-125223.9831)	Arch Hard Loss 1.7414 (2.0380)	Arch Beta Loss 1252.7375 (1252.2602)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.7%, 96.3%)	
11/22 07:51:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.6656 (0.7590)	Arch Loss -125365.2500 (-125271.6193)	Arch Hard Loss 1.9587 (2.0218)	Arch Beta Loss 1253.6721 (1252.7364)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.4%, 96.3%)	
11/22 07:52:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.8124 (0.7730)	Arch Loss -125455.0938 (-125318.0440)	Arch Hard Loss 2.0669 (2.0269)	Arch Beta Loss 1254.5717 (1253.2007)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.0%, 96.1%)	
11/22 07:53:24午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 1.0364 (0.7785)	Arch Loss -125533.1797 (-125358.8467)	Arch Hard Loss 2.0706 (2.0246)	Arch Beta Loss 1255.3525 (1253.6087)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.8%, 96.1%)	
11/22 07:53:25午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 75.8480%
11/22 07:53:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.3743	Prec@(1,5) (46.0%, 76.0%)
11/22 07:53:41午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.3687	Prec@(1,5) (46.4%, 76.2%)
11/22 07:53:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.3754	Prec@(1,5) (46.5%, 76.3%)
11/22 07:53:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.3846	Prec@(1,5) (46.4%, 76.2%)
11/22 07:53:56午後 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 46.3680%
11/22 07:53:56午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:53:56午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 07:54:50午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.7472 (0.6834)	Arch Loss -125617.5938 (-125576.5416)	Arch Hard Loss 2.1985 (2.0837)	Arch Beta Loss 1256.1979 (1255.7863)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.8%, 96.8%)	
11/22 07:55:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.9286 (0.7021)	Arch Loss -125698.0547 (-125617.6085)	Arch Hard Loss 2.2954 (2.0608)	Arch Beta Loss 1257.0035 (1256.1967)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.9%, 96.7%)	
11/22 07:56:38午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.7532 (0.7227)	Arch Loss -125776.2422 (-125657.6500)	Arch Hard Loss 1.6710 (2.0496)	Arch Beta Loss 1257.7792 (1256.5970)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.1%, 96.5%)	
11/22 07:57:26午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 1.1621 (0.7348)	Arch Loss -125843.5469 (-125692.8458)	Arch Hard Loss 1.7284 (2.0389)	Arch Beta Loss 1258.4528 (1256.9488)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.7%, 96.4%)	
11/22 07:57:27午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 77.6720%
11/22 07:57:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 5.3781	Prec@(1,5) (16.3%, 38.5%)
11/22 07:57:43午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 5.3530	Prec@(1,5) (16.7%, 38.5%)
11/22 07:57:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 5.3472	Prec@(1,5) (16.7%, 38.5%)
11/22 07:57:58午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 5.3364	Prec@(1,5) (16.5%, 38.6%)
11/22 07:57:58午後 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 16.5520%
11/22 07:57:58午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 07:57:58午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 07:58:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.5707 (0.6605)	Arch Loss -125916.5156 (-125880.6720)	Arch Hard Loss 1.7509 (2.0485)	Arch Beta Loss 1259.1826 (1258.8272)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.3%, 97.2%)	
11/22 07:59:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.6367 (0.6786)	Arch Loss -125985.9375 (-125916.1067)	Arch Hard Loss 1.8692 (2.0496)	Arch Beta Loss 1259.8781 (1259.1816)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.8%, 97.0%)	
11/22 08:00:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.7368 (0.6825)	Arch Loss -126052.8516 (-125950.6733)	Arch Hard Loss 1.9909 (2.0528)	Arch Beta Loss 1260.5485 (1259.5273)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.7%, 96.9%)	
11/22 08:01:29午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 1.0416 (0.6961)	Arch Loss -126111.1328 (-125981.0693)	Arch Hard Loss 1.9310 (2.0506)	Arch Beta Loss 1261.1306 (1259.8312)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.3%, 96.9%)	
11/22 08:01:29午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 78.2640%
11/22 08:01:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.2386	Prec@(1,5) (49.4%, 77.9%)
11/22 08:01:45午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.2226	Prec@(1,5) (48.9%, 78.1%)
11/22 08:01:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.2054	Prec@(1,5) (49.1%, 78.4%)
11/22 08:02:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.2000	Prec@(1,5) (49.1%, 78.5%)
11/22 08:02:00午後 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 49.1400%
11/22 08:02:00午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:02:00午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:02:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.6773 (0.6232)	Arch Loss -126174.1641 (-126143.3495)	Arch Hard Loss 1.9694 (2.0661)	Arch Beta Loss 1261.7614 (1261.4542)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.7%, 97.6%)	
11/22 08:03:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.6289 (0.6338)	Arch Loss -126233.7812 (-126174.0270)	Arch Hard Loss 2.5108 (2.0288)	Arch Beta Loss 1262.3629 (1261.7606)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.5%, 97.5%)	
11/22 08:04:43午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.6940 (0.6437)	Arch Loss -126292.0625 (-126203.9072)	Arch Hard Loss 2.2271 (2.0469)	Arch Beta Loss 1262.9429 (1262.0595)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.2%, 97.5%)	
11/22 08:05:31午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.6823 (0.6443)	Arch Loss -126342.3906 (-126230.1972)	Arch Hard Loss 2.2968 (2.0503)	Arch Beta Loss 1263.4469 (1262.3225)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.2%, 97.4%)	
11/22 08:05:32午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 80.2160%
11/22 08:05:40午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.1765	Prec@(1,5) (49.3%, 79.1%)
11/22 08:05:47午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.2250	Prec@(1,5) (48.7%, 78.3%)
11/22 08:05:55午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.2267	Prec@(1,5) (48.9%, 78.2%)
11/22 08:06:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.2381	Prec@(1,5) (48.4%, 78.2%)
11/22 08:06:02午後 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 48.4320%
11/22 08:06:02午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:06:03午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:06:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.5671 (0.5917)	Arch Loss -126397.1094 (-126370.6213)	Arch Hard Loss 2.1711 (2.0591)	Arch Beta Loss 1263.9928 (1263.7268)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.0%, 97.7%)	
11/22 08:07:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.8019 (0.6047)	Arch Loss -126449.5156 (-126397.1568)	Arch Hard Loss 1.8481 (2.0553)	Arch Beta Loss 1264.5137 (1263.9921)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.8%, 97.7%)	
11/22 08:08:45午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.6189 (0.6076)	Arch Loss -126499.1484 (-126423.0479)	Arch Hard Loss 2.4538 (2.0596)	Arch Beta Loss 1265.0160 (1264.2511)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.5%, 97.7%)	
11/22 08:09:33午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.5032 (0.6137)	Arch Loss -126543.4609 (-126445.8204)	Arch Hard Loss 1.8126 (2.0651)	Arch Beta Loss 1265.4528 (1264.4789)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.2%, 97.5%)	
11/22 08:09:34午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 81.1640%
11/22 08:09:42午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 2.8771	Prec@(1,5) (40.7%, 71.4%)
11/22 08:09:50午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.8611	Prec@(1,5) (40.5%, 71.5%)
11/22 08:09:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.8668	Prec@(1,5) (40.1%, 71.5%)
11/22 08:10:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.8825	Prec@(1,5) (40.2%, 71.2%)
11/22 08:10:05午後 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 40.2160%
11/22 08:10:05午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:10:05午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:10:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.5633 (0.5727)	Arch Loss -126590.1719 (-126567.5054)	Arch Hard Loss 2.4507 (2.0620)	Arch Beta Loss 1265.9263 (1265.6957)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.2%, 97.9%)	
11/22 08:11:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.9484 (0.5820)	Arch Loss -126634.7422 (-126590.5139)	Arch Hard Loss 3.0755 (2.0574)	Arch Beta Loss 1266.3782 (1265.9257)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.1%, 97.8%)	
11/22 08:12:47午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.4919 (0.5929)	Arch Loss -126678.8594 (-126612.9488)	Arch Hard Loss 2.5121 (2.0798)	Arch Beta Loss 1266.8137 (1266.1503)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.7%, 97.7%)	
11/22 08:13:36午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.4620 (0.5953)	Arch Loss -126717.3047 (-126632.7176)	Arch Hard Loss 1.9629 (2.0674)	Arch Beta Loss 1267.1926 (1266.3478)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.6%, 97.7%)	
11/22 08:13:36午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 81.6360%
11/22 08:13:44午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.5071	Prec@(1,5) (44.3%, 74.9%)
11/22 08:13:52午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.5272	Prec@(1,5) (44.1%, 74.5%)
11/22 08:14:00午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.5437	Prec@(1,5) (43.9%, 74.5%)
11/22 08:14:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.5624	Prec@(1,5) (43.7%, 74.4%)
11/22 08:14:07午後 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 43.6760%
11/22 08:14:07午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:14:07午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:15:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.5270 (0.5688)	Arch Loss -126758.5156 (-126738.2940)	Arch Hard Loss 1.8542 (2.0521)	Arch Beta Loss 1267.6036 (1267.4035)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.8%, 97.8%)	
11/22 08:15:55午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.6934 (0.5694)	Arch Loss -126797.2188 (-126758.2475)	Arch Hard Loss 2.3723 (2.0659)	Arch Beta Loss 1267.9960 (1267.6031)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.5%, 97.8%)	
11/22 08:16:49午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.6668 (0.5680)	Arch Loss -126835.0156 (-126777.7418)	Arch Hard Loss 2.4012 (2.0680)	Arch Beta Loss 1268.3741 (1267.7981)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.8%)	
11/22 08:17:37午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.5503 (0.5678)	Arch Loss -126868.0781 (-126794.8888)	Arch Hard Loss 2.2610 (2.0765)	Arch Beta Loss 1268.7034 (1267.9697)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.8%)	
11/22 08:17:38午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 82.6440%
11/22 08:17:46午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 3.1085	Prec@(1,5) (43.5%, 73.2%)
11/22 08:17:54午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 3.2295	Prec@(1,5) (43.1%, 72.5%)
11/22 08:18:02午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 3.2418	Prec@(1,5) (43.1%, 72.6%)
11/22 08:18:09午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 3.2359	Prec@(1,5) (43.1%, 72.9%)
11/22 08:18:09午後 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 43.0600%
11/22 08:18:09午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:18:09午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:19:04午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.6632 (0.5171)	Arch Loss -126904.2500 (-126886.5409)	Arch Hard Loss 1.7775 (2.1000)	Arch Beta Loss 1269.0603 (1268.8864)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.3%, 98.2%)	
11/22 08:19:57午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.5114 (0.5334)	Arch Loss -126938.1406 (-126903.9148)	Arch Hard Loss 1.9716 (2.0760)	Arch Beta Loss 1269.4011 (1269.0599)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.9%, 98.2%)	
11/22 08:20:51午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.3809 (0.5319)	Arch Loss -126971.2188 (-126920.8562)	Arch Hard Loss 1.7807 (2.0780)	Arch Beta Loss 1269.7300 (1269.2293)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.9%, 98.2%)	
11/22 08:21:39午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.4253 (0.5410)	Arch Loss -126999.3750 (-126935.7649)	Arch Hard Loss 2.2530 (2.0801)	Arch Beta Loss 1270.0162 (1269.3785)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.6%, 98.1%)	
11/22 08:21:40午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 83.5600%
11/22 08:21:48午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 3.1177	Prec@(1,5) (38.3%, 69.0%)
11/22 08:21:56午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 3.0957	Prec@(1,5) (38.3%, 68.9%)
11/22 08:22:03午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 3.1069	Prec@(1,5) (38.0%, 68.7%)
11/22 08:22:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 3.1108	Prec@(1,5) (38.0%, 68.6%)
11/22 08:22:10午後 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 37.9800%
11/22 08:22:10午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:22:11午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:23:05午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.4706 (0.5120)	Arch Loss -127031.4531 (-127015.4775)	Arch Hard Loss 1.1912 (2.0580)	Arch Beta Loss 1270.3264 (1270.1754)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.3%, 98.2%)	
11/22 08:23:59午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.4448 (0.5188)	Arch Loss -127060.4609 (-127030.5347)	Arch Hard Loss 1.8431 (2.0933)	Arch Beta Loss 1270.6230 (1270.3263)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.1%, 98.2%)	
11/22 08:24:53午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.4833 (0.5261)	Arch Loss -127088.8672 (-127045.2876)	Arch Hard Loss 2.0595 (2.0779)	Arch Beta Loss 1270.9093 (1270.4737)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.8%, 98.2%)	
11/22 08:25:41午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.4975 (0.5313)	Arch Loss -127114.0156 (-127058.2504)	Arch Hard Loss 1.8241 (2.0877)	Arch Beta Loss 1271.1583 (1270.6034)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.6%, 98.2%)	
11/22 08:25:41午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 83.6160%
11/22 08:25:49午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.4985	Prec@(1,5) (45.8%, 75.8%)
11/22 08:25:57午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.5003	Prec@(1,5) (45.6%, 75.5%)
11/22 08:26:05午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.5042	Prec@(1,5) (46.0%, 75.6%)
11/22 08:26:12午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.5117	Prec@(1,5) (45.7%, 75.5%)
11/22 08:26:12午後 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 45.7200%
11/22 08:26:12午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:26:12午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:27:07午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.7126 (0.5083)	Arch Loss -127140.7891 (-127127.6142)	Arch Hard Loss 2.0505 (2.0630)	Arch Beta Loss 1271.4283 (1271.2968)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.7%, 98.3%)	
11/22 08:28:00午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.5001 (0.5150)	Arch Loss -127165.8984 (-127140.7112)	Arch Hard Loss 2.7578 (2.1019)	Arch Beta Loss 1271.6865 (1271.4281)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.3%)	
11/22 08:28:54午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.5009 (0.5254)	Arch Loss -127191.7812 (-127153.5593)	Arch Hard Loss 1.7613 (2.0833)	Arch Beta Loss 1271.9354 (1271.5564)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.0%, 98.2%)	
11/22 08:29:42午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.7361 (0.5284)	Arch Loss -127213.0781 (-127164.8454)	Arch Hard Loss 2.1564 (2.0911)	Arch Beta Loss 1272.1523 (1271.6694)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.0%, 98.2%)	
11/22 08:29:43午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 83.9760%
11/22 08:29:51午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.6070	Prec@(1,5) (44.4%, 74.3%)
11/22 08:29:59午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 2.5736	Prec@(1,5) (44.7%, 74.6%)
11/22 08:30:07午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 2.5602	Prec@(1,5) (44.7%, 74.6%)
11/22 08:30:14午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 2.5673	Prec@(1,5) (44.7%, 74.5%)
11/22 08:30:14午後 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 44.6800%
11/22 08:30:14午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:30:14午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:31:08午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.4301 (0.5009)	Arch Loss -127236.5078 (-127225.2280)	Arch Hard Loss 2.2961 (2.0836)	Arch Beta Loss 1272.3881 (1272.2731)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.6%)	
11/22 08:32:02午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.3799 (0.5014)	Arch Loss -127259.2656 (-127236.6640)	Arch Hard Loss 1.9904 (2.0904)	Arch Beta Loss 1272.6125 (1272.3875)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.8%, 98.6%)	
11/22 08:32:56午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.3150 (0.5066)	Arch Loss -127281.0078 (-127247.8548)	Arch Hard Loss 1.9574 (2.0757)	Arch Beta Loss 1272.8297 (1272.4993)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.8%, 98.5%)	
11/22 08:33:44午後 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.6369 (0.5144)	Arch Loss -127300.3828 (-127257.6852)	Arch Hard Loss 1.4862 (2.0853)	Arch Beta Loss 1273.0187 (1272.5977)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (84.5%, 98.4%)	
11/22 08:33:45午後 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 84.5120%
11/22 08:33:53午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 13.7435	Prec@(1,5) (45.2%, 74.1%)
11/22 08:34:01午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 13.2761	Prec@(1,5) (44.8%, 73.9%)
11/22 08:34:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 13.3873	Prec@(1,5) (44.8%, 73.9%)
11/22 08:34:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 12.5082	Prec@(1,5) (45.0%, 74.1%)
11/22 08:34:16午後 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 45.0360%
11/22 08:34:16午後 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 10)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
11/22 08:34:16午後 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.1680%
11/22 08:34:16午後 trainer_runner.py:110 [INFO] Final best Prec@1 = 49.1680%
11/22 08:34:16午後 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('avg_pool_3x3', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[10, 11])
