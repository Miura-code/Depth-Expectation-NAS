11/21 11:16:47PM parser.py:28 [INFO] 
11/21 11:16:47PM parser.py:29 [INFO] Parameters:
11/21 11:16:47PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1/DAG
11/21 11:16:47PM parser.py:31 [INFO] T=10.0
11/21 11:16:47PM parser.py:31 [INFO] ADVANCED=1
11/21 11:16:47PM parser.py:31 [INFO] ALPHA_LR=0.0003
11/21 11:16:47PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/21 11:16:47PM parser.py:31 [INFO] ARCH_CRITERION=expected
11/21 11:16:47PM parser.py:31 [INFO] BATCH_SIZE=64
11/21 11:16:47PM parser.py:31 [INFO] CASCADE=0
11/21 11:16:47PM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/21 11:16:47PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/21 11:16:47PM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/21 11:16:47PM parser.py:31 [INFO] DATA_PATH=../data/
11/21 11:16:47PM parser.py:31 [INFO] DATASET=cifar100
11/21 11:16:47PM parser.py:31 [INFO] DEPTH_COEF=0.0
11/21 11:16:47PM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/21 11:16:47PM parser.py:31 [INFO] DISCRETE=0
11/21 11:16:47PM parser.py:31 [INFO] EPOCHS=2
11/21 11:16:47PM parser.py:31 [INFO] EVAL_EPOCHS=100
11/21 11:16:47PM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g0.1
11/21 11:16:47PM parser.py:31 [INFO] FINAL_L=0.0
11/21 11:16:47PM parser.py:31 [INFO] G=0.1
11/21 11:16:47PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/21 11:16:47PM parser.py:31 [INFO] GPUS=[0]
11/21 11:16:47PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/21 11:16:47PM parser.py:31 [INFO] INIT_CHANNELS=16
11/21 11:16:47PM parser.py:31 [INFO] L=0.0
11/21 11:16:47PM parser.py:31 [INFO] LAYERS=32
11/21 11:16:47PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/21 11:16:47PM parser.py:31 [INFO] NAME=Pruning
11/21 11:16:47PM parser.py:31 [INFO] NONKD=1
11/21 11:16:47PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1
11/21 11:16:47PM parser.py:31 [INFO] PCDARTS=0
11/21 11:16:47PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1/plots
11/21 11:16:47PM parser.py:31 [INFO] PRINT_FREQ=100
11/21 11:16:47PM parser.py:31 [INFO] RESET=0
11/21 11:16:47PM parser.py:31 [INFO] RESUME_PATH=None
11/21 11:16:47PM parser.py:31 [INFO] SAVE=s0-expected-sw3-g0.1
11/21 11:16:47PM parser.py:31 [INFO] SEED=0
11/21 11:16:47PM parser.py:31 [INFO] SHARE_STAGE=0
11/21 11:16:47PM parser.py:31 [INFO] SLIDE_WINDOW=3
11/21 11:16:47PM parser.py:31 [INFO] SPEC_CELL=1
11/21 11:16:47PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/21 11:16:47PM parser.py:31 [INFO] TEACHER_NAME=none
11/21 11:16:47PM parser.py:31 [INFO] TEACHER_PATH=none
11/21 11:16:47PM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/21 11:16:47PM parser.py:31 [INFO] TYPE=Pruning
11/21 11:16:47PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/21 11:16:47PM parser.py:31 [INFO] W_LR=0.025
11/21 11:16:47PM parser.py:31 [INFO] W_LR_MIN=0.001
11/21 11:16:47PM parser.py:31 [INFO] W_MOMENTUM=0.9
11/21 11:16:47PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/21 11:16:47PM parser.py:31 [INFO] WORKERS=4
11/21 11:16:47PM parser.py:32 [INFO] 
11/21 11:16:49PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/21 11:17:46PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.3955 (4.5565)	Arch Loss 39.6108 (40.1412)	Arch Hard Loss 4.4356 (4.5455)	Arch Beta Loss 351.7523 (355.9564)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.1%, 9.4%)	
11/22 02:42:37AM parser.py:28 [INFO] 
11/22 02:42:37AM parser.py:29 [INFO] Parameters:
11/22 02:42:37AM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1/DAG
11/22 02:42:37AM parser.py:31 [INFO] T=10.0
11/22 02:42:37AM parser.py:31 [INFO] ADVANCED=1
11/22 02:42:37AM parser.py:31 [INFO] ALPHA_LR=0.0003
11/22 02:42:37AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
11/22 02:42:37AM parser.py:31 [INFO] ARCH_CRITERION=expected
11/22 02:42:37AM parser.py:31 [INFO] BATCH_SIZE=64
11/22 02:42:37AM parser.py:31 [INFO] CASCADE=0
11/22 02:42:37AM parser.py:31 [INFO] CHECKPOINT_RESET=False
11/22 02:42:37AM parser.py:31 [INFO] CURRICULUM_EPOCHS=[]
11/22 02:42:37AM parser.py:31 [INFO] CUTOUT_LENGTH=0
11/22 02:42:37AM parser.py:31 [INFO] DATA_PATH=../data/
11/22 02:42:37AM parser.py:31 [INFO] DATASET=cifar100
11/22 02:42:37AM parser.py:31 [INFO] DEPTH_COEF=0.0
11/22 02:42:37AM parser.py:31 [INFO] DESCRIPTION=search_with_-Beta-expected-Depth-constraint_slidewindow-3
11/22 02:42:37AM parser.py:31 [INFO] DISCRETE=0
11/22 02:42:37AM parser.py:31 [INFO] EPOCHS=50
11/22 02:42:37AM parser.py:31 [INFO] EVAL_EPOCHS=100
11/22 02:42:37AM parser.py:31 [INFO] EXP_NAME=s0-expected-sw3-g0.1
11/22 02:42:37AM parser.py:31 [INFO] FINAL_L=0.0
11/22 02:42:37AM parser.py:31 [INFO] G=0.1
11/22 02:42:37AM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
11/22 02:42:37AM parser.py:31 [INFO] GPUS=[0]
11/22 02:42:37AM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
11/22 02:42:37AM parser.py:31 [INFO] INIT_CHANNELS=16
11/22 02:42:37AM parser.py:31 [INFO] L=0.0
11/22 02:42:37AM parser.py:31 [INFO] LAYERS=32
11/22 02:42:37AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
11/22 02:42:37AM parser.py:31 [INFO] NAME=Pruning
11/22 02:42:37AM parser.py:31 [INFO] NONKD=1
11/22 02:42:37AM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1
11/22 02:42:37AM parser.py:31 [INFO] PCDARTS=0
11/22 02:42:37AM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected-sw3-g0.1/plots
11/22 02:42:37AM parser.py:31 [INFO] PRINT_FREQ=100
11/22 02:42:37AM parser.py:31 [INFO] RESET=0
11/22 02:42:37AM parser.py:31 [INFO] RESUME_PATH=None
11/22 02:42:37AM parser.py:31 [INFO] SAVE=s0-expected-sw3-g0.1
11/22 02:42:37AM parser.py:31 [INFO] SEED=0
11/22 02:42:37AM parser.py:31 [INFO] SHARE_STAGE=0
11/22 02:42:37AM parser.py:31 [INFO] SLIDE_WINDOW=3
11/22 02:42:37AM parser.py:31 [INFO] SPEC_CELL=1
11/22 02:42:37AM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
11/22 02:42:37AM parser.py:31 [INFO] TEACHER_NAME=none
11/22 02:42:37AM parser.py:31 [INFO] TEACHER_PATH=none
11/22 02:42:37AM parser.py:31 [INFO] TRAIN_PORTION=0.5
11/22 02:42:37AM parser.py:31 [INFO] TYPE=Pruning
11/22 02:42:37AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
11/22 02:42:37AM parser.py:31 [INFO] W_LR=0.025
11/22 02:42:37AM parser.py:31 [INFO] W_LR_MIN=0.001
11/22 02:42:37AM parser.py:31 [INFO] W_MOMENTUM=0.9
11/22 02:42:37AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
11/22 02:42:37AM parser.py:31 [INFO] WORKERS=4
11/22 02:42:37AM parser.py:32 [INFO] 
11/22 02:42:38AM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
11/22 02:43:34AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.3114 (4.5126)	Arch Loss 39.4675 (40.1178)	Arch Hard Loss 4.2923 (4.5222)	Arch Beta Loss 351.7527 (355.9565)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.2%, 10.6%)	
11/22 02:44:27AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0129 (4.3784)	Arch Loss 38.6110 (39.5541)	Arch Hard Loss 4.2708 (4.3802)	Arch Beta Loss 343.4013 (351.7386)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.5%, 14.4%)	
11/22 02:45:20AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0056 (4.2777)	Arch Loss 37.5417 (39.0422)	Arch Hard Loss 4.0188 (4.2843)	Arch Beta Loss 335.2287 (347.5788)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.3%, 17.3%)	
11/22 02:46:08AM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.9759 (4.2154)	Arch Loss 36.7362 (38.6032)	Arch Hard Loss 3.9329 (4.2146)	Arch Beta Loss 328.0332 (343.8864)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.0%, 19.3%)	
11/22 02:46:10AM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 5.0560%
11/22 02:46:18AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9951	Prec@(1,5) (7.7%, 26.2%)
11/22 02:46:26AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9837	Prec@(1,5) (7.6%, 26.4%)
11/22 02:46:33AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9799	Prec@(1,5) (7.7%, 26.6%)
11/22 02:46:41AM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9803	Prec@(1,5) (7.7%, 26.6%)
11/22 02:46:41AM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 7.6720%
11/22 02:46:41AM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('skip_connect', 5), ('avg_pool_3x3', 7)], [('skip_connect', 7), ('avg_pool_3x3', 8)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 10)]], DAG2_concat=[2, 4], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 4), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 02:46:41AM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 7.6720%
11/22 02:47:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.8614 (3.9156)	Arch Loss 36.1727 (36.3175)	Arch Hard Loss 4.1580 (3.9179)	Arch Beta Loss 320.1475 (323.9953)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.3%, 28.2%)	
11/22 02:48:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.8533 (3.8844)	Arch Loss 35.2437 (35.8873)	Arch Hard Loss 3.9905 (3.8733)	Arch Beta Loss 312.5313 (320.1403)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.8%, 29.8%)	
11/22 02:49:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7267 (3.8456)	Arch Loss 33.9970 (35.4699)	Arch Hard Loss 3.4860 (3.8350)	Arch Beta Loss 305.1095 (316.3492)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.5%, 31.1%)	
11/22 02:50:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.8931 (3.8052)	Arch Loss 33.7445 (35.0988)	Arch Hard Loss 3.8852 (3.7996)	Arch Beta Loss 298.5928 (312.9921)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.3%, 32.4%)	
11/22 02:50:13午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 10.3160%
11/22 02:50:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6554	Prec@(1,5) (12.3%, 36.4%)
11/22 02:50:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6415	Prec@(1,5) (12.9%, 36.7%)
11/22 02:50:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6462	Prec@(1,5) (12.7%, 36.7%)
11/22 02:50:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6433	Prec@(1,5) (12.7%, 36.9%)
11/22 02:50:44午前 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 12.7400%
11/22 02:50:44午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 02:50:45午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 12.7400%
11/22 02:51:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.8676 (3.6383)	Arch Loss 32.7608 (33.1271)	Arch Hard Loss 3.6148 (3.6332)	Arch Beta Loss 291.4595 (294.9393)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (12.8%, 36.9%)	
11/22 02:52:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5974 (3.6090)	Arch Loss 32.0774 (32.7658)	Arch Hard Loss 3.6197 (3.6203)	Arch Beta Loss 284.5764 (291.4545)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.6%, 37.9%)	
11/22 02:53:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.4721 (3.5750)	Arch Loss 31.2462 (32.3892)	Arch Hard Loss 3.4589 (3.5864)	Arch Beta Loss 277.8724 (288.0284)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.2%, 39.0%)	
11/22 02:54:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.6069 (3.5523)	Arch Loss 30.5549 (32.0563)	Arch Hard Loss 3.3568 (3.5568)	Arch Beta Loss 271.9814 (284.9946)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.7%, 39.9%)	
11/22 02:54:16午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 14.6840%
11/22 02:54:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4895	Prec@(1,5) (15.6%, 41.6%)
11/22 02:54:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.5009	Prec@(1,5) (15.6%, 41.5%)
11/22 02:54:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4996	Prec@(1,5) (15.4%, 41.7%)
11/22 02:54:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4991	Prec@(1,5) (15.4%, 41.8%)
11/22 02:54:48午前 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 15.3760%
11/22 02:54:48午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 02:54:48午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 15.3760%
11/22 02:55:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.4763 (3.3990)	Arch Loss 29.8247 (30.2703)	Arch Hard Loss 3.2705 (3.4019)	Arch Beta Loss 265.5413 (268.6833)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.9%, 44.3%)	
11/22 02:56:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.1417 (3.3672)	Arch Loss 29.4709 (29.9366)	Arch Hard Loss 3.5382 (3.3830)	Arch Beta Loss 259.3271 (265.5366)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.8%, 45.5%)	
11/22 02:57:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1831 (3.3315)	Arch Loss 28.8977 (29.5940)	Arch Hard Loss 3.5708 (3.3497)	Arch Beta Loss 253.2692 (262.4426)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.4%, 46.5%)	
11/22 02:58:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.1988 (3.3114)	Arch Loss 27.9417 (29.2978)	Arch Hard Loss 3.1476 (3.3276)	Arch Beta Loss 247.9417 (259.7018)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.8%, 47.1%)	
11/22 02:58:20午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 18.8240%
11/22 02:58:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.3038	Prec@(1,5) (19.3%, 47.7%)
11/22 02:58:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2931	Prec@(1,5) (19.3%, 47.6%)
11/22 02:58:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.3038	Prec@(1,5) (19.1%, 47.4%)
11/22 02:58:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.3089	Prec@(1,5) (19.0%, 47.3%)
11/22 02:58:51午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 18.9960%
11/22 02:58:51午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('max_pool_3x3', 5)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 02:58:51午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 18.9960%
11/22 02:59:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.1149 (3.1462)	Arch Loss 27.2872 (27.7321)	Arch Hard Loss 3.0775 (3.2372)	Arch Beta Loss 242.0971 (244.9481)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.9%, 51.5%)	
11/22 03:00:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.0701 (3.1319)	Arch Loss 26.7031 (27.4045)	Arch Hard Loss 3.0581 (3.1954)	Arch Beta Loss 236.4499 (242.0913)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.1%, 51.9%)	
11/22 03:01:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.7969 (3.1215)	Arch Loss 26.2660 (27.1009)	Arch Hard Loss 3.1729 (3.1730)	Arch Beta Loss 230.9306 (239.2782)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.1%, 52.2%)	
11/22 03:02:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 3.1168 (3.1131)	Arch Loss 25.5205 (26.8248)	Arch Hard Loss 2.9136 (3.1465)	Arch Beta Loss 226.0692 (236.7827)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.2%, 52.5%)	
11/22 03:02:24午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 22.2000%
11/22 03:02:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.1479	Prec@(1,5) (22.0%, 51.7%)
11/22 03:02:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.1070	Prec@(1,5) (22.3%, 53.0%)
11/22 03:02:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0952	Prec@(1,5) (22.5%, 53.3%)
11/22 03:02:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0935	Prec@(1,5) (22.7%, 53.6%)
11/22 03:02:55午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 22.7320%
11/22 03:02:55午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('max_pool_3x3', 5), ('skip_connect', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 03:02:55午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 22.7320%
11/22 03:03:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1940 (2.9478)	Arch Loss 25.0034 (25.3379)	Arch Hard Loss 2.9300 (3.0040)	Arch Beta Loss 220.7343 (223.3387)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.8%, 56.4%)	
11/22 03:04:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.0975 (2.9496)	Arch Loss 24.8330 (25.0992)	Arch Hard Loss 3.2762 (3.0266)	Arch Beta Loss 215.5688 (220.7261)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.6%, 56.4%)	
11/22 03:05:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.8359 (2.9448)	Arch Loss 23.7623 (24.8179)	Arch Hard Loss 2.7094 (3.0024)	Arch Beta Loss 210.5292 (218.1551)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.6%, 56.5%)	
11/22 03:06:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.6124 (2.9296)	Arch Loss 23.5768 (24.5732)	Arch Hard Loss 2.9675 (2.9856)	Arch Beta Loss 206.0933 (215.8758)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.8%, 57.0%)	
11/22 03:06:27午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 25.7880%
11/22 03:06:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.9404	Prec@(1,5) (25.6%, 57.2%)
11/22 03:06:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9612	Prec@(1,5) (25.7%, 56.6%)
11/22 03:06:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9656	Prec@(1,5) (25.5%, 56.7%)
11/22 03:06:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9610	Prec@(1,5) (25.7%, 56.8%)
11/22 03:06:58午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 25.6600%
11/22 03:06:58午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 4], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 03:06:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 25.6600%
11/22 03:07:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.6649 (2.7615)	Arch Loss 22.9276 (23.2346)	Arch Hard Loss 2.8052 (2.8745)	Arch Beta Loss 201.2247 (203.6012)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.9%, 61.3%)	
11/22 03:08:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.9551 (2.7806)	Arch Loss 22.6924 (22.9796)	Arch Hard Loss 3.0389 (2.8571)	Arch Beta Loss 196.5351 (201.2246)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.5%, 60.7%)	
11/22 03:09:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.9170 (2.7710)	Arch Loss 21.8504 (22.7244)	Arch Hard Loss 2.6538 (2.8354)	Arch Beta Loss 191.9661 (198.8893)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.6%, 61.2%)	
11/22 03:10:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.8016 (2.7614)	Arch Loss 21.5614 (22.5191)	Arch Hard Loss 2.7659 (2.8369)	Arch Beta Loss 187.9545 (196.8217)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.9%, 61.4%)	
11/22 03:10:30午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 28.8720%
11/22 03:10:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.7780	Prec@(1,5) (30.3%, 60.8%)
11/22 03:10:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.7721	Prec@(1,5) (30.0%, 61.0%)
11/22 03:10:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.7853	Prec@(1,5) (29.7%, 60.8%)
11/22 03:11:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.7904	Prec@(1,5) (29.5%, 60.7%)
11/22 03:11:01午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 29.5640%
11/22 03:11:01午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 4])
11/22 03:11:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 29.5640%
11/22 03:11:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.7971 (2.6158)	Arch Loss 21.0269 (21.3301)	Arch Hard Loss 2.6703 (2.7594)	Arch Beta Loss 183.5657 (185.7074)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.7%, 64.2%)	
11/22 03:12:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.5495 (2.6222)	Arch Loss 20.7414 (21.0961)	Arch Hard Loss 2.8076 (2.7397)	Arch Beta Loss 179.3379 (183.5642)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.5%, 64.0%)	
11/22 03:13:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.8524 (2.6097)	Arch Loss 20.6225 (20.8694)	Arch Hard Loss 3.0989 (2.7232)	Arch Beta Loss 175.2357 (181.4617)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.0%, 64.6%)	
11/22 03:14:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.6296 (2.5982)	Arch Loss 19.7975 (20.6625)	Arch Hard Loss 2.6333 (2.7022)	Arch Beta Loss 171.6423 (179.6038)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.2%, 64.8%)	
11/22 03:14:33午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 32.2040%
11/22 03:14:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6954	Prec@(1,5) (30.8%, 62.8%)
11/22 03:14:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.6682	Prec@(1,5) (31.1%, 63.7%)
11/22 03:14:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6655	Prec@(1,5) (31.2%, 63.9%)
11/22 03:15:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6767	Prec@(1,5) (31.3%, 63.6%)
11/22 03:15:03午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 31.2960%
11/22 03:15:03午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 7), ('skip_connect', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 03:15:04午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 31.2960%
11/22 03:15:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.3887 (2.4691)	Arch Loss 19.6377 (19.5746)	Arch Hard Loss 2.8664 (2.6115)	Arch Beta Loss 167.7131 (169.6302)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.2%, 68.2%)	
11/22 03:16:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.2870 (2.4649)	Arch Loss 19.2892 (19.3806)	Arch Hard Loss 2.8957 (2.6092)	Arch Beta Loss 163.9351 (167.7141)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.4%, 68.1%)	
11/22 03:17:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.4891 (2.4689)	Arch Loss 18.7003 (19.1873)	Arch Hard Loss 2.6739 (2.6039)	Arch Beta Loss 160.2636 (165.8337)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.4%, 68.1%)	
11/22 03:18:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.2964 (2.4606)	Arch Loss 18.3766 (19.0074)	Arch Hard Loss 2.6717 (2.5902)	Arch Beta Loss 157.0495 (164.1718)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.4%, 68.3%)	
11/22 03:18:35午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 35.3600%
11/22 03:18:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.5121	Prec@(1,5) (34.4%, 67.9%)
11/22 03:18:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.5189	Prec@(1,5) (34.5%, 67.6%)
11/22 03:18:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.5301	Prec@(1,5) (34.7%, 67.0%)
11/22 03:19:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.5265	Prec@(1,5) (34.7%, 67.2%)
11/22 03:19:07午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 34.6880%
11/22 03:19:07午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 03:19:07午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 34.6880%
11/22 03:20:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.4694 (2.3407)	Arch Loss 18.2620 (18.0688)	Arch Hard Loss 2.9093 (2.5443)	Arch Beta Loss 153.5275 (155.2453)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.4%, 70.7%)	
11/22 03:20:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.3332 (2.3575)	Arch Loss 17.5247 (17.8741)	Arch Hard Loss 2.5102 (2.5213)	Arch Beta Loss 150.1444 (153.5283)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.4%, 70.7%)	
11/22 03:21:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.3388 (2.3508)	Arch Loss 17.0380 (17.7000)	Arch Hard Loss 2.3532 (2.5156)	Arch Beta Loss 146.8476 (151.8431)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.6%, 70.6%)	
11/22 03:22:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.3416 (2.3450)	Arch Loss 16.9029 (17.5374)	Arch Hard Loss 2.5070 (2.5022)	Arch Beta Loss 143.9587 (150.3516)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.7%, 70.7%)	
11/22 03:22:39午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 37.6480%
11/22 03:22:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.4732	Prec@(1,5) (35.2%, 68.3%)
11/22 03:22:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.4583	Prec@(1,5) (35.5%, 68.6%)
11/22 03:23:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.4679	Prec@(1,5) (35.4%, 68.6%)
11/22 03:23:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.4695	Prec@(1,5) (35.5%, 68.5%)
11/22 03:23:10午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 35.4720%
11/22 03:23:10午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 03:23:10午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 35.4720%
11/22 03:24:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.3499 (2.2244)	Arch Loss 16.5348 (16.7079)	Arch Hard Loss 2.4545 (2.4736)	Arch Beta Loss 140.8032 (142.3426)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.5%, 73.6%)	
11/22 03:24:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.5304 (2.2446)	Arch Loss 15.8352 (16.5195)	Arch Hard Loss 2.0596 (2.4394)	Arch Beta Loss 137.7556 (140.8010)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.0%, 72.8%)	
11/22 03:25:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.2802 (2.2523)	Arch Loss 15.6953 (16.3596)	Arch Hard Loss 2.2165 (2.4312)	Arch Beta Loss 134.7886 (139.2842)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.8%, 72.7%)	
11/22 03:26:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.2537 (2.2443)	Arch Loss 15.6111 (16.2111)	Arch Hard Loss 2.3935 (2.4170)	Arch Beta Loss 132.1764 (137.9407)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.8%, 72.9%)	
11/22 03:26:41午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 39.8240%
11/22 03:26:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.3832	Prec@(1,5) (37.8%, 69.7%)
11/22 03:26:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.3474	Prec@(1,5) (38.1%, 70.7%)
11/22 03:27:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.3674	Prec@(1,5) (37.9%, 70.4%)
11/22 03:27:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.3611	Prec@(1,5) (38.0%, 70.5%)
11/22 03:27:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 37.9800%
11/22 03:27:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 03:27:13午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.9800%
11/22 03:28:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 1.8916 (2.1480)	Arch Loss 15.5537 (15.4401)	Arch Hard Loss 2.6218 (2.3689)	Arch Beta Loss 129.3199 (130.7121)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.2%, 74.8%)	
11/22 03:29:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.5109 (2.1309)	Arch Loss 15.2248 (15.2777)	Arch Hard Loss 2.5691 (2.3462)	Arch Beta Loss 126.5569 (129.3149)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.3%, 75.1%)	
11/22 03:29:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.2317 (2.1289)	Arch Loss 14.4842 (15.1334)	Arch Hard Loss 2.0984 (2.3395)	Arch Beta Loss 123.8584 (127.9395)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.3%, 75.2%)	
11/22 03:30:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 1.9562 (2.1383)	Arch Loss 14.2450 (15.0110)	Arch Hard Loss 2.0968 (2.3390)	Arch Beta Loss 121.4818 (126.7196)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.1%, 75.0%)	
11/22 03:30:46午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 42.1000%
11/22 03:30:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.3133	Prec@(1,5) (39.0%, 71.6%)
11/22 03:31:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.2865	Prec@(1,5) (39.7%, 72.2%)
11/22 03:31:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.2833	Prec@(1,5) (39.6%, 72.3%)
11/22 03:31:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.2841	Prec@(1,5) (39.7%, 72.3%)
11/22 03:31:17午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 39.6600%
11/22 03:31:17午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 03:31:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 39.6600%
11/22 03:32:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.0047 (2.0190)	Arch Loss 13.9231 (14.2962)	Arch Hard Loss 2.0348 (2.2811)	Arch Beta Loss 118.8837 (120.1512)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.7%, 77.8%)	
11/22 03:33:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.1900 (2.0427)	Arch Loss 13.7719 (14.1767)	Arch Hard Loss 2.1349 (2.2887)	Arch Beta Loss 116.3706 (118.8807)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.5%, 77.2%)	
11/22 03:34:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 1.9035 (2.0480)	Arch Loss 13.6937 (14.0374)	Arch Hard Loss 2.3032 (2.2746)	Arch Beta Loss 113.9048 (117.6277)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.4%, 76.9%)	
11/22 03:34:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.9318 (2.0495)	Arch Loss 13.4168 (13.9136)	Arch Hard Loss 2.2431 (2.2620)	Arch Beta Loss 111.7374 (116.5153)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.3%, 76.9%)	
11/22 03:34:50午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 44.2440%
11/22 03:34:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.2306	Prec@(1,5) (41.3%, 73.0%)
11/22 03:35:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.2307	Prec@(1,5) (41.2%, 73.0%)
11/22 03:35:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.2301	Prec@(1,5) (41.0%, 73.2%)
11/22 03:35:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.2377	Prec@(1,5) (41.0%, 73.1%)
11/22 03:35:21午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 40.9600%
11/22 03:35:21午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 2), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 03:35:21午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.9600%
11/22 03:36:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.0867 (1.9375)	Arch Loss 13.3076 (13.2655)	Arch Hard Loss 2.3714 (2.2134)	Arch Beta Loss 109.3617 (110.5208)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.1%, 79.0%)	
11/22 03:37:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.7062 (1.9825)	Arch Loss 12.9126 (13.1447)	Arch Hard Loss 2.2059 (2.2088)	Arch Beta Loss 107.0669 (109.3595)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.1%, 78.3%)	
11/22 03:38:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8025 (1.9801)	Arch Loss 12.6608 (13.0339)	Arch Hard Loss 2.1778 (2.2122)	Arch Beta Loss 104.8299 (108.2168)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.5%, 78.3%)	
11/22 03:38:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.7766 (1.9804)	Arch Loss 12.0875 (12.9279)	Arch Hard Loss 1.8015 (2.2074)	Arch Beta Loss 102.8601 (107.2049)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.3%, 78.3%)	
11/22 03:38:54午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 45.3560%
11/22 03:39:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.1865	Prec@(1,5) (41.7%, 74.4%)
11/22 03:39:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2057	Prec@(1,5) (41.3%, 73.8%)
11/22 03:39:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.1912	Prec@(1,5) (41.5%, 74.0%)
11/22 03:39:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.1921	Prec@(1,5) (41.6%, 74.0%)
11/22 03:39:25午前 searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 41.6040%
11/22 03:39:25午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('skip_connect', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 03:39:26午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 41.6040%
11/22 03:40:20午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 1.7897 (1.8640)	Arch Loss 12.2840 (12.3547)	Arch Hard Loss 2.2148 (2.1796)	Arch Beta Loss 100.6927 (101.7511)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (48.0%, 80.8%)	
11/22 03:41:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 1.9489 (1.8881)	Arch Loss 11.9046 (12.2420)	Arch Hard Loss 2.0435 (2.1726)	Arch Beta Loss 98.6115 (100.6939)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.9%, 79.9%)	
11/22 03:42:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 2.1162 (1.8936)	Arch Loss 11.9409 (12.1300)	Arch Hard Loss 2.2834 (2.1644)	Arch Beta Loss 96.5749 (99.6562)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.8%, 79.7%)	
11/22 03:42:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.8640 (1.8939)	Arch Loss 11.7082 (12.0254)	Arch Hard Loss 2.2289 (2.1518)	Arch Beta Loss 94.7933 (98.7361)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (47.9%, 79.7%)	
11/22 03:42:58午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 47.8600%
11/22 03:43:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.1271	Prec@(1,5) (43.4%, 75.7%)
11/22 03:43:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.1452	Prec@(1,5) (42.9%, 75.3%)
11/22 03:43:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.1534	Prec@(1,5) (42.7%, 75.2%)
11/22 03:43:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.1529	Prec@(1,5) (42.9%, 75.1%)
11/22 03:43:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 42.8960%
11/22 03:43:29午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('max_pool_3x3', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 7)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 03:43:29午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.8960%
11/22 03:44:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.1487 (1.8062)	Arch Loss 11.2536 (11.5071)	Arch Hard Loss 1.9707 (2.1284)	Arch Beta Loss 92.8296 (93.7868)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.3%, 81.4%)	
11/22 03:45:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 1.5987 (1.8080)	Arch Loss 11.1318 (11.4167)	Arch Hard Loss 2.0380 (2.1339)	Arch Beta Loss 90.9383 (92.8278)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.0%, 81.4%)	
11/22 03:46:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.7510 (1.8119)	Arch Loss 10.8965 (11.3208)	Arch Hard Loss 1.9875 (2.1323)	Arch Beta Loss 89.0900 (91.8850)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.9%, 81.4%)	
11/22 03:47:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.5272 (1.8229)	Arch Loss 10.5984 (11.2321)	Arch Hard Loss 1.8518 (2.1272)	Arch Beta Loss 87.4653 (91.0498)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (49.5%, 81.3%)	
11/22 03:47:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 49.4880%
11/22 03:47:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.1197	Prec@(1,5) (43.4%, 75.7%)
11/22 03:47:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.0936	Prec@(1,5) (44.0%, 76.1%)
11/22 03:47:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.1144	Prec@(1,5) (43.7%, 75.7%)
11/22 03:47:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.1035	Prec@(1,5) (43.9%, 75.9%)
11/22 03:47:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 43.8920%
11/22 03:47:31午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 7), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 03:47:32午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.8920%
11/22 03:48:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.4962 (1.7226)	Arch Loss 10.4023 (10.7397)	Arch Hard Loss 1.8339 (2.0843)	Arch Beta Loss 85.6835 (86.5539)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.3%, 82.6%)	
11/22 03:49:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.7306 (1.7505)	Arch Loss 10.2336 (10.6428)	Arch Hard Loss 1.8370 (2.0744)	Arch Beta Loss 83.9654 (85.6833)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.3%, 82.6%)	
11/22 03:50:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.8236 (1.7585)	Arch Loss 10.6226 (10.5676)	Arch Hard Loss 2.3934 (2.0849)	Arch Beta Loss 82.2914 (84.8270)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.1%, 82.5%)	
11/22 03:51:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.8002 (1.7676)	Arch Loss 10.2883 (10.4828)	Arch Hard Loss 2.2060 (2.0758)	Arch Beta Loss 80.8226 (84.0699)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (50.9%, 82.2%)	
11/22 03:51:04午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 50.9040%
11/22 03:51:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1592	Prec@(1,5) (43.4%, 74.6%)
11/22 03:51:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1312	Prec@(1,5) (43.6%, 75.2%)
11/22 03:51:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1296	Prec@(1,5) (43.6%, 75.5%)
11/22 03:51:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1143	Prec@(1,5) (43.7%, 75.8%)
11/22 03:51:35午前 searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 43.6760%
11/22 03:51:35午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 7)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 03:51:36午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.8920%
11/22 03:52:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.5789 (1.6485)	Arch Loss 9.8703 (10.0552)	Arch Hard Loss 1.9498 (2.0555)	Arch Beta Loss 79.2056 (79.9972)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.8%, 84.2%)	
11/22 03:53:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.6562 (1.6867)	Arch Loss 9.9399 (9.9777)	Arch Hard Loss 2.1749 (2.0570)	Arch Beta Loss 77.6495 (79.2072)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.4%, 83.5%)	
11/22 03:54:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.7316 (1.6988)	Arch Loss 9.2446 (9.9052)	Arch Hard Loss 1.6321 (2.0623)	Arch Beta Loss 76.1254 (78.4296)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (52.2%, 83.1%)	
11/22 03:55:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.3988 (1.7092)	Arch Loss 9.4211 (9.8297)	Arch Hard Loss 1.9430 (2.0557)	Arch Beta Loss 74.7808 (77.7401)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (51.9%, 83.1%)	
11/22 03:55:08午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 51.9000%
11/22 03:55:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.0008	Prec@(1,5) (46.0%, 77.7%)
11/22 03:55:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 1.9848	Prec@(1,5) (46.4%, 78.1%)
11/22 03:55:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 1.9926	Prec@(1,5) (46.4%, 78.0%)
11/22 03:55:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 1.9998	Prec@(1,5) (46.0%, 77.9%)
11/22 03:55:39午前 searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 45.9960%
11/22 03:55:39午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 03:55:39午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.9960%
11/22 03:56:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.5613 (1.5913)	Arch Loss 9.4998 (9.3878)	Arch Hard Loss 2.1686 (1.9849)	Arch Beta Loss 73.3120 (74.0298)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.6%, 85.2%)	
11/22 03:57:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.4789 (1.6358)	Arch Loss 9.1260 (9.3483)	Arch Hard Loss 1.9378 (2.0175)	Arch Beta Loss 71.8821 (73.3081)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.5%, 84.3%)	
11/22 03:58:22午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.6566 (1.6355)	Arch Loss 9.0234 (9.2769)	Arch Hard Loss 1.9746 (2.0172)	Arch Beta Loss 70.4885 (72.5973)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.6%, 84.3%)	
11/22 03:59:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.8099 (1.6467)	Arch Loss 8.2989 (9.2028)	Arch Hard Loss 1.3726 (2.0061)	Arch Beta Loss 69.2629 (71.9667)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (53.4%, 84.1%)	
11/22 03:59:11午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 53.3760%
11/22 03:59:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 1.9649	Prec@(1,5) (47.3%, 79.0%)
11/22 03:59:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 1.9688	Prec@(1,5) (47.3%, 78.6%)
11/22 03:59:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 1.9657	Prec@(1,5) (47.3%, 78.3%)
11/22 03:59:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 1.9688	Prec@(1,5) (47.2%, 78.5%)
11/22 03:59:42午前 searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 47.2200%
11/22 03:59:42午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 03:59:42午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2200%
11/22 04:00:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.2746 (1.5350)	Arch Loss 9.0117 (8.8628)	Arch Hard Loss 2.2205 (2.0057)	Arch Beta Loss 67.9123 (68.5712)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.4%, 86.2%)	
11/22 04:01:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.7986 (1.5686)	Arch Loss 8.9065 (8.8059)	Arch Hard Loss 2.2457 (2.0148)	Arch Beta Loss 66.6077 (67.9107)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.5%, 85.6%)	
11/22 04:02:26午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.1201 (1.5917)	Arch Loss 8.2947 (8.7142)	Arch Hard Loss 1.7620 (1.9882)	Arch Beta Loss 65.3268 (67.2598)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (55.0%, 85.1%)	
11/22 04:03:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.5936 (1.6076)	Arch Loss 8.2903 (8.6470)	Arch Hard Loss 1.8699 (1.9788)	Arch Beta Loss 64.2043 (66.6817)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (54.7%, 84.7%)	
11/22 04:03:14午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 54.7280%
11/22 04:03:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.0479	Prec@(1,5) (45.9%, 76.8%)
11/22 04:03:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.0562	Prec@(1,5) (45.9%, 76.5%)
11/22 04:03:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.0468	Prec@(1,5) (46.1%, 76.9%)
11/22 04:03:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.0418	Prec@(1,5) (46.2%, 76.9%)
11/22 04:03:46午前 searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 46.1760%
11/22 04:03:46午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 04:03:46午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.2200%
11/22 04:04:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.7080 (1.5002)	Arch Loss 8.1352 (8.3201)	Arch Hard Loss 1.8381 (1.9626)	Arch Beta Loss 62.9705 (63.5744)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.1%, 86.5%)	
11/22 04:05:35午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.5339 (1.5157)	Arch Loss 8.8339 (8.2610)	Arch Hard Loss 2.6558 (1.9638)	Arch Beta Loss 61.7819 (62.9719)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.6%, 86.3%)	
11/22 04:06:29午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.0741 (1.5268)	Arch Loss 7.6381 (8.1942)	Arch Hard Loss 1.5774 (1.9565)	Arch Beta Loss 60.6068 (62.3764)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.6%, 86.1%)	
11/22 04:07:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.6683 (1.5530)	Arch Loss 7.7056 (8.1391)	Arch Hard Loss 1.7484 (1.9544)	Arch Beta Loss 59.5717 (61.8471)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (56.0%, 85.6%)	
11/22 04:07:19午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 56.0360%
11/22 04:07:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 1.9425	Prec@(1,5) (48.4%, 79.4%)
11/22 04:07:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 1.9445	Prec@(1,5) (48.0%, 79.2%)
11/22 04:07:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 1.9439	Prec@(1,5) (48.0%, 79.0%)
11/22 04:07:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 1.9461	Prec@(1,5) (48.0%, 78.8%)
11/22 04:07:49午前 searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 48.0000%
11/22 04:07:49午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 04:07:50午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.0000%
11/22 04:08:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.3877 (1.4427)	Arch Loss 8.1037 (7.8496)	Arch Hard Loss 2.2601 (1.9507)	Arch Beta Loss 58.4361 (58.9891)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.4%, 87.9%)	
11/22 04:09:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.5905 (1.4699)	Arch Loss 7.9498 (7.7849)	Arch Hard Loss 2.2158 (1.9413)	Arch Beta Loss 57.3401 (58.4353)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.7%, 87.2%)	
11/22 04:10:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.4686 (1.4930)	Arch Loss 7.3313 (7.7219)	Arch Hard Loss 1.7039 (1.9329)	Arch Beta Loss 56.2743 (57.8900)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.4%, 86.7%)	
11/22 04:11:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.5350 (1.5011)	Arch Loss 7.2246 (7.6653)	Arch Hard Loss 1.6907 (1.9246)	Arch Beta Loss 55.3391 (57.4075)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (57.3%, 86.6%)	
11/22 04:11:22午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 57.2480%
11/22 04:11:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 1.8688	Prec@(1,5) (50.0%, 79.6%)
11/22 04:11:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 1.8690	Prec@(1,5) (49.7%, 80.1%)
11/22 04:11:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 1.8690	Prec@(1,5) (49.6%, 80.2%)
11/22 04:11:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 1.8648	Prec@(1,5) (49.7%, 80.3%)
11/22 04:11:53午前 searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 49.6800%
11/22 04:11:53午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('skip_connect', 9), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 04:11:53午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.6800%
11/22 04:12:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.4110 (1.3937)	Arch Loss 7.8133 (7.4158)	Arch Hard Loss 2.3825 (1.9348)	Arch Beta Loss 54.3080 (54.8103)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.6%)	
11/22 04:13:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.4378 (1.4288)	Arch Loss 7.1772 (7.3448)	Arch Hard Loss 1.8464 (1.9142)	Arch Beta Loss 53.3082 (54.3062)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.0%, 87.7%)	
11/22 04:14:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.2655 (1.4418)	Arch Loss 7.4767 (7.2918)	Arch Hard Loss 2.2435 (1.9110)	Arch Beta Loss 52.3321 (53.8080)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.9%, 87.5%)	
11/22 04:15:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.4558 (1.4550)	Arch Loss 7.0225 (7.2433)	Arch Hard Loss 1.8749 (1.9066)	Arch Beta Loss 51.4757 (53.3673)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (58.5%, 87.4%)	
11/22 04:15:25午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 58.4920%
11/22 04:15:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 1.9008	Prec@(1,5) (49.0%, 79.2%)
11/22 04:15:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 1.9054	Prec@(1,5) (48.4%, 79.3%)
11/22 04:15:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 1.8940	Prec@(1,5) (48.6%, 79.9%)
11/22 04:15:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 1.9085	Prec@(1,5) (48.5%, 79.7%)
11/22 04:15:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 48.5160%
11/22 04:15:56午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 04:15:56午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.6800%
11/22 04:16:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.5485 (1.3379)	Arch Loss 6.6854 (6.9668)	Arch Hard Loss 1.6316 (1.8672)	Arch Beta Loss 50.5379 (50.9961)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.3%, 89.1%)	
11/22 04:17:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.3079 (1.3784)	Arch Loss 6.9047 (6.9330)	Arch Hard Loss 1.9422 (1.8795)	Arch Beta Loss 49.6251 (50.5350)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.6%, 88.4%)	
11/22 04:18:39午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.5127 (1.3938)	Arch Loss 6.6162 (6.9028)	Arch Hard Loss 1.7430 (1.8947)	Arch Beta Loss 48.7314 (50.0811)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.2%)	
11/22 04:19:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.5617 (1.4110)	Arch Loss 6.8158 (6.8626)	Arch Hard Loss 2.0213 (1.8948)	Arch Beta Loss 47.9450 (49.6774)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (59.7%, 88.0%)	
11/22 04:19:28午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 59.6960%
11/22 04:19:36午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.8646	Prec@(1,5) (49.5%, 80.0%)
11/22 04:19:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.8771	Prec@(1,5) (49.3%, 80.4%)
11/22 04:19:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.8823	Prec@(1,5) (49.2%, 80.3%)
11/22 04:19:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.8897	Prec@(1,5) (48.9%, 80.1%)
11/22 04:19:59午前 searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 48.9440%
11/22 04:19:59午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 04:19:59午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.6800%
11/22 04:20:54午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.0110 (1.3154)	Arch Loss 6.7112 (6.6375)	Arch Hard Loss 2.0028 (1.8871)	Arch Beta Loss 47.0844 (47.5040)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.6%, 89.9%)	
11/22 04:21:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.3517 (1.3433)	Arch Loss 6.6697 (6.5881)	Arch Hard Loss 2.0442 (1.8797)	Arch Beta Loss 46.2546 (47.0838)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.0%, 89.3%)	
11/22 04:22:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.7126 (1.3558)	Arch Loss 6.5617 (6.5466)	Arch Hard Loss 2.0183 (1.8797)	Arch Beta Loss 45.4333 (46.6690)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.5%, 89.1%)	
11/22 04:23:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.2844 (1.3680)	Arch Loss 6.1791 (6.5126)	Arch Hard Loss 1.7073 (1.8826)	Arch Beta Loss 44.7171 (46.2998)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.9%)	
11/22 04:23:31午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 60.2520%
11/22 04:23:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 1.9043	Prec@(1,5) (49.5%, 80.0%)
11/22 04:23:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 1.8964	Prec@(1,5) (49.5%, 80.3%)
11/22 04:23:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 1.9086	Prec@(1,5) (49.4%, 80.0%)
11/22 04:24:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 1.8947	Prec@(1,5) (49.5%, 80.3%)
11/22 04:24:02午前 searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 49.4480%
11/22 04:24:02午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 5), ('skip_connect', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 04:24:02午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.6800%
11/22 04:24:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.4152 (1.2730)	Arch Loss 6.1938 (6.2783)	Arch Hard Loss 1.8008 (1.8471)	Arch Beta Loss 43.9297 (44.3117)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.7%, 90.0%)	
11/22 04:25:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.4748 (1.3032)	Arch Loss 6.5055 (6.2336)	Arch Hard Loss 2.1876 (1.8405)	Arch Beta Loss 43.1792 (43.9311)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.4%, 89.7%)	
11/22 04:26:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.1201 (1.3172)	Arch Loss 5.7140 (6.2074)	Arch Hard Loss 1.4706 (1.8519)	Arch Beta Loss 42.4347 (43.5551)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.0%, 89.5%)	
11/22 04:27:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.1907 (1.3283)	Arch Loss 5.8016 (6.1717)	Arch Hard Loss 1.6240 (1.8498)	Arch Beta Loss 41.7762 (43.2195)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (61.6%, 89.3%)	
11/22 04:27:35午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 61.6280%
11/22 04:27:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 1.8658	Prec@(1,5) (49.7%, 80.7%)
11/22 04:27:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 1.8547	Prec@(1,5) (50.1%, 80.6%)
11/22 04:27:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 1.8351	Prec@(1,5) (50.4%, 80.9%)
11/22 04:28:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 1.8302	Prec@(1,5) (50.7%, 81.1%)
11/22 04:28:06午前 searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 50.6720%
11/22 04:28:06午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 5)], [('skip_connect', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('max_pool_3x3', 7)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 04:28:06午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.6720%
11/22 04:29:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.6905 (1.2427)	Arch Loss 5.7094 (5.9259)	Arch Hard Loss 1.6040 (1.7852)	Arch Beta Loss 41.0540 (41.4067)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.2%, 90.5%)	
11/22 04:29:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.7982 (1.2656)	Arch Loss 5.9957 (5.9236)	Arch Hard Loss 1.9601 (1.8183)	Arch Beta Loss 40.3564 (41.0535)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.6%, 90.5%)	
11/22 04:30:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.2819 (1.2637)	Arch Loss 5.3913 (5.9011)	Arch Hard Loss 1.4246 (1.8307)	Arch Beta Loss 39.6669 (40.7040)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.6%, 90.4%)	
11/22 04:31:38午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.1879 (1.2851)	Arch Loss 5.9480 (5.8803)	Arch Hard Loss 2.0416 (1.8409)	Arch Beta Loss 39.0639 (40.3939)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (62.8%, 90.0%)	
11/22 04:31:38午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 62.8240%
11/22 04:31:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.8095	Prec@(1,5) (51.2%, 81.8%)
11/22 04:31:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.7966	Prec@(1,5) (51.6%, 81.9%)
11/22 04:32:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.8034	Prec@(1,5) (51.5%, 81.6%)
11/22 04:32:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.8056	Prec@(1,5) (51.3%, 81.6%)
11/22 04:32:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 51.3360%
11/22 04:32:09午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 7)], [('avg_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 04:32:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.3360%
11/22 04:33:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.9370 (1.1779)	Arch Loss 5.9509 (5.7024)	Arch Hard Loss 2.1106 (1.8300)	Arch Beta Loss 38.4024 (38.7244)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 91.4%)	
11/22 04:33:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.5807 (1.2110)	Arch Loss 5.3811 (5.6607)	Arch Hard Loss 1.6051 (1.8208)	Arch Beta Loss 37.7601 (38.3992)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.8%, 91.0%)	
11/22 04:34:53午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.9819 (1.2309)	Arch Loss 5.6872 (5.6324)	Arch Hard Loss 1.9743 (1.8245)	Arch Beta Loss 37.1292 (38.0797)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.1%, 90.6%)	
11/22 04:35:41午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.1578 (1.2466)	Arch Loss 5.1654 (5.6047)	Arch Hard Loss 1.5078 (1.8251)	Arch Beta Loss 36.5751 (37.7960)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (63.7%, 90.3%)	
11/22 04:35:42午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 63.7000%
11/22 04:35:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.8230	Prec@(1,5) (51.4%, 81.1%)
11/22 04:35:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.8156	Prec@(1,5) (51.5%, 81.2%)
11/22 04:36:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.8197	Prec@(1,5) (51.3%, 81.4%)
11/22 04:36:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.8179	Prec@(1,5) (51.3%, 81.3%)
11/22 04:36:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 51.2720%
11/22 04:36:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 04:36:13午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.3360%
11/22 04:37:08午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.2521 (1.1808)	Arch Loss 5.4084 (5.4028)	Arch Hard Loss 1.8124 (1.7768)	Arch Beta Loss 35.9602 (36.2600)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.2%, 91.7%)	
11/22 04:38:02午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.2318 (1.1933)	Arch Loss 5.0103 (5.3832)	Arch Hard Loss 1.4731 (1.7871)	Arch Beta Loss 35.3718 (35.9612)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.5%, 91.4%)	
11/22 04:38:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.4406 (1.2017)	Arch Loss 5.3035 (5.3711)	Arch Hard Loss 1.8236 (1.8041)	Arch Beta Loss 34.7991 (35.6693)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.1%, 91.2%)	
11/22 04:39:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.1612 (1.2207)	Arch Loss 4.9025 (5.3487)	Arch Hard Loss 1.4730 (1.8078)	Arch Beta Loss 34.2947 (35.4091)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (64.7%, 90.9%)	
11/22 04:39:45午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 64.6720%
11/22 04:39:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.7901	Prec@(1,5) (51.2%, 82.1%)
11/22 04:40:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 1.8107	Prec@(1,5) (50.7%, 81.7%)
11/22 04:40:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.7936	Prec@(1,5) (51.4%, 81.8%)
11/22 04:40:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.7860	Prec@(1,5) (51.6%, 81.9%)
11/22 04:40:16午前 searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 51.6280%
11/22 04:40:16午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 7)], [('skip_connect', 8), ('skip_connect', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 04:40:17午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 51.6280%
11/22 04:41:12午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.0877 (1.1355)	Arch Loss 5.0647 (5.2111)	Arch Hard Loss 1.6908 (1.8102)	Arch Beta Loss 33.7390 (34.0090)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.0%, 92.2%)	
11/22 04:42:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.0188 (1.1526)	Arch Loss 4.7743 (5.1877)	Arch Hard Loss 1.4547 (1.8140)	Arch Beta Loss 33.1959 (33.7365)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.5%, 91.8%)	
11/22 04:43:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.0149 (1.1700)	Arch Loss 5.2526 (5.1488)	Arch Hard Loss 1.9863 (1.8022)	Arch Beta Loss 32.6626 (33.4663)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.3%, 91.5%)	
11/22 04:43:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.1293 (1.1835)	Arch Loss 4.9885 (5.1223)	Arch Hard Loss 1.7688 (1.7997)	Arch Beta Loss 32.1964 (33.2262)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (65.8%, 91.4%)	
11/22 04:43:49午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 65.7600%
11/22 04:43:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 1.7967	Prec@(1,5) (53.3%, 81.3%)
11/22 04:44:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 1.7792	Prec@(1,5) (53.1%, 81.6%)
11/22 04:44:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 1.7956	Prec@(1,5) (52.8%, 81.4%)
11/22 04:44:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 1.7816	Prec@(1,5) (52.8%, 81.7%)
11/22 04:44:20午前 searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 52.8320%
11/22 04:44:20午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('max_pool_3x3', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 04:44:21午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.8320%
11/22 04:45:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.1031 (1.1037)	Arch Loss 4.7404 (4.9472)	Arch Hard Loss 1.5717 (1.7538)	Arch Beta Loss 31.6871 (31.9344)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.5%, 92.3%)	
11/22 04:46:09午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.3389 (1.1043)	Arch Loss 5.3239 (4.9523)	Arch Hard Loss 2.2041 (1.7837)	Arch Beta Loss 31.1975 (31.6856)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.3%, 92.6%)	
11/22 04:47:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.2238 (1.1252)	Arch Loss 4.7662 (4.9259)	Arch Hard Loss 1.6944 (1.7818)	Arch Beta Loss 30.7178 (31.4418)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.9%, 92.2%)	
11/22 04:47:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.2449 (1.1429)	Arch Loss 5.1857 (4.9099)	Arch Hard Loss 2.1565 (1.7874)	Arch Beta Loss 30.2924 (31.2246)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (66.5%, 91.9%)	
11/22 04:47:52午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 66.4400%
11/22 04:48:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 1.7886	Prec@(1,5) (52.4%, 81.5%)
11/22 04:48:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 1.7840	Prec@(1,5) (52.7%, 81.8%)
11/22 04:48:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 1.7913	Prec@(1,5) (52.3%, 81.7%)
11/22 04:48:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 1.7894	Prec@(1,5) (52.3%, 81.7%)
11/22 04:48:23午前 searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 52.3400%
11/22 04:48:23午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 04:48:23午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 52.8320%
11/22 04:49:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.9125 (1.0814)	Arch Loss 4.9155 (4.7786)	Arch Hard Loss 1.9330 (1.7735)	Arch Beta Loss 29.8245 (30.0516)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.6%, 92.6%)	
11/22 04:50:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.9265 (1.0916)	Arch Loss 4.6415 (4.7783)	Arch Hard Loss 1.7035 (1.7959)	Arch Beta Loss 29.3803 (29.8239)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.2%, 92.7%)	
11/22 04:51:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.2340 (1.1139)	Arch Loss 5.0389 (4.7592)	Arch Hard Loss 2.1444 (1.7989)	Arch Beta Loss 28.9450 (29.6026)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.4%, 92.4%)	
11/22 04:51:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.1844 (1.1256)	Arch Loss 4.8070 (4.7324)	Arch Hard Loss 1.9508 (1.7918)	Arch Beta Loss 28.5625 (29.4061)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.0%, 92.2%)	
11/22 04:51:57午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 66.9960%
11/22 04:52:05午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 1.7193	Prec@(1,5) (53.6%, 83.1%)
11/22 04:52:13午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 1.7331	Prec@(1,5) (53.5%, 82.9%)
11/22 04:52:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 1.7419	Prec@(1,5) (53.2%, 82.7%)
11/22 04:52:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 1.7495	Prec@(1,5) (53.1%, 82.5%)
11/22 04:52:28午前 searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 53.0480%
11/22 04:52:28午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('skip_connect', 8), ('max_pool_3x3', 9)], [('skip_connect', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 04:52:28午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.0480%
11/22 04:53:23午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.3843 (1.0729)	Arch Loss 4.4196 (4.5837)	Arch Hard Loss 1.6060 (1.7493)	Arch Beta Loss 28.1366 (28.3439)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.9%, 92.9%)	
11/22 04:54:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.9649 (1.0748)	Arch Loss 4.6173 (4.5895)	Arch Hard Loss 1.8451 (1.7760)	Arch Beta Loss 27.7213 (28.1352)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.4%, 92.9%)	
11/22 04:55:11午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.1067 (1.0769)	Arch Loss 4.3951 (4.5694)	Arch Hard Loss 1.6623 (1.7763)	Arch Beta Loss 27.3271 (27.9307)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.3%, 92.9%)	
11/22 04:56:00午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.0691 (1.0865)	Arch Loss 4.3864 (4.5431)	Arch Hard Loss 1.6893 (1.7681)	Arch Beta Loss 26.9710 (27.7496)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.0%, 92.7%)	
11/22 04:56:01午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 68.0120%
11/22 04:56:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 1.7536	Prec@(1,5) (53.3%, 83.0%)
11/22 04:56:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 1.7323	Prec@(1,5) (53.6%, 82.9%)
11/22 04:56:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 1.7294	Prec@(1,5) (53.4%, 82.8%)
11/22 04:56:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 1.7290	Prec@(1,5) (53.5%, 82.8%)
11/22 04:56:31午前 searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 53.5520%
11/22 04:56:31午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 04:56:32午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.5520%
11/22 04:57:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.0516 (0.9948)	Arch Loss 4.1341 (4.4172)	Arch Hard Loss 1.4760 (1.7400)	Arch Beta Loss 26.5808 (26.7722)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.6%, 93.9%)	
11/22 04:58:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.9087 (1.0257)	Arch Loss 4.1774 (4.3946)	Arch Hard Loss 1.5562 (1.7362)	Arch Beta Loss 26.2117 (26.5835)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.5%)	
11/22 04:59:15午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.2055 (1.0434)	Arch Loss 4.7995 (4.4007)	Arch Hard Loss 2.2152 (1.7610)	Arch Beta Loss 25.8422 (26.3970)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.3%, 93.2%)	
11/22 05:00:03午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.2347 (1.0556)	Arch Loss 4.1144 (4.3843)	Arch Hard Loss 1.5620 (1.7612)	Arch Beta Loss 25.5245 (26.2315)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.0%, 93.1%)	
11/22 05:00:04午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 68.9640%
11/22 05:00:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 1.7374	Prec@(1,5) (53.5%, 82.6%)
11/22 05:00:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 1.7225	Prec@(1,5) (53.7%, 82.7%)
11/22 05:00:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 1.7289	Prec@(1,5) (53.3%, 82.9%)
11/22 05:00:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 1.7301	Prec@(1,5) (53.4%, 82.8%)
11/22 05:00:35午前 searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 53.4240%
11/22 05:00:35午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 05:00:35午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.5520%
11/22 05:01:30午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.7603 (0.9867)	Arch Loss 4.3327 (4.2511)	Arch Hard Loss 1.8168 (1.7173)	Arch Beta Loss 25.1594 (25.3376)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.2%, 94.0%)	
11/22 05:02:24午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.7691 (1.0145)	Arch Loss 4.4884 (4.2586)	Arch Hard Loss 2.0070 (1.7425)	Arch Beta Loss 24.8138 (25.1607)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.4%, 93.5%)	
11/22 05:03:18午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.1009 (1.0176)	Arch Loss 4.5215 (4.2509)	Arch Hard Loss 2.0730 (1.7521)	Arch Beta Loss 24.4850 (24.9887)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.2%, 93.5%)	
11/22 05:04:06午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.0689 (1.0318)	Arch Loss 4.1963 (4.2377)	Arch Hard Loss 1.7775 (1.7539)	Arch Beta Loss 24.1872 (24.8378)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.7%, 93.3%)	
11/22 05:04:07午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 69.6800%
11/22 05:04:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 1.7595	Prec@(1,5) (54.0%, 82.7%)
11/22 05:04:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 1.7584	Prec@(1,5) (53.6%, 82.4%)
11/22 05:04:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 1.7492	Prec@(1,5) (53.8%, 82.6%)
11/22 05:04:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 1.7464	Prec@(1,5) (53.6%, 82.6%)
11/22 05:04:38午前 searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 53.5920%
11/22 05:04:38午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 9)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 05:04:38午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 53.5920%
11/22 05:05:33午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 1.1625 (0.9411)	Arch Loss 3.9691 (4.1671)	Arch Hard Loss 1.5830 (1.7649)	Arch Beta Loss 23.8609 (24.0221)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.3%)	
11/22 05:06:27午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.0883 (0.9807)	Arch Loss 3.7122 (4.1555)	Arch Hard Loss 1.3576 (1.7694)	Arch Beta Loss 23.5467 (23.8619)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.5%, 93.8%)	
11/22 05:07:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.1257 (0.9885)	Arch Loss 4.2820 (4.1139)	Arch Hard Loss 1.9568 (1.7431)	Arch Beta Loss 23.2514 (23.7074)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.0%, 93.8%)	
11/22 05:08:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.1958 (0.9986)	Arch Loss 3.8311 (4.1049)	Arch Hard Loss 1.5334 (1.7478)	Arch Beta Loss 22.9767 (23.5704)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.6%, 93.6%)	
11/22 05:08:10午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 70.5520%
11/22 05:08:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 1.6973	Prec@(1,5) (54.6%, 82.8%)
11/22 05:08:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 1.7081	Prec@(1,5) (54.5%, 82.9%)
11/22 05:08:34午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 1.7084	Prec@(1,5) (54.4%, 83.0%)
11/22 05:08:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 1.7139	Prec@(1,5) (54.2%, 83.0%)
11/22 05:08:41午前 searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 54.1680%
11/22 05:08:41午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 7)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 9), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 05:08:42午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.1680%
11/22 05:09:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.7692 (0.9298)	Arch Loss 4.5559 (4.0793)	Arch Hard Loss 2.2887 (1.7971)	Arch Beta Loss 22.6723 (22.8219)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.5%)	
11/22 05:10:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.8043 (0.9549)	Arch Loss 3.7582 (4.0217)	Arch Hard Loss 1.5195 (1.7543)	Arch Beta Loss 22.3867 (22.6739)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.7%, 94.2%)	
11/22 05:11:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 1.0137 (0.9670)	Arch Loss 4.1178 (4.0073)	Arch Hard Loss 1.9068 (1.7541)	Arch Beta Loss 22.1094 (22.5316)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.6%, 94.1%)	
11/22 05:12:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.9123 (0.9743)	Arch Loss 3.7614 (3.9896)	Arch Hard Loss 1.5756 (1.7491)	Arch Beta Loss 21.8585 (22.4047)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.4%, 94.0%)	
11/22 05:12:14午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 71.3680%
11/22 05:12:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 1.7089	Prec@(1,5) (54.2%, 83.7%)
11/22 05:12:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 1.7033	Prec@(1,5) (54.8%, 83.4%)
11/22 05:12:38午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 1.7193	Prec@(1,5) (54.5%, 83.2%)
11/22 05:12:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 1.7259	Prec@(1,5) (54.3%, 83.1%)
11/22 05:12:45午前 searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 54.2640%
11/22 05:12:45午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('skip_connect', 5), ('skip_connect', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 10), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('max_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 05:12:45午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.2640%
11/22 05:13:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.9211 (0.9030)	Arch Loss 3.9834 (3.9145)	Arch Hard Loss 1.8252 (1.7425)	Arch Beta Loss 21.5824 (21.7197)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.4%, 95.0%)	
11/22 05:14:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9376 (0.9120)	Arch Loss 4.1438 (3.8873)	Arch Hard Loss 2.0131 (1.7291)	Arch Beta Loss 21.3068 (21.5820)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.2%, 95.0%)	
11/22 05:15:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.8354 (0.9297)	Arch Loss 3.6617 (3.8787)	Arch Hard Loss 1.5573 (1.7341)	Arch Beta Loss 21.0443 (21.4461)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.6%)	
11/22 05:16:17午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.7482 (0.9393)	Arch Loss 3.6313 (3.8675)	Arch Hard Loss 1.5503 (1.7349)	Arch Beta Loss 20.8098 (21.3258)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.4%, 94.4%)	
11/22 05:16:17午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 72.4000%
11/22 05:16:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 1.7054	Prec@(1,5) (54.2%, 83.5%)
11/22 05:16:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 1.7094	Prec@(1,5) (54.4%, 83.3%)
11/22 05:16:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 1.6922	Prec@(1,5) (54.7%, 83.4%)
11/22 05:16:48午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 1.7034	Prec@(1,5) (54.5%, 83.3%)
11/22 05:16:48午前 searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 54.5280%
11/22 05:16:48午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 05:16:49午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.5280%
11/22 05:17:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.6465 (0.8920)	Arch Loss 3.5564 (3.8114)	Arch Hard Loss 1.4999 (1.7433)	Arch Beta Loss 20.5648 (20.6816)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.8%, 94.7%)	
11/22 05:18:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.7912 (0.9066)	Arch Loss 3.7291 (3.7688)	Arch Hard Loss 1.6974 (1.7127)	Arch Beta Loss 20.3168 (20.5611)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.6%, 94.6%)	
11/22 05:19:32午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.8638 (0.9138)	Arch Loss 3.8399 (3.7552)	Arch Hard Loss 1.8330 (1.7116)	Arch Beta Loss 20.0692 (20.4361)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.3%, 94.6%)	
11/22 05:20:21午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.8846 (0.9186)	Arch Loss 3.6836 (3.7591)	Arch Hard Loss 1.6978 (1.7265)	Arch Beta Loss 19.8573 (20.3261)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.0%, 94.6%)	
11/22 05:20:21午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 73.0400%
11/22 05:20:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 1.7263	Prec@(1,5) (55.0%, 82.5%)
11/22 05:20:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 1.6992	Prec@(1,5) (54.9%, 83.1%)
11/22 05:20:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 1.6990	Prec@(1,5) (54.8%, 83.3%)
11/22 05:20:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 1.6986	Prec@(1,5) (54.6%, 83.4%)
11/22 05:20:52午前 searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 54.5760%
11/22 05:20:52午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)], [('max_pool_3x3', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 7), ('skip_connect', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 05:20:53午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.5760%
11/22 05:21:48午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.8434 (0.8539)	Arch Loss 3.5537 (3.7190)	Arch Hard Loss 1.5907 (1.7447)	Arch Beta Loss 19.6293 (19.7428)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.5%)	
11/22 05:22:42午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.7624 (0.8686)	Arch Loss 3.6025 (3.6923)	Arch Hard Loss 1.6615 (1.7295)	Arch Beta Loss 19.4092 (19.6286)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.6%, 95.3%)	
11/22 05:23:36午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.8220 (0.8900)	Arch Loss 3.6727 (3.6893)	Arch Hard Loss 1.7546 (1.7376)	Arch Beta Loss 19.1815 (19.5171)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.0%, 95.0%)	
11/22 05:24:25午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.9723 (0.8975)	Arch Loss 3.7403 (3.6796)	Arch Hard Loss 1.8405 (1.7378)	Arch Beta Loss 18.9979 (19.4177)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.8%, 94.9%)	
11/22 05:24:25午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 73.7520%
11/22 05:24:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 1.7031	Prec@(1,5) (54.4%, 83.9%)
11/22 05:24:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 1.6960	Prec@(1,5) (54.9%, 83.6%)
11/22 05:24:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 1.6898	Prec@(1,5) (55.1%, 83.7%)
11/22 05:24:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 1.6964	Prec@(1,5) (54.9%, 83.5%)
11/22 05:24:56午前 searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 54.9480%
11/22 05:24:56午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 05:24:57午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 54.9480%
11/22 05:25:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.9031 (0.8576)	Arch Loss 3.8567 (3.6620)	Arch Hard Loss 1.9780 (1.7731)	Arch Beta Loss 18.7873 (18.8898)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.5%)	
11/22 05:26:46午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.8076 (0.8562)	Arch Loss 3.6545 (3.6288)	Arch Hard Loss 1.7959 (1.7499)	Arch Beta Loss 18.5865 (18.7888)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.5%)	
11/22 05:27:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.7065 (0.8780)	Arch Loss 3.4713 (3.6160)	Arch Hard Loss 1.6321 (1.7471)	Arch Beta Loss 18.3920 (18.6889)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.3%, 95.1%)	
11/22 05:28:28午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 1.2439 (0.8802)	Arch Loss 3.2465 (3.5949)	Arch Hard Loss 1.4249 (1.7349)	Arch Beta Loss 18.2161 (18.6000)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.2%, 95.1%)	
11/22 05:28:29午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 74.2240%
11/22 05:28:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 1.6672	Prec@(1,5) (55.3%, 84.5%)
11/22 05:28:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 1.6801	Prec@(1,5) (55.3%, 83.9%)
11/22 05:28:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 1.6798	Prec@(1,5) (55.3%, 83.8%)
11/22 05:29:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 1.6857	Prec@(1,5) (55.3%, 83.7%)
11/22 05:29:00午前 searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 55.3560%
11/22 05:29:00午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 05:29:00午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.3560%
11/22 05:29:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.7393 (0.8230)	Arch Loss 3.5574 (3.5394)	Arch Hard Loss 1.7550 (1.7280)	Arch Beta Loss 18.0236 (18.1141)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.9%, 96.1%)	
11/22 05:30:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.7658 (0.8393)	Arch Loss 3.4079 (3.5309)	Arch Hard Loss 1.6243 (1.7288)	Arch Beta Loss 17.8353 (18.0210)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.4%, 95.7%)	
11/22 05:31:43午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.7218 (0.8449)	Arch Loss 3.3661 (3.5253)	Arch Hard Loss 1.6003 (1.7325)	Arch Beta Loss 17.6580 (17.9280)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.4%, 95.6%)	
11/22 05:32:31午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.9754 (0.8574)	Arch Loss 3.3763 (3.5165)	Arch Hard Loss 1.6257 (1.7317)	Arch Beta Loss 17.5061 (17.8477)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.0%, 95.5%)	
11/22 05:32:32午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 75.0240%
11/22 05:32:40午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 1.7101	Prec@(1,5) (55.0%, 83.2%)
11/22 05:32:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 1.6966	Prec@(1,5) (55.0%, 83.6%)
11/22 05:32:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 1.6928	Prec@(1,5) (55.2%, 83.5%)
11/22 05:33:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 1.6936	Prec@(1,5) (55.2%, 83.5%)
11/22 05:33:02午前 searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 55.2040%
11/22 05:33:02午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('skip_connect', 7), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 9), ('skip_connect', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 5)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 10), ('skip_connect', 8)]], DAG3_concat=[2, 3])
11/22 05:33:03午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.3560%
11/22 05:33:57午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.7468 (0.8131)	Arch Loss 3.3542 (3.4721)	Arch Hard Loss 1.6203 (1.7299)	Arch Beta Loss 17.3398 (17.4215)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.9%, 96.0%)	
11/22 05:34:51午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.8863 (0.8211)	Arch Loss 3.3527 (3.4385)	Arch Hard Loss 1.6347 (1.7045)	Arch Beta Loss 17.1799 (17.3398)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.9%, 95.7%)	
11/22 05:35:45午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.8268 (0.8331)	Arch Loss 3.4398 (3.4371)	Arch Hard Loss 1.7379 (1.7112)	Arch Beta Loss 17.0183 (17.2586)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.6%, 95.7%)	
11/22 05:36:34午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.7569 (0.8385)	Arch Loss 3.5027 (3.4315)	Arch Hard Loss 1.8152 (1.7129)	Arch Beta Loss 16.8750 (17.1864)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.5%, 95.6%)	
11/22 05:36:34午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 75.5280%
11/22 05:36:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 1.6265	Prec@(1,5) (55.4%, 84.9%)
11/22 05:36:50午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 1.6697	Prec@(1,5) (54.9%, 84.2%)
11/22 05:36:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 1.6713	Prec@(1,5) (55.2%, 84.0%)
11/22 05:37:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 1.6741	Prec@(1,5) (55.3%, 84.0%)
11/22 05:37:06午前 searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 55.3440%
11/22 05:37:06午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 8), ('skip_connect', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('max_pool_3x3', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('skip_connect', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 05:37:06午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.3560%
11/22 05:38:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.9561 (0.8068)	Arch Loss 3.4283 (3.4006)	Arch Hard Loss 1.7556 (1.7208)	Arch Beta Loss 16.7278 (16.7980)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.2%, 96.3%)	
11/22 05:38:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.7741 (0.8275)	Arch Loss 3.2168 (3.3913)	Arch Hard Loss 1.5592 (1.7189)	Arch Beta Loss 16.5765 (16.7242)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.3%, 95.9%)	
11/22 05:39:49午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.8739 (0.8265)	Arch Loss 3.5837 (3.3800)	Arch Hard Loss 1.9396 (1.7149)	Arch Beta Loss 16.4411 (16.6515)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 95.8%)	
11/22 05:40:37午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.7396 (0.8278)	Arch Loss 3.0897 (3.3802)	Arch Hard Loss 1.4592 (1.7215)	Arch Beta Loss 16.3048 (16.5870)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 95.8%)	
11/22 05:40:38午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 76.1800%
11/22 05:40:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 1.6458	Prec@(1,5) (56.3%, 84.2%)
11/22 05:40:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 1.6445	Prec@(1,5) (56.1%, 84.1%)
11/22 05:41:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 1.6443	Prec@(1,5) (55.8%, 84.3%)
11/22 05:41:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 1.6594	Prec@(1,5) (55.7%, 84.0%)
11/22 05:41:09午前 searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 55.7200%
11/22 05:41:09午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('skip_connect', 8), ('avg_pool_3x3', 6)], [('skip_connect', 8), ('skip_connect', 9)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG3_concat=[2, 3])
11/22 05:41:09午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.7200%
11/22 05:42:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.7843 (0.8111)	Arch Loss 3.4849 (3.3487)	Arch Hard Loss 1.8677 (1.7247)	Arch Beta Loss 16.1719 (16.2397)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 95.9%)	
11/22 05:42:58午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 1.1664 (0.8080)	Arch Loss 3.6149 (3.3412)	Arch Hard Loss 2.0116 (1.7243)	Arch Beta Loss 16.0330 (16.1691)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 95.9%)	
11/22 05:43:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.7472 (0.8208)	Arch Loss 3.3125 (3.3358)	Arch Hard Loss 1.7219 (1.7257)	Arch Beta Loss 15.9058 (16.1017)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 95.8%)	
11/22 05:44:40午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.5996 (0.8211)	Arch Loss 3.3800 (3.3273)	Arch Hard Loss 1.8012 (1.7230)	Arch Beta Loss 15.7881 (16.0427)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.6%, 95.8%)	
11/22 05:44:41午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 76.5600%
11/22 05:44:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 1.6147	Prec@(1,5) (56.5%, 84.4%)
11/22 05:44:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 1.6473	Prec@(1,5) (55.9%, 84.2%)
11/22 05:45:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 1.6560	Prec@(1,5) (56.0%, 84.1%)
11/22 05:45:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 1.6641	Prec@(1,5) (55.7%, 84.0%)
11/22 05:45:12午前 searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 55.7040%
11/22 05:45:12午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('avg_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('max_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 05:45:12午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.7200%
11/22 05:46:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.6721 (0.8045)	Arch Loss 2.8101 (3.2697)	Arch Hard Loss 1.2435 (1.6971)	Arch Beta Loss 15.6657 (15.7255)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.4%, 96.1%)	
11/22 05:47:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 1.0517 (0.8034)	Arch Loss 3.5895 (3.2754)	Arch Hard Loss 2.0352 (1.7086)	Arch Beta Loss 15.5433 (15.6673)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.1%)	
11/22 05:47:55午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 1.0335 (0.8116)	Arch Loss 3.5484 (3.2625)	Arch Hard Loss 2.0057 (1.7019)	Arch Beta Loss 15.4273 (15.6062)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 95.9%)	
11/22 05:48:44午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.7259 (0.8122)	Arch Loss 3.3477 (3.2607)	Arch Hard Loss 1.8152 (1.7054)	Arch Beta Loss 15.3241 (15.5526)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 95.9%)	
11/22 05:48:44午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 76.8320%
11/22 05:48:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 1.6148	Prec@(1,5) (56.6%, 84.6%)
11/22 05:49:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 1.6354	Prec@(1,5) (56.0%, 84.5%)
11/22 05:49:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 1.6571	Prec@(1,5) (55.6%, 84.3%)
11/22 05:49:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 1.6557	Prec@(1,5) (55.6%, 84.2%)
11/22 05:49:15午前 searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 55.6240%
11/22 05:49:15午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('skip_connect', 7)], [('max_pool_3x3', 7), ('skip_connect', 9)], [('skip_connect', 8), ('avg_pool_3x3', 9)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('skip_connect', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('max_pool_3x3', 8)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 8), ('skip_connect', 9)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 9)], [('avg_pool_3x3', 8), ('max_pool_3x3', 10)]], DAG3_concat=[2, 3])
11/22 05:49:15午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.7200%
11/22 05:50:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 1.0116 (0.7798)	Arch Loss 3.3378 (3.2358)	Arch Hard Loss 1.8166 (1.7090)	Arch Beta Loss 15.2116 (15.2675)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.1%, 96.3%)	
11/22 05:51:05午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.5940 (0.7953)	Arch Loss 3.2280 (3.2256)	Arch Hard Loss 1.7175 (1.7044)	Arch Beta Loss 15.1046 (15.2118)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.5%, 96.1%)	
11/22 05:51:59午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.6594 (0.7958)	Arch Loss 3.1592 (3.2209)	Arch Hard Loss 1.6604 (1.7053)	Arch Beta Loss 14.9882 (15.1563)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.5%, 96.2%)	
11/22 05:52:47午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.6993 (0.8045)	Arch Loss 3.3983 (3.2168)	Arch Hard Loss 1.9092 (1.7062)	Arch Beta Loss 14.8909 (15.1061)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.2%, 96.1%)	
11/22 05:52:48午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 77.2520%
11/22 05:52:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 1.6875	Prec@(1,5) (55.7%, 83.5%)
11/22 05:53:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 1.6652	Prec@(1,5) (55.8%, 83.7%)
11/22 05:53:11午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 1.6652	Prec@(1,5) (55.7%, 83.9%)
11/22 05:53:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 1.6595	Prec@(1,5) (55.8%, 84.1%)
11/22 05:53:18午前 searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 55.7640%
11/22 05:53:18午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 6), ('skip_connect', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 7)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/22 05:53:19午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.7640%
11/22 05:54:14午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.8152 (0.8015)	Arch Loss 2.7117 (3.1839)	Arch Hard Loss 1.2331 (1.7001)	Arch Beta Loss 14.7858 (14.8380)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.2%)	
11/22 05:55:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.6430 (0.8006)	Arch Loss 2.9098 (3.2017)	Arch Hard Loss 1.4420 (1.7233)	Arch Beta Loss 14.6778 (14.7845)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.9%, 96.2%)	
11/22 05:56:01午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.7431 (0.8057)	Arch Loss 2.9629 (3.1857)	Arch Hard Loss 1.5054 (1.7127)	Arch Beta Loss 14.5748 (14.7305)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.7%, 96.1%)	
11/22 05:56:50午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.9982 (0.8092)	Arch Loss 3.1884 (3.1797)	Arch Hard Loss 1.7394 (1.7112)	Arch Beta Loss 14.4896 (14.6845)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.7%, 96.0%)	
11/22 05:56:50午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 76.7360%
11/22 05:56:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 1.6637	Prec@(1,5) (55.0%, 84.3%)
11/22 05:57:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 1.6545	Prec@(1,5) (55.6%, 84.4%)
11/22 05:57:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 1.6645	Prec@(1,5) (55.5%, 84.2%)
11/22 05:57:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 1.6688	Prec@(1,5) (55.5%, 84.1%)
11/22 05:57:21午前 searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 55.4480%
11/22 05:57:21午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('skip_connect', 4)], [('max_pool_3x3', 7), ('skip_connect', 5)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('skip_connect', 9), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 8), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('skip_connect', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('avg_pool_3x3', 6)], [('skip_connect', 9), ('skip_connect', 8)], [('max_pool_3x3', 9), ('skip_connect', 10)]], DAG3_concat=[2, 3])
11/22 05:57:21午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.7640%
11/22 05:58:16午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.7540 (0.7892)	Arch Loss 3.1736 (3.1307)	Arch Hard Loss 1.7346 (1.6871)	Arch Beta Loss 14.3899 (14.4362)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.4%, 96.5%)	
11/22 05:59:10午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.8249 (0.7905)	Arch Loss 3.5743 (3.1450)	Arch Hard Loss 2.1450 (1.7062)	Arch Beta Loss 14.2928 (14.3889)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.5%, 96.4%)	
11/22 06:00:04午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.9490 (0.8035)	Arch Loss 3.0677 (3.1451)	Arch Hard Loss 1.6473 (1.7110)	Arch Beta Loss 14.2044 (14.3412)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.3%, 96.0%)	
11/22 06:00:52午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.8137 (0.8076)	Arch Loss 3.0711 (3.1444)	Arch Hard Loss 1.6589 (1.7144)	Arch Beta Loss 14.1225 (14.2998)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.9%, 96.0%)	
11/22 06:00:53午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 76.8960%
11/22 06:01:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 1.6758	Prec@(1,5) (56.0%, 84.0%)
11/22 06:01:09午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 1.6614	Prec@(1,5) (56.0%, 84.1%)
11/22 06:01:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 1.6493	Prec@(1,5) (56.0%, 84.3%)
11/22 06:01:24午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 1.6560	Prec@(1,5) (55.9%, 84.1%)
11/22 06:01:24午前 searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 55.8760%
11/22 06:01:24午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=[2, 3])
11/22 06:01:24午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.8760%
11/22 06:02:19午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.7201 (0.8028)	Arch Loss 3.0323 (3.1105)	Arch Hard Loss 1.6295 (1.7032)	Arch Beta Loss 14.0280 (14.0727)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.5%, 96.4%)	
11/22 06:03:13午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.6472 (0.7958)	Arch Loss 3.3940 (3.1081)	Arch Hard Loss 2.0008 (1.7056)	Arch Beta Loss 13.9327 (14.0251)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.7%, 96.3%)	
11/22 06:04:07午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.6830 (0.8003)	Arch Loss 3.3236 (3.1103)	Arch Hard Loss 1.9398 (1.7124)	Arch Beta Loss 13.8381 (13.9786)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.1%, 96.2%)	
11/22 06:04:56午前 searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.8913 (0.8080)	Arch Loss 2.7041 (3.1081)	Arch Hard Loss 1.3275 (1.7143)	Arch Beta Loss 13.7660 (13.9381)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.9%, 96.1%)	
11/22 06:04:56午前 searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 76.8880%
11/22 06:05:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 1.6364	Prec@(1,5) (56.3%, 84.5%)
11/22 06:05:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 1.6620	Prec@(1,5) (55.5%, 84.2%)
11/22 06:05:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 1.6745	Prec@(1,5) (55.5%, 83.8%)
11/22 06:05:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 1.6648	Prec@(1,5) (55.6%, 83.9%)
11/22 06:05:27午前 searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 55.6160%
11/22 06:05:27午前 trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('skip_connect', 8), ('skip_connect', 10)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('max_pool_3x3', 6), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 7)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[2, 3])
11/22 06:05:27午前 trainer_runner.py:105 [INFO] Until now, best Prec@1 = 55.8760%
11/22 06:05:28午前 trainer_runner.py:110 [INFO] Final best Prec@1 = 55.8760%
11/22 06:05:28午前 trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 4)], [('max_pool_3x3', 3), ('max_pool_3x3', 5)], [('skip_connect', 4), ('max_pool_3x3', 6)], [('max_pool_3x3', 6), ('avg_pool_3x3', 7)], [('max_pool_3x3', 7), ('avg_pool_3x3', 6)], [('max_pool_3x3', 7), ('max_pool_3x3', 9)], [('max_pool_3x3', 8), ('avg_pool_3x3', 10)]], DAG1_concat=[2, 3], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 4), ('skip_connect', 5)], [('avg_pool_3x3', 5), ('skip_connect', 6)], [('skip_connect', 8), ('skip_connect', 6)], [('max_pool_3x3', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 8)]], DAG2_concat=[2, 3], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 7)], [('max_pool_3x3', 8), ('max_pool_3x3', 6)], [('max_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 8)]], DAG3_concat=[2, 3])
