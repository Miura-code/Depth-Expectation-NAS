08/23 11:51:34PM parser.py:28 [INFO] 
08/23 11:51:34PM parser.py:29 [INFO] Parameters:
08/23 11:51:34PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar10/noSWDL/s4-BaselineBestCell/DAG
08/23 11:51:34PM parser.py:31 [INFO] T=10.0
08/23 11:51:34PM parser.py:31 [INFO] ADVANCED=False
08/23 11:51:34PM parser.py:31 [INFO] ALPHA_LR=0.0003
08/23 11:51:34PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
08/23 11:51:34PM parser.py:31 [INFO] BATCH_SIZE=64
08/23 11:51:34PM parser.py:31 [INFO] CASCADE=False
08/23 11:51:34PM parser.py:31 [INFO] CHECKPOINT_RESET=False
08/23 11:51:34PM parser.py:31 [INFO] CUTOUT_LENGTH=0
08/23 11:51:34PM parser.py:31 [INFO] DATA_PATH=../data/
08/23 11:51:34PM parser.py:31 [INFO] DATASET=cifar10
08/23 11:51:34PM parser.py:31 [INFO] DEPTH_COEF=0.0
08/23 11:51:34PM parser.py:31 [INFO] DESCRIPTION=search_architecture_on_noKD_without_depth_loss_with_large_slidewindoe
08/23 11:51:34PM parser.py:31 [INFO] EPOCHS=50
08/23 11:51:34PM parser.py:31 [INFO] EXP_NAME=s4-BaselineBestCell
08/23 11:51:34PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
08/23 11:51:34PM parser.py:31 [INFO] GPUS=[0]
08/23 11:51:34PM parser.py:31 [INFO] INIT_CHANNELS=16
08/23 11:51:34PM parser.py:31 [INFO] L=0.5
08/23 11:51:34PM parser.py:31 [INFO] LAYERS=20
08/23 11:51:34PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
08/23 11:51:34PM parser.py:31 [INFO] NAME=noSWDL
08/23 11:51:34PM parser.py:31 [INFO] NONKD=True
08/23 11:51:34PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar10/noSWDL/s4-BaselineBestCell
08/23 11:51:34PM parser.py:31 [INFO] PCDARTS=False
08/23 11:51:34PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar10/noSWDL/s4-BaselineBestCell/plots
08/23 11:51:34PM parser.py:31 [INFO] PRINT_FREQ=50
08/23 11:51:34PM parser.py:31 [INFO] RESUME_PATH=None
08/23 11:51:34PM parser.py:31 [INFO] SAVE=s4-BaselineBestCell
08/23 11:51:34PM parser.py:31 [INFO] SEED=4
08/23 11:51:34PM parser.py:31 [INFO] SHARE_STAGE=False
08/23 11:51:34PM parser.py:31 [INFO] SLIDE_WINDOW=8
08/23 11:51:34PM parser.py:31 [INFO] SPEC_CELL=True
08/23 11:51:34PM parser.py:31 [INFO] TEACHER_NAME=non
08/23 11:51:34PM parser.py:31 [INFO] TEACHER_PATH=non
08/23 11:51:34PM parser.py:31 [INFO] TRAIN_PORTION=0.5
08/23 11:51:34PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
08/23 11:51:34PM parser.py:31 [INFO] W_LR=0.025
08/23 11:51:34PM parser.py:31 [INFO] W_LR_MIN=0.001
08/23 11:51:34PM parser.py:31 [INFO] W_MOMENTUM=0.9
08/23 11:51:34PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
08/23 11:51:34PM parser.py:31 [INFO] WORKERS=4
08/23 11:51:34PM parser.py:32 [INFO] 
08/23 11:51:52PM searchStage_trainer.py:137 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2500, 0.2505, 0.2501],
        [0.2503, 0.2499, 0.2496, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2500, 0.2502],
        [0.2501, 0.2501, 0.2499, 0.2500],
        [0.2497, 0.2503, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2501, 0.2499],
        [0.2498, 0.2503, 0.2502, 0.2497],
        [0.2502, 0.2498, 0.2504, 0.2497],
        [0.2499, 0.2501, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2499, 0.2503],
        [0.2498, 0.2498, 0.2503, 0.2502],
        [0.2501, 0.2505, 0.2496, 0.2498],
        [0.2502, 0.2500, 0.2499, 0.2500],
        [0.2503, 0.2501, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2503, 0.2503, 0.2496],
        [0.2499, 0.2496, 0.2506, 0.2499],
        [0.2502, 0.2500, 0.2497, 0.2501],
        [0.2498, 0.2501, 0.2498, 0.2504],
        [0.2498, 0.2503, 0.2502, 0.2497],
        [0.2501, 0.2498, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2504, 0.2498],
        [0.2503, 0.2501, 0.2497, 0.2500],
        [0.2500, 0.2499, 0.2502, 0.2499],
        [0.2498, 0.2501, 0.2501, 0.2500],
        [0.2503, 0.2501, 0.2499, 0.2497],
        [0.2502, 0.2500, 0.2500, 0.2497],
        [0.2499, 0.2501, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2498, 0.2501, 0.2496],
        [0.2500, 0.2500, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2500, 0.2498, 0.2499],
        [0.2497, 0.2502, 0.2500, 0.2501],
        [0.2500, 0.2501, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2499, 0.2499, 0.2505],
        [0.2496, 0.2502, 0.2501, 0.2500],
        [0.2497, 0.2503, 0.2498, 0.2502],
        [0.2501, 0.2499, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2499, 0.2499],
        [0.2501, 0.2498, 0.2500, 0.2501],
        [0.2504, 0.2498, 0.2500, 0.2498],
        [0.2500, 0.2500, 0.2500, 0.2501],
        [0.2500, 0.2502, 0.2498, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2496, 0.2500, 0.2503],
        [0.2498, 0.2503, 0.2502, 0.2497],
        [0.2500, 0.2502, 0.2494, 0.2503],
        [0.2501, 0.2499, 0.2499, 0.2502],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2500, 0.2498],
        [0.2500, 0.2496, 0.2501, 0.2503],
        [0.2501, 0.2499, 0.2498, 0.2502],
        [0.2499, 0.2500, 0.2502, 0.2499],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2502, 0.2499, 0.2501, 0.2498],
        [0.2497, 0.2504, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2495, 0.2503, 0.2500],
        [0.2497, 0.2499, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2498, 0.2496, 0.2505],
        [0.2497, 0.2499, 0.2500, 0.2504],
        [0.2499, 0.2501, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2496, 0.2502],
        [0.2502, 0.2501, 0.2501, 0.2496],
        [0.2499, 0.2501, 0.2501, 0.2499],
        [0.2498, 0.2505, 0.2498, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2496, 0.2501, 0.2499],
        [0.2499, 0.2498, 0.2501, 0.2502],
        [0.2501, 0.2501, 0.2497, 0.2500],
        [0.2502, 0.2502, 0.2497, 0.2499],
        [0.2498, 0.2499, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2506, 0.2502, 0.2496],
        [0.2501, 0.2497, 0.2501, 0.2502],
        [0.2503, 0.2497, 0.2499, 0.2501],
        [0.2498, 0.2500, 0.2501, 0.2501],
        [0.2502, 0.2502, 0.2494, 0.2502],
        [0.2502, 0.2498, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2500, 0.2501, 0.2503],
        [0.2500, 0.2500, 0.2500, 0.2500],
        [0.2501, 0.2498, 0.2500, 0.2500],
        [0.2499, 0.2501, 0.2504, 0.2496],
        [0.2497, 0.2503, 0.2500, 0.2500],
        [0.2499, 0.2500, 0.2501, 0.2500],
        [0.2500, 0.2500, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/23 11:52:08PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 1.9400 (2.2719)	Hard Loss 1.9400 (2.2719)	Soft Loss 1.9400 (2.2719)	Arch Loss 2.0652 (2.2740)	Arch Hard Loss 2.0652 (2.2740)	Arch Soft Loss 2.0652 (2.2740)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (20.0%, 71.5%)	
08/23 11:52:23PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 1.7887 (2.1511)	Hard Loss 1.7887 (2.1511)	Soft Loss 1.7887 (2.1511)	Arch Loss 1.8215 (2.1464)	Arch Hard Loss 1.8215 (2.1464)	Arch Soft Loss 1.8215 (2.1464)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.9%, 76.6%)	
08/23 11:52:38PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.6997 (2.0441)	Hard Loss 1.6997 (2.0441)	Soft Loss 1.6997 (2.0441)	Arch Loss 2.0695 (2.0521)	Arch Hard Loss 2.0695 (2.0521)	Arch Soft Loss 2.0695 (2.0521)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (26.1%, 79.9%)	
08/23 11:52:53PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.4891 (1.9760)	Hard Loss 1.4891 (1.9760)	Soft Loss 1.4891 (1.9760)	Arch Loss 1.7633 (1.9849)	Arch Hard Loss 1.7633 (1.9849)	Arch Soft Loss 1.7633 (1.9849)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.1%, 81.9%)	
08/23 11:53:08PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.7094 (1.9219)	Hard Loss 1.7094 (1.9219)	Soft Loss 1.7094 (1.9219)	Arch Loss 1.7542 (1.9324)	Arch Hard Loss 1.7542 (1.9324)	Arch Soft Loss 1.7542 (1.9324)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (29.8%, 83.1%)	
08/23 11:53:23PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.5543 (1.8703)	Hard Loss 1.5543 (1.8703)	Soft Loss 1.5543 (1.8703)	Arch Loss 1.5464 (1.8833)	Arch Hard Loss 1.5464 (1.8833)	Arch Soft Loss 1.5464 (1.8833)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.6%, 84.1%)	
08/23 11:53:38PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.4966 (1.8325)	Hard Loss 1.4966 (1.8325)	Soft Loss 1.4966 (1.8325)	Arch Loss 1.5618 (1.8438)	Arch Hard Loss 1.5618 (1.8438)	Arch Soft Loss 1.5618 (1.8438)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.7%, 85.0%)	
08/23 11:53:50PM searchStage_trainer.py:268 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.5644 (1.8052)	Hard Loss 1.5644 (1.8052)	Soft Loss 1.5644 (1.8052)	Arch Loss 1.4301 (1.8121)	Arch Hard Loss 1.4301 (1.8121)	Arch Soft Loss 1.4301 (1.8121)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.7%, 85.4%)	
08/23 11:53:51PM searchStage_trainer.py:284 [INFO] Train: [  0/49] Final Prec@1 33.6840%
08/23 11:53:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.5509	Prec@(1,5) (42.7%, 91.2%)
08/23 11:53:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.5417	Prec@(1,5) (43.2%, 90.9%)
08/23 11:53:58PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.5413	Prec@(1,5) (43.6%, 90.5%)
08/23 11:54:00PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.5373	Prec@(1,5) (44.0%, 90.5%)
08/23 11:54:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.5353	Prec@(1,5) (44.1%, 90.6%)
08/23 11:54:04PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.5330	Prec@(1,5) (44.2%, 90.7%)
08/23 11:54:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.5318	Prec@(1,5) (44.1%, 90.8%)
08/23 11:54:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.5319	Prec@(1,5) (44.0%, 90.8%)
08/23 11:54:08PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 44.0200%
08/23 11:54:08PM searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 4)], [('skip_connect', 3), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)]], DAG3_concat=range(6, 8))
08/23 11:54:09PM searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 44.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2499, 0.2475, 0.2541, 0.2486],
        [0.2504, 0.2445, 0.2531, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2479, 0.2513, 0.2498],
        [0.2510, 0.2476, 0.2515, 0.2498],
        [0.2490, 0.2510, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2514, 0.2518],
        [0.2520, 0.2491, 0.2507, 0.2481],
        [0.2502, 0.2490, 0.2502, 0.2505],
        [0.2499, 0.2493, 0.2510, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2490, 0.2512, 0.2494],
        [0.2513, 0.2489, 0.2515, 0.2483],
        [0.2501, 0.2491, 0.2498, 0.2510],
        [0.2496, 0.2498, 0.2495, 0.2511],
        [0.2502, 0.2502, 0.2499, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2468, 0.2577, 0.2436],
        [0.2513, 0.2453, 0.2587, 0.2448],
        [0.2508, 0.2468, 0.2518, 0.2506],
        [0.2488, 0.2479, 0.2502, 0.2531],
        [0.2474, 0.2483, 0.2496, 0.2547],
        [0.2477, 0.2504, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2411, 0.2568, 0.2435],
        [0.2576, 0.2435, 0.2566, 0.2423],
        [0.2568, 0.2429, 0.2496, 0.2507],
        [0.2526, 0.2467, 0.2497, 0.2509],
        [0.2531, 0.2457, 0.2474, 0.2538],
        [0.2524, 0.2465, 0.2483, 0.2529],
        [0.2529, 0.2463, 0.2472, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2461, 0.2530, 0.2498],
        [0.2506, 0.2471, 0.2526, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2472, 0.2536, 0.2486],
        [0.2497, 0.2477, 0.2522, 0.2504],
        [0.2513, 0.2483, 0.2498, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2484, 0.2518, 0.2473],
        [0.2520, 0.2483, 0.2516, 0.2481],
        [0.2493, 0.2485, 0.2514, 0.2507],
        [0.2506, 0.2485, 0.2468, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2504, 0.2511, 0.2453],
        [0.2521, 0.2487, 0.2505, 0.2486],
        [0.2494, 0.2489, 0.2466, 0.2551],
        [0.2490, 0.2490, 0.2523, 0.2497],
        [0.2482, 0.2502, 0.2513, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2488, 0.2513, 0.2479],
        [0.2536, 0.2502, 0.2510, 0.2452],
        [0.2504, 0.2477, 0.2483, 0.2536],
        [0.2481, 0.2490, 0.2511, 0.2517],
        [0.2481, 0.2498, 0.2518, 0.2503],
        [0.2487, 0.2492, 0.2504, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2488, 0.2508, 0.2475],
        [0.2536, 0.2484, 0.2502, 0.2479],
        [0.2511, 0.2482, 0.2503, 0.2504],
        [0.2511, 0.2485, 0.2500, 0.2505],
        [0.2481, 0.2491, 0.2533, 0.2494],
        [0.2488, 0.2514, 0.2510, 0.2487],
        [0.2503, 0.2493, 0.2456, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2407, 0.2457, 0.2629, 0.2507],
        [0.2420, 0.2466, 0.2633, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.2451, 0.2593, 0.2527],
        [0.2449, 0.2468, 0.2606, 0.2477],
        [0.2435, 0.2482, 0.2575, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2480, 0.2580, 0.2475],
        [0.2468, 0.2486, 0.2579, 0.2467],
        [0.2445, 0.2484, 0.2541, 0.2530],
        [0.2449, 0.2501, 0.2531, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2485, 0.2559, 0.2479],
        [0.2466, 0.2471, 0.2543, 0.2520],
        [0.2471, 0.2498, 0.2539, 0.2491],
        [0.2457, 0.2492, 0.2537, 0.2514],
        [0.2452, 0.2489, 0.2554, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2491, 0.2557, 0.2428],
        [0.2512, 0.2463, 0.2534, 0.2491],
        [0.2437, 0.2462, 0.2576, 0.2525],
        [0.2425, 0.2457, 0.2609, 0.2510],
        [0.2447, 0.2467, 0.2529, 0.2557],
        [0.2428, 0.2466, 0.2580, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2486, 0.2558, 0.2438],
        [0.2507, 0.2488, 0.2564, 0.2441],
        [0.2443, 0.2490, 0.2568, 0.2499],
        [0.2431, 0.2470, 0.2576, 0.2523],
        [0.2436, 0.2470, 0.2543, 0.2551],
        [0.2430, 0.2467, 0.2567, 0.2536],
        [0.2453, 0.2474, 0.2533, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/23 11:54:24午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.5387 (1.5096)	Hard Loss 1.5387 (1.5096)	Soft Loss 1.5387 (1.5096)	Arch Loss 1.5133 (1.5199)	Arch Hard Loss 1.5133 (1.5199)	Arch Soft Loss 1.5133 (1.5199)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.8%, 91.0%)	
08/23 11:54:39午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.3138 (1.4866)	Hard Loss 1.3138 (1.4866)	Soft Loss 1.3138 (1.4866)	Arch Loss 1.4110 (1.5200)	Arch Hard Loss 1.4110 (1.5200)	Arch Soft Loss 1.4110 (1.5200)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.7%, 91.4%)	
08/23 11:54:54午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.3349 (1.4763)	Hard Loss 1.3349 (1.4763)	Soft Loss 1.3349 (1.4763)	Arch Loss 1.3430 (1.4960)	Arch Hard Loss 1.3430 (1.4960)	Arch Soft Loss 1.3430 (1.4960)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.0%, 91.7%)	
08/23 11:55:09午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.5710 (1.4680)	Hard Loss 1.5710 (1.4680)	Soft Loss 1.5710 (1.4680)	Arch Loss 1.2858 (1.4806)	Arch Hard Loss 1.2858 (1.4806)	Arch Soft Loss 1.2858 (1.4806)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.1%, 91.8%)	
08/23 11:55:24午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.4259 (1.4472)	Hard Loss 1.4259 (1.4472)	Soft Loss 1.4259 (1.4472)	Arch Loss 1.5550 (1.4637)	Arch Hard Loss 1.5550 (1.4637)	Arch Soft Loss 1.5550 (1.4637)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.7%, 92.2%)	
08/23 11:55:38午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4212 (1.4389)	Hard Loss 1.4212 (1.4389)	Soft Loss 1.4212 (1.4389)	Arch Loss 1.5073 (1.4446)	Arch Hard Loss 1.5073 (1.4446)	Arch Soft Loss 1.5073 (1.4446)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.8%, 92.4%)	
08/23 11:55:53午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.5444 (1.4235)	Hard Loss 1.5444 (1.4235)	Soft Loss 1.5444 (1.4235)	Arch Loss 1.3550 (1.4336)	Arch Hard Loss 1.3550 (1.4336)	Arch Soft Loss 1.3550 (1.4336)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.5%, 92.5%)	
08/23 11:56:05午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.4335 (1.4115)	Hard Loss 1.4335 (1.4115)	Soft Loss 1.4335 (1.4115)	Arch Loss 1.2172 (1.4239)	Arch Hard Loss 1.2172 (1.4239)	Arch Soft Loss 1.2172 (1.4239)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.9%, 92.7%)	
08/23 11:56:06午後 searchStage_trainer.py:284 [INFO] Train: [  1/49] Final Prec@1 47.8840%
08/23 11:56:08午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.2972	Prec@(1,5) (52.2%, 93.8%)
08/23 11:56:10午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.3024	Prec@(1,5) (52.6%, 93.6%)
08/23 11:56:13午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.3068	Prec@(1,5) (53.0%, 93.6%)
08/23 11:56:15午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.3018	Prec@(1,5) (53.4%, 93.7%)
08/23 11:56:17午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.3090	Prec@(1,5) (53.1%, 93.6%)
08/23 11:56:19午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.3067	Prec@(1,5) (53.1%, 93.6%)
08/23 11:56:21午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.3136	Prec@(1,5) (52.9%, 93.5%)
08/23 11:56:23午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.3094	Prec@(1,5) (53.0%, 93.6%)
08/23 11:56:23午後 searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 53.0200%
08/23 11:56:23午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/23 11:56:24午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 53.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2487, 0.2395, 0.2638, 0.2480],
        [0.2492, 0.2346, 0.2636, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2427, 0.2569, 0.2496],
        [0.2501, 0.2423, 0.2580, 0.2496],
        [0.2486, 0.2491, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2432, 0.2601, 0.2481],
        [0.2518, 0.2462, 0.2560, 0.2460],
        [0.2481, 0.2481, 0.2514, 0.2524],
        [0.2501, 0.2487, 0.2493, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2461, 0.2547, 0.2472],
        [0.2532, 0.2459, 0.2553, 0.2456],
        [0.2511, 0.2475, 0.2485, 0.2528],
        [0.2495, 0.2478, 0.2497, 0.2530],
        [0.2485, 0.2494, 0.2514, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2331, 0.2816, 0.2377],
        [0.2501, 0.2323, 0.2813, 0.2363],
        [0.2485, 0.2437, 0.2584, 0.2493],
        [0.2465, 0.2445, 0.2508, 0.2582],
        [0.2445, 0.2451, 0.2477, 0.2628],
        [0.2428, 0.2485, 0.2504, 0.2583]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2592, 0.2252, 0.2778, 0.2378],
        [0.2584, 0.2302, 0.2765, 0.2349],
        [0.2561, 0.2364, 0.2561, 0.2514],
        [0.2504, 0.2447, 0.2526, 0.2524],
        [0.2541, 0.2436, 0.2448, 0.2575],
        [0.2527, 0.2440, 0.2466, 0.2568],
        [0.2500, 0.2452, 0.2450, 0.2598]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2409, 0.2591, 0.2495],
        [0.2509, 0.2414, 0.2585, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2456, 0.2585, 0.2450],
        [0.2494, 0.2455, 0.2568, 0.2483],
        [0.2519, 0.2457, 0.2479, 0.2545]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2500, 0.2539, 0.2444],
        [0.2528, 0.2481, 0.2539, 0.2453],
        [0.2483, 0.2461, 0.2504, 0.2553],
        [0.2482, 0.2489, 0.2485, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2569, 0.2515, 0.2519, 0.2397],
        [0.2542, 0.2493, 0.2519, 0.2447],
        [0.2485, 0.2478, 0.2429, 0.2608],
        [0.2472, 0.2478, 0.2541, 0.2509],
        [0.2479, 0.2488, 0.2508, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2513, 0.2521, 0.2408],
        [0.2577, 0.2512, 0.2519, 0.2392],
        [0.2518, 0.2433, 0.2475, 0.2575],
        [0.2511, 0.2429, 0.2480, 0.2580],
        [0.2504, 0.2475, 0.2526, 0.2494],
        [0.2534, 0.2460, 0.2476, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2566, 0.2498, 0.2503, 0.2433],
        [0.2585, 0.2499, 0.2490, 0.2425],
        [0.2530, 0.2435, 0.2484, 0.2551],
        [0.2516, 0.2449, 0.2460, 0.2576],
        [0.2447, 0.2509, 0.2528, 0.2516],
        [0.2520, 0.2492, 0.2523, 0.2464],
        [0.2517, 0.2488, 0.2475, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2401, 0.2398, 0.2696, 0.2505],
        [0.2424, 0.2398, 0.2693, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2434, 0.2426, 0.2641, 0.2499],
        [0.2428, 0.2426, 0.2644, 0.2502],
        [0.2454, 0.2462, 0.2575, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2455, 0.2599, 0.2494],
        [0.2461, 0.2465, 0.2601, 0.2474],
        [0.2458, 0.2454, 0.2547, 0.2540],
        [0.2465, 0.2498, 0.2546, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2479, 0.2572, 0.2486],
        [0.2474, 0.2465, 0.2534, 0.2528],
        [0.2489, 0.2481, 0.2547, 0.2483],
        [0.2479, 0.2478, 0.2551, 0.2492],
        [0.2469, 0.2470, 0.2541, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2580, 0.2467, 0.2537, 0.2416],
        [0.2547, 0.2441, 0.2521, 0.2491],
        [0.2454, 0.2424, 0.2588, 0.2534],
        [0.2445, 0.2412, 0.2627, 0.2515],
        [0.2449, 0.2443, 0.2543, 0.2564],
        [0.2421, 0.2429, 0.2621, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2460, 0.2542, 0.2402],
        [0.2566, 0.2482, 0.2551, 0.2400],
        [0.2447, 0.2471, 0.2569, 0.2512],
        [0.2437, 0.2444, 0.2583, 0.2535],
        [0.2437, 0.2456, 0.2559, 0.2548],
        [0.2411, 0.2446, 0.2590, 0.2553],
        [0.2431, 0.2422, 0.2550, 0.2597]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/23 11:56:39午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 0.9489 (1.2524)	Hard Loss 0.9489 (1.2524)	Soft Loss 0.9489 (1.2524)	Arch Loss 1.0433 (1.3123)	Arch Hard Loss 1.0433 (1.3123)	Arch Soft Loss 1.0433 (1.3123)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.2%, 95.5%)	
08/23 11:56:54午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.1881 (1.2467)	Hard Loss 1.1881 (1.2467)	Soft Loss 1.1881 (1.2467)	Arch Loss 1.2637 (1.3046)	Arch Hard Loss 1.2637 (1.3046)	Arch Soft Loss 1.2637 (1.3046)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (55.1%, 94.9%)	
08/23 11:57:09午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.1859 (1.2563)	Hard Loss 1.1859 (1.2563)	Soft Loss 1.1859 (1.2563)	Arch Loss 1.2164 (1.2909)	Arch Hard Loss 1.2164 (1.2909)	Arch Soft Loss 1.2164 (1.2909)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.8%, 94.6%)	
08/23 11:57:24午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.0419 (1.2491)	Hard Loss 1.0419 (1.2491)	Soft Loss 1.0419 (1.2491)	Arch Loss 1.0708 (1.2785)	Arch Hard Loss 1.0708 (1.2785)	Arch Soft Loss 1.0708 (1.2785)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.9%, 94.6%)	
08/23 11:57:39午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.1773 (1.2422)	Hard Loss 1.1773 (1.2422)	Soft Loss 1.1773 (1.2422)	Arch Loss 0.9605 (1.2592)	Arch Hard Loss 0.9605 (1.2592)	Arch Soft Loss 0.9605 (1.2592)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (55.2%, 94.7%)	
08/23 11:57:54午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.4129 (1.2339)	Hard Loss 1.4129 (1.2339)	Soft Loss 1.4129 (1.2339)	Arch Loss 1.2744 (1.2477)	Arch Hard Loss 1.2744 (1.2477)	Arch Soft Loss 1.2744 (1.2477)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (55.6%, 94.7%)	
08/23 11:58:09午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.0408 (1.2186)	Hard Loss 1.0408 (1.2186)	Soft Loss 1.0408 (1.2186)	Arch Loss 1.0175 (1.2317)	Arch Hard Loss 1.0175 (1.2317)	Arch Soft Loss 1.0175 (1.2317)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (56.1%, 94.8%)	
08/23 11:58:21午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.0769 (1.2123)	Hard Loss 1.0769 (1.2123)	Soft Loss 1.0769 (1.2123)	Arch Loss 1.1525 (1.2192)	Arch Hard Loss 1.1525 (1.2192)	Arch Soft Loss 1.1525 (1.2192)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (56.3%, 94.9%)	
08/23 11:58:22午後 searchStage_trainer.py:284 [INFO] Train: [  2/49] Final Prec@1 56.3280%
08/23 11:58:24午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.0846	Prec@(1,5) (60.7%, 96.6%)
08/23 11:58:26午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.0986	Prec@(1,5) (60.1%, 96.4%)
08/23 11:58:29午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.0963	Prec@(1,5) (60.3%, 96.2%)
08/23 11:58:31午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.1004	Prec@(1,5) (60.3%, 96.2%)
08/23 11:58:33午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.0960	Prec@(1,5) (60.3%, 96.3%)
08/23 11:58:35午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.1014	Prec@(1,5) (60.2%, 96.2%)
08/23 11:58:37午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.1018	Prec@(1,5) (60.2%, 96.1%)
08/23 11:58:39午後 searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.1068	Prec@(1,5) (60.1%, 96.0%)
08/23 11:58:39午後 searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 60.0600%
08/23 11:58:39午後 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/23 11:58:39午後 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 60.0600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2439, 0.2282, 0.2817, 0.2462],
        [0.2441, 0.2224, 0.2799, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2358, 0.2659, 0.2494],
        [0.2492, 0.2369, 0.2672, 0.2467],
        [0.2482, 0.2444, 0.2529, 0.2545]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2408, 0.2679, 0.2435],
        [0.2504, 0.2444, 0.2647, 0.2405],
        [0.2490, 0.2435, 0.2502, 0.2573],
        [0.2501, 0.2461, 0.2479, 0.2558]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2435, 0.2609, 0.2462],
        [0.2495, 0.2424, 0.2630, 0.2451],
        [0.2482, 0.2471, 0.2524, 0.2523],
        [0.2460, 0.2487, 0.2510, 0.2543],
        [0.2463, 0.2500, 0.2522, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2400, 0.2156, 0.3163, 0.2281],
        [0.2419, 0.2167, 0.3144, 0.2270],
        [0.2428, 0.2364, 0.2677, 0.2531],
        [0.2397, 0.2406, 0.2497, 0.2700],
        [0.2405, 0.2409, 0.2461, 0.2726],
        [0.2404, 0.2471, 0.2517, 0.2607]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2107, 0.3063, 0.2316],
        [0.2503, 0.2157, 0.3047, 0.2293],
        [0.2484, 0.2300, 0.2688, 0.2528],
        [0.2434, 0.2424, 0.2577, 0.2565],
        [0.2457, 0.2417, 0.2471, 0.2655],
        [0.2442, 0.2444, 0.2494, 0.2621],
        [0.2454, 0.2437, 0.2508, 0.2601]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2364, 0.2637, 0.2509],
        [0.2482, 0.2394, 0.2654, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2429, 0.2609, 0.2452],
        [0.2510, 0.2412, 0.2592, 0.2487],
        [0.2523, 0.2433, 0.2495, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2466, 0.2566, 0.2466],
        [0.2520, 0.2492, 0.2555, 0.2433],
        [0.2461, 0.2462, 0.2504, 0.2574],
        [0.2514, 0.2483, 0.2484, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2515, 0.2552, 0.2403],
        [0.2542, 0.2490, 0.2539, 0.2429],
        [0.2489, 0.2464, 0.2422, 0.2625],
        [0.2491, 0.2467, 0.2543, 0.2499],
        [0.2498, 0.2451, 0.2520, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2528, 0.2525, 0.2369],
        [0.2621, 0.2536, 0.2515, 0.2328],
        [0.2525, 0.2413, 0.2431, 0.2630],
        [0.2530, 0.2418, 0.2450, 0.2603],
        [0.2551, 0.2444, 0.2511, 0.2494],
        [0.2579, 0.2427, 0.2455, 0.2539]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2480, 0.2513, 0.2421],
        [0.2629, 0.2457, 0.2498, 0.2416],
        [0.2530, 0.2420, 0.2464, 0.2586],
        [0.2547, 0.2419, 0.2446, 0.2587],
        [0.2480, 0.2480, 0.2525, 0.2514],
        [0.2538, 0.2457, 0.2535, 0.2470],
        [0.2570, 0.2449, 0.2486, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2399, 0.2358, 0.2742, 0.2500],
        [0.2408, 0.2354, 0.2750, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2416, 0.2665, 0.2469],
        [0.2434, 0.2421, 0.2672, 0.2474],
        [0.2475, 0.2433, 0.2540, 0.2551]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2463, 0.2595, 0.2480],
        [0.2473, 0.2466, 0.2602, 0.2459],
        [0.2488, 0.2427, 0.2518, 0.2566],
        [0.2484, 0.2491, 0.2534, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2511, 0.2578, 0.2439],
        [0.2468, 0.2487, 0.2543, 0.2502],
        [0.2507, 0.2462, 0.2517, 0.2514],
        [0.2484, 0.2480, 0.2530, 0.2506],
        [0.2493, 0.2467, 0.2505, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2438, 0.2526, 0.2399],
        [0.2605, 0.2407, 0.2509, 0.2479],
        [0.2504, 0.2390, 0.2563, 0.2543],
        [0.2508, 0.2383, 0.2591, 0.2517],
        [0.2503, 0.2421, 0.2523, 0.2553],
        [0.2455, 0.2410, 0.2603, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2670, 0.2446, 0.2513, 0.2371],
        [0.2652, 0.2463, 0.2504, 0.2381],
        [0.2480, 0.2454, 0.2543, 0.2524],
        [0.2476, 0.2427, 0.2567, 0.2530],
        [0.2478, 0.2443, 0.2517, 0.2561],
        [0.2435, 0.2431, 0.2553, 0.2580],
        [0.2449, 0.2430, 0.2537, 0.2583]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/23 11:58:55午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.1164 (1.0995)	Hard Loss 1.1164 (1.0995)	Soft Loss 1.1164 (1.0995)	Arch Loss 1.3440 (1.1102)	Arch Hard Loss 1.3440 (1.1102)	Arch Soft Loss 1.3440 (1.1102)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (59.8%, 95.9%)	
08/23 11:59:11午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 0.9921 (1.0856)	Hard Loss 0.9921 (1.0856)	Soft Loss 0.9921 (1.0856)	Arch Loss 1.3292 (1.0955)	Arch Hard Loss 1.3292 (1.0955)	Arch Soft Loss 1.3292 (1.0955)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (60.3%, 96.0%)	
08/23 11:59:26午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.0841 (1.0714)	Hard Loss 1.0841 (1.0714)	Soft Loss 1.0841 (1.0714)	Arch Loss 1.0432 (1.1018)	Arch Hard Loss 1.0432 (1.1018)	Arch Soft Loss 1.0432 (1.1018)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.3%, 96.1%)	
08/23 11:59:41午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.0363 (1.0701)	Hard Loss 1.0363 (1.0701)	Soft Loss 1.0363 (1.0701)	Arch Loss 1.0699 (1.1060)	Arch Hard Loss 1.0699 (1.1060)	Arch Soft Loss 1.0699 (1.1060)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.3%, 96.0%)	
08/23 11:59:56午後 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 0.9390 (1.0686)	Hard Loss 0.9390 (1.0686)	Soft Loss 0.9390 (1.0686)	Arch Loss 1.3338 (1.1005)	Arch Hard Loss 1.3338 (1.1005)	Arch Soft Loss 1.3338 (1.1005)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.5%, 96.1%)	
08/24 12:00:11午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.0484 (1.0659)	Hard Loss 1.0484 (1.0659)	Soft Loss 1.0484 (1.0659)	Arch Loss 1.2910 (1.0956)	Arch Hard Loss 1.2910 (1.0956)	Arch Soft Loss 1.2910 (1.0956)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.7%, 96.1%)	
08/24 12:00:26午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.1521 (1.0624)	Hard Loss 1.1521 (1.0624)	Soft Loss 1.1521 (1.0624)	Arch Loss 1.0463 (1.0878)	Arch Hard Loss 1.0463 (1.0878)	Arch Soft Loss 1.0463 (1.0878)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.9%, 96.2%)	
08/24 12:00:38午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 0.9070 (1.0547)	Hard Loss 0.9070 (1.0547)	Soft Loss 0.9070 (1.0547)	Arch Loss 0.8524 (1.0792)	Arch Hard Loss 0.8524 (1.0792)	Arch Soft Loss 0.8524 (1.0792)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (62.1%, 96.3%)	
08/24 12:00:38午前 searchStage_trainer.py:284 [INFO] Train: [  3/49] Final Prec@1 62.1400%
08/24 12:00:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.0291	Prec@(1,5) (64.2%, 96.0%)
08/24 12:00:43午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.0054	Prec@(1,5) (64.7%, 96.1%)
08/24 12:00:45午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.0009	Prec@(1,5) (64.8%, 96.3%)
08/24 12:00:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.0056	Prec@(1,5) (64.6%, 96.4%)
08/24 12:00:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.0140	Prec@(1,5) (64.3%, 96.3%)
08/24 12:00:52午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.0122	Prec@(1,5) (64.3%, 96.3%)
08/24 12:00:54午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.0170	Prec@(1,5) (64.2%, 96.3%)
08/24 12:00:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.0137	Prec@(1,5) (64.2%, 96.3%)
08/24 12:00:56午前 searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 64.2480%
08/24 12:00:56午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/24 12:00:56午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 64.2480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2367, 0.2188, 0.2987, 0.2458],
        [0.2386, 0.2134, 0.2951, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2299, 0.2767, 0.2479],
        [0.2458, 0.2319, 0.2761, 0.2462],
        [0.2466, 0.2428, 0.2550, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2368, 0.2727, 0.2438],
        [0.2485, 0.2413, 0.2699, 0.2403],
        [0.2481, 0.2446, 0.2482, 0.2592],
        [0.2509, 0.2461, 0.2492, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2423, 0.2656, 0.2456],
        [0.2476, 0.2401, 0.2693, 0.2429],
        [0.2463, 0.2466, 0.2543, 0.2528],
        [0.2440, 0.2487, 0.2512, 0.2562],
        [0.2455, 0.2500, 0.2532, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2295, 0.2026, 0.3444, 0.2235],
        [0.2315, 0.2053, 0.3413, 0.2219],
        [0.2422, 0.2298, 0.2697, 0.2583],
        [0.2356, 0.2381, 0.2508, 0.2755],
        [0.2356, 0.2415, 0.2494, 0.2735],
        [0.2391, 0.2464, 0.2513, 0.2632]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.1988, 0.3316, 0.2261],
        [0.2427, 0.2029, 0.3276, 0.2268],
        [0.2459, 0.2283, 0.2726, 0.2532],
        [0.2402, 0.2421, 0.2586, 0.2591],
        [0.2415, 0.2394, 0.2460, 0.2731],
        [0.2392, 0.2454, 0.2508, 0.2646],
        [0.2423, 0.2451, 0.2533, 0.2594]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2329, 0.2684, 0.2506],
        [0.2485, 0.2348, 0.2697, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2402, 0.2623, 0.2459],
        [0.2521, 0.2398, 0.2610, 0.2471],
        [0.2542, 0.2426, 0.2479, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2435, 0.2569, 0.2481],
        [0.2528, 0.2472, 0.2556, 0.2445],
        [0.2475, 0.2441, 0.2491, 0.2593],
        [0.2495, 0.2520, 0.2503, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2478, 0.2575, 0.2421],
        [0.2532, 0.2497, 0.2566, 0.2405],
        [0.2482, 0.2435, 0.2402, 0.2681],
        [0.2474, 0.2481, 0.2553, 0.2491],
        [0.2474, 0.2486, 0.2544, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.2507, 0.2527, 0.2377],
        [0.2658, 0.2513, 0.2523, 0.2306],
        [0.2517, 0.2405, 0.2436, 0.2642],
        [0.2545, 0.2403, 0.2436, 0.2617],
        [0.2538, 0.2468, 0.2527, 0.2468],
        [0.2568, 0.2435, 0.2436, 0.2561]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2567, 0.2469, 0.2546, 0.2418],
        [0.2638, 0.2442, 0.2521, 0.2399],
        [0.2491, 0.2425, 0.2439, 0.2645],
        [0.2558, 0.2411, 0.2436, 0.2595],
        [0.2501, 0.2488, 0.2532, 0.2479],
        [0.2549, 0.2484, 0.2527, 0.2440],
        [0.2635, 0.2407, 0.2461, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2424, 0.2314, 0.2776, 0.2486],
        [0.2412, 0.2298, 0.2786, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2376, 0.2700, 0.2472],
        [0.2435, 0.2379, 0.2703, 0.2482],
        [0.2523, 0.2397, 0.2539, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2454, 0.2629, 0.2467],
        [0.2455, 0.2433, 0.2631, 0.2481],
        [0.2504, 0.2416, 0.2526, 0.2553],
        [0.2501, 0.2460, 0.2542, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2492, 0.2584, 0.2443],
        [0.2463, 0.2481, 0.2569, 0.2488],
        [0.2503, 0.2470, 0.2537, 0.2490],
        [0.2472, 0.2464, 0.2533, 0.2531],
        [0.2513, 0.2452, 0.2480, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2694, 0.2400, 0.2504, 0.2403],
        [0.2657, 0.2372, 0.2494, 0.2477],
        [0.2512, 0.2359, 0.2574, 0.2556],
        [0.2529, 0.2357, 0.2608, 0.2506],
        [0.2517, 0.2398, 0.2513, 0.2573],
        [0.2461, 0.2407, 0.2617, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2728, 0.2401, 0.2500, 0.2371],
        [0.2712, 0.2429, 0.2494, 0.2365],
        [0.2502, 0.2413, 0.2552, 0.2533],
        [0.2487, 0.2392, 0.2596, 0.2526],
        [0.2488, 0.2418, 0.2498, 0.2596],
        [0.2432, 0.2425, 0.2571, 0.2572],
        [0.2452, 0.2430, 0.2541, 0.2577]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:01:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 0.9151 (0.9802)	Hard Loss 0.9151 (0.9802)	Soft Loss 0.9151 (0.9802)	Arch Loss 0.9660 (0.9990)	Arch Hard Loss 0.9660 (0.9990)	Arch Soft Loss 0.9660 (0.9990)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (64.3%, 97.1%)	
08/24 12:01:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 0.9202 (0.9756)	Hard Loss 0.9202 (0.9756)	Soft Loss 0.9202 (0.9756)	Arch Loss 0.8627 (0.9902)	Arch Hard Loss 0.8627 (0.9902)	Arch Soft Loss 0.8627 (0.9902)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (64.9%, 97.1%)	
08/24 12:01:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.0331 (0.9740)	Hard Loss 1.0331 (0.9740)	Soft Loss 1.0331 (0.9740)	Arch Loss 1.0350 (0.9846)	Arch Hard Loss 1.0350 (0.9846)	Arch Soft Loss 1.0350 (0.9846)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.0%, 97.1%)	
08/24 12:01:56午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.2053 (0.9755)	Hard Loss 1.2053 (0.9755)	Soft Loss 1.2053 (0.9755)	Arch Loss 0.9744 (0.9753)	Arch Hard Loss 0.9744 (0.9753)	Arch Soft Loss 0.9744 (0.9753)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.1%, 97.1%)	
08/24 12:02:11午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 0.9696 (0.9742)	Hard Loss 0.9696 (0.9742)	Soft Loss 0.9696 (0.9742)	Arch Loss 1.0110 (0.9721)	Arch Hard Loss 1.0110 (0.9721)	Arch Soft Loss 1.0110 (0.9721)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.4%, 97.0%)	
08/24 12:02:26午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 0.6357 (0.9716)	Hard Loss 0.6357 (0.9716)	Soft Loss 0.6357 (0.9716)	Arch Loss 0.7965 (0.9726)	Arch Hard Loss 0.7965 (0.9726)	Arch Soft Loss 0.7965 (0.9726)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.6%, 97.0%)	
08/24 12:02:41午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 0.8770 (0.9657)	Hard Loss 0.8770 (0.9657)	Soft Loss 0.8770 (0.9657)	Arch Loss 1.0673 (0.9746)	Arch Hard Loss 1.0673 (0.9746)	Arch Soft Loss 1.0673 (0.9746)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.7%, 97.0%)	
08/24 12:02:53午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 0.8893 (0.9606)	Hard Loss 0.8893 (0.9606)	Soft Loss 0.8893 (0.9606)	Arch Loss 0.9480 (0.9744)	Arch Hard Loss 0.9480 (0.9744)	Arch Soft Loss 0.9480 (0.9744)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.8%, 97.1%)	
08/24 12:02:53午前 searchStage_trainer.py:284 [INFO] Train: [  4/49] Final Prec@1 65.8120%
08/24 12:02:56午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 0.9221	Prec@(1,5) (67.4%, 97.5%)
08/24 12:02:58午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 0.9177	Prec@(1,5) (67.1%, 97.4%)
08/24 12:03:00午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 0.9264	Prec@(1,5) (67.0%, 97.3%)
08/24 12:03:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 0.9337	Prec@(1,5) (66.7%, 97.3%)
08/24 12:03:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 0.9324	Prec@(1,5) (66.7%, 97.2%)
08/24 12:03:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 0.9323	Prec@(1,5) (66.7%, 97.2%)
08/24 12:03:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 0.9349	Prec@(1,5) (66.7%, 97.2%)
08/24 12:03:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 0.9350	Prec@(1,5) (66.7%, 97.2%)
08/24 12:03:10午前 searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 66.7400%
08/24 12:03:10午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 1), ('max_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/24 12:03:11午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 66.7400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2325, 0.2101, 0.3136, 0.2438],
        [0.2322, 0.2046, 0.3091, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2237, 0.2831, 0.2483],
        [0.2443, 0.2263, 0.2845, 0.2449],
        [0.2461, 0.2413, 0.2564, 0.2562]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2329, 0.2735, 0.2422],
        [0.2533, 0.2371, 0.2714, 0.2382],
        [0.2476, 0.2406, 0.2472, 0.2646],
        [0.2510, 0.2451, 0.2511, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2420, 0.2685, 0.2443],
        [0.2468, 0.2407, 0.2724, 0.2401],
        [0.2463, 0.2448, 0.2526, 0.2563],
        [0.2428, 0.2490, 0.2530, 0.2551],
        [0.2465, 0.2492, 0.2520, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2163, 0.1896, 0.3776, 0.2165],
        [0.2186, 0.1931, 0.3734, 0.2149],
        [0.2339, 0.2240, 0.2751, 0.2670],
        [0.2332, 0.2354, 0.2528, 0.2785],
        [0.2335, 0.2390, 0.2478, 0.2798],
        [0.2354, 0.2448, 0.2546, 0.2652]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2287, 0.1853, 0.3646, 0.2213],
        [0.2269, 0.1900, 0.3627, 0.2204],
        [0.2367, 0.2274, 0.2811, 0.2548],
        [0.2372, 0.2384, 0.2598, 0.2647],
        [0.2367, 0.2429, 0.2469, 0.2735],
        [0.2355, 0.2440, 0.2535, 0.2670],
        [0.2366, 0.2481, 0.2514, 0.2639]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.2336, 0.2725, 0.2505],
        [0.2462, 0.2349, 0.2724, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2357, 0.2654, 0.2493],
        [0.2546, 0.2383, 0.2621, 0.2450],
        [0.2538, 0.2421, 0.2495, 0.2546]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2438, 0.2554, 0.2500],
        [0.2542, 0.2467, 0.2535, 0.2455],
        [0.2468, 0.2425, 0.2446, 0.2660],
        [0.2505, 0.2565, 0.2532, 0.2398]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2464, 0.2574, 0.2406],
        [0.2555, 0.2476, 0.2568, 0.2400],
        [0.2482, 0.2409, 0.2375, 0.2734],
        [0.2503, 0.2463, 0.2533, 0.2501],
        [0.2486, 0.2494, 0.2560, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2571, 0.2505, 0.2554, 0.2369],
        [0.2668, 0.2522, 0.2527, 0.2283],
        [0.2502, 0.2389, 0.2399, 0.2709],
        [0.2553, 0.2375, 0.2383, 0.2690],
        [0.2537, 0.2512, 0.2560, 0.2391],
        [0.2588, 0.2434, 0.2440, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2482, 0.2528, 0.2394],
        [0.2673, 0.2458, 0.2504, 0.2365],
        [0.2473, 0.2411, 0.2398, 0.2717],
        [0.2577, 0.2381, 0.2377, 0.2664],
        [0.2503, 0.2524, 0.2553, 0.2419],
        [0.2550, 0.2499, 0.2543, 0.2409],
        [0.2632, 0.2422, 0.2438, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2305, 0.2786, 0.2484],
        [0.2415, 0.2289, 0.2792, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2386, 0.2680, 0.2464],
        [0.2465, 0.2367, 0.2682, 0.2486],
        [0.2517, 0.2407, 0.2530, 0.2546]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2456, 0.2628, 0.2472],
        [0.2462, 0.2434, 0.2624, 0.2480],
        [0.2482, 0.2435, 0.2530, 0.2553],
        [0.2490, 0.2475, 0.2544, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2486, 0.2561, 0.2479],
        [0.2474, 0.2462, 0.2538, 0.2527],
        [0.2501, 0.2463, 0.2530, 0.2506],
        [0.2500, 0.2484, 0.2542, 0.2474],
        [0.2523, 0.2469, 0.2493, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2696, 0.2390, 0.2525, 0.2389],
        [0.2669, 0.2355, 0.2502, 0.2474],
        [0.2528, 0.2354, 0.2574, 0.2544],
        [0.2530, 0.2341, 0.2582, 0.2546],
        [0.2528, 0.2396, 0.2505, 0.2570],
        [0.2471, 0.2416, 0.2613, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2732, 0.2375, 0.2509, 0.2383],
        [0.2721, 0.2409, 0.2504, 0.2367],
        [0.2523, 0.2382, 0.2558, 0.2537],
        [0.2501, 0.2377, 0.2610, 0.2512],
        [0.2494, 0.2386, 0.2498, 0.2621],
        [0.2427, 0.2423, 0.2593, 0.2557],
        [0.2469, 0.2431, 0.2540, 0.2560]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:03:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.7403 (0.8863)	Hard Loss 0.7403 (0.8863)	Soft Loss 0.7403 (0.8863)	Arch Loss 0.9879 (0.9074)	Arch Hard Loss 0.9879 (0.9074)	Arch Soft Loss 0.9879 (0.9074)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.2%, 97.3%)	
08/24 12:03:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 0.7206 (0.8811)	Hard Loss 0.7206 (0.8811)	Soft Loss 0.7206 (0.8811)	Arch Loss 0.8073 (0.9371)	Arch Hard Loss 0.8073 (0.9371)	Arch Soft Loss 0.8073 (0.9371)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.6%, 97.5%)	
08/24 12:03:57午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 0.8824 (0.8873)	Hard Loss 0.8824 (0.8873)	Soft Loss 0.8824 (0.8873)	Arch Loss 0.5601 (0.9378)	Arch Hard Loss 0.5601 (0.9378)	Arch Soft Loss 0.5601 (0.9378)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.4%, 97.4%)	
08/24 12:04:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 0.8646 (0.8879)	Hard Loss 0.8646 (0.8879)	Soft Loss 0.8646 (0.8879)	Arch Loss 0.8087 (0.9266)	Arch Hard Loss 0.8087 (0.9266)	Arch Soft Loss 0.8087 (0.9266)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.4%, 97.5%)	
08/24 12:04:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 0.8872 (0.8810)	Hard Loss 0.8872 (0.8810)	Soft Loss 0.8872 (0.8810)	Arch Loss 0.9502 (0.9192)	Arch Hard Loss 0.9502 (0.9192)	Arch Soft Loss 0.9502 (0.9192)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.6%, 97.5%)	
08/24 12:04:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 0.8777 (0.8738)	Hard Loss 0.8777 (0.8738)	Soft Loss 0.8777 (0.8738)	Arch Loss 1.1782 (0.9114)	Arch Hard Loss 1.1782 (0.9114)	Arch Soft Loss 1.1782 (0.9114)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.8%, 97.6%)	
08/24 12:04:57午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 0.8453 (0.8723)	Hard Loss 0.8453 (0.8723)	Soft Loss 0.8453 (0.8723)	Arch Loss 1.0002 (0.9079)	Arch Hard Loss 1.0002 (0.9079)	Arch Soft Loss 1.0002 (0.9079)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (68.9%, 97.6%)	
08/24 12:05:09午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.7195 (0.8721)	Hard Loss 0.7195 (0.8721)	Soft Loss 0.7195 (0.8721)	Arch Loss 0.8013 (0.9035)	Arch Hard Loss 0.8013 (0.9035)	Arch Soft Loss 0.8013 (0.9035)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (69.0%, 97.6%)	
08/24 12:05:09午前 searchStage_trainer.py:284 [INFO] Train: [  5/49] Final Prec@1 68.9960%
08/24 12:05:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 0.8993	Prec@(1,5) (68.4%, 97.4%)
08/24 12:05:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 0.8813	Prec@(1,5) (68.7%, 97.6%)
08/24 12:05:16午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 0.8865	Prec@(1,5) (68.9%, 97.5%)
08/24 12:05:18午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 0.8761	Prec@(1,5) (69.1%, 97.5%)
08/24 12:05:20午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 0.8750	Prec@(1,5) (69.1%, 97.5%)
08/24 12:05:22午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 0.8686	Prec@(1,5) (69.4%, 97.5%)
08/24 12:05:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 0.8646	Prec@(1,5) (69.4%, 97.6%)
08/24 12:05:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 0.8627	Prec@(1,5) (69.5%, 97.6%)
08/24 12:05:26午前 searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 69.4880%
08/24 12:05:26午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 4)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 1), ('max_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/24 12:05:27午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 69.4880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2270, 0.2018, 0.3274, 0.2438],
        [0.2269, 0.1976, 0.3230, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2194, 0.2916, 0.2465],
        [0.2418, 0.2223, 0.2933, 0.2425],
        [0.2453, 0.2401, 0.2558, 0.2588]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2344, 0.2780, 0.2379],
        [0.2505, 0.2360, 0.2766, 0.2369],
        [0.2425, 0.2398, 0.2462, 0.2715],
        [0.2507, 0.2450, 0.2530, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2411, 0.2394, 0.2743, 0.2452],
        [0.2416, 0.2362, 0.2779, 0.2443],
        [0.2450, 0.2452, 0.2550, 0.2547],
        [0.2430, 0.2494, 0.2540, 0.2536],
        [0.2463, 0.2506, 0.2533, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2048, 0.1782, 0.4031, 0.2138],
        [0.2085, 0.1820, 0.3985, 0.2110],
        [0.2282, 0.2217, 0.2837, 0.2664],
        [0.2335, 0.2337, 0.2537, 0.2791],
        [0.2321, 0.2362, 0.2486, 0.2831],
        [0.2335, 0.2403, 0.2559, 0.2703]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2180, 0.1761, 0.3887, 0.2172],
        [0.2157, 0.1801, 0.3889, 0.2153],
        [0.2361, 0.2225, 0.2852, 0.2563],
        [0.2314, 0.2376, 0.2628, 0.2682],
        [0.2357, 0.2407, 0.2473, 0.2763],
        [0.2351, 0.2395, 0.2507, 0.2746],
        [0.2376, 0.2457, 0.2533, 0.2633]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2415, 0.2362, 0.2753, 0.2471],
        [0.2449, 0.2344, 0.2709, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2354, 0.2668, 0.2476],
        [0.2574, 0.2361, 0.2610, 0.2455],
        [0.2532, 0.2435, 0.2479, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2408, 0.2569, 0.2523],
        [0.2513, 0.2434, 0.2570, 0.2483],
        [0.2450, 0.2448, 0.2446, 0.2656],
        [0.2507, 0.2579, 0.2554, 0.2360]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2451, 0.2586, 0.2433],
        [0.2561, 0.2445, 0.2575, 0.2420],
        [0.2462, 0.2424, 0.2386, 0.2727],
        [0.2486, 0.2478, 0.2553, 0.2484],
        [0.2464, 0.2509, 0.2579, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2616, 0.2457, 0.2561, 0.2366],
        [0.2701, 0.2481, 0.2531, 0.2286],
        [0.2452, 0.2418, 0.2424, 0.2706],
        [0.2617, 0.2333, 0.2375, 0.2675],
        [0.2560, 0.2492, 0.2566, 0.2382],
        [0.2636, 0.2387, 0.2402, 0.2575]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2459, 0.2557, 0.2388],
        [0.2685, 0.2440, 0.2526, 0.2348],
        [0.2436, 0.2410, 0.2409, 0.2745],
        [0.2624, 0.2339, 0.2358, 0.2678],
        [0.2507, 0.2511, 0.2571, 0.2410],
        [0.2595, 0.2465, 0.2550, 0.2390],
        [0.2647, 0.2403, 0.2421, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2266, 0.2807, 0.2464],
        [0.2428, 0.2242, 0.2803, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2362, 0.2692, 0.2438],
        [0.2492, 0.2346, 0.2681, 0.2481],
        [0.2505, 0.2389, 0.2532, 0.2573]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2441, 0.2612, 0.2481],
        [0.2460, 0.2415, 0.2617, 0.2508],
        [0.2490, 0.2453, 0.2537, 0.2520],
        [0.2502, 0.2464, 0.2542, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2455, 0.2573, 0.2491],
        [0.2480, 0.2444, 0.2557, 0.2519],
        [0.2486, 0.2466, 0.2529, 0.2520],
        [0.2489, 0.2482, 0.2554, 0.2475],
        [0.2510, 0.2475, 0.2520, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2726, 0.2350, 0.2524, 0.2400],
        [0.2689, 0.2313, 0.2506, 0.2491],
        [0.2541, 0.2334, 0.2599, 0.2527],
        [0.2523, 0.2306, 0.2588, 0.2584],
        [0.2542, 0.2392, 0.2549, 0.2518],
        [0.2463, 0.2391, 0.2646, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2767, 0.2339, 0.2510, 0.2384],
        [0.2744, 0.2376, 0.2514, 0.2366],
        [0.2549, 0.2355, 0.2571, 0.2525],
        [0.2506, 0.2343, 0.2610, 0.2541],
        [0.2523, 0.2378, 0.2515, 0.2584],
        [0.2426, 0.2391, 0.2628, 0.2555],
        [0.2465, 0.2394, 0.2550, 0.2591]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:05:43午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.7189 (0.7890)	Hard Loss 0.7189 (0.7890)	Soft Loss 0.7189 (0.7890)	Arch Loss 0.7596 (0.8406)	Arch Hard Loss 0.7596 (0.8406)	Arch Soft Loss 0.7596 (0.8406)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.5%, 98.1%)	
08/24 12:05:57午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.8314 (0.7993)	Hard Loss 0.8314 (0.7993)	Soft Loss 0.8314 (0.7993)	Arch Loss 0.7371 (0.8465)	Arch Hard Loss 0.7371 (0.8465)	Arch Soft Loss 0.7371 (0.8465)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.3%, 97.9%)	
08/24 12:06:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.9629 (0.8110)	Hard Loss 0.9629 (0.8110)	Soft Loss 0.9629 (0.8110)	Arch Loss 0.8403 (0.8557)	Arch Hard Loss 0.8403 (0.8557)	Arch Soft Loss 0.8403 (0.8557)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.2%, 97.9%)	
08/24 12:06:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.6680 (0.8067)	Hard Loss 0.6680 (0.8067)	Soft Loss 0.6680 (0.8067)	Arch Loss 0.9073 (0.8566)	Arch Hard Loss 0.9073 (0.8566)	Arch Soft Loss 0.9073 (0.8566)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.2%, 98.0%)	
08/24 12:06:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.6957 (0.8031)	Hard Loss 0.6957 (0.8031)	Soft Loss 0.6957 (0.8031)	Arch Loss 0.8218 (0.8569)	Arch Hard Loss 0.8218 (0.8569)	Arch Soft Loss 0.8218 (0.8569)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.1%, 98.0%)	
08/24 12:06:56午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.8419 (0.8027)	Hard Loss 0.8419 (0.8027)	Soft Loss 0.8419 (0.8027)	Arch Loss 0.9279 (0.8528)	Arch Hard Loss 0.9279 (0.8528)	Arch Soft Loss 0.9279 (0.8528)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.2%, 98.0%)	
08/24 12:07:11午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.9473 (0.7985)	Hard Loss 0.9473 (0.7985)	Soft Loss 0.9473 (0.7985)	Arch Loss 0.6027 (0.8477)	Arch Hard Loss 0.6027 (0.8477)	Arch Soft Loss 0.6027 (0.8477)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.4%, 98.0%)	
08/24 12:07:23午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.7605 (0.7950)	Hard Loss 0.7605 (0.7950)	Soft Loss 0.7605 (0.7950)	Arch Loss 0.6125 (0.8406)	Arch Hard Loss 0.6125 (0.8406)	Arch Soft Loss 0.6125 (0.8406)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (72.4%, 98.0%)	
08/24 12:07:24午前 searchStage_trainer.py:284 [INFO] Train: [  6/49] Final Prec@1 72.3560%
08/24 12:07:26午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 0.8255	Prec@(1,5) (70.2%, 97.5%)
08/24 12:07:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 0.8115	Prec@(1,5) (71.2%, 97.9%)
08/24 12:07:31午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 0.8124	Prec@(1,5) (71.2%, 98.0%)
08/24 12:07:33午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 0.8041	Prec@(1,5) (71.6%, 98.0%)
08/24 12:07:35午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 0.8059	Prec@(1,5) (71.5%, 97.9%)
08/24 12:07:37午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 0.8056	Prec@(1,5) (71.6%, 97.8%)
08/24 12:07:39午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 0.8121	Prec@(1,5) (71.4%, 97.8%)
08/24 12:07:41午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 0.8115	Prec@(1,5) (71.5%, 97.9%)
08/24 12:07:41午前 searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 71.4480%
08/24 12:07:41午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 4), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 6), ('max_pool_3x3', 1)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/24 12:07:41午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 71.4480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2232, 0.1956, 0.3373, 0.2439],
        [0.2227, 0.1924, 0.3339, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2388, 0.2153, 0.2961, 0.2497],
        [0.2414, 0.2201, 0.2968, 0.2417],
        [0.2449, 0.2412, 0.2571, 0.2568]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2369, 0.2799, 0.2372],
        [0.2473, 0.2397, 0.2785, 0.2345],
        [0.2389, 0.2415, 0.2440, 0.2756],
        [0.2466, 0.2478, 0.2554, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2415, 0.2414, 0.2719, 0.2452],
        [0.2430, 0.2392, 0.2731, 0.2446],
        [0.2442, 0.2459, 0.2573, 0.2525],
        [0.2434, 0.2502, 0.2525, 0.2539],
        [0.2438, 0.2522, 0.2519, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1951, 0.1719, 0.4236, 0.2095],
        [0.2000, 0.1758, 0.4166, 0.2076],
        [0.2223, 0.2230, 0.2905, 0.2642],
        [0.2270, 0.2347, 0.2527, 0.2856],
        [0.2299, 0.2342, 0.2467, 0.2892],
        [0.2306, 0.2411, 0.2587, 0.2696]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2075, 0.1676, 0.4110, 0.2140],
        [0.2055, 0.1712, 0.4120, 0.2113],
        [0.2340, 0.2192, 0.2900, 0.2568],
        [0.2278, 0.2349, 0.2665, 0.2708],
        [0.2374, 0.2403, 0.2444, 0.2780],
        [0.2374, 0.2395, 0.2498, 0.2734],
        [0.2382, 0.2440, 0.2513, 0.2665]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2405, 0.2350, 0.2771, 0.2474],
        [0.2418, 0.2338, 0.2753, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2354, 0.2698, 0.2457],
        [0.2566, 0.2335, 0.2627, 0.2471],
        [0.2545, 0.2436, 0.2467, 0.2552]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2425, 0.2579, 0.2489],
        [0.2496, 0.2432, 0.2585, 0.2487],
        [0.2408, 0.2456, 0.2404, 0.2731],
        [0.2468, 0.2619, 0.2595, 0.2317]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2441, 0.2591, 0.2432],
        [0.2557, 0.2415, 0.2574, 0.2454],
        [0.2468, 0.2409, 0.2335, 0.2788],
        [0.2483, 0.2505, 0.2569, 0.2444],
        [0.2485, 0.2529, 0.2585, 0.2400]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2640, 0.2450, 0.2567, 0.2344],
        [0.2727, 0.2474, 0.2526, 0.2273],
        [0.2477, 0.2417, 0.2419, 0.2687],
        [0.2624, 0.2302, 0.2318, 0.2756],
        [0.2582, 0.2503, 0.2561, 0.2354],
        [0.2641, 0.2387, 0.2392, 0.2580]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.2507, 0.2596, 0.2309],
        [0.2662, 0.2468, 0.2567, 0.2303],
        [0.2450, 0.2396, 0.2390, 0.2765],
        [0.2647, 0.2334, 0.2328, 0.2691],
        [0.2505, 0.2501, 0.2556, 0.2439],
        [0.2610, 0.2457, 0.2520, 0.2413],
        [0.2686, 0.2366, 0.2412, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2229, 0.2842, 0.2454],
        [0.2429, 0.2200, 0.2836, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2343, 0.2723, 0.2418],
        [0.2492, 0.2324, 0.2712, 0.2472],
        [0.2488, 0.2367, 0.2545, 0.2601]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2420, 0.2633, 0.2478],
        [0.2449, 0.2389, 0.2634, 0.2528],
        [0.2457, 0.2455, 0.2564, 0.2524],
        [0.2488, 0.2460, 0.2580, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2431, 0.2566, 0.2512],
        [0.2484, 0.2435, 0.2568, 0.2513],
        [0.2471, 0.2463, 0.2550, 0.2516],
        [0.2475, 0.2469, 0.2587, 0.2469],
        [0.2488, 0.2462, 0.2555, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2750, 0.2309, 0.2525, 0.2416],
        [0.2700, 0.2269, 0.2502, 0.2529],
        [0.2546, 0.2300, 0.2626, 0.2528],
        [0.2525, 0.2279, 0.2630, 0.2565],
        [0.2566, 0.2348, 0.2585, 0.2501],
        [0.2465, 0.2345, 0.2706, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2793, 0.2311, 0.2520, 0.2377],
        [0.2748, 0.2343, 0.2525, 0.2383],
        [0.2550, 0.2315, 0.2600, 0.2534],
        [0.2495, 0.2307, 0.2650, 0.2548],
        [0.2534, 0.2338, 0.2559, 0.2569],
        [0.2425, 0.2337, 0.2690, 0.2548],
        [0.2477, 0.2377, 0.2537, 0.2608]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:07:57午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 1.0735 (0.7217)	Hard Loss 1.0735 (0.7217)	Soft Loss 1.0735 (0.7217)	Arch Loss 0.6323 (0.8218)	Arch Hard Loss 0.6323 (0.8218)	Arch Soft Loss 0.6323 (0.8218)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.7%, 98.4%)	
08/24 12:08:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.9297 (0.7383)	Hard Loss 0.9297 (0.7383)	Soft Loss 0.9297 (0.7383)	Arch Loss 0.9449 (0.8061)	Arch Hard Loss 0.9449 (0.8061)	Arch Soft Loss 0.9449 (0.8061)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.1%, 98.5%)	
08/24 12:08:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 0.9071 (0.7318)	Hard Loss 0.9071 (0.7318)	Soft Loss 0.9071 (0.7318)	Arch Loss 0.6950 (0.7990)	Arch Hard Loss 0.6950 (0.7990)	Arch Soft Loss 0.6950 (0.7990)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.1%, 98.4%)	
08/24 12:08:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.9188 (0.7366)	Hard Loss 0.9188 (0.7366)	Soft Loss 0.9188 (0.7366)	Arch Loss 0.5262 (0.7984)	Arch Hard Loss 0.5262 (0.7984)	Arch Soft Loss 0.5262 (0.7984)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.2%, 98.3%)	
08/24 12:08:57午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.5884 (0.7375)	Hard Loss 0.5884 (0.7375)	Soft Loss 0.5884 (0.7375)	Arch Loss 0.7584 (0.7892)	Arch Hard Loss 0.7584 (0.7892)	Arch Soft Loss 0.7584 (0.7892)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.1%, 98.3%)	
08/24 12:09:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.7956 (0.7348)	Hard Loss 0.7956 (0.7348)	Soft Loss 0.7956 (0.7348)	Arch Loss 0.8839 (0.7910)	Arch Hard Loss 0.8839 (0.7910)	Arch Soft Loss 0.8839 (0.7910)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.2%, 98.3%)	
08/24 12:09:27午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.7702 (0.7353)	Hard Loss 0.7702 (0.7353)	Soft Loss 0.7702 (0.7353)	Arch Loss 1.0075 (0.7887)	Arch Hard Loss 1.0075 (0.7887)	Arch Soft Loss 1.0075 (0.7887)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.1%, 98.3%)	
08/24 12:09:39午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.9416 (0.7377)	Hard Loss 0.9416 (0.7377)	Soft Loss 0.9416 (0.7377)	Arch Loss 0.7059 (0.7836)	Arch Hard Loss 0.7059 (0.7836)	Arch Soft Loss 0.7059 (0.7836)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (74.1%, 98.3%)	
08/24 12:09:40午前 searchStage_trainer.py:284 [INFO] Train: [  7/49] Final Prec@1 74.0560%
08/24 12:09:42午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.7904	Prec@(1,5) (73.2%, 97.6%)
08/24 12:09:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.7811	Prec@(1,5) (73.0%, 98.0%)
08/24 12:09:47午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.7929	Prec@(1,5) (72.6%, 97.8%)
08/24 12:09:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.8009	Prec@(1,5) (72.1%, 97.9%)
08/24 12:09:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.7990	Prec@(1,5) (72.2%, 97.9%)
08/24 12:09:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.7992	Prec@(1,5) (72.2%, 97.9%)
08/24 12:09:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.8010	Prec@(1,5) (72.1%, 97.9%)
08/24 12:09:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.7970	Prec@(1,5) (72.2%, 98.0%)
08/24 12:09:57午前 searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 72.1760%
08/24 12:09:57午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 3)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 5), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], DAG3_concat=range(6, 8))
08/24 12:09:57午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 72.1760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2213, 0.1943, 0.3418, 0.2426],
        [0.2199, 0.1913, 0.3375, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2348, 0.2145, 0.2986, 0.2521],
        [0.2389, 0.2188, 0.2991, 0.2432],
        [0.2439, 0.2433, 0.2594, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2354, 0.2794, 0.2363],
        [0.2493, 0.2374, 0.2795, 0.2338],
        [0.2414, 0.2393, 0.2432, 0.2761],
        [0.2486, 0.2459, 0.2534, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.2408, 0.2691, 0.2466],
        [0.2439, 0.2393, 0.2704, 0.2464],
        [0.2452, 0.2470, 0.2572, 0.2507],
        [0.2418, 0.2499, 0.2521, 0.2561],
        [0.2450, 0.2530, 0.2528, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1875, 0.1667, 0.4386, 0.2072],
        [0.1923, 0.1712, 0.4322, 0.2044],
        [0.2200, 0.2223, 0.2936, 0.2641],
        [0.2277, 0.2341, 0.2493, 0.2888],
        [0.2288, 0.2328, 0.2471, 0.2913],
        [0.2310, 0.2415, 0.2591, 0.2685]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2006, 0.1617, 0.4261, 0.2116],
        [0.1975, 0.1650, 0.4297, 0.2078],
        [0.2327, 0.2157, 0.2928, 0.2589],
        [0.2284, 0.2326, 0.2670, 0.2720],
        [0.2418, 0.2372, 0.2422, 0.2788],
        [0.2413, 0.2397, 0.2485, 0.2705],
        [0.2414, 0.2415, 0.2482, 0.2689]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2406, 0.2313, 0.2789, 0.2492],
        [0.2440, 0.2311, 0.2774, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2331, 0.2723, 0.2434],
        [0.2574, 0.2310, 0.2659, 0.2457],
        [0.2549, 0.2422, 0.2451, 0.2578]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2391, 0.2594, 0.2488],
        [0.2528, 0.2389, 0.2590, 0.2493],
        [0.2412, 0.2450, 0.2408, 0.2730],
        [0.2450, 0.2616, 0.2615, 0.2319]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2404, 0.2602, 0.2470],
        [0.2561, 0.2386, 0.2582, 0.2472],
        [0.2497, 0.2426, 0.2380, 0.2697],
        [0.2507, 0.2491, 0.2580, 0.2422],
        [0.2490, 0.2495, 0.2555, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.2403, 0.2581, 0.2325],
        [0.2770, 0.2428, 0.2542, 0.2260],
        [0.2464, 0.2399, 0.2427, 0.2711],
        [0.2639, 0.2260, 0.2292, 0.2809],
        [0.2626, 0.2455, 0.2567, 0.2352],
        [0.2704, 0.2342, 0.2399, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2634, 0.2483, 0.2599, 0.2284],
        [0.2699, 0.2438, 0.2575, 0.2288],
        [0.2467, 0.2375, 0.2379, 0.2779],
        [0.2651, 0.2307, 0.2308, 0.2735],
        [0.2515, 0.2469, 0.2546, 0.2471],
        [0.2636, 0.2432, 0.2500, 0.2431],
        [0.2706, 0.2388, 0.2431, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2181, 0.2878, 0.2471],
        [0.2459, 0.2157, 0.2868, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2306, 0.2765, 0.2424],
        [0.2488, 0.2288, 0.2749, 0.2474],
        [0.2479, 0.2362, 0.2566, 0.2592]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2426, 0.2647, 0.2470],
        [0.2432, 0.2391, 0.2646, 0.2532],
        [0.2451, 0.2456, 0.2586, 0.2508],
        [0.2457, 0.2457, 0.2596, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2412, 0.2572, 0.2530],
        [0.2485, 0.2412, 0.2574, 0.2529],
        [0.2483, 0.2448, 0.2575, 0.2493],
        [0.2485, 0.2453, 0.2594, 0.2468],
        [0.2510, 0.2456, 0.2551, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2737, 0.2254, 0.2531, 0.2478],
        [0.2684, 0.2214, 0.2500, 0.2603],
        [0.2548, 0.2268, 0.2694, 0.2489],
        [0.2483, 0.2255, 0.2699, 0.2563],
        [0.2574, 0.2342, 0.2650, 0.2434],
        [0.2454, 0.2314, 0.2787, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2774, 0.2260, 0.2537, 0.2429],
        [0.2738, 0.2291, 0.2542, 0.2429],
        [0.2544, 0.2293, 0.2654, 0.2509],
        [0.2456, 0.2277, 0.2695, 0.2572],
        [0.2549, 0.2342, 0.2628, 0.2481],
        [0.2421, 0.2314, 0.2753, 0.2512],
        [0.2480, 0.2357, 0.2555, 0.2608]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:10:13午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.6651 (0.6955)	Hard Loss 0.6651 (0.6955)	Soft Loss 0.6651 (0.6955)	Arch Loss 0.9074 (0.7405)	Arch Hard Loss 0.9074 (0.7405)	Arch Soft Loss 0.9074 (0.7405)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.7%, 98.8%)	
08/24 12:10:29午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.8579 (0.7063)	Hard Loss 0.8579 (0.7063)	Soft Loss 0.8579 (0.7063)	Arch Loss 0.7198 (0.7436)	Arch Hard Loss 0.7198 (0.7436)	Arch Soft Loss 0.7198 (0.7436)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.3%, 98.7%)	
08/24 12:10:44午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.6183 (0.6966)	Hard Loss 0.6183 (0.6966)	Soft Loss 0.6183 (0.6966)	Arch Loss 0.7720 (0.7391)	Arch Hard Loss 0.7720 (0.7391)	Arch Soft Loss 0.7720 (0.7391)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.7%, 98.6%)	
08/24 12:10:58午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.4660 (0.6974)	Hard Loss 0.4660 (0.6974)	Soft Loss 0.4660 (0.6974)	Arch Loss 0.8219 (0.7374)	Arch Hard Loss 0.8219 (0.7374)	Arch Soft Loss 0.8219 (0.7374)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.7%, 98.6%)	
08/24 12:11:13午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.4804 (0.6936)	Hard Loss 0.4804 (0.6936)	Soft Loss 0.4804 (0.6936)	Arch Loss 0.4200 (0.7337)	Arch Hard Loss 0.4200 (0.7337)	Arch Soft Loss 0.4200 (0.7337)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.8%, 98.6%)	
08/24 12:11:28午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.8215 (0.6882)	Hard Loss 0.8215 (0.6882)	Soft Loss 0.8215 (0.6882)	Arch Loss 0.7286 (0.7365)	Arch Hard Loss 0.7286 (0.7365)	Arch Soft Loss 0.7286 (0.7365)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (76.0%, 98.6%)	
08/24 12:11:42午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.6929 (0.6882)	Hard Loss 0.6929 (0.6882)	Soft Loss 0.6929 (0.6882)	Arch Loss 0.4649 (0.7356)	Arch Hard Loss 0.4649 (0.7356)	Arch Soft Loss 0.4649 (0.7356)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.9%, 98.6%)	
08/24 12:11:54午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.6587 (0.6879)	Hard Loss 0.6587 (0.6879)	Soft Loss 0.6587 (0.6879)	Arch Loss 0.7723 (0.7361)	Arch Hard Loss 0.7723 (0.7361)	Arch Soft Loss 0.7723 (0.7361)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (75.9%, 98.6%)	
08/24 12:11:55午前 searchStage_trainer.py:284 [INFO] Train: [  8/49] Final Prec@1 75.9320%
08/24 12:11:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.7579	Prec@(1,5) (74.2%, 98.2%)
08/24 12:11:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.7683	Prec@(1,5) (73.9%, 98.2%)
08/24 12:12:02午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.7733	Prec@(1,5) (73.6%, 98.2%)
08/24 12:12:04午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.7759	Prec@(1,5) (73.3%, 98.2%)
08/24 12:12:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.7708	Prec@(1,5) (73.6%, 98.2%)
08/24 12:12:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.7711	Prec@(1,5) (73.5%, 98.2%)
08/24 12:12:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.7755	Prec@(1,5) (73.4%, 98.2%)
08/24 12:12:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.7738	Prec@(1,5) (73.5%, 98.2%)
08/24 12:12:12午前 searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 73.4760%
08/24 12:12:12午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 3)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 1)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 1)], [('skip_connect', 5), ('max_pool_3x3', 0)], [('skip_connect', 5), ('max_pool_3x3', 0)]], DAG3_concat=range(6, 8))
08/24 12:12:13午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 73.4760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2183, 0.1936, 0.3452, 0.2430],
        [0.2172, 0.1926, 0.3404, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2154, 0.2934, 0.2550],
        [0.2409, 0.2215, 0.2952, 0.2424],
        [0.2467, 0.2421, 0.2589, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2358, 0.2749, 0.2353],
        [0.2533, 0.2380, 0.2749, 0.2338],
        [0.2418, 0.2379, 0.2456, 0.2747],
        [0.2487, 0.2451, 0.2504, 0.2558]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2424, 0.2442, 0.2692, 0.2442],
        [0.2419, 0.2416, 0.2705, 0.2461],
        [0.2448, 0.2446, 0.2556, 0.2549],
        [0.2400, 0.2507, 0.2521, 0.2572],
        [0.2435, 0.2558, 0.2549, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1829, 0.1601, 0.4548, 0.2022],
        [0.1879, 0.1651, 0.4474, 0.1995],
        [0.2213, 0.2200, 0.2917, 0.2670],
        [0.2281, 0.2349, 0.2501, 0.2869],
        [0.2296, 0.2303, 0.2425, 0.2976],
        [0.2335, 0.2391, 0.2549, 0.2724]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1943, 0.1594, 0.4390, 0.2074],
        [0.1903, 0.1619, 0.4442, 0.2037],
        [0.2291, 0.2146, 0.2920, 0.2643],
        [0.2294, 0.2336, 0.2667, 0.2704],
        [0.2414, 0.2375, 0.2416, 0.2794],
        [0.2439, 0.2379, 0.2444, 0.2738],
        [0.2402, 0.2410, 0.2497, 0.2691]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2396, 0.2302, 0.2816, 0.2485],
        [0.2426, 0.2296, 0.2800, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2321, 0.2724, 0.2438],
        [0.2552, 0.2287, 0.2675, 0.2486],
        [0.2569, 0.2417, 0.2458, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2364, 0.2623, 0.2495],
        [0.2502, 0.2372, 0.2640, 0.2485],
        [0.2416, 0.2448, 0.2425, 0.2711],
        [0.2446, 0.2611, 0.2612, 0.2331]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2407, 0.2621, 0.2448],
        [0.2518, 0.2380, 0.2639, 0.2464],
        [0.2483, 0.2419, 0.2391, 0.2707],
        [0.2500, 0.2493, 0.2574, 0.2432],
        [0.2502, 0.2479, 0.2558, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2702, 0.2405, 0.2612, 0.2281],
        [0.2777, 0.2427, 0.2576, 0.2219],
        [0.2461, 0.2384, 0.2428, 0.2727],
        [0.2626, 0.2254, 0.2283, 0.2837],
        [0.2594, 0.2448, 0.2568, 0.2391],
        [0.2744, 0.2318, 0.2391, 0.2547]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2614, 0.2445, 0.2637, 0.2304],
        [0.2689, 0.2397, 0.2606, 0.2309],
        [0.2454, 0.2378, 0.2401, 0.2768],
        [0.2641, 0.2319, 0.2314, 0.2726],
        [0.2470, 0.2485, 0.2569, 0.2475],
        [0.2647, 0.2436, 0.2505, 0.2412],
        [0.2735, 0.2368, 0.2418, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2426, 0.2154, 0.2961, 0.2460],
        [0.2401, 0.2132, 0.2950, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2282, 0.2824, 0.2424],
        [0.2447, 0.2253, 0.2802, 0.2499],
        [0.2442, 0.2357, 0.2627, 0.2574]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2417, 0.2663, 0.2453],
        [0.2436, 0.2369, 0.2657, 0.2538],
        [0.2446, 0.2437, 0.2612, 0.2506],
        [0.2448, 0.2452, 0.2597, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2402, 0.2602, 0.2521],
        [0.2471, 0.2402, 0.2604, 0.2522],
        [0.2469, 0.2431, 0.2585, 0.2515],
        [0.2460, 0.2450, 0.2636, 0.2454],
        [0.2481, 0.2461, 0.2571, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2720, 0.2226, 0.2567, 0.2487],
        [0.2658, 0.2183, 0.2537, 0.2622],
        [0.2522, 0.2248, 0.2742, 0.2488],
        [0.2427, 0.2225, 0.2738, 0.2609],
        [0.2543, 0.2334, 0.2726, 0.2397],
        [0.2433, 0.2302, 0.2849, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2747, 0.2227, 0.2582, 0.2444],
        [0.2707, 0.2254, 0.2593, 0.2446],
        [0.2508, 0.2275, 0.2712, 0.2506],
        [0.2408, 0.2256, 0.2749, 0.2587],
        [0.2516, 0.2338, 0.2705, 0.2441],
        [0.2388, 0.2301, 0.2820, 0.2490],
        [0.2470, 0.2345, 0.2569, 0.2616]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:12:29午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.5123 (0.6317)	Hard Loss 0.5123 (0.6317)	Soft Loss 0.5123 (0.6317)	Arch Loss 0.9692 (0.7351)	Arch Hard Loss 0.9692 (0.7351)	Arch Soft Loss 0.9692 (0.7351)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.7%, 99.0%)	
08/24 12:12:44午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.7699 (0.6268)	Hard Loss 0.7699 (0.6268)	Soft Loss 0.7699 (0.6268)	Arch Loss 0.6564 (0.7382)	Arch Hard Loss 0.6564 (0.7382)	Arch Soft Loss 0.6564 (0.7382)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.8%, 99.0%)	
08/24 12:12:59午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.6266 (0.6366)	Hard Loss 0.6266 (0.6366)	Soft Loss 0.6266 (0.6366)	Arch Loss 0.5218 (0.7357)	Arch Hard Loss 0.5218 (0.7357)	Arch Soft Loss 0.5218 (0.7357)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.5%, 98.9%)	
08/24 12:13:14午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.9329 (0.6396)	Hard Loss 0.9329 (0.6396)	Soft Loss 0.9329 (0.6396)	Arch Loss 0.7844 (0.7221)	Arch Hard Loss 0.7844 (0.7221)	Arch Soft Loss 0.7844 (0.7221)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.4%, 98.8%)	
08/24 12:13:29午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.7994 (0.6434)	Hard Loss 0.7994 (0.6434)	Soft Loss 0.7994 (0.6434)	Arch Loss 0.6816 (0.7153)	Arch Hard Loss 0.6816 (0.7153)	Arch Soft Loss 0.6816 (0.7153)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.4%, 98.7%)	
08/24 12:13:46午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.4705 (0.6429)	Hard Loss 0.4705 (0.6429)	Soft Loss 0.4705 (0.6429)	Arch Loss 0.8194 (0.7145)	Arch Hard Loss 0.8194 (0.7145)	Arch Soft Loss 0.8194 (0.7145)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.5%, 98.7%)	
08/24 12:14:02午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.7251 (0.6434)	Hard Loss 0.7251 (0.6434)	Soft Loss 0.7251 (0.6434)	Arch Loss 0.8219 (0.7103)	Arch Hard Loss 0.8219 (0.7103)	Arch Soft Loss 0.8219 (0.7103)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.4%, 98.7%)	
08/24 12:14:14午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.5937 (0.6438)	Hard Loss 0.5937 (0.6438)	Soft Loss 0.5937 (0.6438)	Arch Loss 0.6137 (0.7057)	Arch Hard Loss 0.6137 (0.7057)	Arch Soft Loss 0.6137 (0.7057)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (77.5%, 98.7%)	
08/24 12:14:14午前 searchStage_trainer.py:284 [INFO] Train: [  9/49] Final Prec@1 77.5120%
08/24 12:14:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.7052	Prec@(1,5) (76.1%, 98.2%)
08/24 12:14:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.7097	Prec@(1,5) (75.9%, 98.1%)
08/24 12:14:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.7034	Prec@(1,5) (76.1%, 98.2%)
08/24 12:14:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.7061	Prec@(1,5) (76.0%, 98.1%)
08/24 12:14:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.6979	Prec@(1,5) (76.2%, 98.2%)
08/24 12:14:28午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.6917	Prec@(1,5) (76.4%, 98.3%)
08/24 12:14:30午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.6968	Prec@(1,5) (76.2%, 98.3%)
08/24 12:14:32午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.6976	Prec@(1,5) (76.2%, 98.3%)
08/24 12:14:32午前 searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 76.2160%
08/24 12:14:32午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 0)], [('skip_connect', 5), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 3)]], DAG3_concat=range(6, 8))
08/24 12:14:32午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 76.2160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2175, 0.1932, 0.3471, 0.2422],
        [0.2167, 0.1922, 0.3412, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2335, 0.2176, 0.2944, 0.2546],
        [0.2376, 0.2232, 0.2951, 0.2441],
        [0.2445, 0.2433, 0.2615, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2368, 0.2773, 0.2332],
        [0.2511, 0.2368, 0.2768, 0.2354],
        [0.2424, 0.2387, 0.2451, 0.2737],
        [0.2450, 0.2472, 0.2500, 0.2577]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2445, 0.2655, 0.2452],
        [0.2447, 0.2421, 0.2663, 0.2468],
        [0.2457, 0.2446, 0.2558, 0.2539],
        [0.2385, 0.2506, 0.2535, 0.2574],
        [0.2453, 0.2540, 0.2548, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1762, 0.1556, 0.4672, 0.2010],
        [0.1808, 0.1610, 0.4599, 0.1983],
        [0.2172, 0.2196, 0.2931, 0.2701],
        [0.2274, 0.2360, 0.2519, 0.2847],
        [0.2290, 0.2324, 0.2451, 0.2936],
        [0.2315, 0.2395, 0.2568, 0.2722]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1873, 0.1579, 0.4522, 0.2025],
        [0.1834, 0.1604, 0.4572, 0.1991],
        [0.2214, 0.2153, 0.2937, 0.2696],
        [0.2246, 0.2355, 0.2701, 0.2697],
        [0.2391, 0.2408, 0.2432, 0.2769],
        [0.2418, 0.2380, 0.2409, 0.2793],
        [0.2371, 0.2400, 0.2521, 0.2708]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2287, 0.2782, 0.2493],
        [0.2462, 0.2281, 0.2775, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2318, 0.2680, 0.2464],
        [0.2541, 0.2286, 0.2652, 0.2521],
        [0.2568, 0.2432, 0.2482, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2387, 0.2597, 0.2470],
        [0.2510, 0.2387, 0.2623, 0.2479],
        [0.2400, 0.2432, 0.2394, 0.2774],
        [0.2410, 0.2651, 0.2634, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2416, 0.2619, 0.2459],
        [0.2498, 0.2391, 0.2627, 0.2485],
        [0.2459, 0.2447, 0.2422, 0.2673],
        [0.2485, 0.2526, 0.2556, 0.2433],
        [0.2515, 0.2494, 0.2530, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2704, 0.2403, 0.2594, 0.2298],
        [0.2784, 0.2421, 0.2569, 0.2226],
        [0.2417, 0.2413, 0.2430, 0.2740],
        [0.2654, 0.2271, 0.2270, 0.2805],
        [0.2612, 0.2444, 0.2544, 0.2400],
        [0.2739, 0.2334, 0.2394, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2431, 0.2658, 0.2315],
        [0.2662, 0.2401, 0.2624, 0.2314],
        [0.2436, 0.2397, 0.2411, 0.2756],
        [0.2663, 0.2336, 0.2312, 0.2688],
        [0.2451, 0.2508, 0.2563, 0.2478],
        [0.2693, 0.2407, 0.2468, 0.2432],
        [0.2709, 0.2377, 0.2427, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2389, 0.2143, 0.3034, 0.2434],
        [0.2350, 0.2108, 0.3009, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2285, 0.2886, 0.2382],
        [0.2407, 0.2242, 0.2853, 0.2498],
        [0.2399, 0.2348, 0.2646, 0.2607]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.2436, 0.2710, 0.2421],
        [0.2397, 0.2347, 0.2691, 0.2565],
        [0.2412, 0.2439, 0.2632, 0.2516],
        [0.2437, 0.2446, 0.2623, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2406, 0.2620, 0.2510],
        [0.2460, 0.2391, 0.2609, 0.2539],
        [0.2441, 0.2453, 0.2612, 0.2495],
        [0.2441, 0.2450, 0.2657, 0.2452],
        [0.2455, 0.2446, 0.2598, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2670, 0.2208, 0.2643, 0.2480],
        [0.2604, 0.2150, 0.2591, 0.2655],
        [0.2486, 0.2214, 0.2804, 0.2496],
        [0.2390, 0.2186, 0.2789, 0.2635],
        [0.2517, 0.2308, 0.2789, 0.2386],
        [0.2417, 0.2287, 0.2923, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.2215, 0.2650, 0.2432],
        [0.2670, 0.2230, 0.2643, 0.2457],
        [0.2462, 0.2248, 0.2761, 0.2529],
        [0.2377, 0.2234, 0.2796, 0.2594],
        [0.2500, 0.2301, 0.2748, 0.2451],
        [0.2374, 0.2286, 0.2874, 0.2467],
        [0.2499, 0.2339, 0.2565, 0.2596]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:14:50午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.6111 (0.5971)	Hard Loss 0.6111 (0.5971)	Soft Loss 0.6111 (0.5971)	Arch Loss 0.5206 (0.6371)	Arch Hard Loss 0.5206 (0.6371)	Arch Soft Loss 0.5206 (0.6371)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.2%, 99.3%)	
08/24 12:15:06午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.6249 (0.5911)	Hard Loss 0.6249 (0.5911)	Soft Loss 0.6249 (0.5911)	Arch Loss 0.6253 (0.6524)	Arch Hard Loss 0.6253 (0.6524)	Arch Soft Loss 0.6253 (0.6524)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.7%, 99.1%)	
08/24 12:15:23午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.5447 (0.5892)	Hard Loss 0.5447 (0.5892)	Soft Loss 0.5447 (0.5892)	Arch Loss 0.4289 (0.6563)	Arch Hard Loss 0.4289 (0.6563)	Arch Soft Loss 0.4289 (0.6563)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.4%, 99.0%)	
08/24 12:15:38午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.5577 (0.5898)	Hard Loss 0.5577 (0.5898)	Soft Loss 0.5577 (0.5898)	Arch Loss 0.8229 (0.6616)	Arch Hard Loss 0.8229 (0.6616)	Arch Soft Loss 0.8229 (0.6616)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.4%, 98.9%)	
08/24 12:15:53午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.4293 (0.5941)	Hard Loss 0.4293 (0.5941)	Soft Loss 0.4293 (0.5941)	Arch Loss 0.5804 (0.6619)	Arch Hard Loss 0.5804 (0.6619)	Arch Soft Loss 0.5804 (0.6619)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.2%, 98.9%)	
08/24 12:16:11午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.5382 (0.5950)	Hard Loss 0.5382 (0.5950)	Soft Loss 0.5382 (0.5950)	Arch Loss 0.9871 (0.6591)	Arch Hard Loss 0.9871 (0.6591)	Arch Soft Loss 0.9871 (0.6591)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.2%, 98.9%)	
08/24 12:16:26午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.5620 (0.5956)	Hard Loss 0.5620 (0.5956)	Soft Loss 0.5620 (0.5956)	Arch Loss 0.4674 (0.6566)	Arch Hard Loss 0.4674 (0.6566)	Arch Soft Loss 0.4674 (0.6566)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.2%, 98.9%)	
08/24 12:16:41午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.3446 (0.5952)	Hard Loss 0.3446 (0.5952)	Soft Loss 0.3446 (0.5952)	Arch Loss 0.6088 (0.6555)	Arch Hard Loss 0.6088 (0.6555)	Arch Soft Loss 0.6088 (0.6555)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.2%, 98.9%)	
08/24 12:16:42午前 searchStage_trainer.py:284 [INFO] Train: [ 10/49] Final Prec@1 79.2400%
08/24 12:16:44午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.6390	Prec@(1,5) (78.0%, 98.7%)
08/24 12:16:46午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.6575	Prec@(1,5) (77.3%, 98.5%)
08/24 12:16:49午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.6415	Prec@(1,5) (77.8%, 98.6%)
08/24 12:16:51午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.6395	Prec@(1,5) (78.1%, 98.7%)
08/24 12:16:53午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.6377	Prec@(1,5) (78.2%, 98.6%)
08/24 12:16:55午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.6397	Prec@(1,5) (78.0%, 98.7%)
08/24 12:16:57午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.6358	Prec@(1,5) (78.1%, 98.6%)
08/24 12:16:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.6331	Prec@(1,5) (78.2%, 98.7%)
08/24 12:16:59午前 searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 78.1520%
08/24 12:16:59午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 1), ('max_pool_3x3', 3)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 3)]], DAG3_concat=range(6, 8))
08/24 12:16:59午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 78.1520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2180, 0.1936, 0.3461, 0.2422],
        [0.2177, 0.1932, 0.3396, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2339, 0.2184, 0.2922, 0.2555],
        [0.2382, 0.2233, 0.2907, 0.2478],
        [0.2454, 0.2441, 0.2632, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2384, 0.2759, 0.2359],
        [0.2485, 0.2389, 0.2747, 0.2379],
        [0.2412, 0.2428, 0.2482, 0.2678],
        [0.2421, 0.2472, 0.2519, 0.2589]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2462, 0.2642, 0.2446],
        [0.2441, 0.2449, 0.2654, 0.2456],
        [0.2461, 0.2462, 0.2528, 0.2550],
        [0.2391, 0.2513, 0.2546, 0.2549],
        [0.2434, 0.2536, 0.2535, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1717, 0.1499, 0.4750, 0.2034],
        [0.1764, 0.1556, 0.4674, 0.2005],
        [0.2170, 0.2222, 0.2944, 0.2664],
        [0.2293, 0.2403, 0.2512, 0.2792],
        [0.2323, 0.2307, 0.2447, 0.2923],
        [0.2334, 0.2415, 0.2573, 0.2678]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1842, 0.1552, 0.4579, 0.2027],
        [0.1807, 0.1576, 0.4624, 0.1993],
        [0.2211, 0.2158, 0.2942, 0.2689],
        [0.2276, 0.2360, 0.2679, 0.2686],
        [0.2388, 0.2413, 0.2439, 0.2760],
        [0.2399, 0.2375, 0.2384, 0.2843],
        [0.2377, 0.2413, 0.2557, 0.2652]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2265, 0.2802, 0.2501],
        [0.2465, 0.2256, 0.2807, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2338, 0.2659, 0.2448],
        [0.2561, 0.2309, 0.2640, 0.2489],
        [0.2550, 0.2426, 0.2470, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2563, 0.2390, 0.2598, 0.2449],
        [0.2540, 0.2381, 0.2623, 0.2455],
        [0.2372, 0.2434, 0.2396, 0.2797],
        [0.2397, 0.2654, 0.2624, 0.2325]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2417, 0.2639, 0.2432],
        [0.2510, 0.2401, 0.2633, 0.2456],
        [0.2425, 0.2450, 0.2451, 0.2674],
        [0.2480, 0.2525, 0.2562, 0.2433],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2732, 0.2375, 0.2588, 0.2304],
        [0.2808, 0.2411, 0.2571, 0.2209],
        [0.2392, 0.2409, 0.2443, 0.2756],
        [0.2681, 0.2290, 0.2301, 0.2728],
        [0.2609, 0.2424, 0.2519, 0.2448],
        [0.2761, 0.2320, 0.2370, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2614, 0.2448, 0.2652, 0.2286],
        [0.2686, 0.2414, 0.2619, 0.2281],
        [0.2414, 0.2371, 0.2397, 0.2818],
        [0.2686, 0.2358, 0.2339, 0.2617],
        [0.2420, 0.2520, 0.2546, 0.2514],
        [0.2678, 0.2419, 0.2466, 0.2437],
        [0.2656, 0.2393, 0.2432, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2356, 0.2147, 0.3048, 0.2449],
        [0.2334, 0.2122, 0.3032, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2423, 0.2272, 0.2886, 0.2419],
        [0.2382, 0.2244, 0.2859, 0.2515],
        [0.2421, 0.2374, 0.2646, 0.2559]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2412, 0.2431, 0.2722, 0.2435],
        [0.2386, 0.2355, 0.2700, 0.2559],
        [0.2414, 0.2438, 0.2632, 0.2517],
        [0.2422, 0.2452, 0.2645, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2381, 0.2618, 0.2549],
        [0.2449, 0.2372, 0.2609, 0.2570],
        [0.2445, 0.2470, 0.2624, 0.2461],
        [0.2455, 0.2453, 0.2676, 0.2416],
        [0.2439, 0.2449, 0.2610, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2195, 0.2672, 0.2512],
        [0.2554, 0.2128, 0.2616, 0.2702],
        [0.2457, 0.2217, 0.2851, 0.2476],
        [0.2348, 0.2200, 0.2838, 0.2615],
        [0.2492, 0.2320, 0.2813, 0.2375],
        [0.2406, 0.2299, 0.2970, 0.2325]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.2192, 0.2686, 0.2466],
        [0.2616, 0.2201, 0.2670, 0.2513],
        [0.2460, 0.2248, 0.2815, 0.2477],
        [0.2345, 0.2237, 0.2860, 0.2558],
        [0.2476, 0.2303, 0.2773, 0.2448],
        [0.2354, 0.2284, 0.2926, 0.2436],
        [0.2495, 0.2358, 0.2568, 0.2579]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:17:15午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.4703 (0.5473)	Hard Loss 0.4703 (0.5473)	Soft Loss 0.4703 (0.5473)	Arch Loss 0.4799 (0.6397)	Arch Hard Loss 0.4799 (0.6397)	Arch Soft Loss 0.4799 (0.6397)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.2%, 99.1%)	
08/24 12:17:30午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.6537 (0.5803)	Hard Loss 0.6537 (0.5803)	Soft Loss 0.6537 (0.5803)	Arch Loss 0.6516 (0.6440)	Arch Hard Loss 0.6516 (0.6440)	Arch Soft Loss 0.6516 (0.6440)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.7%, 98.9%)	
08/24 12:17:44午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.5645 (0.5874)	Hard Loss 0.5645 (0.5874)	Soft Loss 0.5645 (0.5874)	Arch Loss 0.6709 (0.6426)	Arch Hard Loss 0.6709 (0.6426)	Arch Soft Loss 0.6709 (0.6426)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.6%, 98.9%)	
08/24 12:17:59午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.5104 (0.5824)	Hard Loss 0.5104 (0.5824)	Soft Loss 0.5104 (0.5824)	Arch Loss 0.7380 (0.6510)	Arch Hard Loss 0.7380 (0.6510)	Arch Soft Loss 0.7380 (0.6510)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.6%, 99.0%)	
08/24 12:18:14午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.6212 (0.5773)	Hard Loss 0.6212 (0.5773)	Soft Loss 0.6212 (0.5773)	Arch Loss 0.7467 (0.6567)	Arch Hard Loss 0.7467 (0.6567)	Arch Soft Loss 0.7467 (0.6567)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.8%, 99.0%)	
08/24 12:18:29午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.4685 (0.5790)	Hard Loss 0.4685 (0.5790)	Soft Loss 0.4685 (0.5790)	Arch Loss 0.9622 (0.6638)	Arch Hard Loss 0.9622 (0.6638)	Arch Soft Loss 0.9622 (0.6638)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.7%, 99.1%)	
08/24 12:18:44午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.6195 (0.5798)	Hard Loss 0.6195 (0.5798)	Soft Loss 0.6195 (0.5798)	Arch Loss 0.5273 (0.6615)	Arch Hard Loss 0.5273 (0.6615)	Arch Soft Loss 0.5273 (0.6615)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.7%, 99.0%)	
08/24 12:18:56午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.5155 (0.5771)	Hard Loss 0.5155 (0.5771)	Soft Loss 0.5155 (0.5771)	Arch Loss 0.8784 (0.6602)	Arch Hard Loss 0.8784 (0.6602)	Arch Soft Loss 0.8784 (0.6602)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (79.8%, 99.1%)	
08/24 12:18:57午前 searchStage_trainer.py:284 [INFO] Train: [ 11/49] Final Prec@1 79.8120%
08/24 12:18:59午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.6544	Prec@(1,5) (77.2%, 98.7%)
08/24 12:19:01午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.6655	Prec@(1,5) (76.9%, 98.5%)
08/24 12:19:03午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.6619	Prec@(1,5) (77.1%, 98.4%)
08/24 12:19:06午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.6519	Prec@(1,5) (77.4%, 98.5%)
08/24 12:19:08午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.6434	Prec@(1,5) (77.8%, 98.6%)
08/24 12:19:10午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.6444	Prec@(1,5) (77.9%, 98.6%)
08/24 12:19:12午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.6425	Prec@(1,5) (78.1%, 98.6%)
08/24 12:19:14午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.6431	Prec@(1,5) (78.0%, 98.6%)
08/24 12:19:14午前 searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 78.0240%
08/24 12:19:14午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 3), ('skip_connect', 0)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 3)]], DAG3_concat=range(6, 8))
08/24 12:19:14午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 78.1520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2174, 0.1948, 0.3463, 0.2415],
        [0.2160, 0.1944, 0.3397, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2315, 0.2193, 0.2920, 0.2572],
        [0.2373, 0.2246, 0.2886, 0.2496],
        [0.2461, 0.2466, 0.2627, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2384, 0.2711, 0.2384],
        [0.2502, 0.2391, 0.2698, 0.2408],
        [0.2441, 0.2444, 0.2502, 0.2613],
        [0.2422, 0.2451, 0.2519, 0.2608]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2453, 0.2629, 0.2450],
        [0.2470, 0.2449, 0.2638, 0.2443],
        [0.2497, 0.2439, 0.2515, 0.2550],
        [0.2431, 0.2492, 0.2540, 0.2537],
        [0.2445, 0.2515, 0.2509, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1685, 0.1455, 0.4871, 0.1989],
        [0.1730, 0.1505, 0.4787, 0.1978],
        [0.2157, 0.2207, 0.2932, 0.2704],
        [0.2275, 0.2392, 0.2538, 0.2796],
        [0.2297, 0.2305, 0.2460, 0.2937],
        [0.2353, 0.2410, 0.2559, 0.2678]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1813, 0.1531, 0.4679, 0.1978],
        [0.1769, 0.1550, 0.4727, 0.1954],
        [0.2190, 0.2156, 0.2916, 0.2738],
        [0.2296, 0.2317, 0.2640, 0.2747],
        [0.2386, 0.2375, 0.2447, 0.2793],
        [0.2406, 0.2362, 0.2376, 0.2856],
        [0.2384, 0.2401, 0.2595, 0.2621]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2227, 0.2787, 0.2492],
        [0.2503, 0.2211, 0.2795, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.2327, 0.2637, 0.2447],
        [0.2563, 0.2294, 0.2637, 0.2507],
        [0.2587, 0.2427, 0.2444, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2406, 0.2584, 0.2402],
        [0.2579, 0.2383, 0.2610, 0.2427],
        [0.2360, 0.2397, 0.2352, 0.2891],
        [0.2400, 0.2652, 0.2632, 0.2316]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2407, 0.2650, 0.2410],
        [0.2538, 0.2370, 0.2642, 0.2451],
        [0.2407, 0.2453, 0.2457, 0.2682],
        [0.2446, 0.2545, 0.2579, 0.2430],
        [0.2464, 0.2491, 0.2518, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.2386, 0.2628, 0.2277],
        [0.2773, 0.2397, 0.2613, 0.2217],
        [0.2416, 0.2386, 0.2447, 0.2751],
        [0.2671, 0.2303, 0.2312, 0.2714],
        [0.2626, 0.2421, 0.2522, 0.2432],
        [0.2749, 0.2289, 0.2352, 0.2609]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2619, 0.2451, 0.2688, 0.2243],
        [0.2648, 0.2436, 0.2678, 0.2238],
        [0.2422, 0.2367, 0.2405, 0.2806],
        [0.2699, 0.2357, 0.2359, 0.2584],
        [0.2414, 0.2505, 0.2532, 0.2548],
        [0.2687, 0.2386, 0.2453, 0.2475],
        [0.2645, 0.2375, 0.2413, 0.2568]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2348, 0.2132, 0.3085, 0.2434],
        [0.2313, 0.2105, 0.3058, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2392, 0.2268, 0.2918, 0.2422],
        [0.2351, 0.2241, 0.2898, 0.2510],
        [0.2397, 0.2375, 0.2674, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2380, 0.2415, 0.2747, 0.2458],
        [0.2363, 0.2355, 0.2724, 0.2558],
        [0.2399, 0.2440, 0.2647, 0.2515],
        [0.2404, 0.2454, 0.2684, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2352, 0.2612, 0.2597],
        [0.2437, 0.2345, 0.2601, 0.2617],
        [0.2461, 0.2469, 0.2646, 0.2424],
        [0.2472, 0.2448, 0.2695, 0.2385],
        [0.2457, 0.2452, 0.2607, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2149, 0.2723, 0.2571],
        [0.2483, 0.2077, 0.2647, 0.2793],
        [0.2421, 0.2218, 0.2937, 0.2424],
        [0.2296, 0.2197, 0.2925, 0.2582],
        [0.2465, 0.2299, 0.2872, 0.2364],
        [0.2384, 0.2274, 0.3081, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2579, 0.2137, 0.2743, 0.2541],
        [0.2539, 0.2149, 0.2725, 0.2588],
        [0.2429, 0.2230, 0.2886, 0.2456],
        [0.2310, 0.2222, 0.2954, 0.2514],
        [0.2447, 0.2286, 0.2837, 0.2430],
        [0.2331, 0.2272, 0.3037, 0.2360],
        [0.2545, 0.2366, 0.2541, 0.2548]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:19:30午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.6264 (0.5192)	Hard Loss 0.6264 (0.5192)	Soft Loss 0.6264 (0.5192)	Arch Loss 0.4990 (0.6449)	Arch Hard Loss 0.4990 (0.6449)	Arch Soft Loss 0.4990 (0.6449)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.2%, 99.2%)	
08/24 12:19:45午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.4141 (0.5301)	Hard Loss 0.4141 (0.5301)	Soft Loss 0.4141 (0.5301)	Arch Loss 0.5188 (0.6391)	Arch Hard Loss 0.5188 (0.6391)	Arch Soft Loss 0.5188 (0.6391)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.8%, 99.2%)	
08/24 12:20:00午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.5786 (0.5383)	Hard Loss 0.5786 (0.5383)	Soft Loss 0.5786 (0.5383)	Arch Loss 0.6835 (0.6321)	Arch Hard Loss 0.6835 (0.6321)	Arch Soft Loss 0.6835 (0.6321)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.1%, 99.3%)	
08/24 12:20:15午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.5481 (0.5393)	Hard Loss 0.5481 (0.5393)	Soft Loss 0.5481 (0.5393)	Arch Loss 0.6625 (0.6374)	Arch Hard Loss 0.6625 (0.6374)	Arch Soft Loss 0.6625 (0.6374)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.1%, 99.3%)	
08/24 12:20:30午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.5048 (0.5461)	Hard Loss 0.5048 (0.5461)	Soft Loss 0.5048 (0.5461)	Arch Loss 0.7900 (0.6357)	Arch Hard Loss 0.7900 (0.6357)	Arch Soft Loss 0.7900 (0.6357)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.0%, 99.2%)	
08/24 12:20:45午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.3364 (0.5484)	Hard Loss 0.3364 (0.5484)	Soft Loss 0.3364 (0.5484)	Arch Loss 0.7425 (0.6331)	Arch Hard Loss 0.7425 (0.6331)	Arch Soft Loss 0.7425 (0.6331)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (80.8%, 99.2%)	
08/24 12:21:00午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.7209 (0.5492)	Hard Loss 0.7209 (0.5492)	Soft Loss 0.7209 (0.5492)	Arch Loss 0.5036 (0.6340)	Arch Hard Loss 0.5036 (0.6340)	Arch Soft Loss 0.5036 (0.6340)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (80.8%, 99.2%)	
08/24 12:21:12午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.3186 (0.5504)	Hard Loss 0.3186 (0.5504)	Soft Loss 0.3186 (0.5504)	Arch Loss 0.7123 (0.6297)	Arch Hard Loss 0.7123 (0.6297)	Arch Soft Loss 0.7123 (0.6297)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (80.7%, 99.2%)	
08/24 12:21:12午前 searchStage_trainer.py:284 [INFO] Train: [ 12/49] Final Prec@1 80.7200%
08/24 12:21:15午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.6082	Prec@(1,5) (78.8%, 99.0%)
08/24 12:21:17午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.6026	Prec@(1,5) (79.5%, 98.8%)
08/24 12:21:19午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.5938	Prec@(1,5) (79.7%, 98.9%)
08/24 12:21:21午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.5908	Prec@(1,5) (79.8%, 98.9%)
08/24 12:21:23午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.5945	Prec@(1,5) (79.7%, 98.9%)
08/24 12:21:25午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.5985	Prec@(1,5) (79.5%, 98.9%)
08/24 12:21:27午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.5990	Prec@(1,5) (79.4%, 98.9%)
08/24 12:21:29午前 searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.5999	Prec@(1,5) (79.3%, 98.9%)
08/24 12:21:29午前 searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 79.3240%
08/24 12:21:29午前 searchStage_KD_main.py:58 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 5)], [('max_pool_3x3', 3), ('skip_connect', 0)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)]], DAG3_concat=range(6, 8))
08/24 12:21:30午前 searchStage_KD_main.py:89 [INFO] Until now, best Prec@1 = 79.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2168, 0.1948, 0.3493, 0.2391],
        [0.2144, 0.1932, 0.3407, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2322, 0.2208, 0.2892, 0.2578],
        [0.2373, 0.2258, 0.2851, 0.2518],
        [0.2480, 0.2476, 0.2620, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2385, 0.2712, 0.2384],
        [0.2496, 0.2391, 0.2696, 0.2417],
        [0.2437, 0.2468, 0.2517, 0.2578],
        [0.2426, 0.2432, 0.2504, 0.2638]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2454, 0.2641, 0.2453],
        [0.2461, 0.2451, 0.2638, 0.2450],
        [0.2475, 0.2465, 0.2554, 0.2506],
        [0.2387, 0.2517, 0.2573, 0.2522],
        [0.2437, 0.2504, 0.2482, 0.2577]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1675, 0.1417, 0.4919, 0.1989],
        [0.1719, 0.1467, 0.4822, 0.1993],
        [0.2182, 0.2206, 0.2918, 0.2694],
        [0.2313, 0.2415, 0.2560, 0.2711],
        [0.2301, 0.2299, 0.2443, 0.2957],
        [0.2349, 0.2418, 0.2544, 0.2689]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.1770, 0.1520, 0.4771, 0.1940],
        [0.1728, 0.1531, 0.4820, 0.1920],
        [0.2178, 0.2147, 0.2900, 0.2774],
        [0.2296, 0.2325, 0.2625, 0.2753],
        [0.2366, 0.2397, 0.2447, 0.2789],
        [0.2397, 0.2351, 0.2362, 0.2890],
        [0.2385, 0.2386, 0.2601, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2213, 0.2764, 0.2509],
        [0.2543, 0.2203, 0.2772, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2571, 0.2360, 0.2620, 0.2449],
        [0.2548, 0.2320, 0.2629, 0.2503],
        [0.2577, 0.2431, 0.2449, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2421, 0.2617, 0.2400],
        [0.2530, 0.2395, 0.2636, 0.2439],
        [0.2334, 0.2388, 0.2350, 0.2928],
        [0.2395, 0.2668, 0.2661, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2402, 0.2670, 0.2408],
        [0.2539, 0.2387, 0.2656, 0.2418],
        [0.2412, 0.2446, 0.2470, 0.2672],
        [0.2446, 0.2547, 0.2581, 0.2426],
        [0.2463, 0.2470, 0.2498, 0.2569]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.2354, 0.2648, 0.2291],
        [0.2755, 0.2368, 0.2642, 0.2235],
        [0.2426, 0.2402, 0.2472, 0.2700],
        [0.2662, 0.2318, 0.2326, 0.2694],
        [0.2624, 0.2429, 0.2513, 0.2434],
        [0.2751, 0.2275, 0.2329, 0.2645]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2644, 0.2455, 0.2683, 0.2218],
        [0.2666, 0.2440, 0.2674, 0.2219],
        [0.2428, 0.2364, 0.2408, 0.2801],
        [0.2692, 0.2367, 0.2375, 0.2566],
        [0.2385, 0.2520, 0.2531, 0.2564],
        [0.2683, 0.2370, 0.2430, 0.2518],
        [0.2632, 0.2385, 0.2400, 0.2583]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2276, 0.2108, 0.3160, 0.2456],
        [0.2262, 0.2096, 0.3154, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2352, 0.2254, 0.2969, 0.2425],
        [0.2312, 0.2228, 0.2952, 0.2507],
        [0.2374, 0.2382, 0.2697, 0.2547]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2353, 0.2394, 0.2793, 0.2461],
        [0.2351, 0.2341, 0.2763, 0.2545],
        [0.2378, 0.2447, 0.2664, 0.2512],
        [0.2396, 0.2445, 0.2695, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2422, 0.2354, 0.2625, 0.2599],
        [0.2406, 0.2356, 0.2630, 0.2607],
        [0.2434, 0.2469, 0.2662, 0.2434],
        [0.2442, 0.2454, 0.2714, 0.2390],
        [0.2431, 0.2458, 0.2639, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2108, 0.2758, 0.2617],
        [0.2444, 0.2041, 0.2675, 0.2841],
        [0.2415, 0.2179, 0.2962, 0.2444],
        [0.2291, 0.2175, 0.2987, 0.2547],
        [0.2467, 0.2253, 0.2909, 0.2371],
        [0.2402, 0.2257, 0.3151, 0.2190]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2088, 0.2779, 0.2596],
        [0.2497, 0.2104, 0.2763, 0.2635],
        [0.2428, 0.2193, 0.2918, 0.2461],
        [0.2302, 0.2191, 0.3033, 0.2474],
        [0.2431, 0.2247, 0.2878, 0.2444],
        [0.2346, 0.2254, 0.3114, 0.2286],
        [0.2579, 0.2361, 0.2533, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
08/24 12:21:45午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.4744 (0.5360)	Hard Loss 0.4744 (0.5360)	Soft Loss 0.4744 (0.5360)	Arch Loss 0.6311 (0.5843)	Arch Hard Loss 0.6311 (0.5843)	Arch Soft Loss 0.6311 (0.5843)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (81.3%, 99.3%)	
08/24 12:22:00午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.3860 (0.5162)	Hard Loss 0.3860 (0.5162)	Soft Loss 0.3860 (0.5162)	Arch Loss 0.3867 (0.5930)	Arch Hard Loss 0.3867 (0.5930)	Arch Soft Loss 0.3867 (0.5930)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.1%, 99.2%)	
08/24 12:22:15午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.5372 (0.5057)	Hard Loss 0.5372 (0.5057)	Soft Loss 0.5372 (0.5057)	Arch Loss 0.4564 (0.6072)	Arch Hard Loss 0.4564 (0.6072)	Arch Soft Loss 0.4564 (0.6072)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.6%, 99.3%)	
08/24 12:22:29午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.6514 (0.5084)	Hard Loss 0.6514 (0.5084)	Soft Loss 0.6514 (0.5084)	Arch Loss 0.5225 (0.6241)	Arch Hard Loss 0.5225 (0.6241)	Arch Soft Loss 0.5225 (0.6241)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.5%, 99.3%)	
08/24 12:22:44午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.4334 (0.5115)	Hard Loss 0.4334 (0.5115)	Soft Loss 0.4334 (0.5115)	Arch Loss 0.6654 (0.6146)	Arch Hard Loss 0.6654 (0.6146)	Arch Soft Loss 0.6654 (0.6146)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.2%, 99.2%)	
08/24 12:22:59午前 searchStage_trainer.py:268 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.5949 (0.5132)	Hard Loss 0.5949 (0.5132)	Soft Loss 0.5949 (0.5132)	Arch Loss 0.7086 (0.6129)	Arch Hard Loss 0.7086 (0.6129)	Arch Soft Loss 0.7086 (0.6129)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (82.3%, 99.2%)	
