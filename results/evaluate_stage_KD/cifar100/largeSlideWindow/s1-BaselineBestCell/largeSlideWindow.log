08/23 11:41:10PM parser.py:28 [INFO] 
08/23 11:41:10PM parser.py:29 [INFO] Parameters:
08/23 11:41:10PM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
08/23 11:41:10PM parser.py:31 [INFO] T=10.0
08/23 11:41:10PM parser.py:31 [INFO] ADVANCED=True
08/23 11:41:10PM parser.py:31 [INFO] AUX_WEIGHT=0.4
08/23 11:41:10PM parser.py:31 [INFO] BATCH_SIZE=64
08/23 11:41:10PM parser.py:31 [INFO] CUTOUT_LENGTH=16
08/23 11:41:10PM parser.py:31 [INFO] DATA_PATH=../data/
08/23 11:41:10PM parser.py:31 [INFO] DATASET=cifar100
08/23 11:41:10PM parser.py:31 [INFO] DESCRIPTION=stage_architecure_evaluation_on_nonKD_searching_with_slidewindow-8
08/23 11:41:10PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
08/23 11:41:10PM parser.py:31 [INFO] EPOCHS=100
08/23 11:41:10PM parser.py:31 [INFO] EXP_NAME=s1-BaselineBestCell
08/23 11:41:10PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
08/23 11:41:10PM parser.py:31 [INFO] GPUS=[0]
08/23 11:41:10PM parser.py:31 [INFO] GRAD_CLIP=5.0
08/23 11:41:10PM parser.py:31 [INFO] INIT_CHANNELS=32
08/23 11:41:10PM parser.py:31 [INFO] L=0.5
08/23 11:41:10PM parser.py:31 [INFO] LAYERS=20
08/23 11:41:10PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
08/23 11:41:10PM parser.py:31 [INFO] LR=0.025
08/23 11:41:10PM parser.py:31 [INFO] LR_MIN=0.001
08/23 11:41:10PM parser.py:31 [INFO] MOMENTUM=0.9
08/23 11:41:10PM parser.py:31 [INFO] NAME=largeSlideWindow
08/23 11:41:10PM parser.py:31 [INFO] NONKD=True
08/23 11:41:10PM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/largeSlideWindow/s1-BaselineBestCell
08/23 11:41:10PM parser.py:31 [INFO] PRINT_FREQ=50
08/23 11:41:10PM parser.py:31 [INFO] RESUME_PATH=None
08/23 11:41:10PM parser.py:31 [INFO] SAVE=s1-BaselineBestCell
08/23 11:41:10PM parser.py:31 [INFO] SEED=0
08/23 11:41:10PM parser.py:31 [INFO] SPEC_CELL=True
08/23 11:41:10PM parser.py:31 [INFO] TEACHER_NAME=non
08/23 11:41:10PM parser.py:31 [INFO] TEACHER_PATH=non
08/23 11:41:10PM parser.py:31 [INFO] TRAIN_PORTION=0.9
08/23 11:41:10PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
08/23 11:41:10PM parser.py:31 [INFO] WORKERS=4
08/23 11:41:10PM parser.py:32 [INFO] 
08/23 11:41:12PM evaluateStage_trainer.py:82 [INFO] --> No loaded checkpoint!
08/23 11:41:20PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][50/703]	Step 50	lr 0.025	Loss 6.5064 (6.5154)	Prec@(1,5) (1.4%, 5.9%)	
08/23 11:41:27PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][100/703]	Step 100	lr 0.025	Loss 6.4411 (6.4819)	Prec@(1,5) (1.9%, 8.0%)	
08/23 11:41:34PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][150/703]	Step 150	lr 0.025	Loss 5.9976 (6.3807)	Prec@(1,5) (2.5%, 10.1%)	
08/23 11:41:40PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][200/703]	Step 200	lr 0.025	Loss 5.8000 (6.2874)	Prec@(1,5) (2.8%, 11.8%)	
08/23 11:41:47PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][250/703]	Step 250	lr 0.025	Loss 6.0402 (6.1961)	Prec@(1,5) (3.2%, 13.5%)	
08/23 11:41:53PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][300/703]	Step 300	lr 0.025	Loss 5.6105 (6.1218)	Prec@(1,5) (3.6%, 15.0%)	
08/23 11:42:00PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][350/703]	Step 350	lr 0.025	Loss 5.4796 (6.0614)	Prec@(1,5) (4.0%, 16.1%)	
08/23 11:42:06PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][400/703]	Step 400	lr 0.025	Loss 5.5877 (6.0095)	Prec@(1,5) (4.4%, 17.1%)	
08/23 11:42:13PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][450/703]	Step 450	lr 0.025	Loss 5.5108 (5.9583)	Prec@(1,5) (4.7%, 18.2%)	
08/23 11:42:20PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][500/703]	Step 500	lr 0.025	Loss 5.3950 (5.9146)	Prec@(1,5) (5.0%, 19.2%)	
08/23 11:42:26PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][550/703]	Step 550	lr 0.025	Loss 5.6284 (5.8744)	Prec@(1,5) (5.4%, 20.1%)	
08/23 11:42:33PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][600/703]	Step 600	lr 0.025	Loss 5.3863 (5.8347)	Prec@(1,5) (5.7%, 21.0%)	
08/23 11:42:39PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][650/703]	Step 650	lr 0.025	Loss 5.2937 (5.7990)	Prec@(1,5) (6.0%, 21.7%)	
08/23 11:42:46PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][700/703]	Step 700	lr 0.025	Loss 5.1506 (5.7638)	Prec@(1,5) (6.3%, 22.5%)	
08/23 11:42:46PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][703/703]	Step 703	lr 0.025	Loss 5.4231 (5.7623)	Prec@(1,5) (6.4%, 22.5%)	
08/23 11:42:47PM evaluateStage_trainer.py:148 [INFO] Train: [  0/99] Final Prec@1 6.3644%
08/23 11:42:50PM evaluateStage_trainer.py:183 [INFO] Valid: [  0/99] Final Prec@1 10.5200%
08/23 11:42:51午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 10.5200%
08/23 11:42:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][50/703]	Step 754	lr 0.02499	Loss 5.2626 (5.2621)	Prec@(1,5) (10.7%, 33.2%)	
08/23 11:43:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][100/703]	Step 804	lr 0.02499	Loss 5.2539 (5.2015)	Prec@(1,5) (11.3%, 34.2%)	
08/23 11:43:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][150/703]	Step 854	lr 0.02499	Loss 4.9157 (5.1901)	Prec@(1,5) (11.4%, 34.8%)	
08/23 11:43:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][200/703]	Step 904	lr 0.02499	Loss 5.2864 (5.1729)	Prec@(1,5) (11.6%, 34.9%)	
08/23 11:43:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][250/703]	Step 954	lr 0.02499	Loss 4.9835 (5.1621)	Prec@(1,5) (11.7%, 35.2%)	
08/23 11:43:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][300/703]	Step 1004	lr 0.02499	Loss 5.2735 (5.1396)	Prec@(1,5) (12.0%, 35.9%)	
08/23 11:43:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][350/703]	Step 1054	lr 0.02499	Loss 4.6498 (5.1127)	Prec@(1,5) (12.4%, 36.6%)	
08/23 11:43:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][400/703]	Step 1104	lr 0.02499	Loss 4.5817 (5.0881)	Prec@(1,5) (12.7%, 37.1%)	
08/23 11:43:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][450/703]	Step 1154	lr 0.02499	Loss 4.9733 (5.0675)	Prec@(1,5) (13.0%, 37.7%)	
08/23 11:44:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][500/703]	Step 1204	lr 0.02499	Loss 5.3698 (5.0500)	Prec@(1,5) (13.2%, 38.1%)	
08/23 11:44:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][550/703]	Step 1254	lr 0.02499	Loss 4.3942 (5.0296)	Prec@(1,5) (13.5%, 38.5%)	
08/23 11:44:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][600/703]	Step 1304	lr 0.02499	Loss 4.8040 (5.0079)	Prec@(1,5) (13.9%, 39.1%)	
08/23 11:44:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][650/703]	Step 1354	lr 0.02499	Loss 4.6620 (4.9911)	Prec@(1,5) (14.0%, 39.4%)	
08/23 11:44:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][700/703]	Step 1404	lr 0.02499	Loss 4.7749 (4.9735)	Prec@(1,5) (14.3%, 39.8%)	
08/23 11:44:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][703/703]	Step 1407	lr 0.02499	Loss 4.5302 (4.9725)	Prec@(1,5) (14.3%, 39.8%)	
08/23 11:44:35午後 evaluateStage_trainer.py:148 [INFO] Train: [  1/99] Final Prec@1 14.3467%
08/23 11:44:39午後 evaluateStage_trainer.py:183 [INFO] Valid: [  1/99] Final Prec@1 16.7400%
08/23 11:44:39午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 16.7400%
08/23 11:44:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][50/703]	Step 1458	lr 0.02498	Loss 4.8423 (4.6823)	Prec@(1,5) (19.1%, 47.4%)	
08/23 11:44:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][100/703]	Step 1508	lr 0.02498	Loss 4.6685 (4.6432)	Prec@(1,5) (19.2%, 47.2%)	
08/23 11:45:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][150/703]	Step 1558	lr 0.02498	Loss 4.5160 (4.6093)	Prec@(1,5) (19.7%, 48.1%)	
08/23 11:45:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][200/703]	Step 1608	lr 0.02498	Loss 4.5455 (4.5968)	Prec@(1,5) (19.6%, 48.1%)	
08/23 11:45:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][250/703]	Step 1658	lr 0.02498	Loss 5.4176 (4.5880)	Prec@(1,5) (19.5%, 48.3%)	
08/23 11:45:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][300/703]	Step 1708	lr 0.02498	Loss 4.0581 (4.5686)	Prec@(1,5) (19.7%, 48.8%)	
08/23 11:45:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][350/703]	Step 1758	lr 0.02498	Loss 4.8783 (4.5477)	Prec@(1,5) (19.9%, 49.3%)	
08/23 11:45:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][400/703]	Step 1808	lr 0.02498	Loss 4.3713 (4.5369)	Prec@(1,5) (20.1%, 49.4%)	
08/23 11:45:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][450/703]	Step 1858	lr 0.02498	Loss 4.4899 (4.5240)	Prec@(1,5) (20.3%, 49.6%)	
08/23 11:45:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][500/703]	Step 1908	lr 0.02498	Loss 4.2550 (4.5070)	Prec@(1,5) (20.6%, 49.8%)	
08/23 11:45:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][550/703]	Step 1958	lr 0.02498	Loss 4.5400 (4.4960)	Prec@(1,5) (20.8%, 50.0%)	
08/23 11:46:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][600/703]	Step 2008	lr 0.02498	Loss 4.4041 (4.4781)	Prec@(1,5) (21.0%, 50.3%)	
08/23 11:46:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][650/703]	Step 2058	lr 0.02498	Loss 4.3128 (4.4674)	Prec@(1,5) (21.3%, 50.5%)	
08/23 11:46:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][700/703]	Step 2108	lr 0.02498	Loss 4.1069 (4.4525)	Prec@(1,5) (21.4%, 50.7%)	
08/23 11:46:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][703/703]	Step 2111	lr 0.02498	Loss 4.5593 (4.4519)	Prec@(1,5) (21.4%, 50.7%)	
08/23 11:46:22午後 evaluateStage_trainer.py:148 [INFO] Train: [  2/99] Final Prec@1 21.4222%
08/23 11:46:26午後 evaluateStage_trainer.py:183 [INFO] Valid: [  2/99] Final Prec@1 23.0400%
08/23 11:46:26午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 23.0400%
08/23 11:46:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][50/703]	Step 2162	lr 0.02495	Loss 4.4283 (4.1912)	Prec@(1,5) (24.4%, 56.4%)	
08/23 11:46:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][100/703]	Step 2212	lr 0.02495	Loss 3.7164 (4.1480)	Prec@(1,5) (25.6%, 56.3%)	
08/23 11:46:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][150/703]	Step 2262	lr 0.02495	Loss 4.3229 (4.1419)	Prec@(1,5) (25.5%, 56.5%)	
08/23 11:46:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][200/703]	Step 2312	lr 0.02495	Loss 4.4290 (4.1479)	Prec@(1,5) (25.4%, 56.4%)	
08/23 11:47:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][250/703]	Step 2362	lr 0.02495	Loss 3.7154 (4.1333)	Prec@(1,5) (25.9%, 56.6%)	
08/23 11:47:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][300/703]	Step 2412	lr 0.02495	Loss 3.8101 (4.1254)	Prec@(1,5) (25.9%, 56.7%)	
08/23 11:47:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][350/703]	Step 2462	lr 0.02495	Loss 3.7877 (4.1231)	Prec@(1,5) (25.8%, 56.7%)	
08/23 11:47:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][400/703]	Step 2512	lr 0.02495	Loss 4.4429 (4.1138)	Prec@(1,5) (26.0%, 56.9%)	
08/23 11:47:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][450/703]	Step 2562	lr 0.02495	Loss 4.1632 (4.1043)	Prec@(1,5) (26.2%, 57.2%)	
08/23 11:47:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][500/703]	Step 2612	lr 0.02495	Loss 4.3217 (4.0968)	Prec@(1,5) (26.3%, 57.3%)	
08/23 11:47:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][550/703]	Step 2662	lr 0.02495	Loss 3.6283 (4.0870)	Prec@(1,5) (26.5%, 57.5%)	
08/23 11:47:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][600/703]	Step 2712	lr 0.02495	Loss 3.9670 (4.0750)	Prec@(1,5) (26.7%, 57.7%)	
08/23 11:48:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][650/703]	Step 2762	lr 0.02495	Loss 3.3431 (4.0631)	Prec@(1,5) (26.9%, 58.0%)	
08/23 11:48:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][700/703]	Step 2812	lr 0.02495	Loss 3.5773 (4.0534)	Prec@(1,5) (27.0%, 58.1%)	
08/23 11:48:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][703/703]	Step 2815	lr 0.02495	Loss 3.6513 (4.0527)	Prec@(1,5) (27.0%, 58.2%)	
08/23 11:48:10午後 evaluateStage_trainer.py:148 [INFO] Train: [  3/99] Final Prec@1 27.0133%
08/23 11:48:14午後 evaluateStage_trainer.py:183 [INFO] Valid: [  3/99] Final Prec@1 27.1800%
08/23 11:48:14午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 27.1800%
08/23 11:48:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][50/703]	Step 2866	lr 0.02491	Loss 4.0463 (3.8216)	Prec@(1,5) (30.6%, 62.5%)	
08/23 11:48:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][100/703]	Step 2916	lr 0.02491	Loss 3.9651 (3.8128)	Prec@(1,5) (30.9%, 62.5%)	
08/23 11:48:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][150/703]	Step 2966	lr 0.02491	Loss 4.2471 (3.8173)	Prec@(1,5) (30.4%, 62.2%)	
08/23 11:48:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][200/703]	Step 3016	lr 0.02491	Loss 3.9162 (3.8247)	Prec@(1,5) (30.4%, 61.8%)	
08/23 11:48:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][250/703]	Step 3066	lr 0.02491	Loss 4.2852 (3.8189)	Prec@(1,5) (30.4%, 61.9%)	
08/23 11:48:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][300/703]	Step 3116	lr 0.02491	Loss 3.5545 (3.8052)	Prec@(1,5) (30.6%, 62.2%)	
08/23 11:49:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][350/703]	Step 3166	lr 0.02491	Loss 3.7760 (3.8009)	Prec@(1,5) (30.7%, 62.2%)	
08/23 11:49:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][400/703]	Step 3216	lr 0.02491	Loss 3.9930 (3.7939)	Prec@(1,5) (30.8%, 62.3%)	
08/23 11:49:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][450/703]	Step 3266	lr 0.02491	Loss 3.7918 (3.7855)	Prec@(1,5) (31.0%, 62.4%)	
08/23 11:49:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][500/703]	Step 3316	lr 0.02491	Loss 3.4012 (3.7699)	Prec@(1,5) (31.2%, 62.7%)	
08/23 11:49:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][550/703]	Step 3366	lr 0.02491	Loss 3.8194 (3.7625)	Prec@(1,5) (31.4%, 62.8%)	
08/23 11:49:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][600/703]	Step 3416	lr 0.02491	Loss 4.1863 (3.7484)	Prec@(1,5) (31.5%, 63.0%)	
08/23 11:49:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][650/703]	Step 3466	lr 0.02491	Loss 3.9993 (3.7378)	Prec@(1,5) (31.6%, 63.2%)	
08/23 11:49:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][700/703]	Step 3516	lr 0.02491	Loss 3.9387 (3.7284)	Prec@(1,5) (31.7%, 63.4%)	
08/23 11:49:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][703/703]	Step 3519	lr 0.02491	Loss 3.9557 (3.7282)	Prec@(1,5) (31.7%, 63.4%)	
08/23 11:49:58午後 evaluateStage_trainer.py:148 [INFO] Train: [  4/99] Final Prec@1 31.7289%
08/23 11:50:02午後 evaluateStage_trainer.py:183 [INFO] Valid: [  4/99] Final Prec@1 32.7400%
08/23 11:50:03午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 32.7400%
08/23 11:50:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][50/703]	Step 3570	lr 0.02485	Loss 4.2458 (3.5442)	Prec@(1,5) (33.9%, 66.9%)	
08/23 11:50:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][100/703]	Step 3620	lr 0.02485	Loss 3.4601 (3.5455)	Prec@(1,5) (33.6%, 67.0%)	
08/23 11:50:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][150/703]	Step 3670	lr 0.02485	Loss 3.0275 (3.5184)	Prec@(1,5) (34.3%, 67.4%)	
08/23 11:50:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][200/703]	Step 3720	lr 0.02485	Loss 4.0556 (3.4834)	Prec@(1,5) (34.9%, 67.8%)	
08/23 11:50:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][250/703]	Step 3770	lr 0.02485	Loss 3.7122 (3.4817)	Prec@(1,5) (35.1%, 68.0%)	
08/23 11:50:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][300/703]	Step 3820	lr 0.02485	Loss 3.7878 (3.4895)	Prec@(1,5) (35.1%, 67.9%)	
08/23 11:50:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][350/703]	Step 3870	lr 0.02485	Loss 3.6695 (3.4973)	Prec@(1,5) (34.9%, 67.7%)	
08/23 11:51:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][400/703]	Step 3920	lr 0.02485	Loss 3.7525 (3.4885)	Prec@(1,5) (34.9%, 67.7%)	
08/23 11:51:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][450/703]	Step 3970	lr 0.02485	Loss 3.2474 (3.4809)	Prec@(1,5) (34.9%, 67.9%)	
08/23 11:51:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][500/703]	Step 4020	lr 0.02485	Loss 3.3573 (3.4730)	Prec@(1,5) (35.0%, 68.0%)	
08/23 11:51:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][550/703]	Step 4070	lr 0.02485	Loss 2.8708 (3.4717)	Prec@(1,5) (35.1%, 68.0%)	
08/23 11:51:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][600/703]	Step 4120	lr 0.02485	Loss 3.8940 (3.4728)	Prec@(1,5) (35.1%, 68.0%)	
08/23 11:51:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][650/703]	Step 4170	lr 0.02485	Loss 3.3631 (3.4600)	Prec@(1,5) (35.3%, 68.1%)	
08/23 11:51:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][700/703]	Step 4220	lr 0.02485	Loss 3.0964 (3.4528)	Prec@(1,5) (35.4%, 68.2%)	
08/23 11:51:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][703/703]	Step 4223	lr 0.02485	Loss 3.5697 (3.4534)	Prec@(1,5) (35.4%, 68.2%)	
08/23 11:51:46午後 evaluateStage_trainer.py:148 [INFO] Train: [  5/99] Final Prec@1 35.4044%
08/23 11:51:50午後 evaluateStage_trainer.py:183 [INFO] Valid: [  5/99] Final Prec@1 34.9600%
08/23 11:51:50午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 34.9600%
08/23 11:51:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][50/703]	Step 4274	lr 0.02479	Loss 3.0169 (3.2386)	Prec@(1,5) (38.0%, 71.5%)	
08/23 11:52:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][100/703]	Step 4324	lr 0.02479	Loss 3.2113 (3.2674)	Prec@(1,5) (38.0%, 71.1%)	
08/23 11:52:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][150/703]	Step 4374	lr 0.02479	Loss 2.5327 (3.2710)	Prec@(1,5) (37.7%, 71.4%)	
08/23 11:52:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][200/703]	Step 4424	lr 0.02479	Loss 3.8393 (3.2736)	Prec@(1,5) (37.8%, 71.3%)	
08/23 11:52:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][250/703]	Step 4474	lr 0.02479	Loss 2.7059 (3.2654)	Prec@(1,5) (38.0%, 71.5%)	
08/23 11:52:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][300/703]	Step 4524	lr 0.02479	Loss 3.2937 (3.2621)	Prec@(1,5) (38.3%, 71.4%)	
08/23 11:52:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][350/703]	Step 4574	lr 0.02479	Loss 3.1586 (3.2559)	Prec@(1,5) (38.5%, 71.4%)	
08/23 11:52:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][400/703]	Step 4624	lr 0.02479	Loss 3.0056 (3.2450)	Prec@(1,5) (38.6%, 71.4%)	
08/23 11:52:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][450/703]	Step 4674	lr 0.02479	Loss 2.7321 (3.2400)	Prec@(1,5) (38.6%, 71.6%)	
08/23 11:53:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][500/703]	Step 4724	lr 0.02479	Loss 3.4081 (3.2368)	Prec@(1,5) (38.7%, 71.6%)	
08/23 11:53:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][550/703]	Step 4774	lr 0.02479	Loss 3.5547 (3.2360)	Prec@(1,5) (38.7%, 71.7%)	
08/23 11:53:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][600/703]	Step 4824	lr 0.02479	Loss 3.4010 (3.2380)	Prec@(1,5) (38.7%, 71.6%)	
08/23 11:53:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][650/703]	Step 4874	lr 0.02479	Loss 2.7676 (3.2362)	Prec@(1,5) (38.8%, 71.6%)	
08/23 11:53:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][700/703]	Step 4924	lr 0.02479	Loss 3.1614 (3.2309)	Prec@(1,5) (38.8%, 71.7%)	
08/23 11:53:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][703/703]	Step 4927	lr 0.02479	Loss 3.2887 (3.2297)	Prec@(1,5) (38.8%, 71.7%)	
08/23 11:53:34午後 evaluateStage_trainer.py:148 [INFO] Train: [  6/99] Final Prec@1 38.8200%
08/23 11:53:38午後 evaluateStage_trainer.py:183 [INFO] Valid: [  6/99] Final Prec@1 35.7200%
08/23 11:53:38午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 35.7200%
08/23 11:53:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][50/703]	Step 4978	lr 0.02471	Loss 3.1796 (3.0682)	Prec@(1,5) (40.6%, 74.5%)	
08/23 11:53:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][100/703]	Step 5028	lr 0.02471	Loss 3.0479 (3.0635)	Prec@(1,5) (41.4%, 74.9%)	
08/23 11:54:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][150/703]	Step 5078	lr 0.02471	Loss 3.2573 (3.0548)	Prec@(1,5) (41.4%, 74.6%)	
08/23 11:54:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][200/703]	Step 5128	lr 0.02471	Loss 3.0293 (3.0377)	Prec@(1,5) (41.9%, 74.6%)	
08/23 11:54:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][250/703]	Step 5178	lr 0.02471	Loss 2.7955 (3.0457)	Prec@(1,5) (41.8%, 74.3%)	
08/23 11:54:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][300/703]	Step 5228	lr 0.02471	Loss 2.7512 (3.0626)	Prec@(1,5) (41.4%, 74.1%)	
08/23 11:54:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][350/703]	Step 5278	lr 0.02471	Loss 3.5506 (3.0668)	Prec@(1,5) (41.4%, 74.0%)	
08/23 11:54:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][400/703]	Step 5328	lr 0.02471	Loss 2.7279 (3.0657)	Prec@(1,5) (41.4%, 74.1%)	
08/23 11:54:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][450/703]	Step 5378	lr 0.02471	Loss 3.2797 (3.0752)	Prec@(1,5) (41.3%, 74.0%)	
08/23 11:54:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][500/703]	Step 5428	lr 0.02471	Loss 3.3641 (3.0707)	Prec@(1,5) (41.4%, 74.0%)	
08/23 11:54:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][550/703]	Step 5478	lr 0.02471	Loss 3.0304 (3.0667)	Prec@(1,5) (41.5%, 74.2%)	
08/23 11:55:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][600/703]	Step 5528	lr 0.02471	Loss 2.9296 (3.0612)	Prec@(1,5) (41.7%, 74.2%)	
08/23 11:55:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][650/703]	Step 5578	lr 0.02471	Loss 3.2432 (3.0617)	Prec@(1,5) (41.6%, 74.2%)	
08/23 11:55:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][700/703]	Step 5628	lr 0.02471	Loss 2.2336 (3.0606)	Prec@(1,5) (41.6%, 74.2%)	
08/23 11:55:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][703/703]	Step 5631	lr 0.02471	Loss 3.0291 (3.0599)	Prec@(1,5) (41.6%, 74.2%)	
08/23 11:55:22午後 evaluateStage_trainer.py:148 [INFO] Train: [  7/99] Final Prec@1 41.5867%
08/23 11:55:25午後 evaluateStage_trainer.py:183 [INFO] Valid: [  7/99] Final Prec@1 40.0800%
08/23 11:55:26午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 40.0800%
08/23 11:55:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][50/703]	Step 5682	lr 0.02462	Loss 2.9815 (2.9029)	Prec@(1,5) (43.8%, 76.2%)	
08/23 11:55:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][100/703]	Step 5732	lr 0.02462	Loss 3.0732 (2.9298)	Prec@(1,5) (43.7%, 75.8%)	
08/23 11:55:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][150/703]	Step 5782	lr 0.02462	Loss 2.9048 (2.8985)	Prec@(1,5) (44.1%, 76.1%)	
08/23 11:55:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][200/703]	Step 5832	lr 0.02462	Loss 2.8720 (2.9000)	Prec@(1,5) (44.1%, 75.9%)	
08/23 11:56:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][250/703]	Step 5882	lr 0.02462	Loss 3.6157 (2.9010)	Prec@(1,5) (44.0%, 76.0%)	
08/23 11:56:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][300/703]	Step 5932	lr 0.02462	Loss 2.4469 (2.9104)	Prec@(1,5) (43.9%, 75.9%)	
08/23 11:56:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][350/703]	Step 5982	lr 0.02462	Loss 2.8972 (2.9223)	Prec@(1,5) (43.7%, 75.7%)	
08/23 11:56:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][400/703]	Step 6032	lr 0.02462	Loss 3.1228 (2.9258)	Prec@(1,5) (43.7%, 75.7%)	
08/23 11:56:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][450/703]	Step 6082	lr 0.02462	Loss 2.6366 (2.9238)	Prec@(1,5) (43.9%, 75.8%)	
08/23 11:56:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][500/703]	Step 6132	lr 0.02462	Loss 2.9671 (2.9176)	Prec@(1,5) (43.9%, 75.9%)	
08/23 11:56:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][550/703]	Step 6182	lr 0.02462	Loss 2.5864 (2.9154)	Prec@(1,5) (44.0%, 75.9%)	
08/23 11:56:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][600/703]	Step 6232	lr 0.02462	Loss 2.8098 (2.9071)	Prec@(1,5) (44.2%, 76.1%)	
08/23 11:57:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][650/703]	Step 6282	lr 0.02462	Loss 2.9442 (2.9058)	Prec@(1,5) (44.1%, 76.2%)	
08/23 11:57:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][700/703]	Step 6332	lr 0.02462	Loss 2.7747 (2.9050)	Prec@(1,5) (44.1%, 76.1%)	
08/23 11:57:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][703/703]	Step 6335	lr 0.02462	Loss 2.8041 (2.9055)	Prec@(1,5) (44.1%, 76.1%)	
08/23 11:57:09午後 evaluateStage_trainer.py:148 [INFO] Train: [  8/99] Final Prec@1 44.0733%
08/23 11:57:13午後 evaluateStage_trainer.py:183 [INFO] Valid: [  8/99] Final Prec@1 40.6000%
08/23 11:57:13午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 40.6000%
08/23 11:57:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][50/703]	Step 6386	lr 0.02452	Loss 2.8757 (2.8257)	Prec@(1,5) (45.2%, 77.8%)	
08/23 11:57:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][100/703]	Step 6436	lr 0.02452	Loss 3.2227 (2.8311)	Prec@(1,5) (45.3%, 77.8%)	
08/23 11:57:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][150/703]	Step 6486	lr 0.02452	Loss 3.3183 (2.7972)	Prec@(1,5) (46.0%, 78.0%)	
08/23 11:57:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][200/703]	Step 6536	lr 0.02452	Loss 2.4753 (2.7952)	Prec@(1,5) (46.1%, 78.0%)	
08/23 11:57:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][250/703]	Step 6586	lr 0.02452	Loss 2.9296 (2.8016)	Prec@(1,5) (45.9%, 77.9%)	
08/23 11:57:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][300/703]	Step 6636	lr 0.02452	Loss 2.9282 (2.8032)	Prec@(1,5) (45.8%, 77.9%)	
08/23 11:58:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][350/703]	Step 6686	lr 0.02452	Loss 3.2179 (2.8022)	Prec@(1,5) (45.8%, 78.0%)	
08/23 11:58:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][400/703]	Step 6736	lr 0.02452	Loss 2.7633 (2.7941)	Prec@(1,5) (46.1%, 78.1%)	
08/23 11:58:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][450/703]	Step 6786	lr 0.02452	Loss 2.7250 (2.7912)	Prec@(1,5) (46.2%, 78.2%)	
08/23 11:58:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][500/703]	Step 6836	lr 0.02452	Loss 2.5448 (2.7968)	Prec@(1,5) (46.2%, 78.1%)	
08/23 11:58:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][550/703]	Step 6886	lr 0.02452	Loss 3.1910 (2.7925)	Prec@(1,5) (46.3%, 78.2%)	
08/23 11:58:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][600/703]	Step 6936	lr 0.02452	Loss 2.4906 (2.7893)	Prec@(1,5) (46.2%, 78.2%)	
08/23 11:58:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][650/703]	Step 6986	lr 0.02452	Loss 2.5279 (2.7883)	Prec@(1,5) (46.1%, 78.2%)	
08/23 11:58:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][700/703]	Step 7036	lr 0.02452	Loss 2.7594 (2.7829)	Prec@(1,5) (46.2%, 78.3%)	
08/23 11:58:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][703/703]	Step 7039	lr 0.02452	Loss 2.5956 (2.7828)	Prec@(1,5) (46.2%, 78.3%)	
08/23 11:58:56午後 evaluateStage_trainer.py:148 [INFO] Train: [  9/99] Final Prec@1 46.1756%
08/23 11:59:00午後 evaluateStage_trainer.py:183 [INFO] Valid: [  9/99] Final Prec@1 43.8800%
08/23 11:59:00午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 43.8800%
08/23 11:59:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][50/703]	Step 7090	lr 0.02441	Loss 2.5556 (2.5796)	Prec@(1,5) (49.6%, 80.8%)	
08/23 11:59:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][100/703]	Step 7140	lr 0.02441	Loss 2.7623 (2.6368)	Prec@(1,5) (48.6%, 80.0%)	
08/23 11:59:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][150/703]	Step 7190	lr 0.02441	Loss 2.2112 (2.6518)	Prec@(1,5) (48.4%, 79.8%)	
08/23 11:59:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][200/703]	Step 7240	lr 0.02441	Loss 2.9071 (2.6666)	Prec@(1,5) (48.3%, 79.7%)	
08/23 11:59:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][250/703]	Step 7290	lr 0.02441	Loss 2.5665 (2.6602)	Prec@(1,5) (48.2%, 79.8%)	
08/23 11:59:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][300/703]	Step 7340	lr 0.02441	Loss 2.3881 (2.6585)	Prec@(1,5) (48.2%, 79.7%)	
08/23 11:59:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][350/703]	Step 7390	lr 0.02441	Loss 2.9665 (2.6675)	Prec@(1,5) (47.9%, 79.7%)	
08/23 11:59:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][400/703]	Step 7440	lr 0.02441	Loss 2.4706 (2.6714)	Prec@(1,5) (47.9%, 79.7%)	
08/24 12:00:07午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][450/703]	Step 7490	lr 0.02441	Loss 2.7501 (2.6684)	Prec@(1,5) (47.9%, 79.8%)	
08/24 12:00:14午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][500/703]	Step 7540	lr 0.02441	Loss 3.3200 (2.6702)	Prec@(1,5) (47.8%, 79.8%)	
08/24 12:00:21午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][550/703]	Step 7590	lr 0.02441	Loss 2.0280 (2.6655)	Prec@(1,5) (48.0%, 79.8%)	
08/24 12:00:29午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][600/703]	Step 7640	lr 0.02441	Loss 2.8694 (2.6742)	Prec@(1,5) (47.8%, 79.7%)	
08/24 12:00:36午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][650/703]	Step 7690	lr 0.02441	Loss 2.4704 (2.6788)	Prec@(1,5) (47.7%, 79.6%)	
08/24 12:00:43午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][700/703]	Step 7740	lr 0.02441	Loss 2.3398 (2.6738)	Prec@(1,5) (47.8%, 79.7%)	
08/24 12:00:44午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][703/703]	Step 7743	lr 0.02441	Loss 2.6775 (2.6732)	Prec@(1,5) (47.8%, 79.7%)	
08/24 12:00:44午前 evaluateStage_trainer.py:148 [INFO] Train: [ 10/99] Final Prec@1 47.7667%
08/24 12:00:48午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 10/99] Final Prec@1 43.2600%
08/24 12:00:48午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 43.8800%
08/24 12:00:56午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][50/703]	Step 7794	lr 0.02429	Loss 2.4119 (2.5408)	Prec@(1,5) (50.8%, 80.3%)	
08/24 12:01:03午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][100/703]	Step 7844	lr 0.02429	Loss 2.5929 (2.5522)	Prec@(1,5) (50.6%, 80.4%)	
08/24 12:01:10午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][150/703]	Step 7894	lr 0.02429	Loss 2.6425 (2.5736)	Prec@(1,5) (50.2%, 80.2%)	
08/24 12:01:18午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][200/703]	Step 7944	lr 0.02429	Loss 2.6548 (2.5665)	Prec@(1,5) (50.1%, 80.3%)	
08/24 12:01:25午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][250/703]	Step 7994	lr 0.02429	Loss 2.7340 (2.5832)	Prec@(1,5) (49.8%, 80.4%)	
08/24 12:01:32午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][300/703]	Step 8044	lr 0.02429	Loss 2.7246 (2.6000)	Prec@(1,5) (49.4%, 80.3%)	
08/24 12:01:40午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][350/703]	Step 8094	lr 0.02429	Loss 2.0887 (2.6010)	Prec@(1,5) (49.5%, 80.4%)	
08/24 12:01:47午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][400/703]	Step 8144	lr 0.02429	Loss 2.7406 (2.5931)	Prec@(1,5) (49.4%, 80.5%)	
08/24 12:01:54午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][450/703]	Step 8194	lr 0.02429	Loss 2.6985 (2.5964)	Prec@(1,5) (49.4%, 80.5%)	
08/24 12:02:01午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][500/703]	Step 8244	lr 0.02429	Loss 2.7757 (2.5905)	Prec@(1,5) (49.4%, 80.6%)	
08/24 12:02:09午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][550/703]	Step 8294	lr 0.02429	Loss 3.0045 (2.5931)	Prec@(1,5) (49.4%, 80.5%)	
08/24 12:02:16午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][600/703]	Step 8344	lr 0.02429	Loss 2.3708 (2.5955)	Prec@(1,5) (49.4%, 80.5%)	
08/24 12:02:23午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][650/703]	Step 8394	lr 0.02429	Loss 2.6346 (2.5922)	Prec@(1,5) (49.3%, 80.5%)	
08/24 12:02:30午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][700/703]	Step 8444	lr 0.02429	Loss 2.6028 (2.5898)	Prec@(1,5) (49.4%, 80.6%)	
08/24 12:02:31午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][703/703]	Step 8447	lr 0.02429	Loss 2.9631 (2.5901)	Prec@(1,5) (49.4%, 80.6%)	
08/24 12:02:31午前 evaluateStage_trainer.py:148 [INFO] Train: [ 11/99] Final Prec@1 49.3911%
08/24 12:02:35午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 11/99] Final Prec@1 45.6200%
08/24 12:02:35午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 45.6200%
08/24 12:02:43午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][50/703]	Step 8498	lr 0.02416	Loss 2.1720 (2.5178)	Prec@(1,5) (50.5%, 81.7%)	
08/24 12:02:50午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][100/703]	Step 8548	lr 0.02416	Loss 2.6931 (2.5349)	Prec@(1,5) (50.5%, 81.6%)	
08/24 12:02:58午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][150/703]	Step 8598	lr 0.02416	Loss 2.1868 (2.5307)	Prec@(1,5) (51.1%, 81.1%)	
08/24 12:03:05午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][200/703]	Step 8648	lr 0.02416	Loss 2.8378 (2.5201)	Prec@(1,5) (51.1%, 81.3%)	
08/24 12:03:12午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][250/703]	Step 8698	lr 0.02416	Loss 2.5675 (2.5367)	Prec@(1,5) (50.6%, 81.0%)	
08/24 12:03:20午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][300/703]	Step 8748	lr 0.02416	Loss 2.1716 (2.5311)	Prec@(1,5) (50.7%, 81.1%)	
08/24 12:03:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][350/703]	Step 8798	lr 0.02416	Loss 2.2196 (2.5330)	Prec@(1,5) (50.7%, 81.0%)	
08/24 12:03:34午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][400/703]	Step 8848	lr 0.02416	Loss 2.3831 (2.5288)	Prec@(1,5) (50.7%, 81.2%)	
08/24 12:03:42午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][450/703]	Step 8898	lr 0.02416	Loss 2.7948 (2.5331)	Prec@(1,5) (50.6%, 81.1%)	
08/24 12:03:49午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][500/703]	Step 8948	lr 0.02416	Loss 2.4836 (2.5266)	Prec@(1,5) (50.7%, 81.1%)	
08/24 12:03:56午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][550/703]	Step 8998	lr 0.02416	Loss 2.2454 (2.5218)	Prec@(1,5) (50.8%, 81.2%)	
08/24 12:04:04午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][600/703]	Step 9048	lr 0.02416	Loss 2.6895 (2.5201)	Prec@(1,5) (50.8%, 81.3%)	
08/24 12:04:11午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][650/703]	Step 9098	lr 0.02416	Loss 2.3529 (2.5164)	Prec@(1,5) (50.9%, 81.3%)	
08/24 12:04:18午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][700/703]	Step 9148	lr 0.02416	Loss 2.6599 (2.5147)	Prec@(1,5) (50.9%, 81.3%)	
08/24 12:04:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][703/703]	Step 9151	lr 0.02416	Loss 2.7312 (2.5150)	Prec@(1,5) (50.9%, 81.3%)	
08/24 12:04:19午前 evaluateStage_trainer.py:148 [INFO] Train: [ 12/99] Final Prec@1 50.9111%
08/24 12:04:23午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 12/99] Final Prec@1 48.1800%
08/24 12:04:23午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 48.1800%
08/24 12:04:31午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][50/703]	Step 9202	lr 0.02401	Loss 2.5679 (2.4242)	Prec@(1,5) (52.7%, 82.6%)	
08/24 12:04:39午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][100/703]	Step 9252	lr 0.02401	Loss 2.6232 (2.3862)	Prec@(1,5) (53.0%, 83.1%)	
08/24 12:04:46午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][150/703]	Step 9302	lr 0.02401	Loss 2.6366 (2.4059)	Prec@(1,5) (52.8%, 82.9%)	
08/24 12:04:53午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][200/703]	Step 9352	lr 0.02401	Loss 2.4588 (2.4171)	Prec@(1,5) (52.5%, 82.9%)	
08/24 12:05:00午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][250/703]	Step 9402	lr 0.02401	Loss 3.0632 (2.4159)	Prec@(1,5) (52.7%, 82.9%)	
08/24 12:05:08午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][300/703]	Step 9452	lr 0.02401	Loss 2.6454 (2.4135)	Prec@(1,5) (52.7%, 82.8%)	
08/24 12:05:15午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][350/703]	Step 9502	lr 0.02401	Loss 2.7831 (2.4230)	Prec@(1,5) (52.5%, 82.6%)	
08/24 12:05:22午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][400/703]	Step 9552	lr 0.02401	Loss 2.7265 (2.4348)	Prec@(1,5) (52.3%, 82.6%)	
08/24 12:05:30午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][450/703]	Step 9602	lr 0.02401	Loss 2.3322 (2.4386)	Prec@(1,5) (52.3%, 82.5%)	
08/24 12:05:37午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][500/703]	Step 9652	lr 0.02401	Loss 2.2537 (2.4386)	Prec@(1,5) (52.2%, 82.4%)	
08/24 12:05:44午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][550/703]	Step 9702	lr 0.02401	Loss 2.4456 (2.4370)	Prec@(1,5) (52.1%, 82.4%)	
08/24 12:05:52午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][600/703]	Step 9752	lr 0.02401	Loss 2.5030 (2.4411)	Prec@(1,5) (52.1%, 82.3%)	
08/24 12:05:59午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][650/703]	Step 9802	lr 0.02401	Loss 2.0415 (2.4411)	Prec@(1,5) (52.1%, 82.4%)	
08/24 12:06:07午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][700/703]	Step 9852	lr 0.02401	Loss 2.2377 (2.4435)	Prec@(1,5) (52.1%, 82.3%)	
08/24 12:06:07午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][703/703]	Step 9855	lr 0.02401	Loss 2.4086 (2.4445)	Prec@(1,5) (52.1%, 82.3%)	
08/24 12:06:07午前 evaluateStage_trainer.py:148 [INFO] Train: [ 13/99] Final Prec@1 52.0778%
08/24 12:06:11午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 13/99] Final Prec@1 48.2000%
08/24 12:06:11午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 48.2000%
08/24 12:06:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][50/703]	Step 9906	lr 0.02386	Loss 2.5452 (2.3045)	Prec@(1,5) (53.9%, 84.0%)	
08/24 12:06:26午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][100/703]	Step 9956	lr 0.02386	Loss 2.2749 (2.3232)	Prec@(1,5) (53.9%, 84.3%)	
08/24 12:06:34午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][150/703]	Step 10006	lr 0.02386	Loss 2.7403 (2.3343)	Prec@(1,5) (53.6%, 84.1%)	
08/24 12:06:41午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][200/703]	Step 10056	lr 0.02386	Loss 2.3978 (2.3486)	Prec@(1,5) (53.6%, 83.8%)	
08/24 12:06:48午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][250/703]	Step 10106	lr 0.02386	Loss 2.4152 (2.3690)	Prec@(1,5) (53.1%, 83.6%)	
08/24 12:06:56午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][300/703]	Step 10156	lr 0.02386	Loss 2.4881 (2.3737)	Prec@(1,5) (53.1%, 83.5%)	
08/24 12:07:03午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][350/703]	Step 10206	lr 0.02386	Loss 2.4590 (2.3773)	Prec@(1,5) (53.1%, 83.3%)	
08/24 12:07:10午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][400/703]	Step 10256	lr 0.02386	Loss 2.5558 (2.3767)	Prec@(1,5) (53.2%, 83.2%)	
08/24 12:07:18午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][450/703]	Step 10306	lr 0.02386	Loss 2.3486 (2.3767)	Prec@(1,5) (53.2%, 83.2%)	
08/24 12:07:25午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][500/703]	Step 10356	lr 0.02386	Loss 2.5317 (2.3849)	Prec@(1,5) (53.0%, 83.1%)	
08/24 12:07:32午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][550/703]	Step 10406	lr 0.02386	Loss 2.2219 (2.3778)	Prec@(1,5) (53.0%, 83.2%)	
08/24 12:07:39午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][600/703]	Step 10456	lr 0.02386	Loss 2.2762 (2.3843)	Prec@(1,5) (52.9%, 83.1%)	
08/24 12:07:47午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][650/703]	Step 10506	lr 0.02386	Loss 2.6048 (2.3879)	Prec@(1,5) (52.8%, 83.0%)	
08/24 12:07:54午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][700/703]	Step 10556	lr 0.02386	Loss 2.8072 (2.3880)	Prec@(1,5) (52.8%, 83.1%)	
08/24 12:07:54午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][703/703]	Step 10559	lr 0.02386	Loss 2.0480 (2.3880)	Prec@(1,5) (52.8%, 83.0%)	
08/24 12:07:54午前 evaluateStage_trainer.py:148 [INFO] Train: [ 14/99] Final Prec@1 52.7844%
08/24 12:07:58午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 14/99] Final Prec@1 49.3200%
08/24 12:07:59午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 49.3200%
08/24 12:08:06午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][50/703]	Step 10610	lr 0.02369	Loss 2.4954 (2.2810)	Prec@(1,5) (55.1%, 84.1%)	
08/24 12:08:14午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][100/703]	Step 10660	lr 0.02369	Loss 2.1310 (2.2905)	Prec@(1,5) (54.9%, 84.0%)	
08/24 12:08:21午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][150/703]	Step 10710	lr 0.02369	Loss 2.6078 (2.3213)	Prec@(1,5) (54.3%, 83.8%)	
08/24 12:08:28午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][200/703]	Step 10760	lr 0.02369	Loss 2.3229 (2.3161)	Prec@(1,5) (54.5%, 83.8%)	
08/24 12:08:36午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][250/703]	Step 10810	lr 0.02369	Loss 2.0377 (2.3052)	Prec@(1,5) (54.6%, 83.9%)	
08/24 12:08:43午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][300/703]	Step 10860	lr 0.02369	Loss 2.2319 (2.3152)	Prec@(1,5) (54.6%, 83.7%)	
08/24 12:08:51午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][350/703]	Step 10910	lr 0.02369	Loss 1.7814 (2.3254)	Prec@(1,5) (54.3%, 83.7%)	
08/24 12:08:58午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][400/703]	Step 10960	lr 0.02369	Loss 2.1932 (2.3185)	Prec@(1,5) (54.3%, 83.8%)	
08/24 12:09:05午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][450/703]	Step 11010	lr 0.02369	Loss 2.5598 (2.3314)	Prec@(1,5) (54.1%, 83.7%)	
08/24 12:09:13午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][500/703]	Step 11060	lr 0.02369	Loss 2.0210 (2.3344)	Prec@(1,5) (54.0%, 83.6%)	
08/24 12:09:20午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][550/703]	Step 11110	lr 0.02369	Loss 2.5416 (2.3376)	Prec@(1,5) (54.0%, 83.6%)	
08/24 12:09:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][600/703]	Step 11160	lr 0.02369	Loss 2.6632 (2.3341)	Prec@(1,5) (54.0%, 83.7%)	
08/24 12:09:35午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][650/703]	Step 11210	lr 0.02369	Loss 2.4101 (2.3344)	Prec@(1,5) (54.0%, 83.7%)	
08/24 12:09:42午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][700/703]	Step 11260	lr 0.02369	Loss 2.7754 (2.3348)	Prec@(1,5) (54.0%, 83.7%)	
08/24 12:09:43午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][703/703]	Step 11263	lr 0.02369	Loss 2.8604 (2.3351)	Prec@(1,5) (54.0%, 83.7%)	
08/24 12:09:43午前 evaluateStage_trainer.py:148 [INFO] Train: [ 15/99] Final Prec@1 54.0111%
08/24 12:09:47午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 15/99] Final Prec@1 47.9200%
08/24 12:09:47午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 49.3200%
08/24 12:09:54午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][50/703]	Step 11314	lr 0.02352	Loss 2.2710 (2.1529)	Prec@(1,5) (56.7%, 86.6%)	
08/24 12:10:02午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][100/703]	Step 11364	lr 0.02352	Loss 2.5017 (2.2002)	Prec@(1,5) (56.6%, 85.7%)	
08/24 12:10:09午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][150/703]	Step 11414	lr 0.02352	Loss 2.1813 (2.2236)	Prec@(1,5) (55.9%, 85.2%)	
08/24 12:10:16午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][200/703]	Step 11464	lr 0.02352	Loss 2.4493 (2.2403)	Prec@(1,5) (55.6%, 85.1%)	
08/24 12:10:24午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][250/703]	Step 11514	lr 0.02352	Loss 2.2416 (2.2444)	Prec@(1,5) (55.3%, 85.1%)	
08/24 12:10:31午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][300/703]	Step 11564	lr 0.02352	Loss 2.3234 (2.2563)	Prec@(1,5) (55.1%, 84.7%)	
08/24 12:10:38午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][350/703]	Step 11614	lr 0.02352	Loss 2.0601 (2.2560)	Prec@(1,5) (55.1%, 84.8%)	
08/24 12:10:45午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][400/703]	Step 11664	lr 0.02352	Loss 2.2730 (2.2505)	Prec@(1,5) (55.1%, 84.8%)	
08/24 12:10:53午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][450/703]	Step 11714	lr 0.02352	Loss 2.6255 (2.2586)	Prec@(1,5) (55.1%, 84.6%)	
08/24 12:11:00午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][500/703]	Step 11764	lr 0.02352	Loss 2.4504 (2.2649)	Prec@(1,5) (55.0%, 84.6%)	
08/24 12:11:07午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][550/703]	Step 11814	lr 0.02352	Loss 2.2485 (2.2630)	Prec@(1,5) (54.9%, 84.7%)	
08/24 12:11:15午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][600/703]	Step 11864	lr 0.02352	Loss 2.2200 (2.2662)	Prec@(1,5) (54.9%, 84.7%)	
08/24 12:11:22午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][650/703]	Step 11914	lr 0.02352	Loss 2.2941 (2.2697)	Prec@(1,5) (54.8%, 84.7%)	
08/24 12:11:29午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][700/703]	Step 11964	lr 0.02352	Loss 2.4045 (2.2750)	Prec@(1,5) (54.8%, 84.6%)	
08/24 12:11:30午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][703/703]	Step 11967	lr 0.02352	Loss 2.4521 (2.2763)	Prec@(1,5) (54.8%, 84.6%)	
08/24 12:11:30午前 evaluateStage_trainer.py:148 [INFO] Train: [ 16/99] Final Prec@1 54.7889%
08/24 12:11:34午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 16/99] Final Prec@1 52.9600%
08/24 12:11:34午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 52.9600%
08/24 12:11:42午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][50/703]	Step 12018	lr 0.02333	Loss 2.4923 (2.1280)	Prec@(1,5) (57.2%, 86.3%)	
08/24 12:11:49午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][100/703]	Step 12068	lr 0.02333	Loss 1.7755 (2.1568)	Prec@(1,5) (56.5%, 86.2%)	
08/24 12:11:57午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][150/703]	Step 12118	lr 0.02333	Loss 1.9271 (2.1775)	Prec@(1,5) (56.4%, 85.9%)	
08/24 12:12:04午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][200/703]	Step 12168	lr 0.02333	Loss 2.5774 (2.1879)	Prec@(1,5) (56.3%, 85.9%)	
08/24 12:12:11午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][250/703]	Step 12218	lr 0.02333	Loss 2.2238 (2.1931)	Prec@(1,5) (56.2%, 85.7%)	
08/24 12:12:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][300/703]	Step 12268	lr 0.02333	Loss 2.0960 (2.2051)	Prec@(1,5) (56.0%, 85.5%)	
08/24 12:12:26午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][350/703]	Step 12318	lr 0.02333	Loss 2.2633 (2.2076)	Prec@(1,5) (55.9%, 85.4%)	
08/24 12:12:33午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][400/703]	Step 12368	lr 0.02333	Loss 2.3295 (2.2186)	Prec@(1,5) (55.7%, 85.2%)	
08/24 12:12:41午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][450/703]	Step 12418	lr 0.02333	Loss 1.8752 (2.2224)	Prec@(1,5) (55.5%, 85.2%)	
08/24 12:12:48午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][500/703]	Step 12468	lr 0.02333	Loss 2.3889 (2.2260)	Prec@(1,5) (55.4%, 85.2%)	
08/24 12:12:55午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][550/703]	Step 12518	lr 0.02333	Loss 2.5453 (2.2319)	Prec@(1,5) (55.3%, 85.1%)	
08/24 12:13:02午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][600/703]	Step 12568	lr 0.02333	Loss 2.0381 (2.2308)	Prec@(1,5) (55.4%, 85.1%)	
08/24 12:13:10午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][650/703]	Step 12618	lr 0.02333	Loss 2.0548 (2.2307)	Prec@(1,5) (55.5%, 85.1%)	
08/24 12:13:17午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][700/703]	Step 12668	lr 0.02333	Loss 2.2276 (2.2328)	Prec@(1,5) (55.5%, 85.0%)	
08/24 12:13:18午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][703/703]	Step 12671	lr 0.02333	Loss 1.6336 (2.2324)	Prec@(1,5) (55.5%, 85.0%)	
08/24 12:13:18午前 evaluateStage_trainer.py:148 [INFO] Train: [ 17/99] Final Prec@1 55.4533%
08/24 12:13:22午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 17/99] Final Prec@1 52.2000%
08/24 12:13:22午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 52.9600%
08/24 12:13:29午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][50/703]	Step 12722	lr 0.02313	Loss 2.5180 (2.2006)	Prec@(1,5) (55.5%, 86.2%)	
08/24 12:13:37午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][100/703]	Step 12772	lr 0.02313	Loss 2.4611 (2.1375)	Prec@(1,5) (56.8%, 86.7%)	
08/24 12:13:44午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][150/703]	Step 12822	lr 0.02313	Loss 2.9627 (2.1478)	Prec@(1,5) (56.9%, 86.6%)	
08/24 12:13:51午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][200/703]	Step 12872	lr 0.02313	Loss 2.2750 (2.1471)	Prec@(1,5) (57.1%, 86.3%)	
08/24 12:13:58午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][250/703]	Step 12922	lr 0.02313	Loss 2.4283 (2.1646)	Prec@(1,5) (56.6%, 86.2%)	
08/24 12:14:06午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][300/703]	Step 12972	lr 0.02313	Loss 1.6654 (2.1696)	Prec@(1,5) (56.5%, 86.1%)	
08/24 12:14:13午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][350/703]	Step 13022	lr 0.02313	Loss 2.2412 (2.1738)	Prec@(1,5) (56.4%, 86.0%)	
08/24 12:14:20午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][400/703]	Step 13072	lr 0.02313	Loss 2.2681 (2.1706)	Prec@(1,5) (56.5%, 86.0%)	
08/24 12:14:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][450/703]	Step 13122	lr 0.02313	Loss 2.1780 (2.1748)	Prec@(1,5) (56.5%, 85.8%)	
08/24 12:14:35午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][500/703]	Step 13172	lr 0.02313	Loss 2.0876 (2.1780)	Prec@(1,5) (56.5%, 85.7%)	
08/24 12:14:42午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][550/703]	Step 13222	lr 0.02313	Loss 2.3200 (2.1789)	Prec@(1,5) (56.5%, 85.7%)	
08/24 12:14:49午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][600/703]	Step 13272	lr 0.02313	Loss 2.3668 (2.1831)	Prec@(1,5) (56.4%, 85.7%)	
08/24 12:14:57午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][650/703]	Step 13322	lr 0.02313	Loss 2.3576 (2.1858)	Prec@(1,5) (56.4%, 85.6%)	
08/24 12:15:04午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][700/703]	Step 13372	lr 0.02313	Loss 2.3656 (2.1845)	Prec@(1,5) (56.3%, 85.6%)	
08/24 12:15:05午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][703/703]	Step 13375	lr 0.02313	Loss 2.1930 (2.1845)	Prec@(1,5) (56.3%, 85.6%)	
08/24 12:15:05午前 evaluateStage_trainer.py:148 [INFO] Train: [ 18/99] Final Prec@1 56.3422%
08/24 12:15:09午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 18/99] Final Prec@1 51.2600%
08/24 12:15:09午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 52.9600%
08/24 12:15:16午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][50/703]	Step 13426	lr 0.02292	Loss 2.2380 (2.1510)	Prec@(1,5) (57.3%, 86.0%)	
08/24 12:15:24午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][100/703]	Step 13476	lr 0.02292	Loss 1.8857 (2.1534)	Prec@(1,5) (57.1%, 85.8%)	
08/24 12:15:31午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][150/703]	Step 13526	lr 0.02292	Loss 2.0808 (2.1204)	Prec@(1,5) (58.1%, 86.1%)	
08/24 12:15:39午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][200/703]	Step 13576	lr 0.02292	Loss 2.5136 (2.1330)	Prec@(1,5) (57.8%, 86.0%)	
08/24 12:15:46午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][250/703]	Step 13626	lr 0.02292	Loss 2.0513 (2.1271)	Prec@(1,5) (57.7%, 86.2%)	
08/24 12:15:53午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][300/703]	Step 13676	lr 0.02292	Loss 2.6441 (2.1381)	Prec@(1,5) (57.6%, 86.1%)	
08/24 12:16:01午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][350/703]	Step 13726	lr 0.02292	Loss 2.4032 (2.1349)	Prec@(1,5) (57.6%, 86.1%)	
08/24 12:16:08午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][400/703]	Step 13776	lr 0.02292	Loss 1.5595 (2.1463)	Prec@(1,5) (57.4%, 86.0%)	
08/24 12:16:15午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][450/703]	Step 13826	lr 0.02292	Loss 2.3543 (2.1468)	Prec@(1,5) (57.3%, 86.0%)	
08/24 12:16:22午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][500/703]	Step 13876	lr 0.02292	Loss 2.1514 (2.1487)	Prec@(1,5) (57.2%, 85.9%)	
08/24 12:16:30午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][550/703]	Step 13926	lr 0.02292	Loss 2.0555 (2.1512)	Prec@(1,5) (57.1%, 85.9%)	
08/24 12:16:37午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][600/703]	Step 13976	lr 0.02292	Loss 2.6496 (2.1532)	Prec@(1,5) (57.0%, 85.9%)	
08/24 12:16:44午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][650/703]	Step 14026	lr 0.02292	Loss 2.3729 (2.1529)	Prec@(1,5) (57.0%, 85.9%)	
08/24 12:16:51午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][700/703]	Step 14076	lr 0.02292	Loss 2.0504 (2.1523)	Prec@(1,5) (57.0%, 85.9%)	
08/24 12:16:52午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][703/703]	Step 14079	lr 0.02292	Loss 2.3282 (2.1527)	Prec@(1,5) (57.0%, 85.9%)	
08/24 12:16:52午前 evaluateStage_trainer.py:148 [INFO] Train: [ 19/99] Final Prec@1 56.9800%
08/24 12:16:56午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 19/99] Final Prec@1 53.2600%
08/24 12:16:56午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 53.2600%
08/24 12:17:04午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][50/703]	Step 14130	lr 0.02271	Loss 1.9503 (2.0569)	Prec@(1,5) (58.7%, 86.9%)	
08/24 12:17:11午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][100/703]	Step 14180	lr 0.02271	Loss 2.0241 (2.0551)	Prec@(1,5) (58.6%, 87.1%)	
08/24 12:17:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][150/703]	Step 14230	lr 0.02271	Loss 2.2954 (2.0669)	Prec@(1,5) (58.7%, 87.0%)	
08/24 12:17:26午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][200/703]	Step 14280	lr 0.02271	Loss 2.1973 (2.0674)	Prec@(1,5) (58.5%, 87.0%)	
08/24 12:17:33午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][250/703]	Step 14330	lr 0.02271	Loss 1.8679 (2.0732)	Prec@(1,5) (58.2%, 86.9%)	
08/24 12:17:41午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][300/703]	Step 14380	lr 0.02271	Loss 2.3959 (2.0859)	Prec@(1,5) (58.1%, 86.8%)	
08/24 12:17:48午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][350/703]	Step 14430	lr 0.02271	Loss 2.0842 (2.0894)	Prec@(1,5) (58.0%, 86.7%)	
08/24 12:17:55午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][400/703]	Step 14480	lr 0.02271	Loss 2.1520 (2.0929)	Prec@(1,5) (57.9%, 86.7%)	
08/24 12:18:03午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][450/703]	Step 14530	lr 0.02271	Loss 2.1558 (2.0941)	Prec@(1,5) (57.9%, 86.6%)	
08/24 12:18:10午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][500/703]	Step 14580	lr 0.02271	Loss 1.8741 (2.0934)	Prec@(1,5) (57.8%, 86.7%)	
08/24 12:18:17午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][550/703]	Step 14630	lr 0.02271	Loss 2.1111 (2.1033)	Prec@(1,5) (57.7%, 86.6%)	
08/24 12:18:25午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][600/703]	Step 14680	lr 0.02271	Loss 2.1460 (2.1109)	Prec@(1,5) (57.5%, 86.5%)	
08/24 12:18:32午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][650/703]	Step 14730	lr 0.02271	Loss 2.3459 (2.1139)	Prec@(1,5) (57.4%, 86.4%)	
08/24 12:18:39午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][700/703]	Step 14780	lr 0.02271	Loss 1.8911 (2.1169)	Prec@(1,5) (57.4%, 86.4%)	
08/24 12:18:40午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][703/703]	Step 14783	lr 0.02271	Loss 1.7672 (2.1165)	Prec@(1,5) (57.4%, 86.4%)	
08/24 12:18:40午前 evaluateStage_trainer.py:148 [INFO] Train: [ 20/99] Final Prec@1 57.4089%
08/24 12:18:44午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 20/99] Final Prec@1 51.7600%
08/24 12:18:44午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 53.2600%
08/24 12:18:52午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][50/703]	Step 14834	lr 0.02248	Loss 2.0248 (2.0430)	Prec@(1,5) (59.2%, 86.7%)	
08/24 12:18:59午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][100/703]	Step 14884	lr 0.02248	Loss 1.9072 (1.9925)	Prec@(1,5) (59.8%, 87.7%)	
08/24 12:19:06午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][150/703]	Step 14934	lr 0.02248	Loss 1.7686 (2.0394)	Prec@(1,5) (58.7%, 87.2%)	
08/24 12:19:14午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][200/703]	Step 14984	lr 0.02248	Loss 1.7885 (2.0329)	Prec@(1,5) (58.7%, 87.3%)	
08/24 12:19:21午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][250/703]	Step 15034	lr 0.02248	Loss 1.8017 (2.0476)	Prec@(1,5) (58.6%, 87.0%)	
08/24 12:19:28午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][300/703]	Step 15084	lr 0.02248	Loss 2.0429 (2.0541)	Prec@(1,5) (58.4%, 87.0%)	
08/24 12:19:36午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][350/703]	Step 15134	lr 0.02248	Loss 2.3344 (2.0642)	Prec@(1,5) (58.6%, 86.9%)	
08/24 12:19:43午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][400/703]	Step 15184	lr 0.02248	Loss 2.6855 (2.0727)	Prec@(1,5) (58.3%, 86.8%)	
08/24 12:19:50午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][450/703]	Step 15234	lr 0.02248	Loss 1.3314 (2.0738)	Prec@(1,5) (58.2%, 86.8%)	
08/24 12:19:58午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][500/703]	Step 15284	lr 0.02248	Loss 2.3055 (2.0702)	Prec@(1,5) (58.3%, 86.8%)	
08/24 12:20:05午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][550/703]	Step 15334	lr 0.02248	Loss 2.0408 (2.0627)	Prec@(1,5) (58.4%, 87.0%)	
08/24 12:20:12午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][600/703]	Step 15384	lr 0.02248	Loss 2.3910 (2.0690)	Prec@(1,5) (58.3%, 87.0%)	
08/24 12:20:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][650/703]	Step 15434	lr 0.02248	Loss 2.1306 (2.0755)	Prec@(1,5) (58.3%, 86.9%)	
08/24 12:20:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][700/703]	Step 15484	lr 0.02248	Loss 2.1093 (2.0769)	Prec@(1,5) (58.3%, 86.9%)	
08/24 12:20:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][703/703]	Step 15487	lr 0.02248	Loss 2.2865 (2.0771)	Prec@(1,5) (58.3%, 86.9%)	
08/24 12:20:27午前 evaluateStage_trainer.py:148 [INFO] Train: [ 21/99] Final Prec@1 58.2822%
08/24 12:20:31午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 21/99] Final Prec@1 55.4600%
08/24 12:20:32午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 55.4600%
08/24 12:20:39午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][50/703]	Step 15538	lr 0.02225	Loss 1.7930 (1.9767)	Prec@(1,5) (60.6%, 87.8%)	
08/24 12:20:47午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][100/703]	Step 15588	lr 0.02225	Loss 1.8728 (1.9933)	Prec@(1,5) (60.3%, 87.8%)	
08/24 12:20:54午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][150/703]	Step 15638	lr 0.02225	Loss 1.9077 (1.9898)	Prec@(1,5) (60.1%, 87.7%)	
08/24 12:21:01午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][200/703]	Step 15688	lr 0.02225	Loss 1.9699 (1.9951)	Prec@(1,5) (59.8%, 87.6%)	
08/24 12:21:09午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][250/703]	Step 15738	lr 0.02225	Loss 1.8099 (1.9934)	Prec@(1,5) (59.9%, 87.7%)	
08/24 12:21:16午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][300/703]	Step 15788	lr 0.02225	Loss 2.2170 (2.0138)	Prec@(1,5) (59.5%, 87.4%)	
08/24 12:21:23午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][350/703]	Step 15838	lr 0.02225	Loss 1.9098 (2.0214)	Prec@(1,5) (59.3%, 87.4%)	
08/24 12:21:30午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][400/703]	Step 15888	lr 0.02225	Loss 1.8043 (2.0208)	Prec@(1,5) (59.3%, 87.4%)	
08/24 12:21:38午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][450/703]	Step 15938	lr 0.02225	Loss 1.9694 (2.0222)	Prec@(1,5) (59.3%, 87.4%)	
08/24 12:21:45午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][500/703]	Step 15988	lr 0.02225	Loss 2.0462 (2.0243)	Prec@(1,5) (59.3%, 87.4%)	
08/24 12:21:53午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][550/703]	Step 16038	lr 0.02225	Loss 2.4508 (2.0324)	Prec@(1,5) (59.2%, 87.2%)	
08/24 12:22:00午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][600/703]	Step 16088	lr 0.02225	Loss 1.8889 (2.0404)	Prec@(1,5) (59.0%, 87.2%)	
08/24 12:22:07午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][650/703]	Step 16138	lr 0.02225	Loss 1.4849 (2.0420)	Prec@(1,5) (59.1%, 87.1%)	
08/24 12:22:15午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][700/703]	Step 16188	lr 0.02225	Loss 2.0841 (2.0429)	Prec@(1,5) (59.0%, 87.1%)	
08/24 12:22:15午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][703/703]	Step 16191	lr 0.02225	Loss 2.2021 (2.0429)	Prec@(1,5) (59.0%, 87.1%)	
08/24 12:22:15午前 evaluateStage_trainer.py:148 [INFO] Train: [ 22/99] Final Prec@1 59.0267%
08/24 12:22:19午前 evaluateStage_trainer.py:183 [INFO] Valid: [ 22/99] Final Prec@1 52.9400%
08/24 12:22:19午前 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 55.4600%
08/24 12:22:27午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][50/703]	Step 16242	lr 0.022	Loss 1.9219 (2.0478)	Prec@(1,5) (59.4%, 87.1%)	
08/24 12:22:34午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][100/703]	Step 16292	lr 0.022	Loss 1.9391 (2.0118)	Prec@(1,5) (59.7%, 87.0%)	
08/24 12:22:42午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][150/703]	Step 16342	lr 0.022	Loss 1.7810 (1.9839)	Prec@(1,5) (60.5%, 87.3%)	
08/24 12:22:49午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][200/703]	Step 16392	lr 0.022	Loss 1.7989 (1.9826)	Prec@(1,5) (60.0%, 87.7%)	
08/24 12:22:57午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][250/703]	Step 16442	lr 0.022	Loss 1.5149 (1.9943)	Prec@(1,5) (59.8%, 87.5%)	
08/24 12:23:04午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][300/703]	Step 16492	lr 0.022	Loss 1.8340 (1.9957)	Prec@(1,5) (59.7%, 87.5%)	
08/24 12:23:11午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][350/703]	Step 16542	lr 0.022	Loss 2.1646 (1.9931)	Prec@(1,5) (59.8%, 87.5%)	
08/24 12:23:19午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][400/703]	Step 16592	lr 0.022	Loss 2.2411 (1.9965)	Prec@(1,5) (59.8%, 87.4%)	
08/24 12:23:26午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][450/703]	Step 16642	lr 0.022	Loss 1.8732 (2.0008)	Prec@(1,5) (59.7%, 87.4%)	
08/24 12:23:33午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][500/703]	Step 16692	lr 0.022	Loss 1.8172 (2.0041)	Prec@(1,5) (59.7%, 87.5%)	
08/24 12:23:40午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][550/703]	Step 16742	lr 0.022	Loss 2.1604 (2.0088)	Prec@(1,5) (59.6%, 87.4%)	
08/24 12:23:48午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][600/703]	Step 16792	lr 0.022	Loss 1.8058 (2.0089)	Prec@(1,5) (59.6%, 87.4%)	
08/24 12:23:55午前 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][650/703]	Step 16842	lr 0.022	Loss 2.0873 (2.0124)	Prec@(1,5) (59.6%, 87.4%)	
