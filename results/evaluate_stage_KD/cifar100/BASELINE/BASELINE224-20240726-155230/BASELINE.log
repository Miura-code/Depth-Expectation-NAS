07/26 03:52:30PM parser.py:28 [INFO] 
07/26 03:52:30PM parser.py:29 [INFO] Parameters:
07/26 03:52:30PM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/26 03:52:30PM parser.py:31 [INFO] T=4.0
07/26 03:52:30PM parser.py:31 [INFO] ADVANCED=True
07/26 03:52:30PM parser.py:31 [INFO] AUX_WEIGHT=0.4
07/26 03:52:30PM parser.py:31 [INFO] BATCH_SIZE=64
07/26 03:52:30PM parser.py:31 [INFO] CUTOUT_LENGTH=16
07/26 03:52:30PM parser.py:31 [INFO] DATA_PATH=../data/
07/26 03:52:30PM parser.py:31 [INFO] DATASET=cifar100
07/26 03:52:30PM parser.py:31 [INFO] DESCRIPTION=baseline_stage-level
07/26 03:52:30PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
07/26 03:52:30PM parser.py:31 [INFO] EPOCHS=100
07/26 03:52:30PM parser.py:31 [INFO] EXP_NAME=BASELINE224-20240726-155230
07/26 03:52:30PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('skip_connect', 1), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 2)], [('skip_connect', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 1)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal3_concat=range(2, 6))
07/26 03:52:30PM parser.py:31 [INFO] GPUS=[0]
07/26 03:52:30PM parser.py:31 [INFO] GRAD_CLIP=5.0
07/26 03:52:30PM parser.py:31 [INFO] INIT_CHANNELS=32
07/26 03:52:30PM parser.py:31 [INFO] L=0.6
07/26 03:52:30PM parser.py:31 [INFO] LAYERS=20
07/26 03:52:30PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/26 03:52:30PM parser.py:31 [INFO] LR=0.025
07/26 03:52:30PM parser.py:31 [INFO] LR_MIN=0.001
07/26 03:52:30PM parser.py:31 [INFO] MOMENTUM=0.9
07/26 03:52:30PM parser.py:31 [INFO] NAME=BASELINE
07/26 03:52:30PM parser.py:31 [INFO] NONKD=True
07/26 03:52:30PM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/BASELINE/BASELINE224-20240726-155230
07/26 03:52:30PM parser.py:31 [INFO] PRINT_FREQ=50
07/26 03:52:30PM parser.py:31 [INFO] RESUME_PATH=None
07/26 03:52:30PM parser.py:31 [INFO] SAVE=BASELINE224
07/26 03:52:30PM parser.py:31 [INFO] SEED=0
07/26 03:52:30PM parser.py:31 [INFO] SPEC_CELL=True
07/26 03:52:30PM parser.py:31 [INFO] TEACHER_NAME=non
07/26 03:52:30PM parser.py:31 [INFO] TEACHER_PATH=non
07/26 03:52:30PM parser.py:31 [INFO] TRAIN_PORTION=0.9
07/26 03:52:30PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
07/26 03:52:30PM parser.py:31 [INFO] WORKERS=4
07/26 03:52:30PM parser.py:32 [INFO] 
07/26 03:52:32PM evaluateStage_trainer.py:82 [INFO] --> No loaded checkpoint!
07/26 03:52:42PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][50/703]	Step 50	lr 0.025	Loss 6.3584 (6.4656)	Prec@(1,5) (1.4%, 7.0%)	
07/26 03:52:49PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][100/703]	Step 100	lr 0.025	Loss 6.4526 (6.3866)	Prec@(1,5) (2.2%, 10.0%)	
07/26 03:52:55PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][150/703]	Step 150	lr 0.025	Loss 5.9238 (6.2634)	Prec@(1,5) (3.1%, 13.0%)	
07/26 03:53:02PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][200/703]	Step 200	lr 0.025	Loss 6.0040 (6.1552)	Prec@(1,5) (3.9%, 15.4%)	
07/26 03:53:08PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][250/703]	Step 250	lr 0.025	Loss 5.8811 (6.0508)	Prec@(1,5) (4.6%, 17.8%)	
07/26 03:53:15PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][300/703]	Step 300	lr 0.025	Loss 5.2848 (5.9528)	Prec@(1,5) (5.6%, 20.0%)	
07/26 03:53:21PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][350/703]	Step 350	lr 0.025	Loss 5.9005 (5.8761)	Prec@(1,5) (6.2%, 21.7%)	
07/26 03:53:28PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][400/703]	Step 400	lr 0.025	Loss 5.2137 (5.8044)	Prec@(1,5) (6.8%, 23.2%)	
07/26 03:53:34PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][450/703]	Step 450	lr 0.025	Loss 5.1621 (5.7399)	Prec@(1,5) (7.4%, 24.7%)	
07/26 03:53:40PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][500/703]	Step 500	lr 0.025	Loss 5.0632 (5.6755)	Prec@(1,5) (8.0%, 26.0%)	
07/26 03:53:46PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][550/703]	Step 550	lr 0.025	Loss 5.1811 (5.6237)	Prec@(1,5) (8.4%, 27.2%)	
07/26 03:53:53PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][600/703]	Step 600	lr 0.025	Loss 5.2714 (5.5747)	Prec@(1,5) (8.9%, 28.3%)	
07/26 03:53:59PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][650/703]	Step 650	lr 0.025	Loss 4.9185 (5.5294)	Prec@(1,5) (9.4%, 29.3%)	
07/26 03:54:06PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][700/703]	Step 700	lr 0.025	Loss 4.9884 (5.4888)	Prec@(1,5) (9.8%, 30.1%)	
07/26 03:54:06PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][703/703]	Step 703	lr 0.025	Loss 5.1203 (5.4866)	Prec@(1,5) (9.8%, 30.2%)	
07/26 03:54:08PM evaluateStage_trainer.py:148 [INFO] Train: [  0/99] Final Prec@1 9.7978%
