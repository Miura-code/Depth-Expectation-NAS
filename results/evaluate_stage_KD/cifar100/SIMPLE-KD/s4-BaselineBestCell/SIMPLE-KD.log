09/05 01:36:02PM parser.py:28 [INFO] 
09/05 01:36:02PM parser.py:29 [INFO] Parameters:
09/05 01:36:02PM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 1), ('skip_connect', 0)]], DAG3_concat=range(6, 8))
09/05 01:36:02PM parser.py:31 [INFO] T=10.0
09/05 01:36:02PM parser.py:31 [INFO] ADVANCED=True
09/05 01:36:02PM parser.py:31 [INFO] AUX_WEIGHT=0.4
09/05 01:36:02PM parser.py:31 [INFO] BATCH_SIZE=64
09/05 01:36:02PM parser.py:31 [INFO] CUTOUT_LENGTH=16
09/05 01:36:02PM parser.py:31 [INFO] DATA_PATH=../data/
09/05 01:36:02PM parser.py:31 [INFO] DATASET=cifar100
09/05 01:36:02PM parser.py:31 [INFO] DESCRIPTION=search_architecture_on_simpleKD_without_depth_loss_with_large_slidewindoe
09/05 01:36:02PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
09/05 01:36:02PM parser.py:31 [INFO] EPOCHS=100
09/05 01:36:02PM parser.py:31 [INFO] EXP_NAME=s4-BaselineBestCell
09/05 01:36:02PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
09/05 01:36:02PM parser.py:31 [INFO] GPUS=[0]
09/05 01:36:02PM parser.py:31 [INFO] GRAD_CLIP=5.0
09/05 01:36:02PM parser.py:31 [INFO] INIT_CHANNELS=32
09/05 01:36:02PM parser.py:31 [INFO] L=0.5
09/05 01:36:02PM parser.py:31 [INFO] LAYERS=20
09/05 01:36:02PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
09/05 01:36:02PM parser.py:31 [INFO] LR=0.025
09/05 01:36:02PM parser.py:31 [INFO] LR_MIN=0.001
09/05 01:36:02PM parser.py:31 [INFO] MOMENTUM=0.9
09/05 01:36:02PM parser.py:31 [INFO] NAME=SIMPLE-KD
09/05 01:36:02PM parser.py:31 [INFO] NONKD=True
09/05 01:36:02PM parser.py:31 [INFO] PATH=results/evaluate_stage_KD/cifar100/SIMPLE-KD/s4-BaselineBestCell
09/05 01:36:02PM parser.py:31 [INFO] PRINT_FREQ=50
09/05 01:36:02PM parser.py:31 [INFO] RESUME_PATH=None
09/05 01:36:02PM parser.py:31 [INFO] SAVE=s4-BaselineBestCell
09/05 01:36:02PM parser.py:31 [INFO] SEED=0
09/05 01:36:02PM parser.py:31 [INFO] SPEC_CELL=True
09/05 01:36:02PM parser.py:31 [INFO] TEACHER_NAME=non
09/05 01:36:02PM parser.py:31 [INFO] TEACHER_PATH=non
09/05 01:36:02PM parser.py:31 [INFO] TRAIN_PORTION=0.9
09/05 01:36:02PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
09/05 01:36:02PM parser.py:31 [INFO] WORKERS=4
09/05 01:36:02PM parser.py:32 [INFO] 
09/05 01:36:04PM evaluateStage_trainer.py:82 [INFO] --> No loaded checkpoint!
09/05 01:36:09PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][50/703]	Step 50	lr 0.025	Loss 6.1611 (6.2967)	Prec@(1,5) (3.6%, 13.9%)	
09/05 01:36:13PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][100/703]	Step 100	lr 0.025	Loss 6.0139 (6.0754)	Prec@(1,5) (5.2%, 18.5%)	
09/05 01:36:16PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][150/703]	Step 150	lr 0.025	Loss 5.3079 (5.9349)	Prec@(1,5) (5.7%, 20.9%)	
09/05 01:36:19PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][200/703]	Step 200	lr 0.025	Loss 5.3962 (5.8335)	Prec@(1,5) (6.3%, 23.0%)	
09/05 01:36:22PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][250/703]	Step 250	lr 0.025	Loss 5.3116 (5.7349)	Prec@(1,5) (7.1%, 24.9%)	
09/05 01:36:25PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][300/703]	Step 300	lr 0.025	Loss 5.0111 (5.6672)	Prec@(1,5) (7.8%, 26.2%)	
09/05 01:36:28PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][350/703]	Step 350	lr 0.025	Loss 5.0675 (5.6012)	Prec@(1,5) (8.5%, 27.6%)	
09/05 01:36:31PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][400/703]	Step 400	lr 0.025	Loss 5.0189 (5.5461)	Prec@(1,5) (8.9%, 28.8%)	
09/05 01:36:34PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][450/703]	Step 450	lr 0.025	Loss 4.9658 (5.4908)	Prec@(1,5) (9.4%, 30.0%)	
09/05 01:36:37PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][500/703]	Step 500	lr 0.025	Loss 4.8620 (5.4411)	Prec@(1,5) (9.8%, 31.0%)	
09/05 01:36:40PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][550/703]	Step 550	lr 0.025	Loss 5.1007 (5.3917)	Prec@(1,5) (10.4%, 32.0%)	
09/05 01:36:43PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][600/703]	Step 600	lr 0.025	Loss 4.6792 (5.3445)	Prec@(1,5) (10.9%, 33.1%)	
09/05 01:36:45PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][650/703]	Step 650	lr 0.025	Loss 4.7345 (5.2989)	Prec@(1,5) (11.4%, 34.1%)	
09/05 01:36:48PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][700/703]	Step 700	lr 0.025	Loss 4.3158 (5.2527)	Prec@(1,5) (11.9%, 35.0%)	
09/05 01:36:49PM evaluateStage_trainer.py:141 [INFO] Train: Epoch: [0][703/703]	Step 703	lr 0.025	Loss 4.9277 (5.2510)	Prec@(1,5) (11.9%, 35.0%)	
09/05 01:36:49PM evaluateStage_trainer.py:148 [INFO] Train: [  0/99] Final Prec@1 11.9156%
09/05 01:36:52PM evaluateStage_trainer.py:183 [INFO] Valid: [  0/99] Final Prec@1 17.2600%
09/05 01:36:52午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 17.2600%
09/05 01:36:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][50/703]	Step 754	lr 0.02499	Loss 4.3546 (4.5888)	Prec@(1,5) (18.9%, 48.2%)	
09/05 01:37:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][100/703]	Step 804	lr 0.02499	Loss 4.4634 (4.5297)	Prec@(1,5) (19.8%, 49.0%)	
09/05 01:37:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][150/703]	Step 854	lr 0.02499	Loss 4.0340 (4.4988)	Prec@(1,5) (20.5%, 49.9%)	
09/05 01:37:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][200/703]	Step 904	lr 0.02499	Loss 4.4908 (4.4753)	Prec@(1,5) (20.9%, 50.1%)	
09/05 01:37:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][250/703]	Step 954	lr 0.02499	Loss 4.1743 (4.4621)	Prec@(1,5) (20.9%, 50.5%)	
09/05 01:37:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][300/703]	Step 1004	lr 0.02499	Loss 4.5632 (4.4358)	Prec@(1,5) (21.2%, 51.2%)	
09/05 01:37:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][350/703]	Step 1054	lr 0.02499	Loss 3.9987 (4.4076)	Prec@(1,5) (21.6%, 51.7%)	
09/05 01:37:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][400/703]	Step 1104	lr 0.02499	Loss 3.7339 (4.3803)	Prec@(1,5) (22.0%, 52.3%)	
09/05 01:37:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][450/703]	Step 1154	lr 0.02499	Loss 4.2266 (4.3521)	Prec@(1,5) (22.4%, 52.9%)	
09/05 01:37:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][500/703]	Step 1204	lr 0.02499	Loss 4.5130 (4.3275)	Prec@(1,5) (22.7%, 53.4%)	
09/05 01:37:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][550/703]	Step 1254	lr 0.02499	Loss 4.0546 (4.3028)	Prec@(1,5) (23.1%, 53.9%)	
09/05 01:37:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][600/703]	Step 1304	lr 0.02499	Loss 4.1411 (4.2724)	Prec@(1,5) (23.5%, 54.4%)	
09/05 01:37:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][650/703]	Step 1354	lr 0.02499	Loss 4.0014 (4.2489)	Prec@(1,5) (23.9%, 54.8%)	
09/05 01:37:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][700/703]	Step 1404	lr 0.02499	Loss 3.7443 (4.2254)	Prec@(1,5) (24.3%, 55.2%)	
09/05 01:37:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [1][703/703]	Step 1407	lr 0.02499	Loss 3.7357 (4.2245)	Prec@(1,5) (24.3%, 55.2%)	
09/05 01:37:52午後 evaluateStage_trainer.py:148 [INFO] Train: [  1/99] Final Prec@1 24.2978%
09/05 01:37:54午後 evaluateStage_trainer.py:183 [INFO] Valid: [  1/99] Final Prec@1 27.4800%
09/05 01:37:55午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 27.4800%
09/05 01:37:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][50/703]	Step 1458	lr 0.02498	Loss 4.2118 (3.9190)	Prec@(1,5) (29.6%, 60.9%)	
09/05 01:38:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][100/703]	Step 1508	lr 0.02498	Loss 3.7794 (3.8434)	Prec@(1,5) (30.9%, 62.0%)	
09/05 01:38:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][150/703]	Step 1558	lr 0.02498	Loss 3.4973 (3.8065)	Prec@(1,5) (31.7%, 62.6%)	
09/05 01:38:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][200/703]	Step 1608	lr 0.02498	Loss 3.9486 (3.7909)	Prec@(1,5) (31.7%, 62.8%)	
09/05 01:38:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][250/703]	Step 1658	lr 0.02498	Loss 4.1625 (3.7753)	Prec@(1,5) (31.7%, 63.1%)	
09/05 01:38:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][300/703]	Step 1708	lr 0.02498	Loss 3.2799 (3.7478)	Prec@(1,5) (32.0%, 63.5%)	
09/05 01:38:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][350/703]	Step 1758	lr 0.02498	Loss 4.1305 (3.7273)	Prec@(1,5) (32.1%, 64.0%)	
09/05 01:38:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][400/703]	Step 1808	lr 0.02498	Loss 3.3389 (3.7175)	Prec@(1,5) (32.3%, 64.2%)	
09/05 01:38:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][450/703]	Step 1858	lr 0.02498	Loss 3.5906 (3.7023)	Prec@(1,5) (32.3%, 64.5%)	
09/05 01:38:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][500/703]	Step 1908	lr 0.02498	Loss 3.4614 (3.6821)	Prec@(1,5) (32.6%, 64.8%)	
09/05 01:38:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][550/703]	Step 1958	lr 0.02498	Loss 3.3963 (3.6720)	Prec@(1,5) (32.7%, 64.9%)	
09/05 01:38:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][600/703]	Step 2008	lr 0.02498	Loss 3.5808 (3.6557)	Prec@(1,5) (33.0%, 65.2%)	
09/05 01:38:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][650/703]	Step 2058	lr 0.02498	Loss 3.3287 (3.6374)	Prec@(1,5) (33.3%, 65.4%)	
09/05 01:38:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][700/703]	Step 2108	lr 0.02498	Loss 3.3598 (3.6218)	Prec@(1,5) (33.4%, 65.7%)	
09/05 01:38:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [2][703/703]	Step 2111	lr 0.02498	Loss 3.6454 (3.6208)	Prec@(1,5) (33.4%, 65.7%)	
09/05 01:38:54午後 evaluateStage_trainer.py:148 [INFO] Train: [  2/99] Final Prec@1 33.4111%
09/05 01:38:57午後 evaluateStage_trainer.py:183 [INFO] Valid: [  2/99] Final Prec@1 33.3400%
09/05 01:38:57午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 33.3400%
09/05 01:39:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][50/703]	Step 2162	lr 0.02495	Loss 3.2678 (3.3519)	Prec@(1,5) (37.5%, 71.0%)	
09/05 01:39:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][100/703]	Step 2212	lr 0.02495	Loss 2.7975 (3.2639)	Prec@(1,5) (39.0%, 71.8%)	
09/05 01:39:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][150/703]	Step 2262	lr 0.02495	Loss 3.4909 (3.2915)	Prec@(1,5) (38.7%, 71.2%)	
09/05 01:39:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][200/703]	Step 2312	lr 0.02495	Loss 3.2855 (3.2918)	Prec@(1,5) (38.5%, 71.0%)	
09/05 01:39:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][250/703]	Step 2362	lr 0.02495	Loss 3.3540 (3.2843)	Prec@(1,5) (38.6%, 71.0%)	
09/05 01:39:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][300/703]	Step 2412	lr 0.02495	Loss 3.0429 (3.2762)	Prec@(1,5) (38.5%, 71.2%)	
09/05 01:39:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][350/703]	Step 2462	lr 0.02495	Loss 2.8689 (3.2860)	Prec@(1,5) (38.2%, 70.9%)	
09/05 01:39:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][400/703]	Step 2512	lr 0.02495	Loss 3.6810 (3.2803)	Prec@(1,5) (38.2%, 70.9%)	
09/05 01:39:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][450/703]	Step 2562	lr 0.02495	Loss 3.4019 (3.2717)	Prec@(1,5) (38.4%, 71.0%)	
09/05 01:39:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][500/703]	Step 2612	lr 0.02495	Loss 3.5007 (3.2665)	Prec@(1,5) (38.3%, 71.2%)	
09/05 01:39:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][550/703]	Step 2662	lr 0.02495	Loss 2.7636 (3.2539)	Prec@(1,5) (38.6%, 71.5%)	
09/05 01:39:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][600/703]	Step 2712	lr 0.02495	Loss 2.8586 (3.2443)	Prec@(1,5) (38.7%, 71.6%)	
09/05 01:39:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][650/703]	Step 2762	lr 0.02495	Loss 2.9495 (3.2317)	Prec@(1,5) (38.9%, 71.8%)	
09/05 01:39:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][700/703]	Step 2812	lr 0.02495	Loss 2.8899 (3.2273)	Prec@(1,5) (39.0%, 71.9%)	
09/05 01:39:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [3][703/703]	Step 2815	lr 0.02495	Loss 2.9335 (3.2262)	Prec@(1,5) (39.0%, 71.9%)	
09/05 01:39:55午後 evaluateStage_trainer.py:148 [INFO] Train: [  3/99] Final Prec@1 38.9911%
09/05 01:39:57午後 evaluateStage_trainer.py:183 [INFO] Valid: [  3/99] Final Prec@1 36.4800%
09/05 01:39:57午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 36.4800%
09/05 01:40:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][50/703]	Step 2866	lr 0.02491	Loss 3.4345 (3.0376)	Prec@(1,5) (41.1%, 74.4%)	
09/05 01:40:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][100/703]	Step 2916	lr 0.02491	Loss 3.5305 (3.0097)	Prec@(1,5) (42.2%, 74.9%)	
09/05 01:40:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][150/703]	Step 2966	lr 0.02491	Loss 3.4219 (3.0261)	Prec@(1,5) (42.0%, 74.5%)	
09/05 01:40:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][200/703]	Step 3016	lr 0.02491	Loss 2.8683 (3.0280)	Prec@(1,5) (42.2%, 74.4%)	
09/05 01:40:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][250/703]	Step 3066	lr 0.02491	Loss 3.3278 (3.0197)	Prec@(1,5) (42.4%, 74.7%)	
09/05 01:40:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][300/703]	Step 3116	lr 0.02491	Loss 2.5363 (3.0114)	Prec@(1,5) (42.5%, 74.8%)	
09/05 01:40:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][350/703]	Step 3166	lr 0.02491	Loss 2.8496 (3.0016)	Prec@(1,5) (42.6%, 74.9%)	
09/05 01:40:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][400/703]	Step 3216	lr 0.02491	Loss 3.0937 (2.9991)	Prec@(1,5) (42.6%, 75.0%)	
09/05 01:40:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][450/703]	Step 3266	lr 0.02491	Loss 3.2882 (2.9968)	Prec@(1,5) (42.8%, 75.0%)	
09/05 01:40:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][500/703]	Step 3316	lr 0.02491	Loss 2.7187 (2.9889)	Prec@(1,5) (42.9%, 75.2%)	
09/05 01:40:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][550/703]	Step 3366	lr 0.02491	Loss 2.9778 (2.9848)	Prec@(1,5) (43.0%, 75.2%)	
09/05 01:40:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][600/703]	Step 3416	lr 0.02491	Loss 3.3492 (2.9735)	Prec@(1,5) (43.2%, 75.3%)	
09/05 01:40:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][650/703]	Step 3466	lr 0.02491	Loss 3.1978 (2.9630)	Prec@(1,5) (43.3%, 75.5%)	
09/05 01:40:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][700/703]	Step 3516	lr 0.02491	Loss 3.1306 (2.9554)	Prec@(1,5) (43.4%, 75.6%)	
09/05 01:40:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [4][703/703]	Step 3519	lr 0.02491	Loss 3.4847 (2.9558)	Prec@(1,5) (43.4%, 75.6%)	
09/05 01:40:58午後 evaluateStage_trainer.py:148 [INFO] Train: [  4/99] Final Prec@1 43.4200%
09/05 01:41:00午後 evaluateStage_trainer.py:183 [INFO] Valid: [  4/99] Final Prec@1 39.8800%
09/05 01:41:01午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 39.8800%
09/05 01:41:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][50/703]	Step 3570	lr 0.02485	Loss 3.4124 (2.8003)	Prec@(1,5) (45.9%, 78.0%)	
09/05 01:41:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][100/703]	Step 3620	lr 0.02485	Loss 3.0623 (2.7903)	Prec@(1,5) (46.2%, 78.1%)	
09/05 01:41:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][150/703]	Step 3670	lr 0.02485	Loss 2.0639 (2.7725)	Prec@(1,5) (46.0%, 78.2%)	
09/05 01:41:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][200/703]	Step 3720	lr 0.02485	Loss 3.2670 (2.7555)	Prec@(1,5) (46.4%, 78.2%)	
09/05 01:41:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][250/703]	Step 3770	lr 0.02485	Loss 3.0485 (2.7591)	Prec@(1,5) (46.4%, 78.3%)	
09/05 01:41:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][300/703]	Step 3820	lr 0.02485	Loss 3.1290 (2.7666)	Prec@(1,5) (46.4%, 78.2%)	
09/05 01:41:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][350/703]	Step 3870	lr 0.02485	Loss 3.0174 (2.7757)	Prec@(1,5) (46.2%, 78.1%)	
09/05 01:41:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][400/703]	Step 3920	lr 0.02485	Loss 3.2133 (2.7703)	Prec@(1,5) (46.3%, 78.2%)	
09/05 01:41:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][450/703]	Step 3970	lr 0.02485	Loss 2.9443 (2.7724)	Prec@(1,5) (46.2%, 78.2%)	
09/05 01:41:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][500/703]	Step 4020	lr 0.02485	Loss 2.8840 (2.7694)	Prec@(1,5) (46.3%, 78.3%)	
09/05 01:41:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][550/703]	Step 4070	lr 0.02485	Loss 2.2751 (2.7699)	Prec@(1,5) (46.3%, 78.2%)	
09/05 01:41:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][600/703]	Step 4120	lr 0.02485	Loss 3.0935 (2.7779)	Prec@(1,5) (46.1%, 78.2%)	
09/05 01:41:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][650/703]	Step 4170	lr 0.02485	Loss 2.5420 (2.7718)	Prec@(1,5) (46.3%, 78.2%)	
09/05 01:42:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][700/703]	Step 4220	lr 0.02485	Loss 2.4772 (2.7686)	Prec@(1,5) (46.3%, 78.2%)	
09/05 01:42:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [5][703/703]	Step 4223	lr 0.02485	Loss 2.7576 (2.7690)	Prec@(1,5) (46.3%, 78.2%)	
09/05 01:42:00午後 evaluateStage_trainer.py:148 [INFO] Train: [  5/99] Final Prec@1 46.3222%
09/05 01:42:03午後 evaluateStage_trainer.py:183 [INFO] Valid: [  5/99] Final Prec@1 43.8000%
09/05 01:42:03午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 43.8000%
09/05 01:42:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][50/703]	Step 4274	lr 0.02479	Loss 2.5628 (2.5868)	Prec@(1,5) (49.2%, 80.2%)	
09/05 01:42:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][100/703]	Step 4324	lr 0.02479	Loss 2.4223 (2.6056)	Prec@(1,5) (49.2%, 80.1%)	
09/05 01:42:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][150/703]	Step 4374	lr 0.02479	Loss 1.9204 (2.6125)	Prec@(1,5) (48.9%, 80.1%)	
09/05 01:42:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][200/703]	Step 4424	lr 0.02479	Loss 2.9939 (2.6152)	Prec@(1,5) (48.9%, 80.0%)	
09/05 01:42:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][250/703]	Step 4474	lr 0.02479	Loss 2.0600 (2.6195)	Prec@(1,5) (48.7%, 79.9%)	
09/05 01:42:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][300/703]	Step 4524	lr 0.02479	Loss 2.5107 (2.6210)	Prec@(1,5) (48.6%, 79.9%)	
09/05 01:42:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][350/703]	Step 4574	lr 0.02479	Loss 2.7237 (2.6225)	Prec@(1,5) (48.6%, 79.8%)	
09/05 01:42:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][400/703]	Step 4624	lr 0.02479	Loss 2.4295 (2.6137)	Prec@(1,5) (48.9%, 80.0%)	
09/05 01:42:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][450/703]	Step 4674	lr 0.02479	Loss 1.9651 (2.6137)	Prec@(1,5) (48.8%, 80.0%)	
09/05 01:42:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][500/703]	Step 4724	lr 0.02479	Loss 2.8190 (2.6133)	Prec@(1,5) (48.9%, 80.0%)	
09/05 01:42:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][550/703]	Step 4774	lr 0.02479	Loss 2.8329 (2.6147)	Prec@(1,5) (48.9%, 80.0%)	
09/05 01:42:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][600/703]	Step 4824	lr 0.02479	Loss 3.0165 (2.6199)	Prec@(1,5) (48.8%, 79.9%)	
09/05 01:42:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][650/703]	Step 4874	lr 0.02479	Loss 2.3375 (2.6136)	Prec@(1,5) (48.9%, 80.0%)	
09/05 01:42:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][700/703]	Step 4924	lr 0.02479	Loss 2.4627 (2.6108)	Prec@(1,5) (48.8%, 80.1%)	
09/05 01:42:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [6][703/703]	Step 4927	lr 0.02479	Loss 2.5215 (2.6097)	Prec@(1,5) (48.8%, 80.1%)	
09/05 01:42:55午後 evaluateStage_trainer.py:148 [INFO] Train: [  6/99] Final Prec@1 48.8378%
09/05 01:42:57午後 evaluateStage_trainer.py:183 [INFO] Valid: [  6/99] Final Prec@1 41.0800%
09/05 01:42:58午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 43.8000%
09/05 01:43:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][50/703]	Step 4978	lr 0.02471	Loss 2.7814 (2.4859)	Prec@(1,5) (50.7%, 82.9%)	
09/05 01:43:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][100/703]	Step 5028	lr 0.02471	Loss 2.3904 (2.4955)	Prec@(1,5) (50.5%, 82.2%)	
09/05 01:43:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][150/703]	Step 5078	lr 0.02471	Loss 2.7743 (2.4807)	Prec@(1,5) (50.6%, 82.3%)	
09/05 01:43:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][200/703]	Step 5128	lr 0.02471	Loss 2.6937 (2.4700)	Prec@(1,5) (50.8%, 82.4%)	
09/05 01:43:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][250/703]	Step 5178	lr 0.02471	Loss 2.0392 (2.4675)	Prec@(1,5) (50.8%, 82.3%)	
09/05 01:43:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][300/703]	Step 5228	lr 0.02471	Loss 2.2006 (2.4895)	Prec@(1,5) (50.4%, 82.0%)	
09/05 01:43:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][350/703]	Step 5278	lr 0.02471	Loss 2.9103 (2.4942)	Prec@(1,5) (50.4%, 82.0%)	
09/05 01:43:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][400/703]	Step 5328	lr 0.02471	Loss 1.9558 (2.4988)	Prec@(1,5) (50.2%, 81.9%)	
09/05 01:43:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][450/703]	Step 5378	lr 0.02471	Loss 2.9887 (2.5072)	Prec@(1,5) (50.3%, 81.7%)	
09/05 01:43:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][500/703]	Step 5428	lr 0.02471	Loss 2.6209 (2.5089)	Prec@(1,5) (50.3%, 81.7%)	
09/05 01:43:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][550/703]	Step 5478	lr 0.02471	Loss 2.6594 (2.5019)	Prec@(1,5) (50.5%, 81.8%)	
09/05 01:43:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][600/703]	Step 5528	lr 0.02471	Loss 2.4835 (2.4969)	Prec@(1,5) (50.6%, 81.8%)	
09/05 01:43:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][650/703]	Step 5578	lr 0.02471	Loss 2.5473 (2.4972)	Prec@(1,5) (50.6%, 81.8%)	
09/05 01:43:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][700/703]	Step 5628	lr 0.02471	Loss 2.0699 (2.4982)	Prec@(1,5) (50.6%, 81.8%)	
09/05 01:43:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [7][703/703]	Step 5631	lr 0.02471	Loss 2.4830 (2.4974)	Prec@(1,5) (50.6%, 81.8%)	
09/05 01:43:56午後 evaluateStage_trainer.py:148 [INFO] Train: [  7/99] Final Prec@1 50.5844%
09/05 01:43:58午後 evaluateStage_trainer.py:183 [INFO] Valid: [  7/99] Final Prec@1 46.4400%
09/05 01:43:59午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 46.4400%
09/05 01:44:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][50/703]	Step 5682	lr 0.02462	Loss 2.7802 (2.3618)	Prec@(1,5) (52.5%, 83.6%)	
09/05 01:44:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][100/703]	Step 5732	lr 0.02462	Loss 2.7260 (2.3984)	Prec@(1,5) (52.3%, 82.8%)	
09/05 01:44:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][150/703]	Step 5782	lr 0.02462	Loss 2.4895 (2.3605)	Prec@(1,5) (52.9%, 83.2%)	
09/05 01:44:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][200/703]	Step 5832	lr 0.02462	Loss 2.3114 (2.3633)	Prec@(1,5) (52.9%, 83.2%)	
09/05 01:44:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][250/703]	Step 5882	lr 0.02462	Loss 3.2659 (2.3770)	Prec@(1,5) (52.6%, 83.0%)	
09/05 01:44:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][300/703]	Step 5932	lr 0.02462	Loss 1.8902 (2.3840)	Prec@(1,5) (52.6%, 82.9%)	
09/05 01:44:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][350/703]	Step 5982	lr 0.02462	Loss 2.2432 (2.3955)	Prec@(1,5) (52.5%, 82.6%)	
09/05 01:44:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][400/703]	Step 6032	lr 0.02462	Loss 2.5607 (2.4021)	Prec@(1,5) (52.4%, 82.5%)	
09/05 01:44:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][450/703]	Step 6082	lr 0.02462	Loss 1.9525 (2.3960)	Prec@(1,5) (52.5%, 82.7%)	
09/05 01:44:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][500/703]	Step 6132	lr 0.02462	Loss 2.5550 (2.3982)	Prec@(1,5) (52.4%, 82.7%)	
09/05 01:44:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][550/703]	Step 6182	lr 0.02462	Loss 2.0071 (2.3979)	Prec@(1,5) (52.5%, 82.7%)	
09/05 01:44:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][600/703]	Step 6232	lr 0.02462	Loss 2.0040 (2.3933)	Prec@(1,5) (52.6%, 82.8%)	
09/05 01:44:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][650/703]	Step 6282	lr 0.02462	Loss 2.7359 (2.3935)	Prec@(1,5) (52.6%, 82.8%)	
09/05 01:44:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][700/703]	Step 6332	lr 0.02462	Loss 2.1911 (2.3951)	Prec@(1,5) (52.5%, 82.7%)	
09/05 01:44:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [8][703/703]	Step 6335	lr 0.02462	Loss 2.3125 (2.3965)	Prec@(1,5) (52.5%, 82.7%)	
09/05 01:44:55午後 evaluateStage_trainer.py:148 [INFO] Train: [  8/99] Final Prec@1 52.4889%
09/05 01:44:58午後 evaluateStage_trainer.py:183 [INFO] Valid: [  8/99] Final Prec@1 48.5400%
09/05 01:44:58午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 48.5400%
09/05 01:45:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][50/703]	Step 6386	lr 0.02452	Loss 2.4579 (2.2710)	Prec@(1,5) (54.7%, 84.6%)	
09/05 01:45:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][100/703]	Step 6436	lr 0.02452	Loss 2.4261 (2.2877)	Prec@(1,5) (54.0%, 84.4%)	
09/05 01:45:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][150/703]	Step 6486	lr 0.02452	Loss 2.6207 (2.2718)	Prec@(1,5) (54.2%, 84.6%)	
09/05 01:45:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][200/703]	Step 6536	lr 0.02452	Loss 2.3451 (2.2833)	Prec@(1,5) (54.1%, 84.4%)	
09/05 01:45:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][250/703]	Step 6586	lr 0.02452	Loss 2.3366 (2.2915)	Prec@(1,5) (54.1%, 84.2%)	
09/05 01:45:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][300/703]	Step 6636	lr 0.02452	Loss 2.2938 (2.2892)	Prec@(1,5) (54.3%, 84.1%)	
09/05 01:45:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][350/703]	Step 6686	lr 0.02452	Loss 2.6266 (2.2934)	Prec@(1,5) (54.1%, 84.1%)	
09/05 01:45:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][400/703]	Step 6736	lr 0.02452	Loss 2.2558 (2.2943)	Prec@(1,5) (54.2%, 84.0%)	
09/05 01:45:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][450/703]	Step 6786	lr 0.02452	Loss 2.0475 (2.2968)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][500/703]	Step 6836	lr 0.02452	Loss 2.3976 (2.3015)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][550/703]	Step 6886	lr 0.02452	Loss 2.5208 (2.2992)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][600/703]	Step 6936	lr 0.02452	Loss 2.1298 (2.3014)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][650/703]	Step 6986	lr 0.02452	Loss 2.1544 (2.3026)	Prec@(1,5) (54.1%, 83.8%)	
09/05 01:45:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][700/703]	Step 7036	lr 0.02452	Loss 2.2227 (2.2984)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [9][703/703]	Step 7039	lr 0.02452	Loss 2.0545 (2.2979)	Prec@(1,5) (54.1%, 83.9%)	
09/05 01:45:58午後 evaluateStage_trainer.py:148 [INFO] Train: [  9/99] Final Prec@1 54.1356%
09/05 01:46:01午後 evaluateStage_trainer.py:183 [INFO] Valid: [  9/99] Final Prec@1 49.1800%
09/05 01:46:01午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 49.1800%
09/05 01:46:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][50/703]	Step 7090	lr 0.02441	Loss 2.0590 (2.1384)	Prec@(1,5) (57.7%, 85.2%)	
09/05 01:46:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][100/703]	Step 7140	lr 0.02441	Loss 2.3848 (2.1555)	Prec@(1,5) (57.4%, 85.3%)	
09/05 01:46:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][150/703]	Step 7190	lr 0.02441	Loss 1.8371 (2.1818)	Prec@(1,5) (56.9%, 85.0%)	
09/05 01:46:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][200/703]	Step 7240	lr 0.02441	Loss 2.4325 (2.1938)	Prec@(1,5) (56.4%, 85.0%)	
09/05 01:46:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][250/703]	Step 7290	lr 0.02441	Loss 2.0694 (2.1817)	Prec@(1,5) (56.6%, 85.2%)	
09/05 01:46:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][300/703]	Step 7340	lr 0.02441	Loss 2.0141 (2.1860)	Prec@(1,5) (56.3%, 85.1%)	
09/05 01:46:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][350/703]	Step 7390	lr 0.02441	Loss 2.1875 (2.1958)	Prec@(1,5) (56.0%, 85.0%)	
09/05 01:46:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][400/703]	Step 7440	lr 0.02441	Loss 2.0324 (2.2049)	Prec@(1,5) (55.7%, 84.9%)	
09/05 01:46:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][450/703]	Step 7490	lr 0.02441	Loss 2.5516 (2.2060)	Prec@(1,5) (55.7%, 84.8%)	
09/05 01:46:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][500/703]	Step 7540	lr 0.02441	Loss 2.4929 (2.2080)	Prec@(1,5) (55.7%, 84.8%)	
09/05 01:46:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][550/703]	Step 7590	lr 0.02441	Loss 1.6460 (2.2077)	Prec@(1,5) (55.8%, 84.8%)	
09/05 01:46:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][600/703]	Step 7640	lr 0.02441	Loss 2.3562 (2.2155)	Prec@(1,5) (55.7%, 84.8%)	
09/05 01:46:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][650/703]	Step 7690	lr 0.02441	Loss 2.1745 (2.2247)	Prec@(1,5) (55.5%, 84.7%)	
09/05 01:47:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][700/703]	Step 7740	lr 0.02441	Loss 1.9942 (2.2254)	Prec@(1,5) (55.5%, 84.6%)	
09/05 01:47:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [10][703/703]	Step 7743	lr 0.02441	Loss 2.2797 (2.2252)	Prec@(1,5) (55.5%, 84.6%)	
09/05 01:47:01午後 evaluateStage_trainer.py:148 [INFO] Train: [ 10/99] Final Prec@1 55.4844%
09/05 01:47:03午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 10/99] Final Prec@1 49.5400%
09/05 01:47:04午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 49.5400%
09/05 01:47:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][50/703]	Step 7794	lr 0.02429	Loss 1.8341 (2.0732)	Prec@(1,5) (58.6%, 86.2%)	
09/05 01:47:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][100/703]	Step 7844	lr 0.02429	Loss 2.0481 (2.0861)	Prec@(1,5) (58.2%, 86.3%)	
09/05 01:47:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][150/703]	Step 7894	lr 0.02429	Loss 2.0884 (2.1091)	Prec@(1,5) (57.7%, 86.3%)	
09/05 01:47:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][200/703]	Step 7944	lr 0.02429	Loss 2.0885 (2.0972)	Prec@(1,5) (57.9%, 86.4%)	
09/05 01:47:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][250/703]	Step 7994	lr 0.02429	Loss 2.6279 (2.1157)	Prec@(1,5) (57.4%, 86.2%)	
09/05 01:47:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][300/703]	Step 8044	lr 0.02429	Loss 2.0019 (2.1410)	Prec@(1,5) (56.7%, 86.0%)	
09/05 01:47:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][350/703]	Step 8094	lr 0.02429	Loss 1.6461 (2.1481)	Prec@(1,5) (56.6%, 85.9%)	
09/05 01:47:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][400/703]	Step 8144	lr 0.02429	Loss 2.1145 (2.1516)	Prec@(1,5) (56.6%, 85.8%)	
09/05 01:47:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][450/703]	Step 8194	lr 0.02429	Loss 2.1839 (2.1553)	Prec@(1,5) (56.6%, 85.8%)	
09/05 01:47:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][500/703]	Step 8244	lr 0.02429	Loss 2.0006 (2.1485)	Prec@(1,5) (56.7%, 85.9%)	
09/05 01:47:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][550/703]	Step 8294	lr 0.02429	Loss 2.2187 (2.1533)	Prec@(1,5) (56.5%, 85.9%)	
09/05 01:47:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][600/703]	Step 8344	lr 0.02429	Loss 2.2075 (2.1513)	Prec@(1,5) (56.6%, 85.9%)	
09/05 01:47:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][650/703]	Step 8394	lr 0.02429	Loss 2.6148 (2.1520)	Prec@(1,5) (56.6%, 85.9%)	
09/05 01:48:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][700/703]	Step 8444	lr 0.02429	Loss 2.3197 (2.1522)	Prec@(1,5) (56.6%, 85.9%)	
09/05 01:48:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [11][703/703]	Step 8447	lr 0.02429	Loss 2.5989 (2.1523)	Prec@(1,5) (56.6%, 85.9%)	
09/05 01:48:03午後 evaluateStage_trainer.py:148 [INFO] Train: [ 11/99] Final Prec@1 56.5622%
09/05 01:48:06午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 11/99] Final Prec@1 51.5000%
09/05 01:48:06午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 51.5000%
09/05 01:48:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][50/703]	Step 8498	lr 0.02416	Loss 1.8502 (2.0319)	Prec@(1,5) (58.1%, 87.9%)	
09/05 01:48:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][100/703]	Step 8548	lr 0.02416	Loss 2.2310 (2.0492)	Prec@(1,5) (58.7%, 87.3%)	
09/05 01:48:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][150/703]	Step 8598	lr 0.02416	Loss 1.8157 (2.0549)	Prec@(1,5) (58.8%, 86.7%)	
09/05 01:48:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][200/703]	Step 8648	lr 0.02416	Loss 2.2632 (2.0543)	Prec@(1,5) (58.7%, 86.9%)	
09/05 01:48:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][250/703]	Step 8698	lr 0.02416	Loss 2.0100 (2.0773)	Prec@(1,5) (58.2%, 86.7%)	
09/05 01:48:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][300/703]	Step 8748	lr 0.02416	Loss 1.8694 (2.0815)	Prec@(1,5) (57.9%, 86.7%)	
09/05 01:48:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][350/703]	Step 8798	lr 0.02416	Loss 2.0165 (2.0925)	Prec@(1,5) (57.6%, 86.5%)	
09/05 01:48:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][400/703]	Step 8848	lr 0.02416	Loss 2.2598 (2.0927)	Prec@(1,5) (57.7%, 86.6%)	
09/05 01:48:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][450/703]	Step 8898	lr 0.02416	Loss 2.1887 (2.1045)	Prec@(1,5) (57.6%, 86.4%)	
09/05 01:48:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][500/703]	Step 8948	lr 0.02416	Loss 2.2130 (2.1014)	Prec@(1,5) (57.6%, 86.4%)	
09/05 01:48:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][550/703]	Step 8998	lr 0.02416	Loss 1.8915 (2.0992)	Prec@(1,5) (57.5%, 86.5%)	
09/05 01:48:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][600/703]	Step 9048	lr 0.02416	Loss 2.0786 (2.0968)	Prec@(1,5) (57.6%, 86.5%)	
09/05 01:49:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][650/703]	Step 9098	lr 0.02416	Loss 2.0797 (2.0968)	Prec@(1,5) (57.6%, 86.5%)	
09/05 01:49:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][700/703]	Step 9148	lr 0.02416	Loss 2.1940 (2.0963)	Prec@(1,5) (57.6%, 86.5%)	
09/05 01:49:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [12][703/703]	Step 9151	lr 0.02416	Loss 2.2261 (2.0966)	Prec@(1,5) (57.6%, 86.5%)	
09/05 01:49:05午後 evaluateStage_trainer.py:148 [INFO] Train: [ 12/99] Final Prec@1 57.5867%
09/05 01:49:07午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 12/99] Final Prec@1 53.2600%
09/05 01:49:08午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 53.2600%
09/05 01:49:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][50/703]	Step 9202	lr 0.02401	Loss 1.9287 (2.0210)	Prec@(1,5) (60.1%, 86.8%)	
09/05 01:49:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][100/703]	Step 9252	lr 0.02401	Loss 2.0863 (1.9899)	Prec@(1,5) (59.9%, 87.7%)	
09/05 01:49:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][150/703]	Step 9302	lr 0.02401	Loss 2.5141 (2.0195)	Prec@(1,5) (59.1%, 87.5%)	
09/05 01:49:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][200/703]	Step 9352	lr 0.02401	Loss 1.7553 (2.0247)	Prec@(1,5) (59.0%, 87.2%)	
09/05 01:49:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][250/703]	Step 9402	lr 0.02401	Loss 2.4284 (2.0256)	Prec@(1,5) (59.1%, 87.2%)	
09/05 01:49:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][300/703]	Step 9452	lr 0.02401	Loss 2.2897 (2.0200)	Prec@(1,5) (59.3%, 87.2%)	
09/05 01:49:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][350/703]	Step 9502	lr 0.02401	Loss 2.2255 (2.0254)	Prec@(1,5) (59.1%, 87.2%)	
09/05 01:49:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][400/703]	Step 9552	lr 0.02401	Loss 2.1192 (2.0401)	Prec@(1,5) (58.8%, 86.9%)	
09/05 01:49:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][450/703]	Step 9602	lr 0.02401	Loss 2.0640 (2.0459)	Prec@(1,5) (58.7%, 86.9%)	
09/05 01:49:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][500/703]	Step 9652	lr 0.02401	Loss 2.1385 (2.0459)	Prec@(1,5) (58.7%, 86.9%)	
09/05 01:49:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][550/703]	Step 9702	lr 0.02401	Loss 2.0476 (2.0440)	Prec@(1,5) (58.8%, 86.8%)	
09/05 01:49:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][600/703]	Step 9752	lr 0.02401	Loss 2.3066 (2.0471)	Prec@(1,5) (58.7%, 86.8%)	
09/05 01:49:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][650/703]	Step 9802	lr 0.02401	Loss 1.7597 (2.0476)	Prec@(1,5) (58.6%, 86.8%)	
09/05 01:50:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][700/703]	Step 9852	lr 0.02401	Loss 1.8969 (2.0506)	Prec@(1,5) (58.6%, 86.8%)	
09/05 01:50:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [13][703/703]	Step 9855	lr 0.02401	Loss 2.0037 (2.0515)	Prec@(1,5) (58.6%, 86.8%)	
09/05 01:50:03午後 evaluateStage_trainer.py:148 [INFO] Train: [ 13/99] Final Prec@1 58.5756%
09/05 01:50:06午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 13/99] Final Prec@1 50.0200%
09/05 01:50:06午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 53.2600%
09/05 01:50:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][50/703]	Step 9906	lr 0.02386	Loss 1.9196 (1.9171)	Prec@(1,5) (60.6%, 88.2%)	
09/05 01:50:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][100/703]	Step 9956	lr 0.02386	Loss 1.8548 (1.9237)	Prec@(1,5) (60.6%, 88.0%)	
09/05 01:50:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][150/703]	Step 10006	lr 0.02386	Loss 2.2973 (1.9354)	Prec@(1,5) (60.7%, 88.0%)	
09/05 01:50:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][200/703]	Step 10056	lr 0.02386	Loss 1.7136 (1.9517)	Prec@(1,5) (60.1%, 87.8%)	
09/05 01:50:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][250/703]	Step 10106	lr 0.02386	Loss 1.8525 (1.9638)	Prec@(1,5) (60.1%, 87.7%)	
09/05 01:50:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][300/703]	Step 10156	lr 0.02386	Loss 1.9849 (1.9690)	Prec@(1,5) (60.0%, 87.7%)	
09/05 01:50:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][350/703]	Step 10206	lr 0.02386	Loss 1.7345 (1.9712)	Prec@(1,5) (59.9%, 87.6%)	
09/05 01:50:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][400/703]	Step 10256	lr 0.02386	Loss 2.3780 (1.9786)	Prec@(1,5) (59.9%, 87.5%)	
09/05 01:50:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][450/703]	Step 10306	lr 0.02386	Loss 1.8949 (1.9850)	Prec@(1,5) (59.7%, 87.4%)	
09/05 01:50:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][500/703]	Step 10356	lr 0.02386	Loss 2.2440 (1.9932)	Prec@(1,5) (59.6%, 87.3%)	
09/05 01:50:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][550/703]	Step 10406	lr 0.02386	Loss 2.0643 (1.9894)	Prec@(1,5) (59.6%, 87.3%)	
09/05 01:50:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][600/703]	Step 10456	lr 0.02386	Loss 2.0958 (1.9967)	Prec@(1,5) (59.4%, 87.3%)	
09/05 01:51:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][650/703]	Step 10506	lr 0.02386	Loss 2.2227 (1.9979)	Prec@(1,5) (59.5%, 87.2%)	
09/05 01:51:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][700/703]	Step 10556	lr 0.02386	Loss 2.4776 (1.9995)	Prec@(1,5) (59.4%, 87.2%)	
09/05 01:51:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [14][703/703]	Step 10559	lr 0.02386	Loss 1.6609 (1.9998)	Prec@(1,5) (59.4%, 87.2%)	
09/05 01:51:05午後 evaluateStage_trainer.py:148 [INFO] Train: [ 14/99] Final Prec@1 59.4311%
09/05 01:51:08午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 14/99] Final Prec@1 52.8600%
09/05 01:51:08午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 53.2600%
09/05 01:51:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][50/703]	Step 10610	lr 0.02369	Loss 2.3262 (1.8875)	Prec@(1,5) (62.0%, 87.8%)	
09/05 01:51:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][100/703]	Step 10660	lr 0.02369	Loss 1.9682 (1.9076)	Prec@(1,5) (61.4%, 87.9%)	
09/05 01:51:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][150/703]	Step 10710	lr 0.02369	Loss 2.0898 (1.9130)	Prec@(1,5) (61.2%, 88.1%)	
09/05 01:51:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][200/703]	Step 10760	lr 0.02369	Loss 2.2495 (1.9236)	Prec@(1,5) (61.0%, 87.9%)	
09/05 01:51:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][250/703]	Step 10810	lr 0.02369	Loss 1.8524 (1.9185)	Prec@(1,5) (60.9%, 88.1%)	
09/05 01:51:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][300/703]	Step 10860	lr 0.02369	Loss 1.8492 (1.9353)	Prec@(1,5) (60.7%, 87.8%)	
09/05 01:51:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][350/703]	Step 10910	lr 0.02369	Loss 1.6240 (1.9411)	Prec@(1,5) (60.5%, 87.7%)	
09/05 01:51:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][400/703]	Step 10960	lr 0.02369	Loss 2.0677 (1.9401)	Prec@(1,5) (60.5%, 87.8%)	
09/05 01:51:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][450/703]	Step 11010	lr 0.02369	Loss 2.1089 (1.9548)	Prec@(1,5) (60.2%, 87.7%)	
09/05 01:51:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][500/703]	Step 11060	lr 0.02369	Loss 1.8145 (1.9598)	Prec@(1,5) (60.1%, 87.6%)	
09/05 01:51:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][550/703]	Step 11110	lr 0.02369	Loss 1.9714 (1.9661)	Prec@(1,5) (60.0%, 87.4%)	
09/05 01:51:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][600/703]	Step 11160	lr 0.02369	Loss 1.9512 (1.9609)	Prec@(1,5) (60.1%, 87.5%)	
09/05 01:52:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][650/703]	Step 11210	lr 0.02369	Loss 2.0313 (1.9636)	Prec@(1,5) (60.2%, 87.5%)	
09/05 01:52:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][700/703]	Step 11260	lr 0.02369	Loss 2.2969 (1.9653)	Prec@(1,5) (60.1%, 87.5%)	
09/05 01:52:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [15][703/703]	Step 11263	lr 0.02369	Loss 2.5989 (1.9654)	Prec@(1,5) (60.1%, 87.5%)	
09/05 01:52:08午後 evaluateStage_trainer.py:148 [INFO] Train: [ 15/99] Final Prec@1 60.0800%
09/05 01:52:10午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 15/99] Final Prec@1 54.0800%
09/05 01:52:10午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 54.0800%
09/05 01:52:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][50/703]	Step 11314	lr 0.02352	Loss 1.9011 (1.8285)	Prec@(1,5) (61.7%, 89.5%)	
09/05 01:52:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][100/703]	Step 11364	lr 0.02352	Loss 1.8451 (1.8476)	Prec@(1,5) (61.9%, 89.1%)	
09/05 01:52:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][150/703]	Step 11414	lr 0.02352	Loss 1.9834 (1.8700)	Prec@(1,5) (61.7%, 88.7%)	
09/05 01:52:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][200/703]	Step 11464	lr 0.02352	Loss 2.2049 (1.8840)	Prec@(1,5) (61.0%, 88.7%)	
09/05 01:52:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][250/703]	Step 11514	lr 0.02352	Loss 1.9209 (1.8846)	Prec@(1,5) (61.1%, 88.7%)	
09/05 01:52:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][300/703]	Step 11564	lr 0.02352	Loss 2.2125 (1.8924)	Prec@(1,5) (61.0%, 88.6%)	
09/05 01:52:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][350/703]	Step 11614	lr 0.02352	Loss 1.9858 (1.8998)	Prec@(1,5) (60.9%, 88.4%)	
09/05 01:52:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][400/703]	Step 11664	lr 0.02352	Loss 1.7790 (1.8952)	Prec@(1,5) (61.0%, 88.4%)	
09/05 01:52:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][450/703]	Step 11714	lr 0.02352	Loss 2.0850 (1.9019)	Prec@(1,5) (60.9%, 88.3%)	
09/05 01:52:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][500/703]	Step 11764	lr 0.02352	Loss 1.7883 (1.9056)	Prec@(1,5) (60.8%, 88.2%)	
09/05 01:52:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][550/703]	Step 11814	lr 0.02352	Loss 2.1455 (1.9089)	Prec@(1,5) (60.9%, 88.2%)	
09/05 01:53:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][600/703]	Step 11864	lr 0.02352	Loss 1.7813 (1.9114)	Prec@(1,5) (60.8%, 88.2%)	
09/05 01:53:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][650/703]	Step 11914	lr 0.02352	Loss 1.9009 (1.9134)	Prec@(1,5) (60.8%, 88.1%)	
09/05 01:53:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][700/703]	Step 11964	lr 0.02352	Loss 2.1408 (1.9196)	Prec@(1,5) (60.7%, 88.1%)	
09/05 01:53:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [16][703/703]	Step 11967	lr 0.02352	Loss 2.4369 (1.9207)	Prec@(1,5) (60.6%, 88.0%)	
09/05 01:53:10午後 evaluateStage_trainer.py:148 [INFO] Train: [ 16/99] Final Prec@1 60.6444%
09/05 01:53:12午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 16/99] Final Prec@1 54.8200%
09/05 01:53:13午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 54.8200%
09/05 01:53:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][50/703]	Step 12018	lr 0.02333	Loss 1.9672 (1.8166)	Prec@(1,5) (62.2%, 88.7%)	
09/05 01:53:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][100/703]	Step 12068	lr 0.02333	Loss 1.4012 (1.8273)	Prec@(1,5) (62.2%, 88.8%)	
09/05 01:53:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][150/703]	Step 12118	lr 0.02333	Loss 1.6447 (1.8302)	Prec@(1,5) (62.3%, 89.3%)	
09/05 01:53:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][200/703]	Step 12168	lr 0.02333	Loss 2.0163 (1.8436)	Prec@(1,5) (61.9%, 89.2%)	
09/05 01:53:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][250/703]	Step 12218	lr 0.02333	Loss 2.0496 (1.8423)	Prec@(1,5) (62.0%, 89.2%)	
09/05 01:53:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][300/703]	Step 12268	lr 0.02333	Loss 1.7554 (1.8568)	Prec@(1,5) (61.6%, 89.0%)	
09/05 01:53:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][350/703]	Step 12318	lr 0.02333	Loss 1.8389 (1.8527)	Prec@(1,5) (61.7%, 89.0%)	
09/05 01:53:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][400/703]	Step 12368	lr 0.02333	Loss 2.3027 (1.8693)	Prec@(1,5) (61.6%, 88.8%)	
09/05 01:53:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][450/703]	Step 12418	lr 0.02333	Loss 1.7665 (1.8774)	Prec@(1,5) (61.4%, 88.7%)	
09/05 01:53:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][500/703]	Step 12468	lr 0.02333	Loss 1.6654 (1.8813)	Prec@(1,5) (61.3%, 88.7%)	
09/05 01:54:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][550/703]	Step 12518	lr 0.02333	Loss 2.1018 (1.8858)	Prec@(1,5) (61.2%, 88.6%)	
09/05 01:54:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][600/703]	Step 12568	lr 0.02333	Loss 1.4535 (1.8858)	Prec@(1,5) (61.3%, 88.5%)	
09/05 01:54:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][650/703]	Step 12618	lr 0.02333	Loss 1.6077 (1.8839)	Prec@(1,5) (61.3%, 88.6%)	
09/05 01:54:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][700/703]	Step 12668	lr 0.02333	Loss 1.9551 (1.8852)	Prec@(1,5) (61.2%, 88.6%)	
09/05 01:54:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [17][703/703]	Step 12671	lr 0.02333	Loss 1.4978 (1.8851)	Prec@(1,5) (61.2%, 88.6%)	
09/05 01:54:12午後 evaluateStage_trainer.py:148 [INFO] Train: [ 17/99] Final Prec@1 61.1689%
09/05 01:54:14午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 17/99] Final Prec@1 57.2800%
09/05 01:54:15午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.2800%
09/05 01:54:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][50/703]	Step 12722	lr 0.02313	Loss 2.0272 (1.8116)	Prec@(1,5) (62.1%, 90.2%)	
09/05 01:54:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][100/703]	Step 12772	lr 0.02313	Loss 1.9586 (1.7809)	Prec@(1,5) (63.2%, 90.4%)	
09/05 01:54:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][150/703]	Step 12822	lr 0.02313	Loss 2.5692 (1.8137)	Prec@(1,5) (62.6%, 89.8%)	
09/05 01:54:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][200/703]	Step 12872	lr 0.02313	Loss 2.0953 (1.8190)	Prec@(1,5) (62.6%, 89.7%)	
09/05 01:54:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][250/703]	Step 12922	lr 0.02313	Loss 2.0105 (1.8318)	Prec@(1,5) (62.2%, 89.6%)	
09/05 01:54:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][300/703]	Step 12972	lr 0.02313	Loss 1.6838 (1.8325)	Prec@(1,5) (62.0%, 89.6%)	
09/05 01:54:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][350/703]	Step 13022	lr 0.02313	Loss 1.8077 (1.8393)	Prec@(1,5) (62.0%, 89.5%)	
09/05 01:54:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][400/703]	Step 13072	lr 0.02313	Loss 1.5509 (1.8355)	Prec@(1,5) (62.0%, 89.5%)	
09/05 01:54:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][450/703]	Step 13122	lr 0.02313	Loss 1.9521 (1.8433)	Prec@(1,5) (61.9%, 89.3%)	
09/05 01:54:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][500/703]	Step 13172	lr 0.02313	Loss 1.6847 (1.8480)	Prec@(1,5) (61.9%, 89.2%)	
09/05 01:55:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][550/703]	Step 13222	lr 0.02313	Loss 2.1302 (1.8470)	Prec@(1,5) (61.9%, 89.2%)	
09/05 01:55:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][600/703]	Step 13272	lr 0.02313	Loss 2.1176 (1.8497)	Prec@(1,5) (61.8%, 89.2%)	
09/05 01:55:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][650/703]	Step 13322	lr 0.02313	Loss 1.8631 (1.8500)	Prec@(1,5) (61.9%, 89.1%)	
09/05 01:55:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][700/703]	Step 13372	lr 0.02313	Loss 2.0390 (1.8536)	Prec@(1,5) (61.9%, 89.0%)	
09/05 01:55:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [18][703/703]	Step 13375	lr 0.02313	Loss 2.0044 (1.8535)	Prec@(1,5) (61.9%, 89.0%)	
09/05 01:55:14午後 evaluateStage_trainer.py:148 [INFO] Train: [ 18/99] Final Prec@1 61.8844%
09/05 01:55:16午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 18/99] Final Prec@1 56.2600%
09/05 01:55:16午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.2800%
09/05 01:55:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][50/703]	Step 13426	lr 0.02292	Loss 1.7003 (1.7858)	Prec@(1,5) (63.8%, 89.5%)	
09/05 01:55:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][100/703]	Step 13476	lr 0.02292	Loss 1.7947 (1.7943)	Prec@(1,5) (63.1%, 89.4%)	
09/05 01:55:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][150/703]	Step 13526	lr 0.02292	Loss 1.6083 (1.7845)	Prec@(1,5) (63.5%, 89.5%)	
09/05 01:55:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][200/703]	Step 13576	lr 0.02292	Loss 1.8915 (1.7844)	Prec@(1,5) (63.4%, 89.6%)	
09/05 01:55:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][250/703]	Step 13626	lr 0.02292	Loss 1.5764 (1.7847)	Prec@(1,5) (63.4%, 89.7%)	
09/05 01:55:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][300/703]	Step 13676	lr 0.02292	Loss 2.1715 (1.7925)	Prec@(1,5) (63.3%, 89.7%)	
09/05 01:55:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][350/703]	Step 13726	lr 0.02292	Loss 2.4176 (1.7896)	Prec@(1,5) (63.3%, 89.7%)	
09/05 01:55:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][400/703]	Step 13776	lr 0.02292	Loss 1.3055 (1.7996)	Prec@(1,5) (63.1%, 89.6%)	
09/05 01:55:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][450/703]	Step 13826	lr 0.02292	Loss 1.8956 (1.8069)	Prec@(1,5) (62.9%, 89.4%)	
09/05 01:55:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][500/703]	Step 13876	lr 0.02292	Loss 2.0352 (1.8085)	Prec@(1,5) (62.9%, 89.4%)	
09/05 01:56:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][550/703]	Step 13926	lr 0.02292	Loss 1.8139 (1.8120)	Prec@(1,5) (62.8%, 89.3%)	
09/05 01:56:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][600/703]	Step 13976	lr 0.02292	Loss 2.4023 (1.8143)	Prec@(1,5) (62.8%, 89.2%)	
09/05 01:56:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][650/703]	Step 14026	lr 0.02292	Loss 2.2429 (1.8173)	Prec@(1,5) (62.8%, 89.3%)	
09/05 01:56:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][700/703]	Step 14076	lr 0.02292	Loss 2.2582 (1.8187)	Prec@(1,5) (62.8%, 89.3%)	
09/05 01:56:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [19][703/703]	Step 14079	lr 0.02292	Loss 2.0559 (1.8185)	Prec@(1,5) (62.8%, 89.3%)	
09/05 01:56:16午後 evaluateStage_trainer.py:148 [INFO] Train: [ 19/99] Final Prec@1 62.7822%
09/05 01:56:18午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 19/99] Final Prec@1 54.6000%
09/05 01:56:18午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.2800%
09/05 01:56:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][50/703]	Step 14130	lr 0.02271	Loss 1.5180 (1.7292)	Prec@(1,5) (64.5%, 90.2%)	
09/05 01:56:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][100/703]	Step 14180	lr 0.02271	Loss 1.6822 (1.7114)	Prec@(1,5) (64.5%, 90.6%)	
09/05 01:56:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][150/703]	Step 14230	lr 0.02271	Loss 1.7594 (1.7258)	Prec@(1,5) (64.5%, 90.3%)	
09/05 01:56:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][200/703]	Step 14280	lr 0.02271	Loss 1.8544 (1.7308)	Prec@(1,5) (64.3%, 90.2%)	
09/05 01:56:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][250/703]	Step 14330	lr 0.02271	Loss 1.3795 (1.7275)	Prec@(1,5) (64.3%, 90.2%)	
09/05 01:56:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][300/703]	Step 14380	lr 0.02271	Loss 1.9702 (1.7361)	Prec@(1,5) (64.0%, 90.1%)	
09/05 01:56:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][350/703]	Step 14430	lr 0.02271	Loss 2.1618 (1.7575)	Prec@(1,5) (63.6%, 89.9%)	
09/05 01:56:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][400/703]	Step 14480	lr 0.02271	Loss 1.9175 (1.7672)	Prec@(1,5) (63.4%, 89.9%)	
09/05 01:56:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][450/703]	Step 14530	lr 0.02271	Loss 2.0650 (1.7709)	Prec@(1,5) (63.5%, 89.8%)	
09/05 01:56:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][500/703]	Step 14580	lr 0.02271	Loss 1.6228 (1.7703)	Prec@(1,5) (63.6%, 89.8%)	
09/05 01:57:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][550/703]	Step 14630	lr 0.02271	Loss 2.1710 (1.7813)	Prec@(1,5) (63.4%, 89.7%)	
09/05 01:57:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][600/703]	Step 14680	lr 0.02271	Loss 1.7828 (1.7879)	Prec@(1,5) (63.3%, 89.7%)	
09/05 01:57:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][650/703]	Step 14730	lr 0.02271	Loss 1.6630 (1.7918)	Prec@(1,5) (63.2%, 89.6%)	
09/05 01:57:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][700/703]	Step 14780	lr 0.02271	Loss 1.6243 (1.7947)	Prec@(1,5) (63.2%, 89.6%)	
09/05 01:57:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [20][703/703]	Step 14783	lr 0.02271	Loss 1.4728 (1.7940)	Prec@(1,5) (63.2%, 89.6%)	
09/05 01:57:16午後 evaluateStage_trainer.py:148 [INFO] Train: [ 20/99] Final Prec@1 63.2000%
09/05 01:57:18午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 20/99] Final Prec@1 57.9000%
09/05 01:57:19午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 01:57:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][50/703]	Step 14834	lr 0.02248	Loss 1.8681 (1.7124)	Prec@(1,5) (64.5%, 90.3%)	
09/05 01:57:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][100/703]	Step 14884	lr 0.02248	Loss 1.5118 (1.6773)	Prec@(1,5) (65.0%, 90.6%)	
09/05 01:57:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][150/703]	Step 14934	lr 0.02248	Loss 1.6418 (1.7160)	Prec@(1,5) (64.5%, 90.5%)	
09/05 01:57:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][200/703]	Step 14984	lr 0.02248	Loss 1.4392 (1.7177)	Prec@(1,5) (64.4%, 90.4%)	
09/05 01:57:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][250/703]	Step 15034	lr 0.02248	Loss 1.4449 (1.7289)	Prec@(1,5) (64.3%, 90.2%)	
09/05 01:57:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][300/703]	Step 15084	lr 0.02248	Loss 1.5928 (1.7306)	Prec@(1,5) (64.2%, 90.2%)	
09/05 01:57:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][350/703]	Step 15134	lr 0.02248	Loss 1.6468 (1.7414)	Prec@(1,5) (64.2%, 90.2%)	
09/05 01:57:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][400/703]	Step 15184	lr 0.02248	Loss 2.2520 (1.7449)	Prec@(1,5) (64.1%, 90.2%)	
09/05 01:57:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][450/703]	Step 15234	lr 0.02248	Loss 1.3572 (1.7533)	Prec@(1,5) (63.9%, 90.1%)	
09/05 01:58:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][500/703]	Step 15284	lr 0.02248	Loss 1.9875 (1.7507)	Prec@(1,5) (63.9%, 90.1%)	
09/05 01:58:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][550/703]	Step 15334	lr 0.02248	Loss 1.5631 (1.7461)	Prec@(1,5) (63.9%, 90.2%)	
09/05 01:58:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][600/703]	Step 15384	lr 0.02248	Loss 1.8111 (1.7513)	Prec@(1,5) (63.8%, 90.1%)	
09/05 01:58:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][650/703]	Step 15434	lr 0.02248	Loss 1.8861 (1.7578)	Prec@(1,5) (63.6%, 90.1%)	
09/05 01:58:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][700/703]	Step 15484	lr 0.02248	Loss 1.6013 (1.7613)	Prec@(1,5) (63.5%, 90.0%)	
09/05 01:58:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [21][703/703]	Step 15487	lr 0.02248	Loss 1.7486 (1.7606)	Prec@(1,5) (63.5%, 90.0%)	
09/05 01:58:15午後 evaluateStage_trainer.py:148 [INFO] Train: [ 21/99] Final Prec@1 63.5289%
09/05 01:58:17午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 21/99] Final Prec@1 54.9200%
09/05 01:58:18午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 01:58:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][50/703]	Step 15538	lr 0.02225	Loss 1.6429 (1.6755)	Prec@(1,5) (65.7%, 90.3%)	
09/05 01:58:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][100/703]	Step 15588	lr 0.02225	Loss 1.5533 (1.6675)	Prec@(1,5) (65.5%, 90.6%)	
09/05 01:58:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][150/703]	Step 15638	lr 0.02225	Loss 1.9118 (1.6689)	Prec@(1,5) (65.3%, 90.7%)	
09/05 01:58:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][200/703]	Step 15688	lr 0.02225	Loss 1.8454 (1.6770)	Prec@(1,5) (65.3%, 90.6%)	
09/05 01:58:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][250/703]	Step 15738	lr 0.02225	Loss 1.7717 (1.6772)	Prec@(1,5) (65.3%, 90.5%)	
09/05 01:58:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][300/703]	Step 15788	lr 0.02225	Loss 1.5721 (1.6917)	Prec@(1,5) (65.0%, 90.3%)	
09/05 01:58:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][350/703]	Step 15838	lr 0.02225	Loss 1.8032 (1.6991)	Prec@(1,5) (64.8%, 90.2%)	
09/05 01:58:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][400/703]	Step 15888	lr 0.02225	Loss 1.6951 (1.7028)	Prec@(1,5) (64.8%, 90.3%)	
09/05 01:58:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][450/703]	Step 15938	lr 0.02225	Loss 1.2711 (1.7015)	Prec@(1,5) (64.7%, 90.3%)	
09/05 01:59:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][500/703]	Step 15988	lr 0.02225	Loss 1.8534 (1.7028)	Prec@(1,5) (64.7%, 90.3%)	
09/05 01:59:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][550/703]	Step 16038	lr 0.02225	Loss 2.1720 (1.7118)	Prec@(1,5) (64.6%, 90.2%)	
09/05 01:59:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][600/703]	Step 16088	lr 0.02225	Loss 1.8239 (1.7233)	Prec@(1,5) (64.4%, 90.1%)	
09/05 01:59:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][650/703]	Step 16138	lr 0.02225	Loss 1.3291 (1.7261)	Prec@(1,5) (64.4%, 90.1%)	
09/05 01:59:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][700/703]	Step 16188	lr 0.02225	Loss 1.9275 (1.7294)	Prec@(1,5) (64.3%, 90.1%)	
09/05 01:59:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [22][703/703]	Step 16191	lr 0.02225	Loss 1.9682 (1.7297)	Prec@(1,5) (64.3%, 90.1%)	
09/05 01:59:17午後 evaluateStage_trainer.py:148 [INFO] Train: [ 22/99] Final Prec@1 64.3067%
09/05 01:59:20午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 22/99] Final Prec@1 56.6400%
09/05 01:59:20午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 01:59:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][50/703]	Step 16242	lr 0.022	Loss 1.5425 (1.7651)	Prec@(1,5) (64.3%, 89.1%)	
09/05 01:59:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][100/703]	Step 16292	lr 0.022	Loss 1.6902 (1.7182)	Prec@(1,5) (64.8%, 89.6%)	
09/05 01:59:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][150/703]	Step 16342	lr 0.022	Loss 1.4574 (1.6721)	Prec@(1,5) (65.8%, 90.2%)	
09/05 01:59:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][200/703]	Step 16392	lr 0.022	Loss 1.3230 (1.6651)	Prec@(1,5) (65.6%, 90.5%)	
09/05 01:59:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][250/703]	Step 16442	lr 0.022	Loss 1.1601 (1.6732)	Prec@(1,5) (65.4%, 90.5%)	
09/05 01:59:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][300/703]	Step 16492	lr 0.022	Loss 1.5334 (1.6686)	Prec@(1,5) (65.6%, 90.6%)	
09/05 01:59:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][350/703]	Step 16542	lr 0.022	Loss 1.7515 (1.6669)	Prec@(1,5) (65.6%, 90.7%)	
09/05 01:59:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][400/703]	Step 16592	lr 0.022	Loss 1.3639 (1.6689)	Prec@(1,5) (65.7%, 90.7%)	
09/05 01:59:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][450/703]	Step 16642	lr 0.022	Loss 1.4402 (1.6720)	Prec@(1,5) (65.6%, 90.6%)	
09/05 02:00:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][500/703]	Step 16692	lr 0.022	Loss 1.2964 (1.6821)	Prec@(1,5) (65.4%, 90.5%)	
09/05 02:00:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][550/703]	Step 16742	lr 0.022	Loss 1.5960 (1.6847)	Prec@(1,5) (65.2%, 90.5%)	
09/05 02:00:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][600/703]	Step 16792	lr 0.022	Loss 1.6758 (1.6860)	Prec@(1,5) (65.1%, 90.5%)	
09/05 02:00:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][650/703]	Step 16842	lr 0.022	Loss 1.7304 (1.6928)	Prec@(1,5) (65.0%, 90.4%)	
09/05 02:00:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][700/703]	Step 16892	lr 0.022	Loss 1.4178 (1.6944)	Prec@(1,5) (65.0%, 90.5%)	
09/05 02:00:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [23][703/703]	Step 16895	lr 0.022	Loss 1.4206 (1.6943)	Prec@(1,5) (65.0%, 90.5%)	
09/05 02:00:20午後 evaluateStage_trainer.py:148 [INFO] Train: [ 23/99] Final Prec@1 64.9733%
09/05 02:00:22午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 23/99] Final Prec@1 56.5600%
09/05 02:00:22午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 02:00:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][50/703]	Step 16946	lr 0.02175	Loss 1.6327 (1.5747)	Prec@(1,5) (67.6%, 91.2%)	
09/05 02:00:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][100/703]	Step 16996	lr 0.02175	Loss 1.5936 (1.6083)	Prec@(1,5) (66.5%, 91.0%)	
09/05 02:00:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][150/703]	Step 17046	lr 0.02175	Loss 1.5371 (1.6057)	Prec@(1,5) (66.6%, 91.2%)	
09/05 02:00:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][200/703]	Step 17096	lr 0.02175	Loss 1.6177 (1.6074)	Prec@(1,5) (66.6%, 91.2%)	
09/05 02:00:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][250/703]	Step 17146	lr 0.02175	Loss 1.7677 (1.6108)	Prec@(1,5) (66.5%, 91.3%)	
09/05 02:00:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][300/703]	Step 17196	lr 0.02175	Loss 1.7090 (1.6312)	Prec@(1,5) (66.0%, 91.2%)	
09/05 02:00:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][350/703]	Step 17246	lr 0.02175	Loss 1.8308 (1.6278)	Prec@(1,5) (66.2%, 91.1%)	
09/05 02:00:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][400/703]	Step 17296	lr 0.02175	Loss 1.9744 (1.6394)	Prec@(1,5) (65.8%, 91.0%)	
09/05 02:01:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][450/703]	Step 17346	lr 0.02175	Loss 1.5346 (1.6530)	Prec@(1,5) (65.6%, 90.9%)	
09/05 02:01:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][500/703]	Step 17396	lr 0.02175	Loss 2.4842 (1.6644)	Prec@(1,5) (65.4%, 90.7%)	
09/05 02:01:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][550/703]	Step 17446	lr 0.02175	Loss 2.0565 (1.6687)	Prec@(1,5) (65.3%, 90.7%)	
09/05 02:01:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][600/703]	Step 17496	lr 0.02175	Loss 1.4575 (1.6760)	Prec@(1,5) (65.1%, 90.5%)	
09/05 02:01:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][650/703]	Step 17546	lr 0.02175	Loss 1.9837 (1.6817)	Prec@(1,5) (65.0%, 90.5%)	
09/05 02:01:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][700/703]	Step 17596	lr 0.02175	Loss 1.7676 (1.6867)	Prec@(1,5) (64.9%, 90.5%)	
09/05 02:01:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [24][703/703]	Step 17599	lr 0.02175	Loss 2.0524 (1.6872)	Prec@(1,5) (64.9%, 90.5%)	
09/05 02:01:22午後 evaluateStage_trainer.py:148 [INFO] Train: [ 24/99] Final Prec@1 64.9022%
09/05 02:01:25午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 24/99] Final Prec@1 55.5800%
09/05 02:01:25午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 02:01:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][50/703]	Step 17650	lr 0.02149	Loss 1.3998 (1.5875)	Prec@(1,5) (67.3%, 91.9%)	
09/05 02:01:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][100/703]	Step 17700	lr 0.02149	Loss 1.5077 (1.5987)	Prec@(1,5) (66.9%, 91.7%)	
09/05 02:01:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][150/703]	Step 17750	lr 0.02149	Loss 1.9547 (1.6081)	Prec@(1,5) (66.8%, 91.4%)	
09/05 02:01:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][200/703]	Step 17800	lr 0.02149	Loss 1.6846 (1.6124)	Prec@(1,5) (66.6%, 91.3%)	
09/05 02:01:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][250/703]	Step 17850	lr 0.02149	Loss 1.5405 (1.6158)	Prec@(1,5) (66.4%, 91.3%)	
09/05 02:01:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][300/703]	Step 17900	lr 0.02149	Loss 1.7447 (1.6141)	Prec@(1,5) (66.4%, 91.3%)	
09/05 02:01:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][350/703]	Step 17950	lr 0.02149	Loss 1.9852 (1.6194)	Prec@(1,5) (66.3%, 91.3%)	
09/05 02:01:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][400/703]	Step 18000	lr 0.02149	Loss 1.3371 (1.6265)	Prec@(1,5) (66.1%, 91.3%)	
09/05 02:02:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][450/703]	Step 18050	lr 0.02149	Loss 2.1746 (1.6319)	Prec@(1,5) (66.0%, 91.2%)	
09/05 02:02:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][500/703]	Step 18100	lr 0.02149	Loss 1.5098 (1.6366)	Prec@(1,5) (65.9%, 91.2%)	
09/05 02:02:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][550/703]	Step 18150	lr 0.02149	Loss 1.9656 (1.6398)	Prec@(1,5) (65.8%, 91.2%)	
09/05 02:02:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][600/703]	Step 18200	lr 0.02149	Loss 1.3784 (1.6396)	Prec@(1,5) (65.7%, 91.1%)	
09/05 02:02:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][650/703]	Step 18250	lr 0.02149	Loss 1.8515 (1.6553)	Prec@(1,5) (65.4%, 90.9%)	
09/05 02:02:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][700/703]	Step 18300	lr 0.02149	Loss 1.6051 (1.6632)	Prec@(1,5) (65.2%, 90.8%)	
09/05 02:02:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [25][703/703]	Step 18303	lr 0.02149	Loss 1.6639 (1.6635)	Prec@(1,5) (65.2%, 90.8%)	
09/05 02:02:24午後 evaluateStage_trainer.py:148 [INFO] Train: [ 25/99] Final Prec@1 65.2111%
09/05 02:02:26午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 25/99] Final Prec@1 56.8400%
09/05 02:02:26午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 57.9000%
09/05 02:02:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][50/703]	Step 18354	lr 0.02121	Loss 1.4577 (1.4623)	Prec@(1,5) (70.0%, 92.9%)	
09/05 02:02:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][100/703]	Step 18404	lr 0.02121	Loss 1.7645 (1.5094)	Prec@(1,5) (68.9%, 92.5%)	
09/05 02:02:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][150/703]	Step 18454	lr 0.02121	Loss 1.6291 (1.5557)	Prec@(1,5) (67.6%, 91.9%)	
09/05 02:02:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][200/703]	Step 18504	lr 0.02121	Loss 2.0871 (1.5771)	Prec@(1,5) (67.1%, 91.7%)	
09/05 02:02:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][250/703]	Step 18554	lr 0.02121	Loss 1.8426 (1.5755)	Prec@(1,5) (67.2%, 91.8%)	
09/05 02:02:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][300/703]	Step 18604	lr 0.02121	Loss 1.4333 (1.5876)	Prec@(1,5) (67.1%, 91.7%)	
09/05 02:02:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][350/703]	Step 18654	lr 0.02121	Loss 1.8144 (1.5935)	Prec@(1,5) (67.0%, 91.7%)	
09/05 02:02:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][400/703]	Step 18704	lr 0.02121	Loss 1.6095 (1.6055)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:03:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][450/703]	Step 18754	lr 0.02121	Loss 1.5663 (1.6156)	Prec@(1,5) (66.7%, 91.3%)	
09/05 02:03:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][500/703]	Step 18804	lr 0.02121	Loss 1.6157 (1.6237)	Prec@(1,5) (66.5%, 91.2%)	
09/05 02:03:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][550/703]	Step 18854	lr 0.02121	Loss 1.4477 (1.6215)	Prec@(1,5) (66.5%, 91.3%)	
09/05 02:03:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][600/703]	Step 18904	lr 0.02121	Loss 1.7658 (1.6264)	Prec@(1,5) (66.4%, 91.2%)	
09/05 02:03:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][650/703]	Step 18954	lr 0.02121	Loss 1.6181 (1.6287)	Prec@(1,5) (66.3%, 91.3%)	
09/05 02:03:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][700/703]	Step 19004	lr 0.02121	Loss 1.6657 (1.6285)	Prec@(1,5) (66.3%, 91.2%)	
09/05 02:03:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [26][703/703]	Step 19007	lr 0.02121	Loss 1.9162 (1.6288)	Prec@(1,5) (66.3%, 91.2%)	
09/05 02:03:26午後 evaluateStage_trainer.py:148 [INFO] Train: [ 26/99] Final Prec@1 66.3200%
09/05 02:03:28午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 26/99] Final Prec@1 59.4000%
09/05 02:03:29午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:03:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][50/703]	Step 19058	lr 0.02094	Loss 1.7964 (1.5581)	Prec@(1,5) (67.8%, 91.7%)	
09/05 02:03:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][100/703]	Step 19108	lr 0.02094	Loss 1.5201 (1.5461)	Prec@(1,5) (67.8%, 92.2%)	
09/05 02:03:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][150/703]	Step 19158	lr 0.02094	Loss 1.6921 (1.5799)	Prec@(1,5) (67.4%, 91.6%)	
09/05 02:03:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][200/703]	Step 19208	lr 0.02094	Loss 1.6299 (1.5851)	Prec@(1,5) (67.0%, 91.5%)	
09/05 02:03:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][250/703]	Step 19258	lr 0.02094	Loss 2.0666 (1.5766)	Prec@(1,5) (67.1%, 91.6%)	
09/05 02:03:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][300/703]	Step 19308	lr 0.02094	Loss 1.2616 (1.5855)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:03:59午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][350/703]	Step 19358	lr 0.02094	Loss 1.3900 (1.5934)	Prec@(1,5) (66.7%, 91.4%)	
09/05 02:04:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][400/703]	Step 19408	lr 0.02094	Loss 1.5949 (1.5965)	Prec@(1,5) (66.7%, 91.3%)	
09/05 02:04:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][450/703]	Step 19458	lr 0.02094	Loss 2.2172 (1.6057)	Prec@(1,5) (66.5%, 91.4%)	
09/05 02:04:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][500/703]	Step 19508	lr 0.02094	Loss 1.3019 (1.6013)	Prec@(1,5) (66.6%, 91.4%)	
09/05 02:04:16午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][550/703]	Step 19558	lr 0.02094	Loss 1.8652 (1.5993)	Prec@(1,5) (66.7%, 91.3%)	
09/05 02:04:20午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][600/703]	Step 19608	lr 0.02094	Loss 1.8556 (1.6047)	Prec@(1,5) (66.5%, 91.4%)	
09/05 02:04:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][650/703]	Step 19658	lr 0.02094	Loss 1.5414 (1.6072)	Prec@(1,5) (66.5%, 91.3%)	
09/05 02:04:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][700/703]	Step 19708	lr 0.02094	Loss 1.7521 (1.6091)	Prec@(1,5) (66.5%, 91.3%)	
09/05 02:04:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [27][703/703]	Step 19711	lr 0.02094	Loss 1.4225 (1.6087)	Prec@(1,5) (66.5%, 91.3%)	
09/05 02:04:29午後 evaluateStage_trainer.py:148 [INFO] Train: [ 27/99] Final Prec@1 66.4956%
09/05 02:04:31午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 27/99] Final Prec@1 58.3400%
09/05 02:04:32午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:04:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][50/703]	Step 19762	lr 0.02065	Loss 1.3961 (1.5242)	Prec@(1,5) (68.7%, 92.2%)	
09/05 02:04:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][100/703]	Step 19812	lr 0.02065	Loss 1.4697 (1.5235)	Prec@(1,5) (68.3%, 92.3%)	
09/05 02:04:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][150/703]	Step 19862	lr 0.02065	Loss 1.2539 (1.5225)	Prec@(1,5) (68.1%, 92.2%)	
09/05 02:04:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][200/703]	Step 19912	lr 0.02065	Loss 2.0979 (1.5385)	Prec@(1,5) (67.8%, 92.1%)	
09/05 02:04:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][250/703]	Step 19962	lr 0.02065	Loss 2.2097 (1.5528)	Prec@(1,5) (67.5%, 91.9%)	
09/05 02:04:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][300/703]	Step 20012	lr 0.02065	Loss 1.7962 (1.5767)	Prec@(1,5) (67.1%, 91.6%)	
09/05 02:05:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][350/703]	Step 20062	lr 0.02065	Loss 1.4714 (1.5841)	Prec@(1,5) (67.0%, 91.5%)	
09/05 02:05:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][400/703]	Step 20112	lr 0.02065	Loss 1.7273 (1.5841)	Prec@(1,5) (67.0%, 91.5%)	
09/05 02:05:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][450/703]	Step 20162	lr 0.02065	Loss 1.1917 (1.5871)	Prec@(1,5) (66.9%, 91.4%)	
09/05 02:05:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][500/703]	Step 20212	lr 0.02065	Loss 1.2869 (1.5864)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:05:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][550/703]	Step 20262	lr 0.02065	Loss 1.7506 (1.5900)	Prec@(1,5) (66.9%, 91.4%)	
09/05 02:05:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][600/703]	Step 20312	lr 0.02065	Loss 1.8965 (1.5868)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:05:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][650/703]	Step 20362	lr 0.02065	Loss 1.4776 (1.5895)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:05:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][700/703]	Step 20412	lr 0.02065	Loss 1.3350 (1.5909)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:05:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [28][703/703]	Step 20415	lr 0.02065	Loss 1.5075 (1.5903)	Prec@(1,5) (66.9%, 91.5%)	
09/05 02:05:32午後 evaluateStage_trainer.py:148 [INFO] Train: [ 28/99] Final Prec@1 66.8711%
09/05 02:05:34午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 28/99] Final Prec@1 58.6600%
09/05 02:05:35午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:05:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][50/703]	Step 20466	lr 0.02035	Loss 1.2913 (1.4557)	Prec@(1,5) (69.2%, 92.7%)	
09/05 02:05:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][100/703]	Step 20516	lr 0.02035	Loss 1.5901 (1.4881)	Prec@(1,5) (68.8%, 92.6%)	
09/05 02:05:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][150/703]	Step 20566	lr 0.02035	Loss 1.4042 (1.5083)	Prec@(1,5) (68.4%, 92.2%)	
09/05 02:05:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][200/703]	Step 20616	lr 0.02035	Loss 1.4736 (1.5151)	Prec@(1,5) (68.3%, 92.4%)	
09/05 02:05:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][250/703]	Step 20666	lr 0.02035	Loss 1.9328 (1.5279)	Prec@(1,5) (68.2%, 92.1%)	
09/05 02:06:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][300/703]	Step 20716	lr 0.02035	Loss 1.4248 (1.5405)	Prec@(1,5) (68.0%, 92.1%)	
09/05 02:06:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][350/703]	Step 20766	lr 0.02035	Loss 1.5008 (1.5504)	Prec@(1,5) (67.7%, 92.0%)	
09/05 02:06:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][400/703]	Step 20816	lr 0.02035	Loss 1.8494 (1.5488)	Prec@(1,5) (67.6%, 92.0%)	
09/05 02:06:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][450/703]	Step 20866	lr 0.02035	Loss 1.8694 (1.5518)	Prec@(1,5) (67.5%, 92.0%)	
09/05 02:06:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][500/703]	Step 20916	lr 0.02035	Loss 1.8155 (1.5534)	Prec@(1,5) (67.4%, 92.0%)	
09/05 02:06:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][550/703]	Step 20966	lr 0.02035	Loss 1.8405 (1.5644)	Prec@(1,5) (67.1%, 92.0%)	
09/05 02:06:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][600/703]	Step 21016	lr 0.02035	Loss 1.5611 (1.5700)	Prec@(1,5) (67.0%, 91.9%)	
09/05 02:06:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][650/703]	Step 21066	lr 0.02035	Loss 1.3715 (1.5752)	Prec@(1,5) (66.9%, 91.9%)	
09/05 02:06:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][700/703]	Step 21116	lr 0.02035	Loss 1.4286 (1.5772)	Prec@(1,5) (66.9%, 91.8%)	
09/05 02:06:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [29][703/703]	Step 21119	lr 0.02035	Loss 1.7041 (1.5776)	Prec@(1,5) (66.8%, 91.8%)	
09/05 02:06:34午後 evaluateStage_trainer.py:148 [INFO] Train: [ 29/99] Final Prec@1 66.8533%
09/05 02:06:37午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 29/99] Final Prec@1 59.1000%
09/05 02:06:37午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:06:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][50/703]	Step 21170	lr 0.02005	Loss 1.7643 (1.4579)	Prec@(1,5) (69.2%, 92.9%)	
09/05 02:06:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][100/703]	Step 21220	lr 0.02005	Loss 1.3708 (1.4689)	Prec@(1,5) (69.1%, 92.7%)	
09/05 02:06:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][150/703]	Step 21270	lr 0.02005	Loss 1.6158 (1.4890)	Prec@(1,5) (68.6%, 92.6%)	
09/05 02:06:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][200/703]	Step 21320	lr 0.02005	Loss 1.5246 (1.5046)	Prec@(1,5) (68.3%, 92.4%)	
09/05 02:06:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][250/703]	Step 21370	lr 0.02005	Loss 1.5926 (1.5071)	Prec@(1,5) (68.4%, 92.4%)	
09/05 02:07:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][300/703]	Step 21420	lr 0.02005	Loss 1.5899 (1.5164)	Prec@(1,5) (68.1%, 92.4%)	
09/05 02:07:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][350/703]	Step 21470	lr 0.02005	Loss 1.6152 (1.5275)	Prec@(1,5) (68.0%, 92.3%)	
09/05 02:07:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][400/703]	Step 21520	lr 0.02005	Loss 2.0245 (1.5339)	Prec@(1,5) (67.8%, 92.1%)	
09/05 02:07:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][450/703]	Step 21570	lr 0.02005	Loss 1.6555 (1.5376)	Prec@(1,5) (67.8%, 92.0%)	
09/05 02:07:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][500/703]	Step 21620	lr 0.02005	Loss 1.3369 (1.5388)	Prec@(1,5) (67.7%, 91.9%)	
09/05 02:07:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][550/703]	Step 21670	lr 0.02005	Loss 1.5435 (1.5418)	Prec@(1,5) (67.7%, 91.9%)	
09/05 02:07:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][600/703]	Step 21720	lr 0.02005	Loss 1.4106 (1.5411)	Prec@(1,5) (67.7%, 92.0%)	
09/05 02:07:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][650/703]	Step 21770	lr 0.02005	Loss 1.7013 (1.5443)	Prec@(1,5) (67.6%, 91.9%)	
09/05 02:07:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][700/703]	Step 21820	lr 0.02005	Loss 1.6587 (1.5539)	Prec@(1,5) (67.5%, 91.8%)	
09/05 02:07:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [30][703/703]	Step 21823	lr 0.02005	Loss 1.5152 (1.5544)	Prec@(1,5) (67.5%, 91.8%)	
09/05 02:07:34午後 evaluateStage_trainer.py:148 [INFO] Train: [ 30/99] Final Prec@1 67.4711%
09/05 02:07:36午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 30/99] Final Prec@1 58.2000%
09/05 02:07:36午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:07:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][50/703]	Step 21874	lr 0.01975	Loss 1.3822 (1.4537)	Prec@(1,5) (68.0%, 93.4%)	
09/05 02:07:44午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][100/703]	Step 21924	lr 0.01975	Loss 1.0447 (1.4391)	Prec@(1,5) (69.2%, 93.4%)	
09/05 02:07:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][150/703]	Step 21974	lr 0.01975	Loss 1.2730 (1.4371)	Prec@(1,5) (69.3%, 93.4%)	
09/05 02:07:52午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][200/703]	Step 22024	lr 0.01975	Loss 1.3528 (1.4510)	Prec@(1,5) (69.2%, 93.1%)	
09/05 02:07:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][250/703]	Step 22074	lr 0.01975	Loss 1.3316 (1.4557)	Prec@(1,5) (69.1%, 93.0%)	
09/05 02:08:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][300/703]	Step 22124	lr 0.01975	Loss 1.4681 (1.4709)	Prec@(1,5) (68.8%, 92.9%)	
09/05 02:08:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][350/703]	Step 22174	lr 0.01975	Loss 1.2171 (1.4757)	Prec@(1,5) (68.8%, 92.8%)	
09/05 02:08:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][400/703]	Step 22224	lr 0.01975	Loss 1.4539 (1.4859)	Prec@(1,5) (68.6%, 92.7%)	
09/05 02:08:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][450/703]	Step 22274	lr 0.01975	Loss 1.6837 (1.4979)	Prec@(1,5) (68.3%, 92.5%)	
09/05 02:08:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][500/703]	Step 22324	lr 0.01975	Loss 1.4787 (1.5043)	Prec@(1,5) (68.2%, 92.5%)	
09/05 02:08:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][550/703]	Step 22374	lr 0.01975	Loss 1.3370 (1.5156)	Prec@(1,5) (67.9%, 92.4%)	
09/05 02:08:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][600/703]	Step 22424	lr 0.01975	Loss 1.5292 (1.5248)	Prec@(1,5) (67.7%, 92.3%)	
09/05 02:08:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][650/703]	Step 22474	lr 0.01975	Loss 1.2554 (1.5300)	Prec@(1,5) (67.7%, 92.2%)	
09/05 02:08:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][700/703]	Step 22524	lr 0.01975	Loss 1.7401 (1.5313)	Prec@(1,5) (67.7%, 92.2%)	
09/05 02:08:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [31][703/703]	Step 22527	lr 0.01975	Loss 1.9889 (1.5336)	Prec@(1,5) (67.6%, 92.2%)	
09/05 02:08:36午後 evaluateStage_trainer.py:148 [INFO] Train: [ 31/99] Final Prec@1 67.6089%
09/05 02:08:38午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 31/99] Final Prec@1 58.6400%
09/05 02:08:38午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 59.4000%
09/05 02:08:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][50/703]	Step 22578	lr 0.01943	Loss 1.9039 (1.4645)	Prec@(1,5) (68.6%, 92.7%)	
09/05 02:08:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][100/703]	Step 22628	lr 0.01943	Loss 1.6899 (1.4533)	Prec@(1,5) (68.9%, 93.0%)	
09/05 02:08:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][150/703]	Step 22678	lr 0.01943	Loss 1.0362 (1.4533)	Prec@(1,5) (69.2%, 92.9%)	
09/05 02:08:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][200/703]	Step 22728	lr 0.01943	Loss 1.5455 (1.4576)	Prec@(1,5) (69.1%, 92.8%)	
09/05 02:09:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][250/703]	Step 22778	lr 0.01943	Loss 1.5271 (1.4705)	Prec@(1,5) (68.9%, 92.6%)	
09/05 02:09:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][300/703]	Step 22828	lr 0.01943	Loss 1.7962 (1.4794)	Prec@(1,5) (68.8%, 92.5%)	
09/05 02:09:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][350/703]	Step 22878	lr 0.01943	Loss 1.5666 (1.4856)	Prec@(1,5) (68.8%, 92.4%)	
09/05 02:09:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][400/703]	Step 22928	lr 0.01943	Loss 1.0993 (1.4834)	Prec@(1,5) (68.8%, 92.4%)	
09/05 02:09:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][450/703]	Step 22978	lr 0.01943	Loss 1.4865 (1.4817)	Prec@(1,5) (68.8%, 92.4%)	
09/05 02:09:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][500/703]	Step 23028	lr 0.01943	Loss 1.3880 (1.4851)	Prec@(1,5) (68.7%, 92.3%)	
09/05 02:09:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][550/703]	Step 23078	lr 0.01943	Loss 1.1331 (1.4925)	Prec@(1,5) (68.5%, 92.3%)	
09/05 02:09:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][600/703]	Step 23128	lr 0.01943	Loss 1.4078 (1.4993)	Prec@(1,5) (68.3%, 92.3%)	
09/05 02:09:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][650/703]	Step 23178	lr 0.01943	Loss 1.5943 (1.5034)	Prec@(1,5) (68.3%, 92.2%)	
09/05 02:09:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][700/703]	Step 23228	lr 0.01943	Loss 1.7384 (1.5068)	Prec@(1,5) (68.3%, 92.2%)	
09/05 02:09:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [32][703/703]	Step 23231	lr 0.01943	Loss 1.3451 (1.5071)	Prec@(1,5) (68.3%, 92.2%)	
09/05 02:09:38午後 evaluateStage_trainer.py:148 [INFO] Train: [ 32/99] Final Prec@1 68.2689%
09/05 02:09:41午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 32/99] Final Prec@1 60.4000%
09/05 02:09:41午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 60.4000%
09/05 02:09:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][50/703]	Step 23282	lr 0.01911	Loss 1.5087 (1.3965)	Prec@(1,5) (71.4%, 93.7%)	
09/05 02:09:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][100/703]	Step 23332	lr 0.01911	Loss 1.4733 (1.4056)	Prec@(1,5) (70.5%, 93.6%)	
09/05 02:09:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][150/703]	Step 23382	lr 0.01911	Loss 1.4022 (1.3999)	Prec@(1,5) (70.5%, 93.8%)	
09/05 02:09:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][200/703]	Step 23432	lr 0.01911	Loss 1.6218 (1.4180)	Prec@(1,5) (70.1%, 93.5%)	
09/05 02:10:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][250/703]	Step 23482	lr 0.01911	Loss 1.3660 (1.4337)	Prec@(1,5) (69.9%, 93.3%)	
09/05 02:10:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][300/703]	Step 23532	lr 0.01911	Loss 1.5586 (1.4423)	Prec@(1,5) (69.7%, 93.2%)	
09/05 02:10:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][350/703]	Step 23582	lr 0.01911	Loss 1.6137 (1.4524)	Prec@(1,5) (69.5%, 93.0%)	
09/05 02:10:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][400/703]	Step 23632	lr 0.01911	Loss 1.6418 (1.4597)	Prec@(1,5) (69.5%, 92.9%)	
09/05 02:10:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][450/703]	Step 23682	lr 0.01911	Loss 1.8263 (1.4673)	Prec@(1,5) (69.3%, 92.8%)	
09/05 02:10:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][500/703]	Step 23732	lr 0.01911	Loss 1.8374 (1.4761)	Prec@(1,5) (69.2%, 92.7%)	
09/05 02:10:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][550/703]	Step 23782	lr 0.01911	Loss 1.5604 (1.4805)	Prec@(1,5) (69.1%, 92.7%)	
09/05 02:10:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][600/703]	Step 23832	lr 0.01911	Loss 1.2872 (1.4836)	Prec@(1,5) (69.0%, 92.6%)	
09/05 02:10:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][650/703]	Step 23882	lr 0.01911	Loss 1.7442 (1.4843)	Prec@(1,5) (68.9%, 92.5%)	
09/05 02:10:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][700/703]	Step 23932	lr 0.01911	Loss 1.6062 (1.4878)	Prec@(1,5) (68.8%, 92.5%)	
09/05 02:10:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [33][703/703]	Step 23935	lr 0.01911	Loss 1.4790 (1.4885)	Prec@(1,5) (68.8%, 92.5%)	
09/05 02:10:40午後 evaluateStage_trainer.py:148 [INFO] Train: [ 33/99] Final Prec@1 68.7711%
09/05 02:10:42午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 33/99] Final Prec@1 58.6200%
09/05 02:10:43午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 60.4000%
09/05 02:10:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][50/703]	Step 23986	lr 0.01878	Loss 1.4985 (1.3833)	Prec@(1,5) (71.2%, 93.8%)	
09/05 02:10:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][100/703]	Step 24036	lr 0.01878	Loss 1.3656 (1.3993)	Prec@(1,5) (70.6%, 93.5%)	
09/05 02:10:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][150/703]	Step 24086	lr 0.01878	Loss 1.7047 (1.4011)	Prec@(1,5) (70.6%, 93.5%)	
09/05 02:11:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][200/703]	Step 24136	lr 0.01878	Loss 0.9778 (1.4138)	Prec@(1,5) (70.2%, 93.4%)	
09/05 02:11:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][250/703]	Step 24186	lr 0.01878	Loss 1.4904 (1.4241)	Prec@(1,5) (69.9%, 93.3%)	
09/05 02:11:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][300/703]	Step 24236	lr 0.01878	Loss 1.2797 (1.4321)	Prec@(1,5) (69.5%, 93.1%)	
09/05 02:11:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][350/703]	Step 24286	lr 0.01878	Loss 1.6414 (1.4334)	Prec@(1,5) (69.6%, 93.0%)	
09/05 02:11:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][400/703]	Step 24336	lr 0.01878	Loss 1.4340 (1.4425)	Prec@(1,5) (69.4%, 92.9%)	
09/05 02:11:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][450/703]	Step 24386	lr 0.01878	Loss 1.4127 (1.4475)	Prec@(1,5) (69.4%, 92.9%)	
09/05 02:11:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][500/703]	Step 24436	lr 0.01878	Loss 1.4193 (1.4546)	Prec@(1,5) (69.2%, 92.8%)	
09/05 02:11:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][550/703]	Step 24486	lr 0.01878	Loss 1.4421 (1.4596)	Prec@(1,5) (69.1%, 92.8%)	
09/05 02:11:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][600/703]	Step 24536	lr 0.01878	Loss 2.0023 (1.4674)	Prec@(1,5) (69.0%, 92.7%)	
09/05 02:11:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][650/703]	Step 24586	lr 0.01878	Loss 1.4332 (1.4641)	Prec@(1,5) (69.1%, 92.7%)	
09/05 02:11:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][700/703]	Step 24636	lr 0.01878	Loss 1.5413 (1.4695)	Prec@(1,5) (69.0%, 92.6%)	
09/05 02:11:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [34][703/703]	Step 24639	lr 0.01878	Loss 1.4246 (1.4689)	Prec@(1,5) (69.0%, 92.6%)	
09/05 02:11:39午後 evaluateStage_trainer.py:148 [INFO] Train: [ 34/99] Final Prec@1 69.0311%
09/05 02:11:42午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 34/99] Final Prec@1 58.1400%
09/05 02:11:42午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 60.4000%
09/05 02:11:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][50/703]	Step 24690	lr 0.01845	Loss 1.4098 (1.4210)	Prec@(1,5) (70.2%, 93.3%)	
09/05 02:11:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][100/703]	Step 24740	lr 0.01845	Loss 1.5712 (1.4050)	Prec@(1,5) (70.7%, 93.4%)	
09/05 02:11:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][150/703]	Step 24790	lr 0.01845	Loss 0.9394 (1.4045)	Prec@(1,5) (70.7%, 93.1%)	
09/05 02:11:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][200/703]	Step 24840	lr 0.01845	Loss 1.0107 (1.4035)	Prec@(1,5) (70.6%, 93.2%)	
09/05 02:12:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][250/703]	Step 24890	lr 0.01845	Loss 1.5203 (1.4040)	Prec@(1,5) (70.5%, 93.3%)	
09/05 02:12:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][300/703]	Step 24940	lr 0.01845	Loss 1.0882 (1.4107)	Prec@(1,5) (70.4%, 93.1%)	
09/05 02:12:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][350/703]	Step 24990	lr 0.01845	Loss 1.5088 (1.4215)	Prec@(1,5) (70.2%, 92.9%)	
09/05 02:12:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][400/703]	Step 25040	lr 0.01845	Loss 1.9765 (1.4143)	Prec@(1,5) (70.3%, 93.0%)	
09/05 02:12:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][450/703]	Step 25090	lr 0.01845	Loss 1.7720 (1.4188)	Prec@(1,5) (70.2%, 93.0%)	
09/05 02:12:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][500/703]	Step 25140	lr 0.01845	Loss 1.4632 (1.4269)	Prec@(1,5) (69.9%, 93.0%)	
09/05 02:12:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][550/703]	Step 25190	lr 0.01845	Loss 1.4282 (1.4286)	Prec@(1,5) (69.9%, 93.0%)	
09/05 02:12:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][600/703]	Step 25240	lr 0.01845	Loss 1.9894 (1.4366)	Prec@(1,5) (69.7%, 92.9%)	
09/05 02:12:31午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][650/703]	Step 25290	lr 0.01845	Loss 1.7116 (1.4425)	Prec@(1,5) (69.6%, 92.9%)	
09/05 02:12:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][700/703]	Step 25340	lr 0.01845	Loss 1.5514 (1.4490)	Prec@(1,5) (69.5%, 92.8%)	
09/05 02:12:35午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [35][703/703]	Step 25343	lr 0.01845	Loss 1.5214 (1.4500)	Prec@(1,5) (69.5%, 92.8%)	
09/05 02:12:35午後 evaluateStage_trainer.py:148 [INFO] Train: [ 35/99] Final Prec@1 69.5156%
09/05 02:12:38午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 35/99] Final Prec@1 60.5800%
09/05 02:12:38午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 60.5800%
09/05 02:12:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][50/703]	Step 25394	lr 0.01811	Loss 1.5834 (1.3079)	Prec@(1,5) (71.7%, 94.2%)	
09/05 02:12:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][100/703]	Step 25444	lr 0.01811	Loss 1.1904 (1.3038)	Prec@(1,5) (71.5%, 94.0%)	
09/05 02:12:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][150/703]	Step 25494	lr 0.01811	Loss 1.2166 (1.3327)	Prec@(1,5) (71.5%, 93.8%)	
09/05 02:12:56午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][200/703]	Step 25544	lr 0.01811	Loss 1.1332 (1.3434)	Prec@(1,5) (71.3%, 93.8%)	
09/05 02:13:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][250/703]	Step 25594	lr 0.01811	Loss 1.5026 (1.3596)	Prec@(1,5) (71.0%, 93.6%)	
09/05 02:13:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][300/703]	Step 25644	lr 0.01811	Loss 1.2169 (1.3693)	Prec@(1,5) (70.9%, 93.5%)	
09/05 02:13:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][350/703]	Step 25694	lr 0.01811	Loss 1.4228 (1.3774)	Prec@(1,5) (70.7%, 93.5%)	
09/05 02:13:12午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][400/703]	Step 25744	lr 0.01811	Loss 1.1414 (1.3914)	Prec@(1,5) (70.5%, 93.4%)	
09/05 02:13:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][450/703]	Step 25794	lr 0.01811	Loss 1.5613 (1.4064)	Prec@(1,5) (70.2%, 93.2%)	
09/05 02:13:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][500/703]	Step 25844	lr 0.01811	Loss 1.2527 (1.4125)	Prec@(1,5) (70.2%, 93.2%)	
09/05 02:13:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][550/703]	Step 25894	lr 0.01811	Loss 1.4910 (1.4163)	Prec@(1,5) (70.1%, 93.1%)	
09/05 02:13:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][600/703]	Step 25944	lr 0.01811	Loss 1.6295 (1.4194)	Prec@(1,5) (70.0%, 93.1%)	
09/05 02:13:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][650/703]	Step 25994	lr 0.01811	Loss 1.0384 (1.4251)	Prec@(1,5) (69.9%, 93.0%)	
09/05 02:13:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][700/703]	Step 26044	lr 0.01811	Loss 1.5598 (1.4293)	Prec@(1,5) (69.8%, 93.0%)	
09/05 02:13:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [36][703/703]	Step 26047	lr 0.01811	Loss 1.4243 (1.4302)	Prec@(1,5) (69.8%, 93.0%)	
09/05 02:13:38午後 evaluateStage_trainer.py:148 [INFO] Train: [ 36/99] Final Prec@1 69.7978%
09/05 02:13:41午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 36/99] Final Prec@1 61.1800%
09/05 02:13:41午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 61.1800%
09/05 02:13:46午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][50/703]	Step 26098	lr 0.01777	Loss 1.2018 (1.3507)	Prec@(1,5) (71.2%, 93.8%)	
09/05 02:13:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][100/703]	Step 26148	lr 0.01777	Loss 1.1146 (1.3459)	Prec@(1,5) (71.5%, 93.9%)	
09/05 02:13:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][150/703]	Step 26198	lr 0.01777	Loss 1.2851 (1.3482)	Prec@(1,5) (71.5%, 93.8%)	
09/05 02:13:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][200/703]	Step 26248	lr 0.01777	Loss 1.3285 (1.3512)	Prec@(1,5) (71.4%, 93.8%)	
09/05 02:14:02午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][250/703]	Step 26298	lr 0.01777	Loss 1.5073 (1.3643)	Prec@(1,5) (71.1%, 93.6%)	
09/05 02:14:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][300/703]	Step 26348	lr 0.01777	Loss 1.4157 (1.3789)	Prec@(1,5) (70.8%, 93.5%)	
09/05 02:14:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][350/703]	Step 26398	lr 0.01777	Loss 1.7256 (1.3777)	Prec@(1,5) (70.7%, 93.5%)	
09/05 02:14:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][400/703]	Step 26448	lr 0.01777	Loss 1.1621 (1.3848)	Prec@(1,5) (70.7%, 93.4%)	
09/05 02:14:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][450/703]	Step 26498	lr 0.01777	Loss 0.9391 (1.3943)	Prec@(1,5) (70.4%, 93.4%)	
09/05 02:14:23午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][500/703]	Step 26548	lr 0.01777	Loss 1.3980 (1.4064)	Prec@(1,5) (70.2%, 93.2%)	
09/05 02:14:27午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][550/703]	Step 26598	lr 0.01777	Loss 1.5553 (1.4116)	Prec@(1,5) (70.0%, 93.2%)	
09/05 02:14:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][600/703]	Step 26648	lr 0.01777	Loss 1.3180 (1.4118)	Prec@(1,5) (70.0%, 93.2%)	
09/05 02:14:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][650/703]	Step 26698	lr 0.01777	Loss 1.8407 (1.4151)	Prec@(1,5) (70.0%, 93.1%)	
09/05 02:14:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][700/703]	Step 26748	lr 0.01777	Loss 1.1336 (1.4135)	Prec@(1,5) (70.0%, 93.2%)	
09/05 02:14:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [37][703/703]	Step 26751	lr 0.01777	Loss 1.6310 (1.4152)	Prec@(1,5) (70.0%, 93.1%)	
09/05 02:14:41午後 evaluateStage_trainer.py:148 [INFO] Train: [ 37/99] Final Prec@1 69.9578%
09/05 02:14:43午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 37/99] Final Prec@1 62.4200%
09/05 02:14:44午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 62.4200%
09/05 02:14:49午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][50/703]	Step 26802	lr 0.01742	Loss 1.3867 (1.3129)	Prec@(1,5) (72.0%, 94.0%)	
09/05 02:14:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][100/703]	Step 26852	lr 0.01742	Loss 1.2868 (1.3287)	Prec@(1,5) (71.6%, 93.9%)	
09/05 02:14:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][150/703]	Step 26902	lr 0.01742	Loss 1.3598 (1.3172)	Prec@(1,5) (72.1%, 93.9%)	
09/05 02:15:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][200/703]	Step 26952	lr 0.01742	Loss 1.6095 (1.3310)	Prec@(1,5) (71.9%, 93.9%)	
09/05 02:15:06午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][250/703]	Step 27002	lr 0.01742	Loss 1.1291 (1.3412)	Prec@(1,5) (71.6%, 93.8%)	
09/05 02:15:10午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][300/703]	Step 27052	lr 0.01742	Loss 1.0724 (1.3504)	Prec@(1,5) (71.3%, 93.7%)	
09/05 02:15:14午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][350/703]	Step 27102	lr 0.01742	Loss 1.4475 (1.3582)	Prec@(1,5) (71.1%, 93.6%)	
09/05 02:15:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][400/703]	Step 27152	lr 0.01742	Loss 1.3796 (1.3583)	Prec@(1,5) (71.0%, 93.7%)	
09/05 02:15:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][450/703]	Step 27202	lr 0.01742	Loss 1.1905 (1.3638)	Prec@(1,5) (70.9%, 93.6%)	
09/05 02:15:25午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][500/703]	Step 27252	lr 0.01742	Loss 1.3826 (1.3676)	Prec@(1,5) (70.8%, 93.6%)	
09/05 02:15:29午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][550/703]	Step 27302	lr 0.01742	Loss 1.5737 (1.3753)	Prec@(1,5) (70.7%, 93.5%)	
09/05 02:15:33午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][600/703]	Step 27352	lr 0.01742	Loss 1.1005 (1.3755)	Prec@(1,5) (70.7%, 93.5%)	
09/05 02:15:37午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][650/703]	Step 27402	lr 0.01742	Loss 1.6115 (1.3794)	Prec@(1,5) (70.6%, 93.5%)	
09/05 02:15:41午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][700/703]	Step 27452	lr 0.01742	Loss 1.5184 (1.3843)	Prec@(1,5) (70.5%, 93.5%)	
09/05 02:15:42午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [38][703/703]	Step 27455	lr 0.01742	Loss 1.5601 (1.3857)	Prec@(1,5) (70.5%, 93.4%)	
09/05 02:15:42午後 evaluateStage_trainer.py:148 [INFO] Train: [ 38/99] Final Prec@1 70.4756%
09/05 02:15:44午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 38/99] Final Prec@1 59.3200%
09/05 02:15:45午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 62.4200%
09/05 02:15:50午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][50/703]	Step 27506	lr 0.01706	Loss 0.9266 (1.2876)	Prec@(1,5) (72.7%, 94.4%)	
09/05 02:15:54午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][100/703]	Step 27556	lr 0.01706	Loss 1.4358 (1.3017)	Prec@(1,5) (72.5%, 94.2%)	
09/05 02:15:58午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][150/703]	Step 27606	lr 0.01706	Loss 1.4177 (1.2968)	Prec@(1,5) (72.8%, 94.2%)	
09/05 02:16:03午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][200/703]	Step 27656	lr 0.01706	Loss 1.0194 (1.3026)	Prec@(1,5) (72.7%, 94.0%)	
09/05 02:16:07午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][250/703]	Step 27706	lr 0.01706	Loss 1.4204 (1.3111)	Prec@(1,5) (72.4%, 93.9%)	
09/05 02:16:11午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][300/703]	Step 27756	lr 0.01706	Loss 1.4915 (1.3163)	Prec@(1,5) (72.3%, 93.9%)	
09/05 02:16:15午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][350/703]	Step 27806	lr 0.01706	Loss 1.2404 (1.3299)	Prec@(1,5) (72.1%, 93.8%)	
09/05 02:16:19午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][400/703]	Step 27856	lr 0.01706	Loss 1.6400 (1.3341)	Prec@(1,5) (72.0%, 93.7%)	
09/05 02:16:24午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][450/703]	Step 27906	lr 0.01706	Loss 0.9956 (1.3400)	Prec@(1,5) (71.8%, 93.6%)	
09/05 02:16:28午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][500/703]	Step 27956	lr 0.01706	Loss 1.5638 (1.3545)	Prec@(1,5) (71.5%, 93.5%)	
09/05 02:16:32午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][550/703]	Step 28006	lr 0.01706	Loss 1.4867 (1.3627)	Prec@(1,5) (71.4%, 93.4%)	
09/05 02:16:36午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][600/703]	Step 28056	lr 0.01706	Loss 1.3880 (1.3659)	Prec@(1,5) (71.3%, 93.4%)	
09/05 02:16:40午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][650/703]	Step 28106	lr 0.01706	Loss 1.8750 (1.3716)	Prec@(1,5) (71.1%, 93.4%)	
09/05 02:16:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][700/703]	Step 28156	lr 0.01706	Loss 1.5573 (1.3743)	Prec@(1,5) (71.1%, 93.4%)	
09/05 02:16:45午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [39][703/703]	Step 28159	lr 0.01706	Loss 1.2088 (1.3743)	Prec@(1,5) (71.1%, 93.4%)	
09/05 02:16:45午後 evaluateStage_trainer.py:148 [INFO] Train: [ 39/99] Final Prec@1 71.0733%
09/05 02:16:47午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 39/99] Final Prec@1 61.6000%
09/05 02:16:48午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 62.4200%
09/05 02:16:53午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][50/703]	Step 28210	lr 0.01671	Loss 1.3147 (1.3129)	Prec@(1,5) (71.3%, 94.7%)	
09/05 02:16:57午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][100/703]	Step 28260	lr 0.01671	Loss 1.2592 (1.2794)	Prec@(1,5) (72.6%, 94.5%)	
09/05 02:17:01午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][150/703]	Step 28310	lr 0.01671	Loss 1.2624 (1.3065)	Prec@(1,5) (72.1%, 94.2%)	
09/05 02:17:05午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][200/703]	Step 28360	lr 0.01671	Loss 0.9835 (1.3097)	Prec@(1,5) (72.3%, 93.9%)	
09/05 02:17:09午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][250/703]	Step 28410	lr 0.01671	Loss 1.2389 (1.3175)	Prec@(1,5) (72.0%, 93.9%)	
09/05 02:17:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][300/703]	Step 28460	lr 0.01671	Loss 1.4878 (1.3302)	Prec@(1,5) (71.8%, 93.7%)	
09/05 02:17:18午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][350/703]	Step 28510	lr 0.01671	Loss 1.4042 (1.3355)	Prec@(1,5) (71.6%, 93.7%)	
09/05 02:17:22午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][400/703]	Step 28560	lr 0.01671	Loss 1.2713 (1.3435)	Prec@(1,5) (71.5%, 93.6%)	
09/05 02:17:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][450/703]	Step 28610	lr 0.01671	Loss 1.4188 (1.3509)	Prec@(1,5) (71.4%, 93.5%)	
09/05 02:17:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][500/703]	Step 28660	lr 0.01671	Loss 1.4581 (1.3540)	Prec@(1,5) (71.3%, 93.5%)	
09/05 02:17:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][550/703]	Step 28710	lr 0.01671	Loss 1.5509 (1.3566)	Prec@(1,5) (71.2%, 93.5%)	
09/05 02:17:39午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][600/703]	Step 28760	lr 0.01671	Loss 1.1997 (1.3599)	Prec@(1,5) (71.1%, 93.5%)	
09/05 02:17:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][650/703]	Step 28810	lr 0.01671	Loss 1.4122 (1.3588)	Prec@(1,5) (71.1%, 93.5%)	
09/05 02:17:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][700/703]	Step 28860	lr 0.01671	Loss 0.8010 (1.3616)	Prec@(1,5) (71.1%, 93.5%)	
09/05 02:17:48午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [40][703/703]	Step 28863	lr 0.01671	Loss 1.5889 (1.3630)	Prec@(1,5) (71.1%, 93.5%)	
09/05 02:17:48午後 evaluateStage_trainer.py:148 [INFO] Train: [ 40/99] Final Prec@1 71.0600%
09/05 02:17:50午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 40/99] Final Prec@1 62.1400%
09/05 02:17:51午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 62.4200%
09/05 02:17:55午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][50/703]	Step 28914	lr 0.01635	Loss 1.2174 (1.2257)	Prec@(1,5) (73.9%, 94.4%)	
09/05 02:18:00午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][100/703]	Step 28964	lr 0.01635	Loss 0.9656 (1.2458)	Prec@(1,5) (73.4%, 94.3%)	
09/05 02:18:04午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][150/703]	Step 29014	lr 0.01635	Loss 1.0736 (1.2787)	Prec@(1,5) (72.8%, 94.2%)	
09/05 02:18:08午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][200/703]	Step 29064	lr 0.01635	Loss 1.4076 (1.2824)	Prec@(1,5) (72.4%, 94.3%)	
09/05 02:18:13午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][250/703]	Step 29114	lr 0.01635	Loss 1.3959 (1.2940)	Prec@(1,5) (72.2%, 94.2%)	
09/05 02:18:17午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][300/703]	Step 29164	lr 0.01635	Loss 1.6748 (1.3071)	Prec@(1,5) (72.1%, 94.0%)	
09/05 02:18:21午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][350/703]	Step 29214	lr 0.01635	Loss 1.7309 (1.3073)	Prec@(1,5) (72.1%, 94.1%)	
09/05 02:18:26午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][400/703]	Step 29264	lr 0.01635	Loss 1.5883 (1.3097)	Prec@(1,5) (72.1%, 94.1%)	
09/05 02:18:30午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][450/703]	Step 29314	lr 0.01635	Loss 1.5642 (1.3200)	Prec@(1,5) (71.8%, 94.0%)	
09/05 02:18:34午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][500/703]	Step 29364	lr 0.01635	Loss 1.5741 (1.3264)	Prec@(1,5) (71.7%, 93.9%)	
09/05 02:18:38午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][550/703]	Step 29414	lr 0.01635	Loss 1.1645 (1.3320)	Prec@(1,5) (71.5%, 93.9%)	
09/05 02:18:43午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][600/703]	Step 29464	lr 0.01635	Loss 1.4475 (1.3374)	Prec@(1,5) (71.4%, 93.8%)	
09/05 02:18:47午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][650/703]	Step 29514	lr 0.01635	Loss 1.3940 (1.3374)	Prec@(1,5) (71.4%, 93.8%)	
09/05 02:18:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][700/703]	Step 29564	lr 0.01635	Loss 0.9944 (1.3428)	Prec@(1,5) (71.3%, 93.7%)	
09/05 02:18:51午後 evaluateStage_trainer.py:141 [INFO] Train: Epoch: [41][703/703]	Step 29567	lr 0.01635	Loss 1.5336 (1.3430)	Prec@(1,5) (71.3%, 93.7%)	
09/05 02:18:52午後 evaluateStage_trainer.py:148 [INFO] Train: [ 41/99] Final Prec@1 71.2711%
09/05 02:18:54午後 evaluateStage_trainer.py:183 [INFO] Valid: [ 41/99] Final Prec@1 62.5600%
09/05 02:18:54午後 evaluateStage_main.py:74 [INFO] Until now, best Prec@1 = 62.5600%
