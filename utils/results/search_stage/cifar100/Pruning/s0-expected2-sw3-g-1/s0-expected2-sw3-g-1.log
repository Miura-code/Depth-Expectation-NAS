12/04 01:00:39PM parser.py:28 [INFO] 
12/04 01:00:39PM parser.py:29 [INFO] Parameters:
12/04 01:00:39PM parser.py:31 [INFO] DAG_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected2-sw3-g-1/DAG
12/04 01:00:39PM parser.py:31 [INFO] T=10.0
12/04 01:00:39PM parser.py:31 [INFO] ADVANCED=1
12/04 01:00:39PM parser.py:31 [INFO] ALPHA_LR=0.0003
12/04 01:00:39PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
12/04 01:00:39PM parser.py:31 [INFO] ARCH_CRITERION=expected
12/04 01:00:39PM parser.py:31 [INFO] BATCH_SIZE=64
12/04 01:00:39PM parser.py:31 [INFO] CASCADE=0
12/04 01:00:39PM parser.py:31 [INFO] CHECKPOINT_RESET=False
12/04 01:00:39PM parser.py:31 [INFO] CURRICULUM_EPOCHS=[30, 20]
12/04 01:00:39PM parser.py:31 [INFO] CUTOUT_LENGTH=0
12/04 01:00:39PM parser.py:31 [INFO] DATA_PATH=../data/
12/04 01:00:39PM parser.py:31 [INFO] DATASET=cifar100
12/04 01:00:39PM parser.py:31 [INFO] DEPTH_COEF=0.0
12/04 01:00:39PM parser.py:31 [INFO] DESCRIPTION=search_and_evaluation_curriculum_learning_with_-Beta-expected-Depth-ecpected-constraint_slidewindow-3
12/04 01:00:39PM parser.py:31 [INFO] DISCRETE=1
12/04 01:00:39PM parser.py:31 [INFO] EPOCHS=50
12/04 01:00:39PM parser.py:31 [INFO] EVAL_EPOCHS=100
12/04 01:00:39PM parser.py:31 [INFO] EXP_NAME=s0-expected2-sw3-g-1
12/04 01:00:39PM parser.py:31 [INFO] FINAL_L=0.0
12/04 01:00:39PM parser.py:31 [INFO] G=-1.0
12/04 01:00:39PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('skip_connect', 0), ('avg_pool_3x3', 2)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
12/04 01:00:39PM parser.py:31 [INFO] GPUS=[0]
12/04 01:00:39PM parser.py:31 [INFO] HINT_EPOCHS=[16, 32]
12/04 01:00:39PM parser.py:31 [INFO] INIT_CHANNELS=16
12/04 01:00:39PM parser.py:31 [INFO] L=0.0
12/04 01:00:39PM parser.py:31 [INFO] LAYERS=32
12/04 01:00:39PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
12/04 01:00:39PM parser.py:31 [INFO] NAME=Pruning
12/04 01:00:39PM parser.py:31 [INFO] NONKD=1
12/04 01:00:39PM parser.py:31 [INFO] PATH=results/search_stage_KD/cifar100/Pruning/s0-expected2-sw3-g-1
12/04 01:00:39PM parser.py:31 [INFO] PCDARTS=0
12/04 01:00:39PM parser.py:31 [INFO] PLOT_PATH=results/search_stage_KD/cifar100/Pruning/s0-expected2-sw3-g-1/plots
12/04 01:00:39PM parser.py:31 [INFO] PRINT_FREQ=100
12/04 01:00:39PM parser.py:31 [INFO] RESET=0
12/04 01:00:39PM parser.py:31 [INFO] RESUME_PATH=None
12/04 01:00:39PM parser.py:31 [INFO] SAVE=s0-expected2-sw3-g-1
12/04 01:00:39PM parser.py:31 [INFO] SEED=0
12/04 01:00:39PM parser.py:31 [INFO] SHARE_STAGE=0
12/04 01:00:39PM parser.py:31 [INFO] SLIDE_WINDOW=3
12/04 01:00:39PM parser.py:31 [INFO] SPEC_CELL=1
12/04 01:00:39PM parser.py:31 [INFO] STAGE_MACS=[8.18, 2.49, 1.88]
12/04 01:00:39PM parser.py:31 [INFO] TEACHER_NAME=none
12/04 01:00:39PM parser.py:31 [INFO] TEACHER_PATH=none
12/04 01:00:39PM parser.py:31 [INFO] TRAIN_PORTION=0.5
12/04 01:00:39PM parser.py:31 [INFO] TYPE=Pruning
12/04 01:00:39PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
12/04 01:00:39PM parser.py:31 [INFO] W_LR=0.025
12/04 01:00:39PM parser.py:31 [INFO] W_LR_MIN=0.001
12/04 01:00:39PM parser.py:31 [INFO] W_MOMENTUM=0.9
12/04 01:00:39PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
12/04 01:00:39PM parser.py:31 [INFO] WORKERS=4
12/04 01:00:39PM parser.py:32 [INFO] 
12/04 01:00:40PM searchStage_trainer.py:134 [INFO] --> No loaded checkpoint!
12/04 01:01:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.3630 (4.4972)	Arch Loss -1.8804 (-1.7441)	Arch Hard Loss 4.4225 (4.5040)	Arch Beta Loss 6.3029 (6.2482)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (2.5%, 10.5%)	
12/04 01:02:21PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0662 (4.3623)	Arch Loss -2.2112 (-1.9476)	Arch Hard Loss 4.2061 (4.3566)	Arch Beta Loss 6.4173 (6.3042)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (3.4%, 14.6%)	
12/04 01:03:09PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 4.0460 (4.2657)	Arch Loss -2.5155 (-2.0892)	Arch Hard Loss 4.0205 (4.2726)	Arch Beta Loss 6.5359 (6.3617)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (4.5%, 17.7%)	
12/04 01:03:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.7502 (4.2028)	Arch Loss -2.7523 (-2.2091)	Arch Hard Loss 3.8938 (4.2056)	Arch Beta Loss 6.6462 (6.4147)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (5.3%, 19.8%)	
12/04 01:03:54PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  0/49] Final Prec@1 5.2880%
12/04 01:04:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 4.0945	Prec@(1,5) (7.3%, 26.3%)
12/04 01:04:07PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 4.0861	Prec@(1,5) (7.5%, 26.3%)
12/04 01:04:14PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 4.0901	Prec@(1,5) (7.5%, 26.3%)
12/04 01:04:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 4.0865	Prec@(1,5) (7.5%, 26.3%)
12/04 01:04:20PM searchStage_trainer.py:323 [INFO] Valid: [  0/49] Final Prec@1 7.4960%
12/04 01:04:20PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[9, 11])
12/04 01:04:21PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 7.4960%
12/04 01:05:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.8207 (3.8923)	Arch Loss -2.6337 (-2.8241)	Arch Hard Loss 4.1400 (3.8868)	Arch Beta Loss 6.7737 (6.7108)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (8.9%, 29.3%)	
12/04 01:06:00PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.6197 (3.8630)	Arch Loss -2.9889 (-2.9289)	Arch Hard Loss 3.9158 (3.8463)	Arch Beta Loss 6.9047 (6.7751)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (9.1%, 30.5%)	
12/04 01:06:49PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.7990 (3.8176)	Arch Loss -3.5976 (-3.0367)	Arch Hard Loss 3.4421 (3.8043)	Arch Beta Loss 7.0397 (6.8409)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.0%, 31.7%)	
12/04 01:07:34PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.8571 (3.7763)	Arch Loss -3.0432 (-3.1324)	Arch Hard Loss 4.1225 (3.7691)	Arch Beta Loss 7.1657 (6.9015)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (10.7%, 33.1%)	
12/04 01:07:34PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  1/49] Final Prec@1 10.6640%
12/04 01:07:41PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6160	Prec@(1,5) (13.5%, 38.1%)
12/04 01:07:48PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.6120	Prec@(1,5) (13.4%, 38.0%)
12/04 01:07:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.6220	Prec@(1,5) (13.2%, 37.9%)
12/04 01:08:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.6203	Prec@(1,5) (13.1%, 38.0%)
12/04 01:08:01PM searchStage_trainer.py:323 [INFO] Valid: [  1/49] Final Prec@1 13.1120%
12/04 01:08:01PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:08:01PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 13.1120%
12/04 01:08:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.6534 (3.5951)	Arch Loss -3.7117 (-3.6440)	Arch Hard Loss 3.5986 (3.5951)	Arch Beta Loss 7.3103 (7.2391)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.1%, 38.5%)	
12/04 01:09:41PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.5671 (3.5715)	Arch Loss -4.0951 (-3.7333)	Arch Hard Loss 3.3630 (3.5787)	Arch Beta Loss 7.4582 (7.3120)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (13.7%, 39.6%)	
12/04 01:10:31PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.3018 (3.5379)	Arch Loss -4.1337 (-3.8380)	Arch Hard Loss 3.4765 (3.5481)	Arch Beta Loss 7.6102 (7.3861)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.3%, 40.4%)	
12/04 01:11:15PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.2771 (3.5162)	Arch Loss -4.4497 (-3.9313)	Arch Hard Loss 3.3002 (3.5228)	Arch Beta Loss 7.7499 (7.4541)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (14.6%, 41.0%)	
12/04 01:11:16PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  2/49] Final Prec@1 14.6440%
12/04 01:11:23PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4383	Prec@(1,5) (16.3%, 44.0%)
12/04 01:11:29PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4575	Prec@(1,5) (15.7%, 43.0%)
12/04 01:11:36PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4572	Prec@(1,5) (15.7%, 43.1%)
12/04 01:11:42PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.4540	Prec@(1,5) (15.8%, 43.4%)
12/04 01:11:42PM searchStage_trainer.py:323 [INFO] Valid: [  2/49] Final Prec@1 15.7520%
12/04 01:11:42PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:11:42PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 15.7520%
12/04 01:12:33PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.4390 (3.3911)	Arch Loss -4.4922 (-4.4471)	Arch Hard Loss 3.4201 (3.3854)	Arch Beta Loss 7.9123 (7.8324)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (16.5%, 44.6%)	
12/04 01:13:21PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.0116 (3.3526)	Arch Loss -4.4235 (-4.5521)	Arch Hard Loss 3.6540 (3.3619)	Arch Beta Loss 8.0776 (7.9140)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (17.7%, 46.0%)	
12/04 01:14:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.2680 (3.3243)	Arch Loss -4.7278 (-4.6602)	Arch Hard Loss 3.5191 (3.3367)	Arch Beta Loss 8.2469 (7.9969)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.2%, 46.6%)	
12/04 01:14:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.1445 (3.3032)	Arch Loss -5.2072 (-4.7491)	Arch Hard Loss 3.1957 (3.3236)	Arch Beta Loss 8.4029 (8.0727)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (18.7%, 47.1%)	
12/04 01:14:54PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  3/49] Final Prec@1 18.6720%
12/04 01:15:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2542	Prec@(1,5) (20.1%, 48.2%)
12/04 01:15:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2604	Prec@(1,5) (19.9%, 48.0%)
12/04 01:15:15PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2623	Prec@(1,5) (19.8%, 47.9%)
12/04 01:15:21PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2654	Prec@(1,5) (19.7%, 47.9%)
12/04 01:15:21PM searchStage_trainer.py:323 [INFO] Valid: [  3/49] Final Prec@1 19.7160%
12/04 01:15:21PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:15:21PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 19.7160%
12/04 01:16:12PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.1617 (3.1447)	Arch Loss -5.5935 (-5.2533)	Arch Hard Loss 2.9886 (3.2405)	Arch Beta Loss 8.5821 (8.4939)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.1%, 51.5%)	
12/04 01:17:02PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.1689 (3.1309)	Arch Loss -5.7412 (-5.3838)	Arch Hard Loss 3.0193 (3.1992)	Arch Beta Loss 8.7605 (8.5829)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (21.8%, 51.7%)	
12/04 01:17:51PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 2.7371 (3.1216)	Arch Loss -5.7452 (-5.4885)	Arch Hard Loss 3.1957 (3.1839)	Arch Beta Loss 8.9409 (8.6725)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.3%, 51.8%)	
12/04 01:18:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.9914 (3.1106)	Arch Loss -6.2719 (-5.5962)	Arch Hard Loss 2.8330 (3.1573)	Arch Beta Loss 9.1049 (8.7535)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (22.6%, 52.2%)	
12/04 01:18:36PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  4/49] Final Prec@1 22.5600%
12/04 01:18:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.1020	Prec@(1,5) (22.8%, 53.3%)
12/04 01:18:50PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0584	Prec@(1,5) (23.5%, 54.3%)
12/04 01:18:57PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.0511	Prec@(1,5) (23.7%, 54.3%)
12/04 01:19:03PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0488	Prec@(1,5) (23.8%, 54.4%)
12/04 01:19:03PM searchStage_trainer.py:323 [INFO] Valid: [  4/49] Final Prec@1 23.7880%
12/04 01:19:03PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:19:04PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 23.7880%
12/04 01:19:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1431 (2.9649)	Arch Loss -6.3109 (-6.1728)	Arch Hard Loss 2.9788 (3.0268)	Arch Beta Loss 9.2897 (9.1996)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.0%, 55.4%)	
12/04 01:20:43PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.1985 (2.9555)	Arch Loss -6.1116 (-6.2464)	Arch Hard Loss 3.3603 (3.0442)	Arch Beta Loss 9.4720 (9.2906)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.2%, 55.9%)	
12/04 01:21:33PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.8588 (2.9602)	Arch Loss -6.8932 (-6.3604)	Arch Hard Loss 2.7624 (3.0215)	Arch Beta Loss 9.6556 (9.3819)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.2%, 55.7%)	
12/04 01:22:17PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.7051 (2.9493)	Arch Loss -6.9265 (-6.4582)	Arch Hard Loss 2.8938 (3.0061)	Arch Beta Loss 9.8203 (9.4643)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (25.2%, 56.3%)	
12/04 01:22:18PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  5/49] Final Prec@1 25.2000%
12/04 01:22:25PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.9603	Prec@(1,5) (25.0%, 57.2%)
12/04 01:22:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9719	Prec@(1,5) (25.1%, 56.8%)
12/04 01:22:38PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9811	Prec@(1,5) (25.1%, 56.6%)
12/04 01:22:44PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9715	Prec@(1,5) (25.3%, 56.7%)
12/04 01:22:44PM searchStage_trainer.py:323 [INFO] Valid: [  5/49] Final Prec@1 25.2920%
12/04 01:22:44PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:22:45PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 25.2920%
12/04 01:23:35PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.7475 (2.8054)	Arch Loss -7.0484 (-7.0035)	Arch Hard Loss 2.9551 (2.9102)	Arch Beta Loss 10.0036 (9.9136)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.5%, 60.0%)	
12/04 01:24:25PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.8093 (2.8248)	Arch Loss -7.1094 (-7.0980)	Arch Hard Loss 3.0755 (2.9065)	Arch Beta Loss 10.1849 (10.0046)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.8%, 59.6%)	
12/04 01:25:14PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.8353 (2.8245)	Arch Loss -7.5387 (-7.2092)	Arch Hard Loss 2.8274 (2.8860)	Arch Beta Loss 10.3661 (10.0952)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (27.7%, 59.6%)	
12/04 01:25:59PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.7733 (2.8203)	Arch Loss -7.6713 (-7.2803)	Arch Hard Loss 2.8564 (2.8961)	Arch Beta Loss 10.5276 (10.1765)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (28.0%, 59.7%)	
12/04 01:26:00PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  6/49] Final Prec@1 27.9560%
12/04 01:26:07PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.8393	Prec@(1,5) (28.3%, 59.4%)
12/04 01:26:13PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.8329	Prec@(1,5) (28.5%, 59.7%)
12/04 01:26:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.8386	Prec@(1,5) (28.2%, 59.6%)
12/04 01:26:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.8448	Prec@(1,5) (28.1%, 59.4%)
12/04 01:26:26PM searchStage_trainer.py:323 [INFO] Valid: [  6/49] Final Prec@1 28.1000%
12/04 01:26:26PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:26:27PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 28.1000%
12/04 01:27:17PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.9297 (2.7041)	Arch Loss -7.8515 (-7.7827)	Arch Hard Loss 2.8547 (2.8364)	Arch Beta Loss 10.7063 (10.6191)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.2%, 62.7%)	
12/04 01:28:07PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.6416 (2.7009)	Arch Loss -8.1246 (-7.8759)	Arch Hard Loss 2.7571 (2.8311)	Arch Beta Loss 10.8817 (10.7070)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.3%, 62.7%)	
12/04 01:28:56PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.8416 (2.7037)	Arch Loss -7.7991 (-7.9714)	Arch Hard Loss 3.2563 (2.8232)	Arch Beta Loss 11.0554 (10.7946)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.3%, 62.6%)	
12/04 01:29:41PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.6846 (2.6959)	Arch Loss -8.4410 (-8.0682)	Arch Hard Loss 2.7674 (2.8044)	Arch Beta Loss 11.2083 (10.8727)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (30.4%, 62.7%)	
12/04 01:29:41PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  7/49] Final Prec@1 30.4040%
12/04 01:29:48PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.7517	Prec@(1,5) (29.8%, 61.5%)
12/04 01:29:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.7358	Prec@(1,5) (29.7%, 61.9%)
12/04 01:30:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.7402	Prec@(1,5) (29.5%, 61.8%)
12/04 01:30:07PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.7481	Prec@(1,5) (29.4%, 61.6%)
12/04 01:30:07PM searchStage_trainer.py:323 [INFO] Valid: [  7/49] Final Prec@1 29.4280%
12/04 01:30:07PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:30:08PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 29.4280%
12/04 01:30:58PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.4168 (2.5979)	Arch Loss -8.4148 (-8.5416)	Arch Hard Loss 2.9613 (2.7524)	Arch Beta Loss 11.3762 (11.2941)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.8%, 64.9%)	
12/04 01:31:47PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.2550 (2.5991)	Arch Loss -8.3556 (-8.6305)	Arch Hard Loss 3.1848 (2.7463)	Arch Beta Loss 11.5404 (11.3767)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.7%, 65.0%)	
12/04 01:32:37PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.6173 (2.6003)	Arch Loss -8.9149 (-8.7169)	Arch Hard Loss 2.7874 (2.7416)	Arch Beta Loss 11.7023 (11.4585)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (31.9%, 64.8%)	
12/04 01:33:22PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.3482 (2.5978)	Arch Loss -9.0014 (-8.8011)	Arch Hard Loss 2.8448 (2.7305)	Arch Beta Loss 11.8462 (11.5316)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (32.1%, 64.9%)	
12/04 01:33:23PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  8/49] Final Prec@1 32.1240%
12/04 01:33:30PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.6145	Prec@(1,5) (32.9%, 65.5%)
12/04 01:33:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.6226	Prec@(1,5) (32.6%, 64.9%)
12/04 01:33:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.6372	Prec@(1,5) (32.5%, 64.4%)
12/04 01:33:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.6357	Prec@(1,5) (32.3%, 64.4%)
12/04 01:33:49PM searchStage_trainer.py:323 [INFO] Valid: [  8/49] Final Prec@1 32.3120%
12/04 01:33:49PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 3)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:33:50PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 32.3120%
12/04 01:34:40PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.4624 (2.5116)	Arch Loss -8.9937 (-9.2447)	Arch Hard Loss 3.0094 (2.6817)	Arch Beta Loss 12.0031 (11.9264)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (34.3%, 66.9%)	
12/04 01:35:30PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.5109 (2.5231)	Arch Loss -9.4089 (-9.3384)	Arch Hard Loss 2.7457 (2.6646)	Arch Beta Loss 12.1546 (12.0030)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.6%, 66.7%)	
12/04 01:36:20PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.4235 (2.5113)	Arch Loss -9.9105 (-9.4249)	Arch Hard Loss 2.3942 (2.6538)	Arch Beta Loss 12.3047 (12.0788)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.9%, 66.8%)	
12/04 01:37:05PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.3039 (2.5105)	Arch Loss -9.8588 (-9.4959)	Arch Hard Loss 2.5778 (2.6504)	Arch Beta Loss 12.4366 (12.1463)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (33.8%, 66.9%)	
12/04 01:37:05PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [  9/49] Final Prec@1 33.8240%
12/04 01:37:12PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.6562	Prec@(1,5) (31.9%, 64.6%)
12/04 01:37:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.6365	Prec@(1,5) (32.2%, 64.7%)
12/04 01:37:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.6464	Prec@(1,5) (32.1%, 64.5%)
12/04 01:37:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.6450	Prec@(1,5) (32.1%, 64.5%)
12/04 01:37:32PM searchStage_trainer.py:323 [INFO] Valid: [  9/49] Final Prec@1 32.1360%
12/04 01:37:32PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:37:32PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 32.3120%
12/04 01:38:22PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.5917 (2.4089)	Arch Loss -9.7485 (-9.8694)	Arch Hard Loss 2.8339 (2.6419)	Arch Beta Loss 12.5824 (12.5113)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.0%, 69.6%)	
12/04 01:39:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.6251 (2.4302)	Arch Loss -10.3548 (-9.9633)	Arch Hard Loss 2.3670 (2.6191)	Arch Beta Loss 12.7217 (12.5823)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.8%, 68.9%)	
12/04 01:40:01PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.5213 (2.4336)	Arch Loss -10.2429 (-10.0419)	Arch Hard Loss 2.6155 (2.6100)	Arch Beta Loss 12.8584 (12.6518)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.9%, 68.8%)	
12/04 01:40:45PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.4248 (2.4336)	Arch Loss -10.5001 (-10.1128)	Arch Hard Loss 2.4786 (2.6007)	Arch Beta Loss 12.9786 (12.7135)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (35.7%, 68.9%)	
12/04 01:40:46PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 10/49] Final Prec@1 35.7360%
12/04 01:40:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.6004	Prec@(1,5) (32.8%, 65.2%)
12/04 01:40:59PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.5705	Prec@(1,5) (33.5%, 65.7%)
12/04 01:41:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.5826	Prec@(1,5) (33.2%, 65.6%)
12/04 01:41:11PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.5737	Prec@(1,5) (33.4%, 65.7%)
12/04 01:41:11PM searchStage_trainer.py:323 [INFO] Valid: [ 10/49] Final Prec@1 33.4200%
12/04 01:41:11PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 3)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:41:12PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 33.4200%
12/04 01:42:03PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.0401 (2.3806)	Arch Loss -10.1685 (-10.4942)	Arch Hard Loss 2.9420 (2.5521)	Arch Beta Loss 13.1105 (13.0463)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.9%, 70.2%)	
12/04 01:42:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 2.8100 (2.3629)	Arch Loss -10.4275 (-10.5664)	Arch Hard Loss 2.8112 (2.5446)	Arch Beta Loss 13.2387 (13.1111)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (36.9%, 70.5%)	
12/04 01:43:42PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.3851 (2.3560)	Arch Loss -11.0531 (-10.6419)	Arch Hard Loss 2.3087 (2.5326)	Arch Beta Loss 13.3618 (13.1744)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.2%, 70.6%)	
12/04 01:44:27PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.2220 (2.3593)	Arch Loss -10.8327 (-10.6943)	Arch Hard Loss 2.6384 (2.5361)	Arch Beta Loss 13.4711 (13.2304)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (37.2%, 70.7%)	
12/04 01:44:27PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 11/49] Final Prec@1 37.1920%
12/04 01:44:34PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.5208	Prec@(1,5) (34.4%, 67.0%)
12/04 01:44:41PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.4987	Prec@(1,5) (34.4%, 67.9%)
12/04 01:44:47PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.4913	Prec@(1,5) (34.6%, 68.1%)
12/04 01:44:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.4936	Prec@(1,5) (34.7%, 68.0%)
12/04 01:44:53PM searchStage_trainer.py:323 [INFO] Valid: [ 11/49] Final Prec@1 34.7080%
12/04 01:44:54PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 3)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:44:54PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 34.7080%
12/04 01:45:45PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.0375 (2.2540)	Arch Loss -11.1459 (-11.0224)	Arch Hard Loss 2.4454 (2.5105)	Arch Beta Loss 13.5913 (13.5329)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.4%, 72.7%)	
12/04 01:46:34PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.3064 (2.2736)	Arch Loss -11.4934 (-11.0907)	Arch Hard Loss 2.2150 (2.5010)	Arch Beta Loss 13.7083 (13.5917)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.9%, 72.3%)	
12/04 01:47:24PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.2553 (2.2849)	Arch Loss -11.3490 (-11.1570)	Arch Hard Loss 2.4706 (2.4925)	Arch Beta Loss 13.8196 (13.6495)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (38.9%, 72.2%)	
12/04 01:48:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.9627 (2.2872)	Arch Loss -11.5777 (-11.2190)	Arch Hard Loss 2.3398 (2.4812)	Arch Beta Loss 13.9176 (13.7002)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.0%, 72.1%)	
12/04 01:48:09PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 12/49] Final Prec@1 38.9520%
12/04 01:48:16PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.4344	Prec@(1,5) (36.4%, 69.3%)
12/04 01:48:22PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.4395	Prec@(1,5) (36.1%, 69.2%)
12/04 01:48:29PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.4335	Prec@(1,5) (36.3%, 69.2%)
12/04 01:48:35PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.4439	Prec@(1,5) (36.1%, 68.9%)
12/04 01:48:35PM searchStage_trainer.py:323 [INFO] Valid: [ 12/49] Final Prec@1 36.0520%
12/04 01:48:35PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:48:36PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 36.0520%
12/04 01:49:27PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1108 (2.1789)	Arch Loss -11.5294 (-11.5227)	Arch Hard Loss 2.4968 (2.4506)	Arch Beta Loss 14.0262 (13.9733)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.8%, 74.1%)	
12/04 01:50:16PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 2.1051 (2.2207)	Arch Loss -11.7686 (-11.5788)	Arch Hard Loss 2.3619 (2.4474)	Arch Beta Loss 14.1305 (14.0262)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (39.9%, 73.2%)	
12/04 01:51:06PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 2.0266 (2.2231)	Arch Loss -11.7759 (-11.6277)	Arch Hard Loss 2.4581 (2.4508)	Arch Beta Loss 14.2340 (14.0784)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.1%, 73.3%)	
12/04 01:51:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 1.9652 (2.2260)	Arch Loss -12.0177 (-11.6821)	Arch Hard Loss 2.3061 (2.4428)	Arch Beta Loss 14.3238 (14.1249)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (40.1%, 73.2%)	
12/04 01:51:50PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 13/49] Final Prec@1 40.0840%
12/04 01:51:57PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.3975	Prec@(1,5) (37.9%, 70.0%)
12/04 01:52:04PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.4107	Prec@(1,5) (37.0%, 69.9%)
12/04 01:52:10PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.3920	Prec@(1,5) (37.5%, 70.0%)
12/04 01:52:16PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.3977	Prec@(1,5) (37.5%, 69.9%)
12/04 01:52:16PM searchStage_trainer.py:323 [INFO] Valid: [ 13/49] Final Prec@1 37.5160%
12/04 01:52:16PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 01:52:17PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 37.5160%
12/04 01:53:07PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.2387 (2.1230)	Arch Loss -11.7616 (-11.9350)	Arch Hard Loss 2.6598 (2.4386)	Arch Beta Loss 14.4214 (14.3736)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.1%, 75.1%)	
12/04 01:53:56PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.1898 (2.1616)	Arch Loss -12.0416 (-11.9855)	Arch Hard Loss 2.4756 (2.4363)	Arch Beta Loss 14.5171 (14.4218)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.4%, 74.1%)	
12/04 01:54:45PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 2.1061 (2.1684)	Arch Loss -12.0832 (-12.0391)	Arch Hard Loss 2.5271 (2.4302)	Arch Beta Loss 14.6103 (14.4693)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.4%, 74.1%)	
12/04 01:55:28PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 2.0702 (2.1686)	Arch Loss -12.3242 (-12.0852)	Arch Hard Loss 2.3673 (2.4261)	Arch Beta Loss 14.6915 (14.5113)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (41.3%, 74.2%)	
12/04 01:55:29PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 14/49] Final Prec@1 41.2920%
12/04 01:55:36PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.3235	Prec@(1,5) (38.8%, 70.9%)
12/04 01:55:42PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.3464	Prec@(1,5) (38.6%, 70.7%)
12/04 01:55:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.3534	Prec@(1,5) (38.5%, 70.6%)
12/04 01:55:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.3542	Prec@(1,5) (38.4%, 70.6%)
12/04 01:55:55PM searchStage_trainer.py:323 [INFO] Valid: [ 14/49] Final Prec@1 38.4000%
12/04 01:55:55PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:55:55PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 38.4000%
12/04 01:56:46PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 2.3797 (2.0640)	Arch Loss -12.3379 (-12.3409)	Arch Hard Loss 2.4429 (2.3964)	Arch Beta Loss 14.7808 (14.7373)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.7%, 75.9%)	
12/04 01:57:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 2.0249 (2.0780)	Arch Loss -12.5050 (-12.3906)	Arch Hard Loss 2.3616 (2.3902)	Arch Beta Loss 14.8666 (14.7808)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.4%, 75.9%)	
12/04 01:58:26PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 2.3599 (2.0840)	Arch Loss -12.5468 (-12.4286)	Arch Hard Loss 2.4029 (2.3947)	Arch Beta Loss 14.9497 (14.8233)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.2%, 76.0%)	
12/04 01:59:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.7484 (2.0949)	Arch Loss -13.0182 (-12.4732)	Arch Hard Loss 2.0039 (2.3879)	Arch Beta Loss 15.0221 (14.8610)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (42.9%, 75.7%)	
12/04 01:59:12PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 15/49] Final Prec@1 42.8240%
12/04 01:59:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.3564	Prec@(1,5) (37.9%, 71.0%)
12/04 01:59:25PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.3278	Prec@(1,5) (39.0%, 71.1%)
12/04 01:59:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.3424	Prec@(1,5) (38.6%, 70.8%)
12/04 01:59:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.3354	Prec@(1,5) (38.7%, 71.1%)
12/04 01:59:38PM searchStage_trainer.py:323 [INFO] Valid: [ 15/49] Final Prec@1 38.7080%
12/04 01:59:38PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 01:59:38PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 38.7080%
12/04 02:00:29PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.7037 (2.0231)	Arch Loss -13.2542 (-12.7071)	Arch Hard Loss 1.8471 (2.3557)	Arch Beta Loss 15.1013 (15.0627)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.9%, 77.5%)	
12/04 02:01:18PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 2.2744 (2.0329)	Arch Loss -12.8599 (-12.7501)	Arch Hard Loss 2.3193 (2.3517)	Arch Beta Loss 15.1792 (15.1018)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.1%, 77.6%)	
12/04 02:02:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.9394 (2.0485)	Arch Loss -12.5481 (-12.7724)	Arch Hard Loss 2.7053 (2.3677)	Arch Beta Loss 15.2534 (15.1402)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.4%, 77.2%)	
12/04 02:02:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9855 (2.0533)	Arch Loss -12.9450 (-12.8218)	Arch Hard Loss 2.3750 (2.3523)	Arch Beta Loss 15.3200 (15.1740)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (43.4%, 77.1%)	
12/04 02:02:53PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 16/49] Final Prec@1 43.4080%
12/04 02:03:00PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.3146	Prec@(1,5) (39.7%, 71.3%)
12/04 02:03:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.2833	Prec@(1,5) (40.2%, 72.0%)
12/04 02:03:13PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.2839	Prec@(1,5) (40.0%, 72.3%)
12/04 02:03:18PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.2842	Prec@(1,5) (39.9%, 72.3%)
12/04 02:03:19PM searchStage_trainer.py:323 [INFO] Valid: [ 16/49] Final Prec@1 39.8800%
12/04 02:03:19PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:03:19PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 39.8800%
12/04 02:04:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.8426 (1.9415)	Arch Loss -12.9281 (-13.0073)	Arch Hard Loss 2.4652 (2.3498)	Arch Beta Loss 15.3934 (15.3571)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.7%, 78.6%)	
12/04 02:04:59PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.9066 (1.9679)	Arch Loss -13.1331 (-13.0633)	Arch Hard Loss 2.3314 (2.3300)	Arch Beta Loss 15.4645 (15.3933)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.7%, 78.3%)	
12/04 02:05:48PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 2.1359 (1.9879)	Arch Loss -13.5290 (-13.0920)	Arch Hard Loss 2.0042 (2.3366)	Arch Beta Loss 15.5332 (15.4286)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (45.1%, 78.0%)	
12/04 02:06:33PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.7029 (2.0021)	Arch Loss -13.4768 (-13.1295)	Arch Hard Loss 2.1170 (2.3303)	Arch Beta Loss 15.5937 (15.4598)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (44.8%, 77.7%)	
12/04 02:06:33PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 17/49] Final Prec@1 44.8120%
12/04 02:06:40PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.2928	Prec@(1,5) (40.4%, 72.5%)
12/04 02:06:47PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.2796	Prec@(1,5) (40.4%, 73.0%)
12/04 02:06:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.2869	Prec@(1,5) (40.2%, 72.5%)
12/04 02:06:59PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.2882	Prec@(1,5) (40.1%, 72.3%)
12/04 02:06:59PM searchStage_trainer.py:323 [INFO] Valid: [ 17/49] Final Prec@1 40.1280%
12/04 02:06:59PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 02:07:00PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.1280%
12/04 02:07:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.8258 (1.8988)	Arch Loss -13.3177 (-13.3545)	Arch Hard Loss 2.3426 (2.2733)	Arch Beta Loss 15.6603 (15.6279)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.6%, 79.3%)	
12/04 02:08:39PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.7028 (1.9390)	Arch Loss -13.5452 (-13.3632)	Arch Hard Loss 2.1774 (2.2965)	Arch Beta Loss 15.7226 (15.6597)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.1%, 78.9%)	
12/04 02:09:28PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 2.0937 (1.9397)	Arch Loss -13.4834 (-13.3935)	Arch Hard Loss 2.3022 (2.2978)	Arch Beta Loss 15.7856 (15.6913)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.0%, 78.9%)	
12/04 02:10:12PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 2.1505 (1.9507)	Arch Loss -14.1178 (-13.4354)	Arch Hard Loss 1.7229 (2.2840)	Arch Beta Loss 15.8407 (15.7195)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (46.6%, 78.7%)	
12/04 02:10:13PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 18/49] Final Prec@1 46.5880%
12/04 02:10:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.3199	Prec@(1,5) (39.7%, 72.2%)
12/04 02:10:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.3052	Prec@(1,5) (39.9%, 72.3%)
12/04 02:10:33PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.3054	Prec@(1,5) (40.0%, 72.0%)
12/04 02:10:38PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.3038	Prec@(1,5) (39.9%, 72.1%)
12/04 02:10:39PM searchStage_trainer.py:323 [INFO] Valid: [ 18/49] Final Prec@1 39.8960%
12/04 02:10:39PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 02:10:39PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.1280%
12/04 02:11:28PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.8063 (1.8441)	Arch Loss -13.3750 (-13.5823)	Arch Hard Loss 2.5259 (2.2893)	Arch Beta Loss 15.9008 (15.8716)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.8%, 80.8%)	
12/04 02:12:16PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 2.1411 (1.8880)	Arch Loss -13.3230 (-13.6136)	Arch Hard Loss 2.6355 (2.2873)	Arch Beta Loss 15.9585 (15.9009)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.9%, 80.1%)	
12/04 02:13:03PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 2.1037 (1.8946)	Arch Loss -14.1754 (-13.6607)	Arch Hard Loss 1.8390 (2.2689)	Arch Beta Loss 16.0144 (15.9296)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.6%, 79.8%)	
12/04 02:13:47PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.9336 (1.9053)	Arch Loss -13.9582 (-13.6899)	Arch Hard Loss 2.1066 (2.2651)	Arch Beta Loss 16.0648 (15.9550)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (47.3%, 79.6%)	
12/04 02:13:47PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 19/49] Final Prec@1 47.3560%
12/04 02:13:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.3001	Prec@(1,5) (40.7%, 72.4%)
12/04 02:14:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.3143	Prec@(1,5) (40.6%, 71.9%)
12/04 02:14:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.2935	Prec@(1,5) (40.6%, 72.3%)
12/04 02:14:14PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.2856	Prec@(1,5) (40.6%, 72.5%)
12/04 02:14:14PM searchStage_trainer.py:323 [INFO] Valid: [ 19/49] Final Prec@1 40.6280%
12/04 02:14:14PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:14:14PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 40.6280%
12/04 02:15:05PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 2.0274 (1.8057)	Arch Loss -13.6938 (-13.8348)	Arch Hard Loss 2.4252 (2.2575)	Arch Beta Loss 16.1190 (16.0923)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.4%, 81.4%)	
12/04 02:15:55PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.9385 (1.8267)	Arch Loss -13.4229 (-13.8638)	Arch Hard Loss 2.7495 (2.2554)	Arch Beta Loss 16.1724 (16.1192)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.1%, 80.8%)	
12/04 02:16:45PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3893 (1.8406)	Arch Loss -14.1279 (-13.8945)	Arch Hard Loss 2.0950 (2.2510)	Arch Beta Loss 16.2229 (16.1455)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.6%, 80.8%)	
12/04 02:17:29PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.7113 (1.8585)	Arch Loss -14.1853 (-13.9217)	Arch Hard Loss 2.0816 (2.2467)	Arch Beta Loss 16.2669 (16.1684)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (48.4%, 80.4%)	
12/04 02:17:30PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 20/49] Final Prec@1 48.4320%
12/04 02:17:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.2446	Prec@(1,5) (41.9%, 73.8%)
12/04 02:17:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 2.2315	Prec@(1,5) (42.0%, 74.0%)
12/04 02:17:50PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 2.2269	Prec@(1,5) (42.0%, 73.8%)
12/04 02:17:56PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 2.2322	Prec@(1,5) (42.0%, 73.8%)
12/04 02:17:56PM searchStage_trainer.py:323 [INFO] Valid: [ 20/49] Final Prec@1 42.0240%
12/04 02:17:56PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:17:56PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.0240%
12/04 02:18:47PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.7914 (1.7403)	Arch Loss -13.7442 (-14.0378)	Arch Hard Loss 2.5717 (2.2545)	Arch Beta Loss 16.3159 (16.2923)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (50.3%, 83.2%)	
12/04 02:19:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.6980 (1.7726)	Arch Loss -13.9951 (-14.0719)	Arch Hard Loss 2.3682 (2.2442)	Arch Beta Loss 16.3632 (16.3161)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.7%, 82.2%)	
12/04 02:20:26PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.9639 (1.7994)	Arch Loss -14.4172 (-14.1178)	Arch Hard Loss 1.9936 (2.2221)	Arch Beta Loss 16.4108 (16.3399)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.4%, 81.8%)	
12/04 02:21:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.7093 (1.8095)	Arch Loss -14.4065 (-14.1432)	Arch Hard Loss 2.0462 (2.2179)	Arch Beta Loss 16.4528 (16.3611)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (49.3%, 81.6%)	
12/04 02:21:11PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 21/49] Final Prec@1 49.2720%
12/04 02:21:18PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 2.1944	Prec@(1,5) (42.0%, 73.9%)
12/04 02:21:24PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 2.1988	Prec@(1,5) (42.0%, 73.8%)
12/04 02:21:31PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 2.1969	Prec@(1,5) (41.8%, 74.0%)
12/04 02:21:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 2.1898	Prec@(1,5) (42.0%, 74.1%)
12/04 02:21:37PM searchStage_trainer.py:323 [INFO] Valid: [ 21/49] Final Prec@1 41.9960%
12/04 02:21:37PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('skip_connect', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 02:21:37PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.0240%
12/04 02:22:28PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.8929 (1.6890)	Arch Loss -14.1456 (-14.2333)	Arch Hard Loss 2.3522 (2.2424)	Arch Beta Loss 16.4978 (16.4757)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (52.9%, 83.5%)	
12/04 02:23:17PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.8491 (1.7263)	Arch Loss -14.2286 (-14.2901)	Arch Hard Loss 2.3141 (2.2080)	Arch Beta Loss 16.5427 (16.4981)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (51.6%, 82.8%)	
12/04 02:24:07PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.5669 (1.7426)	Arch Loss -14.2494 (-14.3154)	Arch Hard Loss 2.3364 (2.2048)	Arch Beta Loss 16.5859 (16.5202)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (51.4%, 82.4%)	
12/04 02:24:51PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.9406 (1.7562)	Arch Loss -14.4585 (-14.3360)	Arch Hard Loss 2.1655 (2.2039)	Arch Beta Loss 16.6241 (16.5399)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (51.1%, 82.3%)	
12/04 02:24:52PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 22/49] Final Prec@1 51.1000%
12/04 02:24:59PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.2015	Prec@(1,5) (41.8%, 74.0%)
12/04 02:25:05PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.2159	Prec@(1,5) (41.5%, 73.5%)
12/04 02:25:12PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.2132	Prec@(1,5) (41.9%, 73.7%)
12/04 02:25:18PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 2.2219	Prec@(1,5) (41.7%, 73.5%)
12/04 02:25:18PM searchStage_trainer.py:323 [INFO] Valid: [ 22/49] Final Prec@1 41.7120%
12/04 02:25:18PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('max_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG3_concat=[10, 11])
12/04 02:25:18PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 42.0240%
12/04 02:26:09PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.6993 (1.6232)	Arch Loss -14.3907 (-14.4563)	Arch Hard Loss 2.2760 (2.1892)	Arch Beta Loss 16.6667 (16.6456)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.4%, 84.9%)	
12/04 02:26:59PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.5462 (1.6781)	Arch Loss -14.2254 (-14.4785)	Arch Hard Loss 2.4818 (2.1877)	Arch Beta Loss 16.7072 (16.6662)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (53.1%, 83.7%)	
12/04 02:27:48PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.7939 (1.6859)	Arch Loss -14.8233 (-14.4997)	Arch Hard Loss 1.9243 (2.1869)	Arch Beta Loss 16.7476 (16.6866)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (52.6%, 83.7%)	
12/04 02:28:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.8454 (1.6964)	Arch Loss -14.2459 (-14.5204)	Arch Hard Loss 2.5370 (2.1844)	Arch Beta Loss 16.7829 (16.7048)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (52.3%, 83.4%)	
12/04 02:28:33PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 23/49] Final Prec@1 52.3040%
12/04 02:28:40PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 2.1333	Prec@(1,5) (43.6%, 75.1%)
12/04 02:28:47PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 2.1535	Prec@(1,5) (43.2%, 75.0%)
12/04 02:28:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 2.1591	Prec@(1,5) (43.4%, 74.9%)
12/04 02:28:59PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 2.1616	Prec@(1,5) (43.3%, 74.9%)
12/04 02:28:59PM searchStage_trainer.py:323 [INFO] Valid: [ 23/49] Final Prec@1 43.3600%
12/04 02:28:59PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:29:00PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 43.3600%
12/04 02:29:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.3977 (1.5902)	Arch Loss -14.5065 (-14.6444)	Arch Hard Loss 2.3159 (2.1587)	Arch Beta Loss 16.8224 (16.8032)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.4%, 85.5%)	
12/04 02:30:40PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.8840 (1.6277)	Arch Loss -14.3053 (-14.6456)	Arch Hard Loss 2.5550 (2.1768)	Arch Beta Loss 16.8604 (16.8224)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (53.4%, 84.6%)	
12/04 02:31:29PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.9570 (1.6344)	Arch Loss -14.8059 (-14.6660)	Arch Hard Loss 2.0919 (2.1754)	Arch Beta Loss 16.8978 (16.8414)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (53.5%, 84.3%)	
12/04 02:32:13PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.6226 (1.6484)	Arch Loss -15.0304 (-14.6846)	Arch Hard Loss 1.9011 (2.1737)	Arch Beta Loss 16.9315 (16.8583)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (53.3%, 84.1%)	
12/04 02:32:14PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 24/49] Final Prec@1 53.3040%
12/04 02:32:21PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 2.1560	Prec@(1,5) (44.4%, 75.2%)
12/04 02:32:27PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 2.1348	Prec@(1,5) (44.7%, 75.8%)
12/04 02:32:34PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 2.1388	Prec@(1,5) (44.5%, 75.6%)
12/04 02:32:40PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 2.1259	Prec@(1,5) (44.6%, 75.9%)
12/04 02:32:40PM searchStage_trainer.py:323 [INFO] Valid: [ 24/49] Final Prec@1 44.6240%
12/04 02:32:40PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:32:40PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.6240%
12/04 02:33:31PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.6213 (1.5630)	Arch Loss -14.6930 (-14.8074)	Arch Hard Loss 2.2760 (2.1434)	Arch Beta Loss 16.9690 (16.9508)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (55.7%, 86.3%)	
12/04 02:34:20PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.7780 (1.5863)	Arch Loss -14.6688 (-14.8168)	Arch Hard Loss 2.3356 (2.1521)	Arch Beta Loss 17.0044 (16.9689)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.8%, 85.6%)	
12/04 02:35:09PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.3597 (1.5907)	Arch Loss -15.3754 (-14.8380)	Arch Hard Loss 1.6629 (2.1485)	Arch Beta Loss 17.0384 (16.9865)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.7%, 85.4%)	
12/04 02:35:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.5460 (1.5987)	Arch Loss -15.1724 (-14.8577)	Arch Hard Loss 1.8972 (2.1445)	Arch Beta Loss 17.0696 (17.0022)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (54.6%, 85.1%)	
12/04 02:35:54PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 25/49] Final Prec@1 54.6080%
12/04 02:36:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 2.2106	Prec@(1,5) (43.7%, 74.7%)
12/04 02:36:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 2.2053	Prec@(1,5) (43.5%, 74.8%)
12/04 02:36:14PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 2.1817	Prec@(1,5) (43.8%, 75.1%)
12/04 02:36:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 2.1848	Prec@(1,5) (43.6%, 75.1%)
12/04 02:36:20PM searchStage_trainer.py:323 [INFO] Valid: [ 25/49] Final Prec@1 43.6320%
12/04 02:36:20PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:36:20PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.6240%
12/04 02:37:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.8603 (1.5113)	Arch Loss -15.3008 (-15.0127)	Arch Hard Loss 1.8023 (2.0739)	Arch Beta Loss 17.1031 (17.0866)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (57.2%, 86.5%)	
12/04 02:38:01PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.2235 (1.5416)	Arch Loss -15.1501 (-14.9940)	Arch Hard Loss 1.9846 (2.1089)	Arch Beta Loss 17.1347 (17.1029)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (56.1%, 86.0%)	
12/04 02:38:49PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.4607 (1.5382)	Arch Loss -15.0610 (-14.9989)	Arch Hard Loss 2.1056 (2.1200)	Arch Beta Loss 17.1666 (17.1188)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (56.2%, 86.1%)	
12/04 02:39:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.3585 (1.5531)	Arch Loss -14.8711 (-15.0025)	Arch Hard Loss 2.3234 (2.1306)	Arch Beta Loss 17.1946 (17.1331)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (55.6%, 85.9%)	
12/04 02:39:33PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 26/49] Final Prec@1 55.5760%
12/04 02:39:40PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 2.1172	Prec@(1,5) (45.2%, 76.0%)
12/04 02:39:46PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 2.1091	Prec@(1,5) (45.4%, 76.0%)
12/04 02:39:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 2.1188	Prec@(1,5) (45.0%, 75.8%)
12/04 02:39:58PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 2.1267	Prec@(1,5) (44.8%, 75.9%)
12/04 02:39:58PM searchStage_trainer.py:323 [INFO] Valid: [ 26/49] Final Prec@1 44.7840%
12/04 02:39:59PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:39:59PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.7840%
12/04 02:40:49PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.4264 (1.4127)	Arch Loss -14.6292 (-15.0575)	Arch Hard Loss 2.5963 (2.1529)	Arch Beta Loss 17.2255 (17.2105)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (59.7%, 87.6%)	
12/04 02:41:37PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.5720 (1.4632)	Arch Loss -15.4449 (-15.1083)	Arch Hard Loss 1.8107 (2.1173)	Arch Beta Loss 17.2556 (17.2256)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (58.3%, 87.0%)	
12/04 02:42:24PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.4464 (1.4807)	Arch Loss -15.0356 (-15.1187)	Arch Hard Loss 2.2473 (2.1214)	Arch Beta Loss 17.2829 (17.2402)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (57.7%, 86.7%)	
12/04 02:43:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.6320 (1.5018)	Arch Loss -15.3396 (-15.1392)	Arch Hard Loss 1.9690 (2.1139)	Arch Beta Loss 17.3087 (17.2531)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (57.0%, 86.5%)	
12/04 02:43:08PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 27/49] Final Prec@1 57.0320%
12/04 02:43:16PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 2.1426	Prec@(1,5) (45.0%, 74.9%)
12/04 02:43:22PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 2.1169	Prec@(1,5) (45.0%, 75.6%)
12/04 02:43:28PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 2.1280	Prec@(1,5) (45.0%, 75.8%)
12/04 02:43:34PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 2.1192	Prec@(1,5) (45.0%, 75.9%)
12/04 02:43:34PM searchStage_trainer.py:323 [INFO] Valid: [ 27/49] Final Prec@1 44.9640%
12/04 02:43:34PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('max_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:43:35PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 44.9640%
12/04 02:44:26PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.5995 (1.3722)	Arch Loss -15.3622 (-15.2911)	Arch Hard Loss 1.9746 (2.0320)	Arch Beta Loss 17.3369 (17.3231)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (60.3%, 88.6%)	
12/04 02:45:16PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 1.1767 (1.4246)	Arch Loss -15.8379 (-15.2690)	Arch Hard Loss 1.5252 (2.0676)	Arch Beta Loss 17.3631 (17.3366)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (58.7%, 88.0%)	
12/04 02:46:05PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.5034 (1.4280)	Arch Loss -15.2954 (-15.2624)	Arch Hard Loss 2.0941 (2.0875)	Arch Beta Loss 17.3895 (17.3499)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (58.8%, 87.9%)	
12/04 02:46:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.3280 (1.4522)	Arch Loss -15.4523 (-15.2644)	Arch Hard Loss 1.9611 (2.0975)	Arch Beta Loss 17.4134 (17.3619)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (58.2%, 87.4%)	
12/04 02:46:50PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 28/49] Final Prec@1 58.1840%
12/04 02:46:58PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 2.0747	Prec@(1,5) (45.5%, 77.1%)
12/04 02:47:04PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 2.1157	Prec@(1,5) (44.5%, 76.4%)
12/04 02:47:11PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 2.0907	Prec@(1,5) (45.3%, 76.8%)
12/04 02:47:17PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 2.0940	Prec@(1,5) (45.3%, 76.8%)
12/04 02:47:17PM searchStage_trainer.py:323 [INFO] Valid: [ 28/49] Final Prec@1 45.2520%
12/04 02:47:17PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:47:17PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 45.2520%
12/04 02:48:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.5816 (1.3314)	Arch Loss -15.4927 (-15.3280)	Arch Hard Loss 1.9460 (2.0980)	Arch Beta Loss 17.4387 (17.4260)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (60.8%, 89.2%)	
12/04 02:48:57PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 1.1383 (1.3481)	Arch Loss -15.3444 (-15.3352)	Arch Hard Loss 2.1185 (2.1032)	Arch Beta Loss 17.4629 (17.4385)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (60.9%, 88.8%)	
12/04 02:49:46PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.4468 (1.3802)	Arch Loss -15.2957 (-15.3518)	Arch Hard Loss 2.1913 (2.0989)	Arch Beta Loss 17.4871 (17.4507)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (60.0%, 88.4%)	
12/04 02:50:30PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.3100 (1.3905)	Arch Loss -15.5170 (-15.3679)	Arch Hard Loss 1.9918 (2.0937)	Arch Beta Loss 17.5088 (17.4616)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (59.7%, 88.3%)	
12/04 02:50:31PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 29/49] Final Prec@1 59.6520%
12/04 02:50:38PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 2.0824	Prec@(1,5) (47.3%, 76.8%)
12/04 02:50:45PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 2.0702	Prec@(1,5) (47.1%, 77.3%)
12/04 02:50:51PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 2.0760	Prec@(1,5) (46.8%, 77.1%)
12/04 02:50:57PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 2.0663	Prec@(1,5) (46.9%, 77.4%)
12/04 02:50:57PM searchStage_trainer.py:323 [INFO] Valid: [ 29/49] Final Prec@1 46.8480%
12/04 02:50:57PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)], [('skip_connect', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:50:58PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8480%
12/04 02:51:48PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 1.1865 (1.2963)	Arch Loss -15.6975 (-15.4645)	Arch Hard Loss 1.8343 (2.0562)	Arch Beta Loss 17.5318 (17.5206)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (62.2%, 89.9%)	
12/04 02:52:37PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 1.5126 (1.2974)	Arch Loss -15.4887 (-15.4470)	Arch Hard Loss 2.0642 (2.0846)	Arch Beta Loss 17.5529 (17.5316)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (62.3%, 89.8%)	
12/04 02:53:26PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.4027 (1.3172)	Arch Loss -15.5558 (-15.4588)	Arch Hard Loss 2.0197 (2.0837)	Arch Beta Loss 17.5755 (17.5425)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.7%, 89.6%)	
12/04 02:54:11PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 1.6146 (1.3428)	Arch Loss -15.2766 (-15.4633)	Arch Hard Loss 2.3180 (2.0890)	Arch Beta Loss 17.5947 (17.5523)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (61.0%, 89.2%)	
12/04 02:54:11PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 30/49] Final Prec@1 61.0360%
12/04 02:54:18PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 2.1935	Prec@(1,5) (44.6%, 75.3%)
12/04 02:54:25PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 2.1839	Prec@(1,5) (44.6%, 75.7%)
12/04 02:54:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 2.1875	Prec@(1,5) (44.5%, 75.6%)
12/04 02:54:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 2.1744	Prec@(1,5) (44.9%, 75.7%)
12/04 02:54:38PM searchStage_trainer.py:323 [INFO] Valid: [ 30/49] Final Prec@1 44.8800%
12/04 02:54:38PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:54:38PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8480%
12/04 02:55:28PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 1.1481 (1.2361)	Arch Loss -15.3543 (-15.5198)	Arch Hard Loss 2.2609 (2.0852)	Arch Beta Loss 17.6152 (17.6049)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (63.8%, 90.5%)	
12/04 02:56:18PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 1.3425 (1.2510)	Arch Loss -15.8186 (-15.5133)	Arch Hard Loss 1.8181 (2.1025)	Arch Beta Loss 17.6367 (17.6158)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (63.3%, 90.4%)	
12/04 02:57:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.4971 (1.2783)	Arch Loss -15.7515 (-15.5311)	Arch Hard Loss 1.9061 (2.0952)	Arch Beta Loss 17.6576 (17.6264)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (62.5%, 90.0%)	
12/04 02:57:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 1.6611 (1.2944)	Arch Loss -15.2698 (-15.5480)	Arch Hard Loss 2.4059 (2.0876)	Arch Beta Loss 17.6757 (17.6356)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (62.0%, 89.8%)	
12/04 02:57:53PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 31/49] Final Prec@1 62.0080%
12/04 02:58:00PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 2.0589	Prec@(1,5) (46.0%, 78.3%)
12/04 02:58:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 2.0520	Prec@(1,5) (46.8%, 78.2%)
12/04 02:58:13PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 2.0730	Prec@(1,5) (46.6%, 77.7%)
12/04 02:58:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 2.0870	Prec@(1,5) (46.4%, 77.5%)
12/04 02:58:19PM searchStage_trainer.py:323 [INFO] Valid: [ 31/49] Final Prec@1 46.3640%
12/04 02:58:19PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 02:58:19PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 46.8480%
12/04 02:59:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 1.4457 (1.1997)	Arch Loss -15.6193 (-15.6497)	Arch Hard Loss 2.0773 (2.0367)	Arch Beta Loss 17.6966 (17.6864)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.0%, 91.5%)	
12/04 03:00:00PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 1.2519 (1.2168)	Arch Loss -15.2915 (-15.6318)	Arch Hard Loss 2.4245 (2.0646)	Arch Beta Loss 17.7160 (17.6964)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (64.1%, 91.0%)	
12/04 03:00:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 1.1418 (1.2237)	Arch Loss -15.9441 (-15.6244)	Arch Hard Loss 1.7904 (2.0818)	Arch Beta Loss 17.7346 (17.7062)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (64.1%, 90.8%)	
12/04 03:01:35PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.0501 (1.2345)	Arch Loss -15.8156 (-15.6377)	Arch Hard Loss 1.9359 (2.0771)	Arch Beta Loss 17.7516 (17.7147)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (63.7%, 90.7%)	
12/04 03:01:35PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 32/49] Final Prec@1 63.7360%
12/04 03:01:42PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 2.0991	Prec@(1,5) (46.7%, 77.8%)
12/04 03:01:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 2.0908	Prec@(1,5) (47.3%, 77.6%)
12/04 03:01:56PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 2.0823	Prec@(1,5) (47.4%, 77.7%)
12/04 03:02:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 2.0666	Prec@(1,5) (47.7%, 77.9%)
12/04 03:02:02PM searchStage_trainer.py:323 [INFO] Valid: [ 32/49] Final Prec@1 47.7240%
12/04 03:02:02PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:02:02PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.7240%
12/04 03:02:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 1.2489 (1.1086)	Arch Loss -15.6547 (-15.6950)	Arch Hard Loss 2.1146 (2.0654)	Arch Beta Loss 17.7693 (17.7604)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (66.9%, 92.3%)	
12/04 03:03:41PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 1.0660 (1.1527)	Arch Loss -15.8337 (-15.7216)	Arch Hard Loss 1.9542 (2.0479)	Arch Beta Loss 17.7879 (17.7695)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.6%, 91.5%)	
12/04 03:04:30PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 1.3388 (1.1737)	Arch Loss -15.3188 (-15.7119)	Arch Hard Loss 2.4864 (2.0666)	Arch Beta Loss 17.8052 (17.7785)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (65.1%, 91.4%)	
12/04 03:05:14PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 1.2181 (1.1867)	Arch Loss -15.9782 (-15.7217)	Arch Hard Loss 1.8423 (2.0647)	Arch Beta Loss 17.8205 (17.7865)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (64.9%, 91.2%)	
12/04 03:05:15PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 33/49] Final Prec@1 64.8840%
12/04 03:05:22PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 2.1118	Prec@(1,5) (46.6%, 77.9%)
12/04 03:05:28PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 2.0781	Prec@(1,5) (47.4%, 77.9%)
12/04 03:05:35PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 2.0898	Prec@(1,5) (47.1%, 77.9%)
12/04 03:05:41PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 2.0959	Prec@(1,5) (47.1%, 77.7%)
12/04 03:05:41PM searchStage_trainer.py:323 [INFO] Valid: [ 33/49] Final Prec@1 47.1200%
12/04 03:05:41PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:05:42PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.7240%
12/04 03:06:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.8802 (1.0765)	Arch Loss -15.8608 (-15.8070)	Arch Hard Loss 1.9773 (2.0228)	Arch Beta Loss 17.8381 (17.8297)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (67.6%, 93.0%)	
12/04 03:07:21PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.8083 (1.1017)	Arch Loss -15.3874 (-15.7712)	Arch Hard Loss 2.4665 (2.0669)	Arch Beta Loss 17.8539 (17.8380)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (67.1%, 92.5%)	
12/04 03:08:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 1.1960 (1.1185)	Arch Loss -15.6751 (-15.7706)	Arch Hard Loss 2.1930 (2.0751)	Arch Beta Loss 17.8681 (17.8457)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (66.7%, 92.3%)	
12/04 03:08:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 1.3503 (1.1338)	Arch Loss -15.9268 (-15.7765)	Arch Hard Loss 1.9549 (2.0759)	Arch Beta Loss 17.8817 (17.8524)	Arch depth Loss 0.0000 (0.0000)	Prec@(1,5) (66.3%, 92.1%)	
12/04 03:08:55PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 34/49] Final Prec@1 66.2600%
12/04 03:09:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 2.2036	Prec@(1,5) (46.0%, 76.4%)
12/04 03:09:09PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 2.2028	Prec@(1,5) (45.7%, 76.1%)
12/04 03:09:15PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 2.1766	Prec@(1,5) (46.1%, 76.4%)
12/04 03:09:21PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 2.1749	Prec@(1,5) (46.1%, 76.4%)
12/04 03:09:21PM searchStage_trainer.py:323 [INFO] Valid: [ 34/49] Final Prec@1 46.0920%
12/04 03:09:21PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:09:22PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 47.7240%
12/04 03:10:12PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 1.1820 (1.0329)	Arch Loss -15.8707 (-15.8158)	Arch Hard Loss 2.0255 (2.0730)	Arch Beta Loss 17.8962 (17.8888)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.0%, 93.1%)	
12/04 03:11:01PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 1.5585 (1.0574)	Arch Loss -15.9642 (-15.8279)	Arch Hard Loss 1.9463 (2.0684)	Arch Beta Loss 17.9104 (17.8963)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.7%, 92.6%)	
12/04 03:11:51PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 1.2049 (1.0718)	Arch Loss -15.5931 (-15.8432)	Arch Hard Loss 2.3323 (2.0603)	Arch Beta Loss 17.9255 (17.9035)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (68.1%, 92.5%)	
12/04 03:12:35PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 1.1954 (1.0764)	Arch Loss -16.0727 (-15.8380)	Arch Hard Loss 1.8644 (2.0719)	Arch Beta Loss 17.9371 (17.9100)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (67.8%, 92.6%)	
12/04 03:12:36PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 35/49] Final Prec@1 67.8280%
12/04 03:12:42PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 2.0786	Prec@(1,5) (48.2%, 77.9%)
12/04 03:12:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 2.0772	Prec@(1,5) (48.1%, 78.3%)
12/04 03:12:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 2.0731	Prec@(1,5) (48.3%, 78.4%)
12/04 03:13:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 2.0725	Prec@(1,5) (48.2%, 78.6%)
12/04 03:13:01PM searchStage_trainer.py:323 [INFO] Valid: [ 35/49] Final Prec@1 48.2560%
12/04 03:13:01PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:13:02PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:13:52PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 1.0815 (0.9590)	Arch Loss -15.6368 (-15.8793)	Arch Hard Loss 2.3133 (2.0645)	Arch Beta Loss 17.9501 (17.9438)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.7%, 94.1%)	
12/04 03:14:42PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.9555 (0.9962)	Arch Loss -16.3577 (-15.8908)	Arch Hard Loss 1.6045 (2.0590)	Arch Beta Loss 17.9621 (17.9498)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (70.2%, 93.6%)	
12/04 03:15:31PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.8834 (1.0061)	Arch Loss -16.1736 (-15.8949)	Arch Hard Loss 1.8000 (2.0610)	Arch Beta Loss 17.9736 (17.9558)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.8%, 93.5%)	
12/04 03:16:16PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 1.0373 (1.0138)	Arch Loss -15.7496 (-15.8901)	Arch Hard Loss 2.2344 (2.0711)	Arch Beta Loss 17.9841 (17.9612)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (69.6%, 93.4%)	
12/04 03:16:16PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 36/49] Final Prec@1 69.5640%
12/04 03:16:24PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 2.1064	Prec@(1,5) (48.3%, 78.5%)
12/04 03:16:30PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 2.1055	Prec@(1,5) (48.5%, 78.3%)
12/04 03:16:37PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 2.1160	Prec@(1,5) (48.3%, 78.1%)
12/04 03:16:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 2.1191	Prec@(1,5) (47.9%, 78.0%)
12/04 03:16:43PM searchStage_trainer.py:323 [INFO] Valid: [ 36/49] Final Prec@1 47.8880%
12/04 03:16:43PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:16:43PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:17:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.8032 (0.9236)	Arch Loss -15.7320 (-15.8918)	Arch Hard Loss 2.2633 (2.0979)	Arch Beta Loss 17.9953 (17.9897)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.7%, 94.2%)	
12/04 03:18:22PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.9692 (0.9390)	Arch Loss -15.7611 (-15.9203)	Arch Hard Loss 2.2441 (2.0749)	Arch Beta Loss 18.0052 (17.9952)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.0%, 94.1%)	
12/04 03:19:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 1.0641 (0.9526)	Arch Loss -16.0863 (-15.9181)	Arch Hard Loss 1.9279 (2.0820)	Arch Beta Loss 18.0141 (18.0001)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.7%, 93.9%)	
12/04 03:19:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.7321 (0.9598)	Arch Loss -16.2048 (-15.9261)	Arch Hard Loss 1.8174 (2.0782)	Arch Beta Loss 18.0222 (18.0043)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (71.4%, 93.8%)	
12/04 03:19:54PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 37/49] Final Prec@1 71.4280%
12/04 03:20:01PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 2.1483	Prec@(1,5) (47.6%, 77.1%)
12/04 03:20:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 2.1651	Prec@(1,5) (47.2%, 77.3%)
12/04 03:20:14PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 2.1567	Prec@(1,5) (47.5%, 77.4%)
12/04 03:20:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 2.1644	Prec@(1,5) (47.4%, 77.2%)
12/04 03:20:20PM searchStage_trainer.py:323 [INFO] Valid: [ 37/49] Final Prec@1 47.4120%
12/04 03:20:20PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:20:21PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:21:10PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.6644 (0.8877)	Arch Loss -16.2820 (-15.9656)	Arch Hard Loss 1.7500 (2.0615)	Arch Beta Loss 18.0320 (18.0272)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.6%, 94.7%)	
12/04 03:21:58PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.7908 (0.8912)	Arch Loss -15.6796 (-15.9934)	Arch Hard Loss 2.3626 (2.0387)	Arch Beta Loss 18.0422 (18.0322)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (73.2%, 94.8%)	
12/04 03:22:45PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 1.0288 (0.8970)	Arch Loss -16.1994 (-15.9755)	Arch Hard Loss 1.8481 (2.0609)	Arch Beta Loss 18.0475 (18.0364)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.9%, 94.8%)	
12/04 03:23:29PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.7163 (0.9031)	Arch Loss -15.6776 (-15.9671)	Arch Hard Loss 2.3764 (2.0726)	Arch Beta Loss 18.0540 (18.0397)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (72.6%, 94.8%)	
12/04 03:23:29PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 38/49] Final Prec@1 72.6240%
12/04 03:23:36PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 2.2770	Prec@(1,5) (46.7%, 76.2%)
12/04 03:23:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 2.2804	Prec@(1,5) (46.2%, 76.2%)
12/04 03:23:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 2.2753	Prec@(1,5) (46.5%, 76.5%)
12/04 03:23:55PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 2.2697	Prec@(1,5) (46.5%, 76.6%)
12/04 03:23:55PM searchStage_trainer.py:323 [INFO] Valid: [ 38/49] Final Prec@1 46.5200%
12/04 03:23:55PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:23:56PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:24:46PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.7862 (0.8315)	Arch Loss -16.2057 (-15.9818)	Arch Hard Loss 1.8556 (2.0758)	Arch Beta Loss 18.0613 (18.0576)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.9%, 95.5%)	
12/04 03:25:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.5606 (0.8389)	Arch Loss -15.9962 (-15.9932)	Arch Hard Loss 2.0717 (2.0681)	Arch Beta Loss 18.0679 (18.0613)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.1%, 95.2%)	
12/04 03:26:24PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.6771 (0.8533)	Arch Loss -15.7810 (-15.9850)	Arch Hard Loss 2.2930 (2.0796)	Arch Beta Loss 18.0741 (18.0646)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.5%, 95.1%)	
12/04 03:27:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.9446 (0.8595)	Arch Loss -15.7228 (-15.9873)	Arch Hard Loss 2.3573 (2.0802)	Arch Beta Loss 18.0801 (18.0675)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (74.3%, 95.0%)	
12/04 03:27:09PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 39/49] Final Prec@1 74.2400%
12/04 03:27:16PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.2413	Prec@(1,5) (46.1%, 76.5%)
12/04 03:27:22PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 2.2315	Prec@(1,5) (46.7%, 76.6%)
12/04 03:27:29PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 2.2303	Prec@(1,5) (46.7%, 76.9%)
12/04 03:27:35PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.2362	Prec@(1,5) (46.7%, 76.8%)
12/04 03:27:35PM searchStage_trainer.py:323 [INFO] Valid: [ 39/49] Final Prec@1 46.6800%
12/04 03:27:35PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:27:36PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:28:26PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.8228 (0.7685)	Arch Loss -15.5891 (-15.9718)	Arch Hard Loss 2.4966 (2.1106)	Arch Beta Loss 18.0857 (18.0824)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.3%, 96.0%)	
12/04 03:29:16PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.8529 (0.7786)	Arch Loss -15.9699 (-15.9895)	Arch Hard Loss 2.1202 (2.0957)	Arch Beta Loss 18.0901 (18.0852)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.9%, 96.0%)	
12/04 03:30:05PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.6805 (0.8008)	Arch Loss -16.2486 (-15.9916)	Arch Hard Loss 1.8461 (2.0960)	Arch Beta Loss 18.0947 (18.0876)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.2%, 95.7%)	
12/04 03:30:50PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 1.0191 (0.8054)	Arch Loss -16.2973 (-16.0106)	Arch Hard Loss 1.8025 (2.0794)	Arch Beta Loss 18.0998 (18.0899)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (75.9%, 95.7%)	
12/04 03:30:50PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 40/49] Final Prec@1 75.9000%
12/04 03:30:57PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 2.2788	Prec@(1,5) (46.0%, 77.0%)
12/04 03:31:04PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 2.3022	Prec@(1,5) (46.3%, 76.4%)
12/04 03:31:10PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 2.3024	Prec@(1,5) (46.4%, 76.3%)
12/04 03:31:16PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 2.3116	Prec@(1,5) (46.3%, 76.3%)
12/04 03:31:16PM searchStage_trainer.py:323 [INFO] Valid: [ 40/49] Final Prec@1 46.2640%
12/04 03:31:16PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:31:17PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:32:08PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.6380 (0.7297)	Arch Loss -15.9787 (-16.0336)	Arch Hard Loss 2.1250 (2.0679)	Arch Beta Loss 18.1036 (18.1015)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.8%, 96.7%)	
12/04 03:32:57PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.7510 (0.7481)	Arch Loss -15.8004 (-16.0313)	Arch Hard Loss 2.3074 (2.0724)	Arch Beta Loss 18.1077 (18.1036)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.6%, 96.4%)	
12/04 03:33:47PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.5692 (0.7517)	Arch Loss -16.2121 (-16.0266)	Arch Hard Loss 1.9003 (2.0791)	Arch Beta Loss 18.1124 (18.1057)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (77.3%, 96.3%)	
12/04 03:34:31PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.8759 (0.7672)	Arch Loss -15.9982 (-16.0264)	Arch Hard Loss 2.1177 (2.0814)	Arch Beta Loss 18.1160 (18.1078)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (76.8%, 96.2%)	
12/04 03:34:32PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 41/49] Final Prec@1 76.7960%
12/04 03:34:39PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.1904	Prec@(1,5) (47.1%, 77.3%)
12/04 03:34:46PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.2022	Prec@(1,5) (47.1%, 77.5%)
12/04 03:34:52PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.1969	Prec@(1,5) (47.4%, 77.7%)
12/04 03:34:58PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.1915	Prec@(1,5) (47.7%, 77.7%)
12/04 03:34:58PM searchStage_trainer.py:323 [INFO] Valid: [ 41/49] Final Prec@1 47.6560%
12/04 03:34:58PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:34:58PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 48.2560%
12/04 03:35:49PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.5742 (0.6984)	Arch Loss -16.1314 (-16.0315)	Arch Hard Loss 1.9880 (2.0862)	Arch Beta Loss 18.1194 (18.1177)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.0%, 97.0%)	
12/04 03:36:38PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.7288 (0.7041)	Arch Loss -16.0159 (-16.0742)	Arch Hard Loss 2.1077 (2.0455)	Arch Beta Loss 18.1235 (18.1197)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.9%, 96.8%)	
12/04 03:37:27PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.6625 (0.7170)	Arch Loss -15.9729 (-16.0647)	Arch Hard Loss 2.1537 (2.0569)	Arch Beta Loss 18.1266 (18.1216)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.4%, 96.7%)	
12/04 03:38:12PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.8235 (0.7251)	Arch Loss -15.9006 (-16.0639)	Arch Hard Loss 2.2289 (2.0591)	Arch Beta Loss 18.1296 (18.1230)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (78.2%, 96.5%)	
12/04 03:38:12PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 42/49] Final Prec@1 78.1760%
12/04 03:38:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.0766	Prec@(1,5) (49.7%, 80.0%)
12/04 03:38:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.1063	Prec@(1,5) (49.8%, 79.3%)
12/04 03:38:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.1011	Prec@(1,5) (49.9%, 79.0%)
12/04 03:38:38PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.1002	Prec@(1,5) (50.0%, 78.9%)
12/04 03:38:38PM searchStage_trainer.py:323 [INFO] Valid: [ 42/49] Final Prec@1 49.9880%
12/04 03:38:38PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:38:39PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 49.9880%
12/04 03:39:30PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.7779 (0.6643)	Arch Loss -15.6971 (-16.0910)	Arch Hard Loss 2.4356 (2.0405)	Arch Beta Loss 18.1327 (18.1314)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.5%, 97.3%)	
12/04 03:40:19PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.5721 (0.6889)	Arch Loss -16.3834 (-16.0922)	Arch Hard Loss 1.7526 (2.0407)	Arch Beta Loss 18.1360 (18.1329)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.8%, 96.8%)	
12/04 03:41:09PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.7768 (0.6903)	Arch Loss -15.8278 (-16.0833)	Arch Hard Loss 2.3108 (2.0512)	Arch Beta Loss 18.1386 (18.1344)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.7%, 96.8%)	
12/04 03:41:54PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.5161 (0.6953)	Arch Loss -15.9192 (-16.0801)	Arch Hard Loss 2.2213 (2.0556)	Arch Beta Loss 18.1404 (18.1357)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (79.4%, 96.8%)	
12/04 03:41:55PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 43/49] Final Prec@1 79.3840%
12/04 03:42:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 2.0258	Prec@(1,5) (50.7%, 80.7%)
12/04 03:42:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.0201	Prec@(1,5) (50.9%, 80.6%)
12/04 03:42:15PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.0470	Prec@(1,5) (50.4%, 80.1%)
12/04 03:42:20PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.0581	Prec@(1,5) (50.4%, 79.8%)
12/04 03:42:21PM searchStage_trainer.py:323 [INFO] Valid: [ 43/49] Final Prec@1 50.4160%
12/04 03:42:21PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:42:21PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4160%
12/04 03:43:12PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.5492 (0.6661)	Arch Loss -15.8689 (-16.0940)	Arch Hard Loss 2.2738 (2.0476)	Arch Beta Loss 18.1426 (18.1416)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.7%, 97.0%)	
12/04 03:44:02PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.8709 (0.6692)	Arch Loss -15.4303 (-16.0949)	Arch Hard Loss 2.7155 (2.0481)	Arch Beta Loss 18.1458 (18.1430)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.2%, 97.1%)	
12/04 03:44:51PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.5599 (0.6749)	Arch Loss -16.1177 (-16.0742)	Arch Hard Loss 2.0307 (2.0702)	Arch Beta Loss 18.1484 (18.1443)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.0%, 97.1%)	
12/04 03:45:35PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.5256 (0.6749)	Arch Loss -16.1520 (-16.0849)	Arch Hard Loss 1.9974 (2.0605)	Arch Beta Loss 18.1494 (18.1454)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.0%, 97.1%)	
12/04 03:45:36PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 44/49] Final Prec@1 80.0040%
12/04 03:45:43PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.3467	Prec@(1,5) (45.6%, 76.3%)
12/04 03:45:49PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.3915	Prec@(1,5) (45.3%, 75.5%)
12/04 03:45:56PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.3887	Prec@(1,5) (45.5%, 75.4%)
12/04 03:46:02PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.4004	Prec@(1,5) (45.4%, 75.2%)
12/04 03:46:02PM searchStage_trainer.py:323 [INFO] Valid: [ 44/49] Final Prec@1 45.3400%
12/04 03:46:02PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:46:02PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4160%
12/04 03:46:53PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.6810 (0.6460)	Arch Loss -16.3639 (-16.1164)	Arch Hard Loss 1.7882 (2.0341)	Arch Beta Loss 18.1521 (18.1506)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.9%, 97.5%)	
12/04 03:47:43PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.7982 (0.6550)	Arch Loss -15.8311 (-16.0942)	Arch Hard Loss 2.3223 (2.0577)	Arch Beta Loss 18.1535 (18.1519)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.5%, 97.4%)	
12/04 03:48:32PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.9492 (0.6533)	Arch Loss -15.5864 (-16.0932)	Arch Hard Loss 2.5693 (2.0597)	Arch Beta Loss 18.1558 (18.1528)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.7%, 97.3%)	
12/04 03:49:17PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.5984 (0.6523)	Arch Loss -16.0139 (-16.0960)	Arch Hard Loss 2.1427 (2.0576)	Arch Beta Loss 18.1565 (18.1535)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (80.7%, 97.3%)	
12/04 03:49:18PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 45/49] Final Prec@1 80.7000%
12/04 03:49:25PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 2.1972	Prec@(1,5) (48.7%, 77.5%)
12/04 03:49:32PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 2.2193	Prec@(1,5) (48.0%, 77.2%)
12/04 03:49:38PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 2.2270	Prec@(1,5) (48.0%, 77.1%)
12/04 03:49:44PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.2225	Prec@(1,5) (48.1%, 77.1%)
12/04 03:49:45PM searchStage_trainer.py:323 [INFO] Valid: [ 45/49] Final Prec@1 48.0680%
12/04 03:49:45PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:49:45PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4160%
12/04 03:50:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.8315 (0.5958)	Arch Loss -16.2906 (-16.1070)	Arch Hard Loss 1.8689 (2.0508)	Arch Beta Loss 18.1594 (18.1578)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.9%)	
12/04 03:51:25PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.5660 (0.6103)	Arch Loss -16.0161 (-16.1205)	Arch Hard Loss 2.1447 (2.0385)	Arch Beta Loss 18.1608 (18.1590)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.1%, 97.8%)	
12/04 03:52:15PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.4546 (0.6138)	Arch Loss -16.1711 (-16.1175)	Arch Hard Loss 1.9909 (2.0422)	Arch Beta Loss 18.1620 (18.1597)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.0%, 97.7%)	
12/04 03:52:58PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.5422 (0.6210)	Arch Loss -15.8748 (-16.1157)	Arch Hard Loss 2.2879 (2.0447)	Arch Beta Loss 18.1627 (18.1604)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.8%, 97.5%)	
12/04 03:52:59PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 46/49] Final Prec@1 81.7720%
12/04 03:53:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.0608	Prec@(1,5) (50.0%, 79.5%)
12/04 03:53:12PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.0515	Prec@(1,5) (50.3%, 79.5%)
12/04 03:53:19PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.0420	Prec@(1,5) (50.5%, 79.7%)
12/04 03:53:25PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.0506	Prec@(1,5) (50.5%, 79.5%)
12/04 03:53:25PM searchStage_trainer.py:323 [INFO] Valid: [ 46/49] Final Prec@1 50.4600%
12/04 03:53:25PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
12/04 03:53:25PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4600%
12/04 03:54:15PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.5696 (0.6048)	Arch Loss -16.3007 (-16.1050)	Arch Hard Loss 1.8632 (2.0581)	Arch Beta Loss 18.1639 (18.1631)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.2%, 97.9%)	
12/04 03:55:05PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.5459 (0.6028)	Arch Loss -16.3086 (-16.0954)	Arch Hard Loss 1.8554 (2.0682)	Arch Beta Loss 18.1640 (18.1636)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.8%)	
12/04 03:55:55PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.5968 (0.6128)	Arch Loss -16.4507 (-16.1151)	Arch Hard Loss 1.7146 (2.0488)	Arch Beta Loss 18.1653 (18.1639)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.3%, 97.6%)	
12/04 03:56:39PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.4851 (0.6149)	Arch Loss -15.8327 (-16.1124)	Arch Hard Loss 2.3331 (2.0519)	Arch Beta Loss 18.1658 (18.1642)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.2%, 97.7%)	
12/04 03:56:40PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 47/49] Final Prec@1 82.1720%
12/04 03:56:47PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.1820	Prec@(1,5) (48.8%, 77.8%)
12/04 03:56:53PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.1794	Prec@(1,5) (49.0%, 77.8%)
12/04 03:57:00PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.1950	Prec@(1,5) (48.9%, 77.5%)
12/04 03:57:06PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.2014	Prec@(1,5) (48.9%, 77.6%)
12/04 03:57:06PM searchStage_trainer.py:323 [INFO] Valid: [ 47/49] Final Prec@1 48.8560%
12/04 03:57:06PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('max_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[9, 10])
12/04 03:57:06PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4600%
12/04 03:57:56PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.6352 (0.6026)	Arch Loss -16.2041 (-16.1520)	Arch Hard Loss 1.9629 (2.0144)	Arch Beta Loss 18.1670 (18.1665)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.6%, 97.6%)	
12/04 03:58:46PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.4959 (0.5991)	Arch Loss -15.9248 (-16.1197)	Arch Hard Loss 2.2429 (2.0472)	Arch Beta Loss 18.1677 (18.1669)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.4%, 97.7%)	
12/04 03:59:35PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.6542 (0.6090)	Arch Loss -16.5340 (-16.1216)	Arch Hard Loss 1.6355 (2.0458)	Arch Beta Loss 18.1695 (18.1673)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.1%, 97.6%)	
12/04 04:00:19PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.5950 (0.6134)	Arch Loss -16.2161 (-16.1197)	Arch Hard Loss 1.9544 (2.0482)	Arch Beta Loss 18.1705 (18.1679)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (81.9%, 97.6%)	
12/04 04:00:20PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 48/49] Final Prec@1 81.9240%
12/04 04:00:27PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.1243	Prec@(1,5) (49.1%, 78.5%)
12/04 04:00:34PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 2.1141	Prec@(1,5) (49.4%, 78.6%)
12/04 04:00:40PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 2.1037	Prec@(1,5) (49.4%, 78.7%)
12/04 04:00:46PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 2.1157	Prec@(1,5) (49.1%, 78.4%)
12/04 04:00:46PM searchStage_trainer.py:323 [INFO] Valid: [ 48/49] Final Prec@1 49.0520%
12/04 04:00:46PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('avg_pool_3x3', 10), ('avg_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('skip_connect', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('avg_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[9, 10])
12/04 04:00:46PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4600%
12/04 04:01:36PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.5405 (0.5942)	Arch Loss -16.2113 (-16.1248)	Arch Hard Loss 1.9606 (2.0465)	Arch Beta Loss 18.1719 (18.1713)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.2%, 97.7%)	
12/04 04:02:25PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.4870 (0.5962)	Arch Loss -15.9132 (-16.1388)	Arch Hard Loss 2.2606 (2.0332)	Arch Beta Loss 18.1738 (18.1720)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (83.0%, 97.8%)	
12/04 04:03:15PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.5243 (0.6016)	Arch Loss -16.0977 (-16.1343)	Arch Hard Loss 2.0765 (2.0382)	Arch Beta Loss 18.1743 (18.1726)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.7%, 97.7%)	
12/04 04:04:00PM searchStage_BetaConcat_trainer.py:133 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.7378 (0.6042)	Arch Loss -16.8407 (-16.1334)	Arch Hard Loss 1.3343 (2.0397)	Arch Beta Loss 18.1750 (18.1730)	Arch depth Loss -0.0000 (0.0000)	Prec@(1,5) (82.5%, 97.7%)	
12/04 04:04:00PM searchStage_BetaConcat_trainer.py:147 [INFO] Train: [ 49/49] Final Prec@1 82.5360%
12/04 04:04:08PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.1280	Prec@(1,5) (49.7%, 78.5%)
12/04 04:04:14PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.1472	Prec@(1,5) (49.0%, 78.4%)
12/04 04:04:21PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.1561	Prec@(1,5) (48.9%, 78.3%)
12/04 04:04:26PM searchStage_trainer.py:312 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.1483	Prec@(1,5) (49.1%, 78.3%)
12/04 04:04:27PM searchStage_trainer.py:323 [INFO] Valid: [ 49/49] Final Prec@1 49.1280%
12/04 04:04:27PM trainer_runner.py:74 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('skip_connect', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('avg_pool_3x3', 8)], [('avg_pool_3x3', 10), ('skip_connect', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('avg_pool_3x3', 6)], [('avg_pool_3x3', 8), ('avg_pool_3x3', 7)], [('skip_connect', 9), ('skip_connect', 8)], [('skip_connect', 10), ('max_pool_3x3', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[9, 10])
12/04 04:04:27PM trainer_runner.py:105 [INFO] Until now, best Prec@1 = 50.4600%
12/04 04:04:27PM trainer_runner.py:110 [INFO] Final best Prec@1 = 50.4600%
12/04 04:04:27PM trainer_runner.py:111 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('avg_pool_3x3', 9), ('max_pool_3x3', 8)], [('avg_pool_3x3', 10), ('max_pool_3x3', 9)]], DAG1_concat=[10, 11], DAG2=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 7), ('max_pool_3x3', 6)], [('avg_pool_3x3', 8), ('max_pool_3x3', 7)], [('skip_connect', 9), ('avg_pool_3x3', 8)], [('skip_connect', 10), ('skip_connect', 9)]], DAG2_concat=[10, 11], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 1)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)], [('skip_connect', 7), ('skip_connect', 5)], [('avg_pool_3x3', 8), ('skip_connect', 6)], [('avg_pool_3x3', 9), ('skip_connect', 8)], [('max_pool_3x3', 10), ('skip_connect', 9)]], DAG3_concat=[10, 11])
